filename,label,text,embedding
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ABackPropagationAlgorithmwithOptimalUseofHiddenUnits.pdf,Optimization,"WINNER-TAKE-ALL NETWORKS OF O(N) COMPLEXITY J. Lazzaro, S. Ryckebusch, M.A. Mahowald, and C. A. Mead California Institute of Technology Pasadena, CA 91125 ABSTRACT We have designed, fabricated, and tested a series of compact CMOS integrated circuits that realize the winner-take-all function. These analog, continuous-time circuits use only O(n) of interconnect to perform this function. We have also modified the winner-take-all circuit, realizing a circuit that computes local nonlinear inhibition. Two general types of inhibition mediate activity in neural systems: subtractive in- hibition, which sets a zero level for the computation, and multiplicative (nonlinear) inhibition, which regulates the gain of the computation. We report a physical real- ization of general nonlinear inhibition in its extreme form, known as winner-take-all. We have designed and fabricated a series of compact, completely functional CMOS integrated circuits that realize the winner-take-all function, using the full analog nature of the medium. This circuit has been used successfully as a component in several VLSI sensory systems that perform auditory localization (Lazzaro and Mead, in press) and visual stereopsis (Mahowald and Delbruck, 1988). Winner- take-all circuits with over 170 inputs function correctly in these sensory systems. We have also modified this global winner-take-all circuit, realizing a circuit that computes local nonlinear inhibition. The circuit allows multiple winners in the net- work, and is well suited for use in systems that represent a feature space topograph- ically and that process several features in parallel. We have designed, fabricated, and tested a CMOS integrated circuit that computes locally the winner-take-all function of spatially ordered input. 703 704 Lazzaro, Ryckebusch, Mahowald and Mead THE WINNER-TAKE-ALL CmCUIT Figure 1 is a schematic diagram of the winner-take-all circuit. A single wire, asso- ciated with the potential Vc, computes the inhibition for the entire circuit; for an n neuron circuit, this wire is O(n) long. To compute the global inhibition, each neuron k contributes a current onto this common wire, using transistor T2a.' To apply this global inhibition locally, each neuron responds to the common wire volt- age Vc, using transistor Tla.' This computation is continuous in time; no clocks are used. The circuit exhibits no hysteresis, and operates with a time constant related to the size of the largest input. The output representation of the circuit is not binary; the winning output encodes the logarithm of its associated input. Figure 1. Schematic diagram of the winner-take-all circuit. Each neuron receives a unidirectional current input 11;; the output voltages VI •.. VB represent the result of the winner-take-all computation. If II; = max(II ••• IB ), then VI; is a logarithmic function of 11;; if Ii <: 11;, then Vi ~ O. A static and dynamic ana.lysis of the two-neuron circuit illustrates these system properties. Figure 2 shows a schematic diagram of a two-neuron winner-take-all circuit. To understand the beha.vior of the circuit, we first consider the input condition II = 12 = 1m. Transistors TIl ~d T12 have identical potentials at gate and source, and are both sinking 1m; thus, the drain potentials VI and V2 must be equal. Transistors T21 and T22 have identical source, drain, and gate potentials, and therefore must sink the identical current ICI = IC2 = Ic/2. In the subthreshold region of operation, the equation 1m = 10 exp(Vc/Vo) describes transistors Til and T12 , where 10 is a fabrication parameter, and Vo = kT/qlt. Likewise, the equation Ic/2 = 10 exp((Vm - Vel/Volt where Vm = VI = V2, describes transistors T21 and T22 . Solving for Vm(Im, Ie) yields Vm = Voln(~:) + Voln(:;). (1) Winner-Take-All Networks ofO(N) Complexity 705 Thus, for equal input currents, the circuit produces equal output voltages; this behavior is desirable for a winner-take-all circuit. In addition, the output voltage V m logarithmically encodes the magnitude of the input current 1m. Figure 2. Schematic diagram of a two-neuron winner-take-all circuit. The input condition II = 1m + Oi, 12 = 1m illustrates the inhibitory action of the circuit. Transistor Til must sink 0, more current than in the previous example; as a result, the gate voltage of Til rises. Transistors Tit and TI2 share a common gate, howeverj thus, TI2 must also sink 1m + 0,. But only 1m is present at the drain of T12 • To compensate, the drain voltage of T12 , V2, must decrease. For small OiS, the Early effect serves to decrease the current through Th , decreasing V2 linearly with 0,. For large o's, TI2 must leave saturation, driving V2 to approximately 0 volts. As desired, the output associated with the smaller input diminishes. For large OiS, Ie2 $!:::f 0, and Iel $!:::f Ie. The equation 1m + 0, = 10 exp(Ve/Vo) describes transistor Til' and the equation Ie = 10 exp((VI - Vel/Yo) describes transistor T21 • Solving for VI yields (2) The winning output encodes the logarithm of the associated input. The symmetrical circuit topology ensures similar behavior for increases in 12 relative to II. Equation 2 predicts the winning response of the circuit; a more complex expression, derived in (Lazzaro et.al., 1989), predicts the losing and crossover response of the circuit. Figure 3 is a plot of this analysis, fit to experimental data. Figure 4 shows the wide dynamic range and logarithmic properties of the circuitj the experiment in Figure 3 is repeated for several values of 12 , ranging over four orders of magnitude. The conductance of transistors Til and T1:a determines the losing response of the circuit. Variants of the winner-take-all circuit shown in (Lazzaro et. aI., 1988) achieve losing responses wider and narrower than Figure 3, using circuit and mask layout techniques. 706 Lazzaro, Ryckebusch, Mahowald and Mead WINNER-TAKE-ALL TIME RESPONSE A good winner-take-all circuit should be stable, and should not exhibit damped oscillations (""ringing"") in response to input changes. This section explores these dynamic properties of our winner-take-all circuit, and predicts the temporal re- sponse of the circuit. Figure 8 shows the two-neuron winner-take-all circuit, with capacitances added to model dynamic behavior. o T 102 Vo Ie Figure 8. Schematic diagram of a two-neuron winner-take-all circuit, with capac- itances added for dynamic analysis. 0 is a large MOS capacitor added to each neuron for smoothingj 0., models the parasitic capacitance contributed by the gates of Tu and T12 , the drains of T21 and T22, and the interconnect. (Lazzaro et. al., 1988) shows a small-signal analysis of this circuit. The transfer function for the circuit has real poles, and thus the circuit is stable and does not ring, if 10 > 41(Oe/O), where 11 RlI2 Rl 1. Figure 9 compares this bound with experimental data. H Ie > 41(00 /0), the circuit exhibits first-order behavior. The time constant OVo/I sets the dynamics of the winning neuron, where Vo = A:T /qK. Rl 40 mV. The time constant OVE/I sets the dynamics of the losing neuron, where VE Rl 50 v. Figure 10 compares these predictions with experimental data. Vl,V, (V) Winner-Take-All Networks ofO(N) Complexity 707 2.6 2.4 2.2 2.0 I.S 1.6 1.4 1.2 1.0+--+--+--+--..... ~I----t~--t---f 0.92 0.94 0.96 0.98 1.00 1.02 1.04 1.06 1.0S II/I, Figure 8. Experimental data (circles) and theory (solid lines) for a two-neuron winner-take-all circuit. II, the input current of the first neuron, is swept about the value of 12, the input current of the second neuron; neuron voltage outputs VI and V2 are plotted versus normalized input current. 2.6 I.S 1.6 1.4 1.2 10-11 10-10 10-0 10-8 IdA) Figure 4. The experiment of Figure 3 is repeated for several values of 12; experi- mental data of output voltage response are plotted versus absolute input current on a log scale. The output voltage VI = V2 is highlighted with a circle for each experi- ment. The dashed line is a theoretical expression confirming logarithmic behavior over four orders of magnitude (Equation 1). 708 Lazzaro, Ryckebusch, Mahowald and Mead 1 Figure 9. Experimental data (circles) and theoretical statements (solid line) for a two-neuron winner-take-all circuit, showing the smallest 10 , for a given I, necessary for a first-order response to a small-signal step input. Figure 10. Experimental data (symbols) and theoretical statements (solid line) for a two-neuron winner-take-all circuit, showing the time constant of the first-order response to a small-signal step input. The winning response (filled circles) and losing response (triangles) of a winner-take-a.ll circuit are shownj the time constants differ by several orders of magnit ude. Winner~Take~AlI Networks ofO(N) Complexity 709 THE LOCAL NONLINEAR INHIBITION CIRCUIT The winner-take-all circuit in Figure 1, as previously explained, locates the largest input to the circuit. Certain applications require a gentler form of nonlinear inhibi- tion. Sometimes, a circuit that can represent multiple intensity scales is necessary. Without circuit modification, the winner-take-all circuit in Figure 1 can perform this task. (Lazzaro et. al., 1988) explains this mode of operation. Other applications require a local winner-take-all computation, with each winner having inHuence over only a limited spatial area. Figure 12 shows a circuit that computes the local winner-taite-all function. The circuit is identical to the original winner-take-all circuit, except that each neuron connects to its nearest neighbors with a nonlinear resistor circuit (Mead, in press). Each resistor conducts a current Ir in response to a voltage ~V across it, where Ir = I.tanh(~V/(2Vo)). 1., the saturating current of the resistor, is a controllable parameter. The current source, 10, present in the original winner-take-all circuit, is distributed between the resistors in the local winner-take-all circuit. Figure 11. Schematic diagram of a section of the local winner-take-all circuit. Each neuron i receives a unidirectional current input Iii the output voltages Vi represent the result of the local winner-take-all computation. To understand the operation of the local winner-take-all circuit, we consider the circuit response to a spatial impulse, defined as 1"" :> 1, where 1 == h~"". 1,,:> 1""-1 and 1,,:> 1""+1, so Ve:,. is much larger than Ve:,._l and Ve:lI+l' and the resistor circuits connecting neuron 1: with neuron 1: - 1 and neuron 1: + 1 saturate. Each resistor sinks 1. current when saturatedj transistor T2,. thus conducts 21. + Ie: current. In the subthreshold region of operation, the equation 1"" = 10 exp(Ve:,. /Vo) describes transistor TI ,., and the equation 21. + Ie = Ioexp((V"" - Ve:,.)/Vo) describes transistor 710 Lazzaro, Ryckebusch, Mahowald and Mead T2,.. Solving for VA: yields VA: = voln((2I. + 10 )/10 ) + voln(IA:/lo). (4) As in the original winner-take-all circuit, the output of a winning neuron encodes the logarithm of that neuron's associated input. As mentioned, the resistor circuit connecting neuron Ie with neuron Ie - 1 sinks 1. CUlTent. The current sources 10 associated with neurons Ie -1, Ie - 2, ... must supply this current. If the current source 10 for neuron Ie - 1 supplies part of this current, the transistor T2,._1 carries no current, and the neuron output VA:-l approaches zero. In this way, a winning neuron inhibits its neighboring neurons. This inhibitory action does not extend throughout the network. Neuron Ie needs only 1. current from neurons Ie - 1, Ie - 2, .... Thus, neurons sufficiently distant from neuron Ie maintain the service of their current source 10, and the outputs of these distant neurons can be active. Since, for a spatial impulse, all neurons Ie - 1, Ie - 2, ... have an equal input current I, all distant neurons have the equal output (5) Similar reasoning applies for neurons Ie + 1, Ie + 2, .... The relative values of 1. and 10 determine the spatial extent of the inhibitory action. Figure 12 shows the spatial impulse response of the local winner-take-all circuit, for different settings of 1./10 , I I I I I o 2 4 6 8 10 12 14 16 Ie (Pollition) Figure 12. Experimental data showing the spatial impulse response of the local winner-take-all circuit, for values of 1./10 ranging over a factor of 12.7. Wider inhibitory responses correspond to larger ratios. For clarity, the plots are vertically displaced in 0.25 volt increments. Winner-Take-All Networks ofO(N) Complexity 711 CONCLUSIONS The circuits described in this paper use the full analog nature of MOS devices to realize an interesting class of neural computations efficiently. The circuits exploit the physics of the medium in many ways. The winner-take-all circuit uses a single wire to compute and communicate inhibition for the entire circuit. Transistor TI,. in the winner-take-all circuit uses two physical phenomena in its computation: its exponential current function encodes the logarithm of the input, and the finite conductance of the transistor defines the losing output response. As evolution exploits all the physical properties of neural devices to optimize system performance, designers of synthetic neural systems should strive to harness the full potential of the physics of their media. Acknow ledgments John Platt, John Wyatt, David Feinstein, Mark Bell, and Dave Gillespie provided mathematical insights in the analysis of the circuit. Lyn Dupre proofread the docu- ment. We thank Hewlett-Packard for computing support, and DARPA and MOSIS for chip fabrication. This work was sponsored by the Office of Naval Research and the System Development Foundation. References Lazzaro, J. P., Ryckebusch, S., Mahowald, M.A., and Mead, C.A. (1989). Winner- Take-All Networks of O(N) Oomplexity, Caltech Computer Science Department Technical Report Caltech-CS-TR-21-88. Lazzaro, J. P., and Mead, C.A. {in press}. Silicon Models of Auditory Localization, Neural Oomputation. Mahowald, M.A., and Delbruck, T.I. (1988). An Analog VLSI Implementation of the Marr-Poggio Stereo Correspondence Algorithm, Abstracts of the First Annual INNS Meeting, Boston, 1988, Vol. I, Supplement I, p. 392. Mead, C. A. (in press). Analog VLSI and Neural Systems. Reading, MA: Addison- Wesley.","[-0.0404229462146759, -0.011248532682657242, 0.032965727150440216, -0.013043306767940521, 0.022712182253599167, -0.06631183624267578, 0.06216628476977348, -0.07551657408475876, 0.012917852029204369, 0.04876958578824997, 0.010651588439941406, -0.043577853590250015, -0.03388878330588341, 0.012920381501317024, -0.0651157796382904, 0.059988535940647125, 0.05395389348268509, 0.08550477027893066, -0.006165561266243458, 0.0014244233025237918, 0.06433134526014328, -0.0015571299009025097, -0.003959086257964373, 0.024334194138646126, 0.011335417628288269, 0.012522122822701931, -0.05858829244971275, -0.05410601571202278, -0.026414228603243828, -0.0772579163312912, 0.00023425488325301558, -0.049368128180503845, 0.10795948654413223, 0.06108356639742851, -0.08705722540616989, 0.006262022070586681, -0.02843625843524933, -0.04276520386338234, 0.0524727888405323, -0.00021790341997984797, 0.008443595841526985, 0.014139371924102306, 0.05419234558939934, 0.008152024820446968, 0.05074275657534599, -0.07752212882041931, 0.028706343844532967, -0.10076042264699936, -0.020237812772393227, -0.025635238736867905, 0.015577849000692368, 0.029482685029506683, 0.005708971060812473, 0.08868170529603958, 0.02051890827715397, 0.1360676884651184, -0.02541401796042919, 0.07715821266174316, -0.028862712904810905, -0.059187911450862885, -0.039411064237356186, 0.02435588464140892, 0.04310659319162369, -0.0009841137798503041, -0.000720936746802181, 0.06166272982954979, 0.046476855874061584, 0.020335976034402847, 0.04324154183268547, -0.00622286694124341, 0.07949002832174301, 0.023483756929636, 0.011301234364509583, -0.007537300232797861, 0.12500277161598206, -0.001169703435152769, 0.051521290093660355, 0.09452248364686966, 0.07844915241003036, 0.05029633268713951, 0.005576746538281441, -0.0640793964266777, 0.029035061597824097, -0.04055982455611229, 0.04839508980512619, 0.030240600928664207, -0.021357417106628418, -0.01149880513548851, 0.10244297981262207, -0.06249982863664627, -0.15301111340522766, -0.10114163160324097, -0.056330110877752304, -0.057045601308345795, 0.022769371047616005, -0.09878017753362656, 0.05244419723749161, 0.02603093720972538, 0.021937547251582146, 0.06710886210203171, 0.02616611123085022, 0.004077116958796978, -0.007498619612306356, 0.023691976442933083, 0.05369429662823677, -0.0063146790489554405, 0.03240372985601425, 0.017366116866469383, -0.057793404906988144, -0.06197880581021309, -0.05262601003050804, 0.02339288592338562, 0.039101164788007736, 0.1011444702744484, 0.011663193814456463, 0.04149897024035454, 0.03802929073572159, 0.009005741216242313, 0.08620632439851761, -0.04646379128098488, -0.09826643764972687, 0.00699842581525445, -0.09329252690076828, 0.02649729698896408, -0.029336437582969666, 0.023876987397670746, -0.09346958249807358, -1.5303015249126558e-33, -0.05612752586603165, 0.016845885664224625, 0.003193989861756563, -0.040462516248226166, 0.032887790352106094, -0.06894613802433014, -0.00865080300718546, -0.014379730448126793, -0.0015552387339994311, 0.037528496235609055, -0.029059581458568573, 0.052615921944379807, 0.00523107685148716, 0.085066057741642, 0.12767758965492249, 0.00813841912895441, 0.03998687490820885, -0.09059305489063263, 0.02875063382089138, -0.09373164176940918, 0.07520631700754166, -0.04833923652768135, -0.05089975520968437, 0.014899595640599728, -0.011127814650535583, -0.025263596326112747, -0.0431898832321167, 0.0016928993863984942, -0.0015365930739790201, 0.014278676360845566, -0.057060450315475464, 0.04521813243627548, 0.0016504998784512281, -0.02859468013048172, 0.043505001813173294, -0.03266492858529091, 0.0531822107732296, -0.012845279648900032, 0.07502071559429169, 0.023139597848057747, -0.11709849536418915, 0.04074086621403694, -0.04475982114672661, -0.025288091972470284, -0.05105286091566086, -0.10386776924133301, -0.01883196458220482, 0.06912193447351456, 0.018296964466571808, -0.01608390174806118, -0.032481756061315536, -0.1018950417637825, 0.022349178791046143, -0.027694137766957283, 0.011796318925917149, -0.06406526267528534, 0.02400301769375801, 0.03899342194199562, 0.08246680349111557, 0.14612483978271484, -0.013307036831974983, -0.037320543080568314, -0.009324691258370876, 0.05601770430803299, -0.06493579596281052, 0.061289653182029724, -0.0711265280842781, -0.006707104854285717, 0.04771621897816658, -0.08830088376998901, 0.0009879284771159291, 0.05654279142618179, -0.0257352776825428, -0.061215512454509735, 0.10635564476251602, 0.016952646896243095, 0.019539572298526764, -0.1201702207326889, 0.0024037158582359552, 0.06808656454086304, 0.08461856096982956, 0.0035554212518036366, -0.022485310211777687, 0.06360338628292084, 0.0038719582371413708, -0.020208464935421944, -0.02142607606947422, -0.09940896928310394, -0.029284518212080002, -0.016720212996006012, -0.0333724319934845, 0.010795408859848976, 0.0751824900507927, -0.09064013510942459, -0.06983955204486847, 9.191943380147962e-34, 0.05941631272435188, -0.047501564025878906, 0.034528475254774094, -0.007454883772879839, -0.039717502892017365, 0.10135889053344727, 0.02860533446073532, -0.06746643036603928, 0.033860690891742706, -0.01698438636958599, 0.0536055862903595, 0.013336898759007454, 0.006457232870161533, -0.024978462606668472, 0.0074670156463980675, 0.0023349574767053127, -0.02387947216629982, -0.011617875657975674, 0.05761924013495445, 0.022024396806955338, 0.028476009145379066, 0.052757278084754944, -0.03667270392179489, -0.01898971199989319, -0.05038956180214882, 0.05986492708325386, -0.09602022171020508, 0.05940094590187073, 0.025394245982170105, 0.005847003310918808, -0.06076778098940849, 0.025575583800673485, -0.00880781002342701, -0.06851349771022797, 0.05779413506388664, 0.1140841618180275, 0.0606129951775074, 0.09611865878105164, -0.07783876359462738, -0.060878511518239975, 0.016625812277197838, -0.06469225138425827, 0.03192726522684097, 0.035542890429496765, 0.040105074644088745, 0.06876872479915619, -0.07106604427099228, -0.04402262344956398, -0.07162053883075714, 0.08738819509744644, 0.011626946739852428, -0.011056577786803246, -0.01396865863353014, -0.021894337609410286, -0.028948161751031876, -0.008793748915195465, -0.031608328223228455, 0.024111047387123108, 0.030682042241096497, -0.03326723352074623, 0.016808439046144485, -0.021365012973546982, -0.04730143025517464, -0.031031835824251175, -0.029573572799563408, 0.06227509304881096, -0.0001401656772941351, 0.08358664065599442, 0.1050255075097084, -0.06260751932859421, -0.0031972620636224747, 0.07753407210111618, 0.03179572895169258, -0.07445695996284485, -0.04270258545875549, 0.005925574339926243, -0.017297198995947838, -0.00032704058685339987, 0.04636277258396149, 0.047562386840581894, 0.02803010120987892, 0.013703057542443275, 0.02540217898786068, -0.08566279709339142, 0.03613341227173805, 0.01293212827295065, -0.006338749546557665, -0.021242110058665276, -0.022064752876758575, -0.026809046044945717, -0.037592701613903046, 0.026874521747231483, -0.008468754589557648, 0.00411567697301507, 0.038955386728048325, -4.984504542449031e-08, 0.031886953860521317, -0.04270273074507713, -0.034825365990400314, -0.015526550821959972, -0.0028684749267995358, -0.07789558172225952, 0.021660231053829193, 0.01211558934301138, -0.021975062787532806, 0.013411001302301884, 0.053339000791311264, 0.06121225655078888, 0.018739284947514534, -0.07052735239267349, -0.003198428777977824, 0.05650670826435089, 0.036731693893671036, 0.015987835824489594, -0.02132021076977253, -0.03994322195649147, -0.0016717032995074987, 0.013091674074530602, -0.008726432919502258, 0.01852831244468689, 0.09302204102277756, -0.1554320752620697, -0.0580013208091259, 0.026369499042630196, 0.005708497483283281, -0.024689145386219025, 0.030756494030356407, -0.007919751107692719, -0.0307107362896204, 0.07871775329113007, 0.004752036649733782, -0.04091775789856911, -0.020092613995075226, 0.04863226041197777, -0.0016596547793596983, -0.04391089081764221, -0.07908854633569717, -0.012623061425983906, -0.07437598705291748, 0.038967009633779526, 0.012570147402584553, -0.05122581869363785, -0.022884193807840347, -0.05336080491542816, 0.014786414802074432, 0.013869100250303745, 0.027328036725521088, 0.08259247988462448, -0.050144873559474945, 0.0252167209982872, -0.02251407690346241, 0.006777788046747446, 0.06190338730812073, -0.07281766086816788, -0.03369319438934326, 0.07771547138690948, -0.01022813469171524, -0.07795727998018265, -0.06524389982223511, -0.010449583642184734]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AComputationallyRobustAnatomicalModelforRetinalDirectionalSelectivity.pdf,Deep Learning,"NEURAL NETWORK RECOGNIZER FOR HAND-WRITTEN ZIP CODE DIGITS J. S. Denker, W. R. Gardner, H. P. Graf, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, H. S. Baird, and I. Guyon AT &T Bell Laboratories Holmdel, New Jersey 07733 ABSTRACT This paper describes the construction of a system that recognizes hand-printed digits, using a combination of classical techniques and neural-net methods. The system has been trained and tested on real-world data, derived from zip codes seen on actual U.S. Mail. The system rejects a small percentage of the examples as unclassifiable, and achieves a very low error rate on the remaining examples. The system compares favorably with other state-of-the art recognizers. While some of the methods are specific to this task, it is hoped that many of the techniques will be applicable to a wide range of recognition tasks. MOTIVATION The problem of recognizing hand-written digits is of enormous practical and the~ retical interest [Kahan, Pavlidis, and Baird 1987; Watanabe 1985; Pavlidis 1982]. This project has forced us to formulate and deal with a number of questions rang- ing from the basic psychophysics of human perception to analog integrated circuit design. This is a topic where ""neural net"" techniques are expected to be relevant, since the task requires closely mimicking human performance, requires massively parallel processing, involves confident conclusions based on low precision data, and requires learning from examples. It is also a task that can benefit from the high throughput potential of neural network hardware. Many different techniques were needed. This motivated us to compare various clas- sical techniques as well as modern neural-net techniques. This provided valuable information about the strengths, weaknesses, and range of applicability of the nu- merous methods. The overall task is extremely complex, so we have broken it down into a great number of simpler steps. Broadly speaking, the recognizer is divided into the pre- processor and the classifier. The two main ideas behind the preprocessor are (1) to remove meaningless variations (i.e. noise) and (2) to capture meaningful variations (i.e. salient features). Most of the results reported in this paper are based on a collection of digits taken from hand-written Zip Codes that appeared on real U.S. Mail passing through the 323 324 Denker, et al (j{OL3l-/.jGIJ JI<=t OI~.?4--:; ~789 d/~~'f.!Jd,7 87 012dL/-S-67 get Figure 1: Typical Data Buffalo, N.Y. post office. Details will be discussed elsewhere [Denker et al., 1989]. Examples of such images are shown in figure 1. The digits were written by many different people, using a great variety of writing styles and instruments, with widely varying levels of care. Important parts of the task can be handled nicely by our lab's custom analog neural network VLSI chip [Gra! et aI., 1987; Gra! & deVegvar, 1987], allowing us to perform the necessary computations in a reasonable time. Also, since the chip was not designed with image processing in mind, this provided a good test of the chips' versatility. THE PREPROCESSOR Acquisition The first step is to create a digital version of the image. One must find where on the envelope the zip code is, which is a hard task in itself (Wang and Srihari 1988]. One must also separate each digit from its neighbors. This would be a relatively simple task if we could assume that a character is contiguous and is disconnected from its neighbors, but neither of these assumptions holds in practice. It is also common to find that there are meaningless stray marks in the image. Acquisition, binarization, location, and preliminary segmentation were performed by Poetal Service contractors. In some images there were extraneous marks, so we developed some simple heuristics to remove them while preserving, in most cases, all segments of a split character. Scaling and Deskewing At this point, the size of the image is typically 40 x 60 pixels, although the scaling routine can accept images that are arbitrarily large, or as small as 5 x 13 pixels. A translation and scale factor are then applied to make the image fit in a rectangle Neural Network Recognizer for Hand-Written Zip Code Digits 325 20 x 32 pixels. The character is centered in the rectangle, and just touches either the horizontal or vertical edges, whichever way fits. It is clear that any extraneous marks must be removed before this step, lest the good part of the image be radically compressed in order to make room for some wild mark. The scaling routine changes the horizontal and vertical size of the image by the same factor, so the aspect ratio of the character is preserved. As shown in figure 1, images can differ greatly in the amount of skew, yet be considered the same digit. This is an extremely significant noise source. To remove this noise, we use the methods of [Casey 1970]; see also [Naylor 1971]. That is, we calculate the XY and YY moments of the image, and apply a linear transformation that drives the XY moment to zero. The transformation is a pure shear, not a rotation, because we find that rotation is much less common than skew. The operations of scaling and deskewing are performed in a single step. This yields a speed advantage, and, more importantly, eliminates the quantization noise that would be introduced by storing the intermediate images as pixel maps, were the calculation carried out in separate steps. Skeletonization For the task of digit recognition, the width of the pen used to make the characters is completely meaningless, and is highly variable. It is important to remove this noise source. By deleting pixels at the boundaries of thick strokes. After a few iterations of this process, each stroke will be as thin as possible. The idea is to remove as many pixels as possible without breaking the connectivity. Connectivity is based on the 8 nearest neighbors. This can be formulated as a pattern matching problem - we search the image looking for situations in which a pixel should be deleted. The qecisions can be expressed as a convolution, using a rather small kernel, since the identical decision process is repeated for each location in the image, and the decision depends on the configuration of the pixel's nearest and next-nearest neighbors. Figure 2 shows an example of a character before (e) and after (I) skeletonization. It also shows some of the templates we use for skeletonization, together with an indication of where (in the given image) that template was active. To visualize the convolution process, imagine taking a template, laying it over the image in each possible place, and asking if the template is ""active"" in that place. (The template is the convolution kernel; we use the two terms practically interchangeably.) The portrayal of the template uses the following code: Black indicates that if the cor- responding pixel in the image is ON, it will contribute +1 to the activity level of this template. Similarly, gray indicates that the corresponding pixel, if ON, will contribute -5, reducing the activity of this template. The rest of the pixels don't matter. If the net activity level exceeds a predetermined threshold, the template is considered active at this location. The outputs of all the skeletonizer templates 326 Denker, et al a) b) c) d) Figure 2: Skeletonization are eombined in a giant logieal OR, that is, whenever any template is aetive, we eonelude that the pixel presently under the eenter of the template should be deleted. The skeletonization eomputation involves six nested loops: for each iteration I for all X in the image (horizontal eoordinate) for all Y in the image (vertical eoordinate) for all T in the set of template shapes for all P in the template (horizontal) for all Q in the template (vertical) compare image element(X +P, Y +Q) with template(T) element(P, Q) The inner three loops (the loops over T, P, and Q) are performed in parallel, in a single cyde of our special-purpose ehip. The outer three loops (1, X, and Y) are performed serially, calling the ehip repeatedly. The X and Y loops eould be performed in parallel with no change in the algorithms. The additional parallelism would require a proportionate increase in hardware. Neural Network Recognizer for Hand-Written Zip Code Digits 327 The purpose of template a is to detect pixels at the top edge of a thick horizontal line. The three ""should be OFF"" (light grey shade in figure 2) template elements enforce the requirement that this should be a boundary, while the three ""should be ON"" (solid black shade in figure 2) template elements enforce the requirement that the line be at least two pixels wide. Template b is analogous to template a, but rotated 90 degrees. Its purpose is to detect pixels at the left edge of a thick vertical line. Template c is similar to, but not exactly the same as, template a rotated 180 degrees. The distinction is necessary because all templates are applied in parallel. A stroke that is only two pixels thick ·must not be attacked from both sides at once, lest it be removed entirely, changing the connectivity of the image. Previous convolutional line-thinning schemes [Naccache 1984] used templates of size 3 x 3, and therefore had to use several serial sub-stages. For parallel operation at least 3 x 4 kernels are needed, and 5 x 5 templates are convenient, powerful, and flexible. Feature Maps Having removed the main sources of meaningless variation, we turn to the task of extracting the meaningful information. It is known from biological studies [Hubel and Wiesel 1962] that the human vision system is sensitive to certain features that occur in images, particularly lines and the ends of lines. We therefore designed detectors for such features. Previous artificial recognizers [\Vatanabe 1985] have used similar feature extractors. Once again we use a convolutional method for locating the features of interest - we check each location in the image to see if each particular feature is present there. Figure 3 shows some of the templates we use, and indicates where they become active in an example image. The feature extractor templates are 7 x 7 pixels - slightly larger than the skeletonizer templates. Feature b is designed to detect the right-hand end of (approximately) horizontal strokes. This can be seen as follows: in order for the template to become active at a particular point, the image must be able to touch the ""should be ON"" pixels at the center of the template without touching the surrounding horseshoe-shaped collection of ""'must be OFF"" pixels. Essentially the only way this can happen is at the right-hand end of a stroke. (An isolated dot in the image will also activate this template, but the images, at this stage, are not supposed to contain dots). Feature d detects (approximately) horizontal strokes. There are 49 different feature extractor templates. The output of each is stored separately. These outputs are called feature maps, since they show what feature(s) occurred where in the image. It is possible, indeed likely, that several different features will occur in the same place. Whereas the outputs of all the skeletonizer templates were combined in a very simple way (a giant OR), the outputs of the feature extractor templates are combined in 328 Denker, et al a) b) c) • I ~------~.~ ~.~------~ Figure 3: Feature Extraction various artful ways. For example, feature"" and a similar one are O~d to form a single combined feature that responds to right-hand ends in general. Certain other features are ANDed to form detectors for arcs (long curved strokes). There are 18 combined features, and these are what is passed to the next stage. We need to create a compact representation, but starting from the skeletonized image, we have, instead, created 18 feature maps of the same size. Fortunately, we can now return to the theme of removing meaningless variation. If a certain image contains a particular feature (say a left-hand stroke end) in the upper left corner, it is not really necessary to specify the location of that feature with great precision. To recognize the Ihope of the feature required considerable precision at the input to the convolution, but the pOlitiora of the feature does not require so much precision at the output of the convolution. We call this Coarse Blocking or Coarse Coding of the feature maps. We find that 3 x 5 is sufficent resolution. CLASSIFIERS If the automatic recognizer is unable to classify a particular zip code digit, it may be possible for the Post Office to determine the correct destination by other means. This is costly, but not nearly so costly as a misclassification (substitution error) that causes the envelope to be sent to the wrong destination. Therefore it is critically Neural Network Recognizer for Hand-Written Zip Code Digits 329 important for the system to provide estimates of its confidence, and to reject digits rather than misclassify them. The objective is not simply to maximize the number of classified digits, nor to minimize the number of errors. The objective is to minimize the cost of the whole operation, and this involves a tradeoff between the rejection rate and the error rate. Preliminary Inves tigations Several different classifiers were tried, including Parzen Windows, K nearest neigh- bors, highly customized layered networks, expert systems, matrix associators, fea- ture spins, and adaptive resonance. We performed preliminary studies to identify the most promising methods. We determined that the top three methods in this list were significantly better suited to our task than the others, and we performed systematic comparisons only among those three. Classical Clustering Methods We used two classical clustering techniques, Parzen Windows (PW) and K Near- est Neighbors (KNN), which are nicely described in Duda and Hart [1973]. In this application, we found (as expected) that they behaved similarly, although PW consistently outperformed KNN by a small margin. These methods have many advantages, not the least of which is that they are well motivated and easily un- derstood in terms of standard Bayesian inference theory. They are well suited to implementation on parallel computers and/or custom hardware. They provide ex- cellent confidence information. Unlike modern adaptive network methods, PW and KNN require no ""learning time"", Furthermore the performance was reproducible and responded smoothly to improvements in the preprocessor and increases in the size of the training set. This is in contrast to the ""noisy"" performance of typical layered networks. This is con- venient, indeed crucial, during exploratory work. Adaptive Network Methods In the early phases of the project, we found that neural network methods gave rather mediocre results. Later, with a high-performance preprocessor, plus a large training database, we found that a layered network gave the best results, surpassing even Parzen Windows. We used a network with two stages of processing (i.e., two layers of weights), with 40 hidden units and using a one-sided objective function (as opposed to LMS) as described in [Denker and Wittner 1987]. The main theoretical advantage of the layered network over the classical methods is that it can form ""higher order"" features - conjunctions and disjunctions of the features provided by our feature extractor. Once the network is trained, it has the advantage that the classification of each input is very rapid compared to PW or KNN. Furthermore, the weights represent a compact distillation of the training data and thus have a smaller memory requirement. The network provides confidence information that is 330 Denker, et al just as good as the classical methods. This is obtained by comparing the activation level of the most active output against the runner-up unit(s). To check on the effectiveness of the preprocessing stages, we applied these three classification schemes (PW, KNN, and the two-layer network) on 256-bit vectors consisting of raw bit maps of the images - with no skeletonization and no feature extraction. For each classification scheme, we found the error rate on the raw bit maps was at least a factor of 5 greater than the error rate on the feature vectors, thus clearly demonstrating the utility of feature extraction. TESTING It is impossible to compare the performance of recognition systems except on iden- tical databases. Using highly motivated ""friendly"" writers, it is possible to get a dataset that is so clean that practically any algorithm would give outstanding re- sults. On the other hand, if the writers are not motivated to write clearly, the result will be not classifiable by machines of any sort (nor by humans for that matter). It would have been much easier to classify digits that were input using a mouse or bitpad, since the lines in the such an image have zero thickness, and stroke-order information is available. It would also have been much easier to recognize digits from a single writer. The most realistic test data we could obtain was provided by the US Postal Service. It consists of approximately 10,000 digits (1000 in each category) obtained from the zip codes on actual envelopes. The data we received had already been binarized and divided into images of individual digits, rather than multi-digit zip codes, but no further processing had been done. On this data set, our best performance is as follows: if 14% of the images are rejected as unclassifiable, only 1% of the remainder are misclassified. If no images are re- jected, approximately 6% are misclassified. Other groups are working with the same dataset, but their results have not yet been published. Informal communications indicate that our results are among the best. CONCLUSIONS We have obtained very good results on this very difficult task. Our methods include low-precision and analog processing, massively parallel computation, extraction of biologically-motivated features, and learning from examples. We feel that this is, therefore, a fine example of a Neural Information Processing System. We empha- size that old-fashioned engineering, classical pattern recognition, and the latest learning-from-examples methods were all absolutely necessary. Without the careful engineering, a direct adaptive network attack would not succeed, but by the same token, without learning from a very large database, it would have been excruciating to engineer a sufficiently accurate representation of the probability space. Neural Network Recognizer for Hand-Written Zip Code Digits 331 Acknowledgements It is a pleasure to acknowledge useful discussions with Patrick Gallinari and tech- nical assistance from Roger Epworth. We thank Tim Barnum of the U.S. Postal Service for making the Zip Code data available to us. References 1. R. G. Casey, ""Moment Normalization of Handprinted Characters"", IBM J. Res. Develop., 548 (1970) 2. J. S. Denker et al., ""Details of the Hand-Written Character Recognizer"", to be published (1989) 3. R. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis, John Wiley and Sons (1973) 4. E. Gullichsen and E. Chang, ""Pattern Classification by Neural Network: An Experimental System for Icon Recognition"", Proc. IEEE First Int. Conf. on Neural Networks, San Diego, IV, 725 (1987) 5. H. P. Graf, W. Hubbard, L. D. Jackel, P.G.N. deVegvar, ""A CMOS Associative Memory Chip"", Proc. IEEE First Int. Conf. on Neural Networks, San Diego, 111,461 (1987) 6. H.P Graf and P. deVegvar, ""A CMOS Implementation of a Neural Network Model"", Proc. 1987 Stanford Conf. Advanced Res. VLSI, P. Losleben (ed.) MIT Press, 351 (1987) 7. D. H. Hubel and T. N. Wiesel, ""Receptive fields, binocular interaction and functional architecture in the cat's visual cortex"", J. Physiology 160, 106 (1962) 8. S. Kahan, T. Pavlidis, and H. S. Baird, ""On the Recognition of Printed Char- acters of Any Font and Size"", IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-9, 274 (1987) 9. N. J. Naccache and R. Shinghal, ''SPTA: A Proposed Algorithm for Thinning Binary Patterns"", IEEE Trans. Systems, Man, and Cybernetics, SMC-14, 409 (1984) 10. W. C. Naylor, ""Some Studies in the Interactive Design of Character Recogni- tion Systems"", IEEE Transactions on Computers, 1075 (1971) 11. T. Pavlidis, Algorithms for Graphics and Image Processing, Computer Science Press (1982) 12. C. Y. Suen, M. Berthod, and S. Mori, ""Automatic Recognition of Handprinted Characters - The State of the Art"", Proceedings of the IEEE 68 4, 469 (1980). 13. C-H. Wang and S. N. Srihari, ""A Framework for Object Recognition in a Vi- sually Complex Environment and its Application to Locating Address Blocks on Mail Pieces"", IntI. J. Computer Vision 2, 125 (1988) 14. S. Watanabe, Pattern Recognition, John Wiley and Sons, New York (1985)","[-0.13734658062458038, -0.019197942689061165, -0.0195452980697155, -0.04927574843168259, -0.06602086871862411, 0.08843628317117691, 0.020314594730734825, -0.0074791987426579, -0.03547811880707741, -0.07757639139890671, -0.040625449270009995, 0.11211231350898743, 0.07530099898576736, 0.04056273773312569, -0.1763543039560318, -0.07218722254037857, -0.012061572633683681, 0.0527307391166687, 0.004609254654496908, 0.019735317677259445, 0.007441478781402111, 0.03117995895445347, 0.014857790432870388, -0.062483303248882294, 0.052826739847660065, 0.04354780912399292, -0.04520726576447487, 0.0105288065969944, 0.0016814288683235645, 0.0010876290034502745, 0.06892078369855881, 0.07820502668619156, 0.04316462203860283, 0.04700561985373497, -0.007614079862833023, -0.0044546956196427345, -0.037741970270872116, 0.023538898676633835, 0.02445903792977333, -0.009367753751575947, -0.0106129739433527, -0.03688646852970123, 0.014335673302412033, 0.08150645345449448, 0.08230305463075638, 0.03363875299692154, 0.000381677586119622, 0.054866667836904526, -0.0456656776368618, -0.03388389199972153, -0.02678280510008335, 0.052663177251815796, -0.025175582617521286, 0.0722164586186409, -0.00581635907292366, -0.03853703662753105, 0.035706937313079834, 0.021587710827589035, -0.13300307095050812, 0.029662692919373512, -0.011160244233906269, 0.03039737418293953, -0.06010197103023529, -0.04871479794383049, 0.021614080294966698, 0.03176601603627205, -0.012015385553240776, 0.011894220486283302, -0.0309612900018692, -0.10682828724384308, 0.06378956139087677, 0.11224843561649323, -0.09032262116670609, 0.04894213750958443, 0.02703377977013588, 0.00620447751134634, 0.020595457404851913, 0.046373941004276276, 0.020841630175709724, -0.029382890090346336, -0.05349281057715416, -0.01769118942320347, 0.0651499480009079, 0.0673174038529396, 0.020662054419517517, 0.027296653017401695, -0.06903695315122604, 0.0920284315943718, -0.031619325280189514, -0.05239002779126167, -0.03488932549953461, -0.07323480397462845, 0.00986809004098177, -0.10094892233610153, -0.019684258848428726, 0.06834077835083008, -0.0006186348618939519, 0.011843668296933174, -0.06944064795970917, 0.08575824648141861, -0.042554501444101334, 0.023692119866609573, 0.03303191438317299, -0.11079339683055878, 0.06490276753902435, 0.017583543434739113, 0.0671694353222847, -0.04546418786048889, 0.05817359685897827, -0.1375543475151062, 0.012061970308423042, -0.016009874641895294, -0.08061809837818146, -0.0371745228767395, 0.0619116872549057, -0.09283946454524994, 0.00613011047244072, 0.02929922379553318, 0.0909251794219017, 0.010908539406955242, -0.10123719274997711, -0.012569976970553398, -0.08139552175998688, 0.03401500731706619, 0.003688865341246128, 0.052350856363773346, -0.02758331224322319, 8.649620796201445e-34, -0.05759228393435478, 0.051622647792100906, 0.01078744139522314, -0.013475123792886734, 0.045061055570840836, -0.021548975259065628, -0.05144843831658363, 0.018936239182949066, 0.022403661161661148, 0.03491251543164253, -0.043041858822107315, 0.04341119900345802, 0.03404848650097847, 0.10284373164176941, 0.0746857076883316, 0.04026755690574646, -0.015267559327185154, -0.003820613259449601, -0.01431282889097929, -0.03312879428267479, 0.0053091044537723064, -0.053251754492521286, 0.10433338582515717, 0.008480273187160492, 0.02380230464041233, -0.026401633396744728, -0.009780624881386757, -0.039515770971775055, 0.06108648329973221, -0.01238712202757597, 0.02724999189376831, 0.006115112453699112, 0.035522136837244034, -0.03854828327894211, 0.020915474742650986, -0.021620212122797966, 0.055132023990154266, 0.010579943656921387, -0.0003220442740712315, 0.00014377744810190052, -0.002018499653786421, 0.007132015191018581, -0.010984999127686024, 0.018894227221608162, -0.015698352828621864, 0.0015494439285248518, 0.04518363997340202, 0.04198039323091507, 0.03380080685019493, -0.03864184021949768, -0.009605104103684425, 0.039119113236665726, -0.0740484669804573, -0.03578897938132286, 0.0077846166677773, -0.0433514378964901, 0.05013960227370262, 0.036609992384910583, 0.033532220870256424, 0.01992366835474968, -0.011357249692082405, 0.04717140272259712, 0.03694775700569153, 0.060797106474637985, 0.019032880663871765, -0.04292536526918411, -0.026953240856528282, 0.014982335269451141, 0.04495678469538689, -0.00610994640737772, -0.007641545031219721, -0.009085374884307384, -0.014803857542574406, -0.06564900279045105, 0.02449130266904831, 0.026967046782374382, 0.07053511589765549, -0.07638977468013763, -0.03039700724184513, -0.008172607980668545, 0.042092449963092804, 0.0160821583122015, -0.014149169437587261, 0.03439607843756676, -0.006947923917323351, 0.00029651724616996944, 0.06435033679008484, -0.04960944876074791, 0.007691868115216494, 0.013099207542836666, -0.026250271126627922, -0.04099937528371811, -0.012645025737583637, -0.040808919817209244, 0.004723045509308577, -1.9817809870726644e-33, -0.03836645931005478, 0.04729839041829109, -0.09141983836889267, -0.029000572860240936, -0.110122911632061, -0.01030307449400425, -0.019805746152997017, 0.006205921061336994, -0.055398449301719666, 0.003061501309275627, -0.01070970669388771, 0.0057974946685135365, 0.020076723769307137, -0.003823742736130953, -0.006736666429787874, -0.031971558928489685, -0.07304921746253967, 0.09094331413507462, 0.07583165168762207, 0.010589206591248512, 0.014308953657746315, 0.07948672026395798, -0.10979561507701874, 0.06127675995230675, -0.007638345472514629, -0.0438191182911396, -0.006356590893119574, 0.024575229734182358, -0.021928630769252777, -0.006340111140161753, -0.05321183428168297, -0.05989104509353638, -0.024048835039138794, 0.04036303982138634, -0.049218472093343735, -0.009985651820898056, 0.06335501372814178, 0.06263861060142517, 0.0008302197675220668, 0.04699863865971565, 0.06843405216932297, 0.026654714718461037, -0.07305778563022614, -0.022687437012791634, -0.006312723737210035, -0.11824257671833038, -0.15103915333747864, 0.047871656715869904, -0.016649775207042694, 0.05419811233878136, 0.005405329167842865, 0.004790209233760834, -0.06742043048143387, -0.03258313611149788, 0.004257810302078724, 0.07751821726560593, -0.03264062479138374, 0.010998751036822796, 0.053205862641334534, 0.05369583144783974, -0.0709429383277893, -0.08528691530227661, 0.059349045157432556, -0.021366000175476074, 0.06306926161050797, -0.07911960780620575, -0.01240477990359068, 0.045353878289461136, -0.026990970596671104, -0.021420519798994064, 0.015017126686871052, -0.007653483655303717, 0.06848165392875671, 0.01312959473580122, -0.015835026279091835, 0.010833927430212498, -0.043308790773153305, 0.04324011877179146, -0.05655497685074806, -0.03338497132062912, 0.013281112536787987, -0.0436389222741127, -0.023691322654485703, 0.10350930690765381, 0.07492788136005402, 0.061493128538131714, 0.04394852742552757, -0.03310193493962288, 0.03758648410439491, -0.003662957577034831, 0.009640461765229702, 0.135030597448349, 0.0049296156503260136, 0.05684814229607582, -0.0434972308576107, -4.9490793685436074e-08, -0.05679628998041153, 0.024734295904636383, 0.008270341902971268, -0.028054561465978622, 0.03804844617843628, 0.016679363325238228, 0.047683797776699066, 0.025831224396824837, -0.041589077562093735, -0.04671601206064224, 0.1102081909775734, -0.09734378010034561, -0.10535632818937302, -0.055323973298072815, 0.0164767038077116, 0.04013001173734665, 0.057127829641103745, -0.047781266272068024, -0.031956084072589874, 0.05994417890906334, 0.11112131923437119, 0.02602395974099636, 0.07756104320287704, 0.04547016695141792, -0.048253417015075684, -0.10162148624658585, -0.05136683210730553, 0.06593151390552521, 0.007457276340574026, 0.011217253282666206, 0.016581542789936066, 0.015159553848206997, 0.0051961541175842285, -0.05718114599585533, 0.05600891634821892, 0.027566002681851387, 0.02878718636929989, -0.021342940628528595, 0.021478919312357903, 0.0060192374512553215, 0.06131496652960777, -0.02193303033709526, -0.10791172832250595, -0.01910751312971115, 0.0632288008928299, -0.10277126729488373, 0.09167139232158661, -0.12463323026895523, -0.009772606194019318, 0.037468601018190384, 0.015587246045470238, 0.02810961753129959, -0.012362735345959663, 0.05497199669480324, 0.01526299025863409, -0.025892436504364014, -0.01082362700253725, -0.05147911235690117, -0.002187918173149228, 0.1087622418999672, -0.023207494989037514, 0.0737316831946373, -0.07191246002912521, -0.0008141875732690096]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AdaptiveNeuralNetworksUsingMOSChargeStorage.pdf,Computer Vision,"A NETWORK FOR IMAGE SEGMENTATION USING COLOR Anya Hurlbert and Tomaso Poggio Center for Biological Information Processing at Whitaker College Department of Brain and Cognitive Science and the MIT AI Laboratory Cambridge, MA 02139 (hur lbert@wheaties.ai.mit.edu) ABSTRACT We propose a parallel network of simple processors to find color boundaries irrespective of spatial changes in illumi- nation, and to spread uniform colors within marked re- . glOns. INTRODUCTION To rely on color as a cue in recognizing objects, a visual system must have at least approximate color constancy. Otherwise it might ascribe different characteristics to the same object under different lights. But the first step in using color for recog- nition, segmenting the scene into regions of different colors, does not require color constancy. In this crucial step color serves simply as a means of distinguishing one object from another in a given scene. Color differences, which mark material boundaries, are essential, while absolute color values are not. The goal of segmen- tation algorithms is to achieve this first step toward object recognition by finding discontinuities in the image irradiance that mark material boundaries. The problems that segmentation algorithms must solve is how to choose color la- bels, how to distinguish material boundaries from other changes in the image that give rise to color edges, and how to fill in uniform regions with the appropriate color labels. (Ideally, the color labels should remain constant under changes in the illumination or scene composition and color edges should occur only at material boundaries.) Rubin and Richards (1984 ) show that algorithms can solve the sec- ond problem under some conditions by comparing the image irradiance signal in distinct spectral channels on either side of an edge. The goal of the segmentation algorithms we discuss here is to find boundaries be- tween regions of different surface spectral reflectances and to spread uniform colors within them, without explicitly requiring the colors to be constant under changes in illumination. The color labels we use are analogous to the CIE chromaticity coordinates x and y. Under the single source assumption, they change across space 297 298 Hurlbert and Poggio only when the surface spectral reflectance changes, except when strong speculari- ties are present. (The algorithms therefore require help at a later stage to identify between color label changes due to specularities, which we have not yet explicitly incorporated.) The color edges themselves are localised with the help of luminance edges, by analogy with psychophysics of segmentation and filling-in. The Koftka Ring illusion, for example, indicates that color is attributed to surfaces by an inter- action between an edge-finding operator and a filling-in operator.1 The interaction is justified by the fact that in the real world changes in surface spectral reflectance are almost always accompanied by changes in brightness. Color Labels We assume that surfaces reflect light according to the neutral-interface-reflection model. In this model (Lee, 1986 , Shaefer, 1984 [3]) the image irradiance I(X,y,A) is the sum of two components, the surface reflection and the body reflection: I(x, y, A) = L(r(x, y), A)[a(r, A)g(6(r)) + bh(6(r))], where A labels wavelength and r( x, y) is the point on the 3D surface to which the image coordinates (x, y) correspond. L(r(x, y), A) is the illumination on the surface. a(r, A) is the spectral reflectance factor of the body reflection component and g(6(r)) its magnitude, which depends on the viewing geometry parameters lumped together in 6(r). The spectral reflectance factor of the specular, or surface reflection, component b is assumed to be constant with respect to A, as is true for inhomogeneous materials such as paints and plastics. For most materials, the magnitude of the specular component h depends strongly on the viewing geometry. Using the single source assumption, we may factor the illumination L into separate spatial and spectral components (L(r, A) L(r)c(A)). Multiplying I by the spectral sensitivities of the color sensors i = 1,2,3 and integrating over wavelength yields the triplet of color values (R, G, B), where and so forth and where the ai and bi are the reflectance factors in the spectral channels defined by the sensor spectral sensitivities. We define the hues u and v as R u= --__ -- R+G+B and 1 Note that Land's original retinex algorithm, which thresholds and swns the differences in image irradiance between adjacent points along many paths, accounts for the contribution of edges to color, without introducing a separate luminance edge detector. A Network for Image Segmentation Using Color 299 G v=----- R+G+B at each pixel. In Lambertian reflection, the specular reflectance factor b is zero. In this case, u and v are piecewise constant: they change in the image only when the ai(x,y) change. Thus u or v mark discontinuities in the surface spectral reflectance function, e.g they mark material boundaries. Conversely, image regions of constant u correspond to regions of constant surface color. Synthetic images generated with standard computer graphics algorithms (using, for example, the Phong reflectance model) behave in this way: u is constant across the visible surface of a shaded sphere. Across specularities, u in general changes but often not much. Thus one approach to the segmentation problem is to find regions of ""constant"" u and their boundaries. The difficulty with this approach is that real u data are noisy and unreliable: u is the quotient of numbers that are not only noisy themselves but also, at least for biological photosensor spectral sensitivities, very close to one another. The goals of segmentation algorithms are therefore to enhance discontinuities in u and, within the regions marked by the discontinuities, to smoothe over the noise and fill in the data where they are unreliable. We have explored several methods of meeting these goals. Segmentation Algorithms One method is to regularize - to eliminate the noise and fill in the data, while preserving the discontinuities. Using an algorithm based on Markov Random Field techniques, we have obtained encouraging results on real images (see Poggio et al., 1988). The MRF technique exploits the constraint that u should be piecewise constant within the discontinuity contours and uses image brightness edges as guides in finding the contours. An alternative to the MRF approach is a cooperative network that fills in data and filters out noise while enforcing the constraint of piecewise constancy. The network, a type of Hopfield net, is similar to the cooperative stereo network of Marr and Poggio (1976). Another approach consists of a one-pass winner-take-all scheme. Both algorithms involve loading the initial hue values into discrete bins, an undesirable and biologically unlikely feature. Although they produce good results on noisy synthetic images and can be improved by modification (see Hurlbert, 1989), another class of algorithms which we now describe are simple and effective, especially on parallel computers such as the Connection Machine. Averaging Network One way to avoid small step changes in hue across a uniform surface resulting from initial loading into discrete bins is to relax the local requirement for piecewise 300 Hurlbert and Poggio b. 41 97 "" 74 Figure 1: (a) Image of a Mondrian-textured sphere - the red channel. (b) Vertical slice through the specularity in a 75 x 75 pixel region of the three-channel image (R + G + B) of the same sphere. constancy and instead require only that hue be smooth within regions delineated by the edge input. We will see that this local smoothness requirement actually yields an iterative algorithm that provides asymptotically piecewise constant hue regions. To implement the local smoothness criterion we use an averaging scheme that simply replaces the value of each pixel in the hue image with the average of its local surround, iterating many times over the whole image. The algorithm takes as input the hue image (either the u-image or the v-image) and one or two edge images, either luminance edges alone, or luminance edges plus u or v edges, or u edges plus v edges. The edge images are obtained by performing Canny edge detection or by using a thresholded directional first derivative. On each iteration, the value at each pixel in the hue image is replaced by the average of its value and those in its contributing neighborhood. A neighboring pixel is allowed to contribute if (i) it is one of the four pixels sharing a full border with the central pixel (ii) it shares the same edge label with the central pixel in all input edge images (iii) its value is non-zero and (iv) its value is within a fixed range of the central pixel value. The last requirement simply reinforces the edge label requirement when a hue image serves as an input edge image - the edge label requirement allows only those pixels that lie on the same side of an edge to be averaged, while the other insures that only those pixels with similar hues are averaged. More formally A Network for Image Segmentation Using Color 301 where Cn(hf,j) is the set of N(Cn) pixels among the next neighbors of i,j that differ from h~. less than a specified amount and are not crossed by an edge in the edge map(s) (on the assumption that the pixel (i,j) does not belong to an edge). The iteration of this operator is similar to nonlinear diffusion and to discontinuous regularization of the type discussed by Blake and Zisserman (1987), Geman and Geman (1984) and Marroquin (9]. The iterative scheme of the above equation can be derived from minimization via gradient descent of the energy function E = L:Ei,j with where V(x, y) = V(x - y) is a quadratic potential around 0 and constant for Ix - yl above a certain value. The local averaging smoothes noise in the hue values and spreads uniform hues across regions marked by the edge inputs. On images with shading but without strong specularities the algorithm performs a clean segmentation into regions of different hues. Conclusions The averaging scheme finds constant hue regions under the assumptions of a single source and no strong specularities. A strong highlight may originate an edge that could then ""break"" the averaging operation. In our limited experience most spec- ularities seem to average out and disappear from the smoothed hue map, largely because even strong specularities in the image are much reduced in the initial hue image. The iterative averaging scheme completely eliminates the remaining gradi- ents in hue. It is possible that more powerful discrimination of specularities will require specialized routines and higher-level knowledge (Hurlbert, 1989). Yet this simple network alone is sufficient to reproduce some psychophysical phe- nomena. In particular, the interaction between brightness and color edges enables the network to mimic such visual ""illusions"" as the Koftka Ring. We replicate the illusion in the following way. A black-and-white Koft'ka Ring (a uniform grey annu- lus against a rectangular bipartite background, one side black and the other white) (Hurlbert and Poggio, 1988b) is filtered through the lightness filter estimated in 302 Hurlbert and Poggio a. c. 9.1989 9 72 9.49679872 9.1122449 9 h. 299 Figure 2: (a) A 75x75 pixel region of the u image, including the specularity. (b) The image obtained after 500 iterations of the averaging network on (a), using as edge input the Canny edges of the luminance image. A threshold on differences in the v image allows only similar v values to be averaged. (c) Vertical slice through center of (a). (d) Vertical slice at same coordinates through (b) (note different scales of (c) and (d». A Network for Image Segmentation Using Color 303 the way described elsewhere (Hurlbert and Poggio, 1988a). (For black-and-white images this step replaces the operation of obtaining u and v: in both cases the goal is to eliminate spatial gradients of in the effective illumination.) The filtered Koffka Ring is then fed to the averaging network together with the brightness edges. When in the input image the boundary between the two parts of the background continues across the annulus, in the output image (after 2000 iterations of the averaging net- work) the annulus splits into two semi-annuli of different colors in the output image, dark grey against the white half, light grey against the black half (Hurlbert, 1989). When the boundary does not continue across the annulus, the annulus remains a uniform grey. These results agree with human perception. Acknowledgements This report describes research done within the Center for Biological Information Processing, in the Department of Brain and Cognitive Sciences, and at the Artifi- cial Intelligence Laboratory. This research is sponsored by a grant from the Office of Naval Research (ONR), Cognitive and Neural Sciences Division; by the Artificial Intelligence Center of Hughes Aircraft Corporation; by the Alfred P. Sloan Foun- dation; by the National Science Foundation; by the Artificial Intelligence Center of Hughes Aircraft Corporation (SI-801534-2); and by the NATO Scientific Affairs Di- vision (0403/87). Support for the A. I. Laboratory's artificial intelligence research is provided by the Advanced Research Projects Agency of the Department of Defense under Army contract DACA76-85-C-001O, and in part by ONR contract NOOOI4- 85-K-0124. Tomaso Foggio is supported by the Uncas and Helen Whitaker Chair at the Massachusetts Institute of Technology, Whitaker College. References John Rubin and Whitman Richards. Colour VISIon: representing material cate- gories. Artificial Intelligence Laboratory Memo 764, Massachusetts Institute of Technology, 1984. Hsien-Che Lee. Method for computing the scene-illuminant chromaticity from spec- ular highlights. Journal of the Optical Society of America, 3:1694-1699, 1986. Steven A. Shafer. Using color to separate reflection components. Color Research and Applications, 10(4):210-218, 1985. Tomaso Poggio, J. Little, E. Gamble, W. Gillett, D. Geiger, D. Weinshall, M. Vil- lalba, N. Larson, T. Cass, H. Biilthoff, M. Drumheller, P. Oppenheimer, W. Yang, and A. Hurlbert. The MIT Vision Machine. In Proceedings Image Understanding Workshop, Cambridge, MA, April 1988. Morgan Kaufmann, San Mateo, CA. David Marr and Tomaso Poggio. Cooperative computation of stereo disparity. Sci- ence, 194:283-287, 1976. Anya C. Hurlbert. The Computation of Color. PhD thesis, Massachusetts Institute of Technology, Cambridge, MA, 1989. Jose L. Marroquin. Probabilistic Solution of Inverse Problems. PhD thesis, Mas- sachusetts Institute of Technology, Cambridge, MA, 1985. 304 Hurlbert and Poggio Andrew Blake and Andrew Zisserman. Visual Reconstruction. MIT Press, Cam- bridge, Mass, 1987. Stuart Geman and Don Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern A nalysis and Machine Intelligence, PAMI-6:721-741, 1984. Anya C. Hurlbert and Tomaso A. Poggio. Learning a color algorithm from examples. In Dana Z. Anderson, editor, Neural Information Processing Systems. American Institute of Physics, 1988. A. C. Hurlbert and T. A. Poggio. Synthesizing a color algorithm from examples. Science, 239:482-485, 1988.","[0.028468240052461624, -0.013177086599171162, 0.057888101786375046, -0.046181004494428635, 0.07169095426797867, -0.07216846197843552, 0.07012870907783508, -0.046842772513628006, 0.03834204375743866, -0.07596070319414139, -0.06802932173013687, -0.08027859032154083, 0.04022059217095375, 0.07485613971948624, -0.03900262340903282, 0.00908189918845892, 0.008403417654335499, 0.1188545748591423, -0.10865148901939392, -0.031229179352521896, 0.02046581357717514, -0.05072671175003052, 0.013396993279457092, -0.03249449282884598, -0.04062278941273689, 0.03905804082751274, 0.04863879829645157, -0.05939518287777901, -0.007688390091061592, -0.010742990300059319, 0.04742792248725891, -0.05472821369767189, 0.028321970254182816, 0.06505949795246124, -0.029890691861510277, -0.00887308456003666, -0.0018137210281565785, -0.022561069577932358, -0.0317709818482399, 0.010504513047635555, -0.02751189097762108, 0.01045245211571455, -0.012094923295080662, -0.01638675108551979, 0.08767063915729523, 0.05158153176307678, 0.05542756989598274, 0.009627608582377434, -0.028540650382637978, -0.05007077008485794, -0.001270797336474061, -0.018101872876286507, -0.07539363950490952, 0.07926030457019806, -0.012209437787532806, 0.05974864587187767, 0.055893488228321075, -0.03389422222971916, 0.03407556936144829, -0.01333486195653677, 0.009541712701320648, -0.07146447151899338, 0.016984600573778152, -0.024502942338585854, -0.024910634383559227, 0.021068187430500984, -0.010837110690772533, -0.0458514504134655, 0.013126084581017494, -0.06590345501899719, 0.06906293332576752, 0.08077201247215271, 0.06637551635503769, 0.03245648741722107, -0.005093865096569061, 0.049981944262981415, 0.1261470913887024, 0.08855650573968887, -0.036201152950525284, -0.12952032685279846, 0.06425817310810089, 0.034557223320007324, 0.022371875122189522, 0.04289717227220535, 0.1476522833108902, -0.00828706007450819, -0.08383894711732864, 0.08970238268375397, -0.03776399418711662, 0.027477676048874855, -0.006012627389281988, 0.004327991511672735, 0.0029492012690752745, -0.04084456339478493, 0.03542347997426987, -0.07038499414920807, 0.01859115995466709, -0.05670235678553581, 0.03791284188628197, 0.04672398790717125, -0.020051805302500725, -0.05655478686094284, 0.03823895379900932, -0.055806469172239304, 0.010234001092612743, -0.004074486903846264, 0.05393586307764053, 0.004829710815101862, 0.09184399992227554, -0.05596001818776131, 0.058592747896909714, 0.004817838780581951, -0.022443091496825218, 0.02450253628194332, 0.009650284424424171, -0.010053430683910847, 0.031040441244840622, 0.03509846329689026, 0.07419849932193756, 0.0303523950278759, 0.020636126399040222, -0.04149334877729416, -0.06672936677932739, 0.0653008371591568, 0.09731283783912659, -0.036533091217279434, -0.033862195909023285, 4.40901519104745e-33, 0.012565182521939278, -0.05102887377142906, 0.00832583848387003, -0.026826871559023857, 0.06056647002696991, 0.06262540072202682, -0.053036753088235855, -0.0128566799685359, -0.03344490006566048, -0.01100880280137062, -0.0010544467950239778, 0.0056573860347270966, -0.02934960089623928, 0.05043816193938255, 0.07795340567827225, -0.04670826718211174, 0.04923687502741814, -0.028042022138834, -0.035620059818029404, -0.00906011089682579, -0.09459049254655838, -0.03846196457743645, 0.03951309993863106, 0.04412276670336723, -0.03187181428074837, 0.019945785403251648, -0.010714071802794933, -0.035363320261240005, 0.05924849212169647, -0.019146669656038284, 0.0007666242308914661, 0.028958026319742203, 0.021300729364156723, 0.10725859552621841, -0.0047712260857224464, 0.0013523087836802006, 0.03757577762007713, 0.025433961302042007, 0.03710095211863518, 0.011589963920414448, -0.025327306240797043, 0.11722775548696518, 0.023605238646268845, -0.029736701399087906, -0.015731647610664368, 0.005650699604302645, 0.034532152116298676, 0.07159022986888885, -0.007833946496248245, 0.08132488280534744, -0.02862462028861046, 0.001991088269278407, 0.03208719566464424, -0.142039954662323, -0.033529117703437805, 0.017595654353499413, 0.008283381350338459, 0.07513437420129776, 0.11092576384544373, -0.044938232749700546, 0.035845205187797546, 0.009386459365487099, 0.01952061429619789, 0.08759589493274689, 0.02481178566813469, 0.01480117253959179, -0.05930250883102417, 0.012763982638716698, 0.016049707308411598, 0.04053984954953194, -0.05110419541597366, 0.05136245861649513, 0.0009280912927351892, -0.03360973298549652, 0.03233593702316284, 0.006562422029674053, 0.0009262540261261165, -0.0467657670378685, -0.0020069899037480354, -0.043910957872867584, -0.10159867256879807, 0.03985501080751419, -0.09370310604572296, -0.09455665946006775, -0.08638457953929901, 0.05615248903632164, 0.04312704876065254, 0.026408758014440536, -0.05577603727579117, -0.06666935980319977, -0.01619429886341095, -0.020159104838967323, 0.010134163312613964, -0.030624067410826683, -0.00022448769595939666, -3.638681784200942e-33, 0.008444109000265598, -0.02346424199640751, -0.07671230286359787, 0.055222202092409134, -0.011464008130133152, -0.06734593957662582, -0.007313723210245371, -0.015583975240588188, -0.05770982801914215, -0.015105762518942356, 0.03563546761870384, 0.05432882532477379, -0.07686767727136612, 0.08876015990972519, -0.056614089757204056, -0.012800927273929119, -0.01934775523841381, 0.07921053469181061, 0.018606700003147125, 0.011162896640598774, -0.09164884686470032, 0.0982983186841011, -0.08995009958744049, -0.06052194535732269, -0.15836642682552338, 0.09386777132749557, -0.07218097895383835, 0.01598755642771721, -0.06505226343870163, -0.06965086609125137, -0.07426087558269501, 0.0074602775275707245, -0.04500637203454971, -0.0414847806096077, 0.021429171785712242, 0.021090181544423103, 0.04352737218141556, -0.07474169135093689, -0.04757989943027496, 0.05888112261891365, 0.06582760810852051, -0.05605504289269447, -0.04612730070948601, -0.003260140074416995, -0.049423910677433014, -0.04278784990310669, -0.03931859880685806, 0.09211736172437668, -0.10867727547883987, 0.06095448136329651, -0.02636115811765194, 0.019364871084690094, -0.07021670788526535, -0.04466154798865318, 0.04876070097088814, 0.042206671088933945, -0.06904381513595581, 0.038421910256147385, 0.013574535958468914, 0.03938673064112663, -0.09678268432617188, -0.0479111447930336, -0.016987435519695282, 0.06193682178854942, 0.03456674888730049, 0.00821787677705288, -0.03495370224118233, 0.03515622392296791, 0.05006352439522743, 0.019915543496608734, 0.029564451426267624, 0.0006172481807880104, 0.037881117314100266, -0.0011529880575835705, 0.03204387426376343, -0.044962890446186066, 0.02775583416223526, -0.02721339277923107, -0.008569687604904175, -0.002198323141783476, -0.045547645539045334, -0.11834240704774857, -0.029513875022530556, 0.06731104850769043, 0.05445648729801178, 0.03855963051319122, 0.00453766155987978, -0.05508854240179062, 0.0759509801864624, -0.04989675059914589, -0.0039579737931489944, 0.01325627975165844, 0.05924883484840393, -0.0033918628469109535, -0.09000685811042786, -5.2674373307581845e-08, -0.0050553325563669205, -0.03624819219112396, 0.022369742393493652, 0.007120031397789717, 0.08458653837442398, -0.039890967309474945, -0.013492046855390072, 0.03126715123653412, -0.03172900900244713, 0.03476850688457489, 0.045134685933589935, -0.08591658622026443, -0.019355082884430885, -0.030348045751452446, -0.026691116392612457, 0.07183124870061874, 0.008666981011629105, -0.05517930909991264, 0.008086657151579857, 0.10255816578865051, -0.075611412525177, -0.009684108197689056, -0.04162866622209549, 0.07025811076164246, 0.044020187109708786, -0.04044002294540405, -0.061068084090948105, 0.07153552770614624, 0.05739044398069382, -0.005263982340693474, 0.06183160841464996, 0.030166400596499443, 0.08104069530963898, 0.036786969751119614, 0.06164337322115898, -0.016417909413576126, -0.03859352320432663, 0.08774125576019287, -0.02678670547902584, -0.05350881814956665, -0.029517266899347305, -0.01904934085905552, 0.02660091035068035, -0.002802231814712286, 0.007882503792643547, -0.027886725962162018, 0.054308775812387466, -0.002805874915793538, -0.001667700707912445, 0.045504212379455566, -0.012332684360444546, 0.01177718210965395, -0.02633301168680191, 0.04933515936136246, -0.04445991665124893, -0.10632461309432983, 0.024388523772358894, -0.0106595354154706, 0.0736912339925766, 0.07142847031354904, 0.007123582065105438, 0.007824469357728958, -0.09473878890275955, -0.07829225808382034]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ALowPowerCMOSCircuitWhichEmulatesTemporalElectricalPropertiesofNeurons.pdf,Computer Vision,"436 SIMULATION AND MEASUREMENT OF THE ELECTRIC FIELDS GENERATED BY WEAKLY ELECTRIC FISH Brian Rasnow1, Christopher Assad2, Mark E. Nelson3 and James M. Bow~ Divisions of Physics1 ,Elecbical Engineerini, and Biolo~ Caltech, Pasadena, 91125 ABSTRACT The weakly electric fish, Gnathonemus peters;;, explores its environment by gener- ating pulsed elecbic fields and detecting small pertwbations in the fields resulting from nearby objects. Accordingly, the fISh detects and discriminates objects on the basis of a sequence of elecbic ""images"" whose temporal and spatial properties depend on the tim- ing of the fish's electric organ discharge and its body position relative to objects in its en- vironmenl We are interested in investigating how these fish utilize timing and body-po- sition during exploration to aid in object discrimination. We have developed a fmite-ele- ment simulation of the fish's self-generated electric fields so as to reconstruct the elec- trosensory consequences of body position and electric organ discharge timing in the fish. This paper describes this finite-element simulation system and presents preliminary elec- tric field measurements which are being used to tune the simulation. INTRODUCTION The active positioning of sensory structures (i.e. eyes, ears, whiskers, nostrils, etc.) is characteristic of the information seeking behavior of all exploratory animals. Yet, in most existing computational models and in many standard experimental paradigms, the active aspects of sensory processing are either eliminated or controlled (e.g. by stimulat- ing fIXed groups of receptors or by stabilizing images). However, it is clear that the ac- tive positioning of receptor surfaces directly affects the content and quality of the sensory infonnation received by the nervous system. Thus. controlling the position of sensors during sensory exploration constitutes an important feature of an animals strategy for making sensory discriminations. Quantitative study of this process could very well shed light on the algorithms and internal representations used by the nervous system in dis- criminating peripheral objects. Studies of the active use of sensory surfaces generally can be expected to pose a number of experimental challenges. This is because, in many animals, the sensory surfac- es involved are themselves structurally complicated, making it difficult to reconstruct p0- sition sequences or the consequences of any repositioning. For example, while the sen- Simulation and Measurement of the Weakly Electric Fish 437 sory systems of rats have been the subjects of a great deal of behavioral (Welker, 1964) and neurophysiological study (Gibson & Welker, 1983), it is extremely difficult to even monitor the movements of the perioral surfaces (lips, snout, whiskers) used by these ani- mals in their exploration of the world let alone reconstruct the sensory consequences. For these reasons we have sought an experimental animal with a sensory system in which these sensory-motor interactions can be more readily quantified. The experimental animal which we have selected for studying the control of sensory surface position during exploration is a member of a family of African freshwater fish (Monniridae) that use self-generated electric fields to detect and discriminate objects in their environment (Bullock & Heiligenberg, 1986). The electrosensory system in these fish relies on an ""electric organ"" in their tails which produces a weak pulsed electric field in the surrounding environment (significant within 1-2 body lengths) that is then detected with an array of electrosensors that are extremely sensitive to voltage drops across the skin. These ""electroreceptors"" allow the fISh to respond to the perturbations in the elec- tric field resulting from objects in the environment which differ in conductivity from the surrounding water (Fig. 1). .. conducting • object IIID electric organ § electroreceptors electric field lines Figure 1. The peripheral electrosensory system of Gnathonemus petersii consists of an ""electric organ"" current source at the base of the tail and sev- eral thousand ""electroreceptor"" cells distributed non uniformly over the fish's body. A conducting object near the fish causes a local increase in the current through the skin. These fISh are nocturnal, and rely more on their electric sense than on any other sensory system in perfonning a wide range of behaviors (eg. detecting and localizing objects such as food). It is also known that these fish execute exploratory movements, changing their body position actively as they attempt an electrosensory discrimination (Toerring & Belbenoit, 1979). Our objective is to understand how these movements change the distri- bution of the electric field on the animals skin, and to determine what, if any, relationship this has to the discrimination process. There are several clear advantages of this system for our studies. First, the electrore- 438 Rasnow, Assad, Nelson and Bower ceptors are in a fixed position with respect to each other on the surface of the animal. Therefore, by knowing the overall body position of the animal it is possible to know the exact spatial relationship of electroreceptors with respect to objects in the environment. Second, the physical equations governing the self-generated electric fIeld in the fish's en- vironment are well understood. As a consequence, it is relatively straightforward to re- construct perturbations in the electric field resulting from objects of different shape and conductance. Third, the electric potential can be readily measured, providing a direct measure of the electric field at a distance from the fish which can be used to reconstruct the potential difference across the animals skin. And finally, in the particular species of fish we have chosen to work with, Gnathonemus petersii, individual animals execute a brief (100 J.1Sec) electric organ discharge (BOD) at intervals of 30 msec to a few seconds. Modification of the firing pattern is 1cnown to be correlated with changes in the electrical environment (Lissmann, 1958). Thus, when the electric organ discharges, it is probable that the animal is interested in ""taking a look"" at its surroundings. In few other sensory systems is there as direct an indication of the attentional state of the subject. Having stated the advantages of this system for the study we have undertaken, it is also the case that considerable effort will still be necessary to answer the questions we have posed. For example, as described in this paper, in order to use electric field mea- surements made at a distance to infer the voltages across the surface of the animal's skin, it is necessary to develop a computer model of the fish and its environment. This will allow us to predict the field on the animal's skin surface given different body poSitions relative to objects in the environment. This paper describes our first steps in constructing this simulation system. Experimental Approach and Methods Simulations of Fish Electric Fields The electric potential, cll(x), generated by the EOD of a weakly electric fish in a fish tank is a solution ofPoisson's equation: Ve(pVell) = f where p(x)and f(x) are the impedance magnitude and source density at each point x in- side and surrounding the fish. Our goal is to solve this equation for ell given the current source density, f, generated by the electric organ and the impedances, p, corresponding to the properties of the fish and external objects (rocks, worms, etc.). Given p and f. this equation can be solved for the potential ell using a variety of iterative approximation schemes. Iterative methods, in general, first discretize the spatial domain of the problem into a set of ""node"" points, and convert Poisson's equation into a set of algebraic equa- tions with the nodal potentials as the unknown parameters. The node values, in this case, each represent an independent degree of freedom of the system and, as a consequence, there are as many equations as there are nodes. This very large system of equations can Simulation and Measurement of the Weakly Electric Fish 439 then be solved using a variety of standard techniques, including relaxation methods, con- jugate gradient minimization, domain decomposition and multi-grid methods. To simulate the electric fields generated by a fish, we currently use a 2-dimensional fmite element domain discretization (Hughes, 1987) and conjugate gradient solver. We chose the finite element method because it allows us to simulate the electric fields at much higher resolution in the area of interest close to the animal's body where the elec- tric field is largest and where errors due to the discretization would be most severe. The fmite element method is based on minimizing a global function that corresponds to the potential energy of the electric field. To compute this energy, the domain is decomposed into a large number of elements, each with uniform impedance (see Fig. 2). The global energy is expressed as a sum of the contributions from each element, where the potential within each element is assumed to be a linear interpolation of the potentials at the nodes or vertices of each element The conjugate gradient solver determines the values of the node potentials which minimize the global energy function. 1\ IVrv'V 1\ 1\ rv:V J J 1\/1\ .J 1\/ 1\11\/1\/1\11\/1\/1\ 1\11\/[\ 1\1 IV V r--.. v 7' v [7 If\ If\ '\ '\ V\ V If\ J\ 1'\ 'w l/\ V 11'\ '\ 1/ :1'\ '\ '\ '\ V '\ 1'\"" '\ 11\ 1/\ \ '\ V Figure 2. The inner region of a fmite element grid constructed for simulat- ing in 2-dimensions the electric field generated by an electric fish. Measurement of Fish Electric Fields Another aspect of our experimental approach involves the direct measurement of the potential generated by a fish's EOD in a fish tank using arrays of small electrodes and differential amplifiers. The electrodes and electronics have a high impedance which min- imizes their influence on the electric fields they are designed to measure. The electrodes are made by pulling a 1mm glass capillary tube across a heated tungsten filament, result- ing in a fine tapered tip through which a 1~ silver wire is run. The end of this wire is melted in a flame leaving a 200J,un ball below the glass insulation. Several electrodes are then mounted as an array on a microdrive attached to a modified X-Yplotter under com- puter control and giving better than 1mm positioning accuracy. Generated potentials are amplified by a factor of 10 - 100, and digitized at a rate of 100kHz per channel with a 12 bit AID converter using a Masscomp 5700 computer. An array processor searches this 440 Rasnow, Assad, Nelson and Bower continuous stream of data for EOD wavefonns. which are extracted and saved along with the position of the electrode array. Results Calibration of the Simulator In order to have confidence in the overall system, it was fD'St necessary to calibrate both the recording and the simulation procedures. To do this we set up relatively simple geometrical arrangements of sources and conductors in a fish tank for which the potential could be found analytically. The calibration source was an electronic ""fake fish"" circuit that generated signals resembling the discharge of Gnathonemus. Point current source A point source in a 2-dimensional box is perhaps the simplest configuration with which to initially test our electric field reconstruction system. The analytic solution for the potential from a point current source centered in a grounded. conducting 2-dimen- sional box is: . (.n7t). (n7tx). h (.n7ty ) 00 sm(""2 sm L sm \L 4>(x. y) = L ri1t n =1 n L cosh(T) Our fmite element simulation. based on a regular 80 x 80 node grid differs from the above expression by less than 1 %. except in the elements adjacent to the source. where the potential change across these elements is large and is not as accurately reconstructed by a linear interpolation (Fig. 3). Smaller elements surrounding the source would im- prove the accuracy. however. one should note the analytic solution is infmite at the loca- tion of the ""point"" source whereas the measured and simulated sources (and real fish) have finite current densities. To measure the real equivalent of a point source in a 2-dimensional box. we used a linear current source (a wire) which ran the full depth of a real 3-dimensional tank. Measurements made in the midplane of the tank agree with the simulation and analytic solution to better than 5% (Fig. 3.). Uncertainty in the positions of the ClUTent source and recording sites relative to the position of the conducting walls probably accounts for much of this difference. Simulation and Measurement of the Weakly Electric Fish 441 1----~~--~--~----~------- o - measured x - simulated 00 2 4 6 8 10 12 14 16 dislaDce from source Figure 3. Electric potential of a point current source centered in a grounded 2-dimensional box. Measurements of Fish Fields and 2-Dimensional Simulations Calibration of our fmite element model of an electric fish requires direct measure- ments of the electric potential close to a discharging fish. Fig. 4 shows a recording of a single EOD sampled with 5 colinear electrodes near a restrained fish. The wavefonn is bipolar, with the fIrst phase positive if recorded near the animals head and negative if re- corded near the tail (relative to a remote reference). We used the peak amplitude of the larger second phase of the wavefonn to quantify the electric potential recorded at each location. Note that the potential reverses sign at a point approximately midway along the tail. This location corresponds to the location of the null potential shown in Fig. 5. 1500 1000 $' 5500 I 0 -1r- -soo -1000 o 200 ~sec Figure 4. EOD waveform sampled simultaneously from 5 electrodes. 442 Rasnow, Assad, Nelson and Bower Measurements of EODs from a restrained fish exhibited an extraordinarily small vari- ance in amplitude and waveform over long periods of time. In fact, the peak-peak ampli- tude of the EOD varied by less than 0.4% in a sample of 40 EOD's randomly chosen dur- ing a 30 minute period. Thus we are able to directly compare waveforms sampled se- quentially without renonnalizing for fluctuations in EOD amplitude. Figure 5 shows equipotential lines reconstructed from a set of 360 measurements made in the midplane of a restrained Gnathonemus. Although the observed potential re- sembles that from a purely dipolar source (Fig. 6), careful inspection reveals an asymme- try between the head and tail of the fISh. This asymmetry can be reproduced in our simu- lations by adjusting the electrical properties of the fish. Qualitatively, the measured fields can be reproduced by assigning a low impedance to the internal body cavity and a high impedance to the skin. However, in order to match the location of the null potential, the skin impedance must vary over the length of the body. We are currently quantifying these parameters, as described in the next section. !!!!m 1'!I!fl!IPf!~m II •• 1 ... ...... 1 ••• !~ ....... . Figure 5. Measured potentials (at peak of second phase of EOD) recorded from a restrained Gnathonemus petersii in the midplane of the fish. Equipotential lines are 20 m V apart. Inset shows relative location of fish and sampling points in the fISh tank. Figure 6. Equipotential lines from a 2-dimensional finite element simula- tion of a dipole using the grid of Fig. 2. The resistivity of the fish was set equal to that of the sWToundings in this simulation. Simulation and Measurement of the Weakly Electric Fish 443 Future Directions There is still a substantial amount of work that remains to be done before we achieve our goal of being able to fully reconstruct the pattern of electroreceptor activa- tion for any arbitrary body position in any particular environment. First. it is clear that we require more information about the electrical structure of the fISh itself. We need an accurate representation of the internal impedance distribution p(x) of the body and skin as well as of the source density f(x) of the electric organ. To some extent this can be ad- dressed as an inverse problem, namely given the measured potential cl>(x), what choice of p(x) and f(x) best reproduces the data. Unfortunately, in the absence of further con- straints, there are many equally valid solution, thus we will need to directly measure the skin and body impedance of the fish. Second, we need to extend our finite-element sim- ulations of the fish to 3-dimensions which, although conceptually straight forward, re- quires substantial technical developments to be able to (a) specify and visualize the space-filling set of 3-dimensional finite-elements (eg. tetrahedrons) for arbitrary configu- rations, (b) compute the solution to the much larger set of equations (typically a factor of 100-1(00) in a reasonable time, and (c) visualize and analyze the resulting solutions for the 3-dimensional electrical fields. As a possible solution to (b), we are developing and testing a parallel processor implementation of the simulator. References Bullock, T. H. & Heiligenberg, W. (Eds.) (1986). ""Electroreception"", Wiley & Sons, New York. Gibson, J. M. & Welker. W. I. (1983). Quantitative Studies of Stimulus Coding in First- Order Vibrissa Afferents of Rats. 1. Receptive Field Properties and Threshold Distributions. Somatosensory Res. 1:51-67. Hughes, T. J. (1987). The Finite Element Method: Linear Static and Dynamic Finite Element Analysis. Prentice-Hall, New Jersey. Lissmann. H.W. (1958). On the function and evolution of electric organs in fish. J. Exp. Bioi. 35:156-191. Toening, M. J. and Belbenoit. P. (1979). Motor Programmes and Electroreception in Monnyrid Fish. Behav. Ecol. Sociobiol. 4:369-379. Welker, W. I. (1964). Analysis of Sniffing of the Albino Rat Behaviour 22:223-244.","[-0.02451806329190731, 0.05599317327141762, 0.019651364535093307, -0.04766228422522545, -0.0006440066499635577, -0.08238308131694794, 0.011632177978754044, -0.0070169284008443356, 0.0023001807276159525, -0.022267378866672516, 0.01742655038833618, -0.15240508317947388, -0.07137980312108994, 0.017006507143378258, -0.01761353388428688, -0.061307791620492935, 0.01661645993590355, 0.035772405564785004, -0.055579472333192825, 0.06527233123779297, -0.004705599043518305, 0.021054139360785484, -0.018309449777007103, -0.07268040627241135, -0.06924200057983398, -0.025437701493501663, -0.05776891112327576, 0.009472227655351162, -0.04283561930060387, -0.07622719556093216, 0.01765650138258934, -0.039934951812028885, 0.0257160272449255, -0.024009451270103455, -0.025818441063165665, 0.014398511499166489, 0.055242713540792465, -0.05514848604798317, -0.009918833151459694, 0.09235165268182755, 0.031805939972400665, 0.02772134728729725, -0.0035533078480511904, 0.019397135823965073, 0.025389328598976135, 0.019308816641569138, 0.08643189072608948, -0.11987052112817764, -0.057702165096998215, -0.015426977537572384, -0.06000939756631851, -0.03160177916288376, 0.03427277132868767, 0.062278542667627335, 0.007686965633183718, 0.047255177050828934, -0.0013470478588715196, -0.0997631773352623, 0.012134670279920101, -0.09038980305194855, 0.0064821140840649605, 0.012162061408162117, 0.044886235147714615, -0.028951143845915794, -0.004928449168801308, -0.04913013428449631, 0.05776459723711014, -0.024853799492120743, 0.009911355562508106, -0.091115303337574, 0.04280421510338783, 0.01868920773267746, -0.0003213222371414304, -0.07850328087806702, -0.04527122154831886, 0.0389414057135582, -0.021210720762610435, 0.011628401465713978, 0.012625490315258503, -0.06258721649646759, 0.017323138192296028, -0.04967673867940903, -0.08908761292695999, -0.016767026856541634, 0.009803076274693012, 0.08566631376743317, 0.024635132402181625, 0.02956601232290268, -0.14986027777194977, 0.008720505982637405, 0.022918296977877617, -0.05814928188920021, -0.08582691103219986, -0.035501524806022644, 0.0020674250554293394, -0.03661999851465225, 0.07655621320009232, -0.013496591709554195, 0.04993867129087448, -0.0016672683414071798, 0.0172629002481699, -0.051687322556972504, 0.001019213581457734, 0.04413604363799095, 0.011565365828573704, -0.03159964084625244, 0.03512580320239067, 0.114957295358181, 0.06853833049535751, -0.04347464069724083, 0.01260798703879118, -0.01853102631866932, -0.04145453870296478, 0.08192592859268188, 0.030161157250404358, 0.11720326542854309, -0.11477980017662048, 0.026281453669071198, 0.14097334444522858, -0.022048577666282654, 0.03919953107833862, -0.07507770508527756, 0.003926796838641167, 0.037202902138233185, 0.08103243261575699, 0.03959694504737854, 0.04667971283197403, 2.4447389678974603e-33, -0.019279686734080315, -0.014156505465507507, -0.058502405881881714, -0.033867381513118744, 0.032165877521038055, -0.0359562523663044, -0.04036001116037369, 0.1261761337518692, 0.04108314588665962, 0.06933789700269699, -0.0030120816081762314, 0.11523351818323135, 0.054406676441431046, -0.01813642494380474, 0.05706414580345154, -0.0578916110098362, -0.035040732473134995, -0.04230985790491104, -0.03940131887793541, 0.017963647842407227, 0.10297145694494247, -0.06345733255147934, 0.0033471796195954084, -0.05033586546778679, -0.00882644858211279, 0.02673269994556904, -0.07654453068971634, -0.04306050017476082, -0.05431686341762543, 0.04994804412126541, -0.03842338174581528, 0.03395041078329086, 0.023331845179200172, 0.04326525703072548, 0.11151382327079773, 0.0058527663350105286, 0.09164945781230927, 0.020523883402347565, -0.05376425012946129, -0.016766797751188278, 0.04117894172668457, 0.009128565900027752, 0.05616400018334389, -0.03344534710049629, -0.004423632752150297, -0.07084182649850845, 0.042822785675525665, 0.09320679306983948, 0.07115485519170761, 0.06883840262889862, 0.04268792271614075, -0.05206459388136864, 0.0336868092417717, -0.0325106605887413, 0.046691905707120895, 0.11886727809906006, -0.10070174932479858, -0.009642666205763817, -0.03168434649705887, 0.0192818995565176, -0.031146405264735222, 0.06989806890487671, 0.02478143200278282, 0.00529085285961628, 0.047296732664108276, 0.035157687962055206, -0.0744357481598854, -0.05892910808324814, 0.043500546365976334, 0.017588108777999878, 0.004851567093282938, -0.019127925857901573, 0.00829752255231142, -0.08941277116537094, 0.010722954757511616, 0.05774719640612602, 0.012025045230984688, 0.04317134991288185, -0.05737321451306343, -0.023843029513955116, -0.007453043479472399, 0.05955183506011963, 0.002581084379926324, -0.0448458306491375, -0.0707135796546936, 0.025290189310908318, 0.09503649175167084, 0.024300159886479378, -0.03170882165431976, -0.0034501655027270317, 0.05538178235292435, 0.020184583961963654, 0.02428768388926983, -0.1220698356628418, 0.004358219914138317, -2.704066429671296e-33, 0.004500557668507099, 0.018331004306674004, -0.09015285968780518, 0.02948964573442936, 0.043071623891592026, 0.007604231126606464, 0.05262654274702072, 0.10494178533554077, 0.014337314292788506, -0.03543882444500923, -0.027360424399375916, 0.012719991616904736, -0.053013671189546585, 0.004964010324329138, 0.02961895242333412, -0.02628423273563385, -0.028503669425845146, -0.04357405751943588, 0.07275591790676117, -0.04347394034266472, 0.0010678964899852872, -0.006625200621783733, 0.05536224693059921, 0.04968280345201492, 0.01691175438463688, 0.07335338741540909, 0.006365921813994646, -0.04873378574848175, -0.03680773824453354, -0.0866350457072258, -0.07950339466333389, 0.0924551784992218, 0.024804910644888878, 0.024102186784148216, -0.0635717362165451, 0.016566628590226173, 0.024772806093096733, 0.08649863302707672, 0.011500365100800991, -0.06372197717428207, 0.0678359717130661, 0.04280482605099678, 0.011829174123704433, 0.03405284136533737, -0.03405104577541351, 0.07244890928268433, -0.03610822185873985, 0.0499068908393383, -0.024727895855903625, 0.048910606652498245, -0.0668000876903534, -0.015689093619585037, -0.0281489510089159, -0.05830519273877144, 0.027674976736307144, 0.0393013060092926, 0.02415260672569275, -0.0758642926812172, -0.01766095869243145, -0.03604445233941078, -0.03620860353112221, -0.048198677599430084, 0.046803075820207596, 0.05623982101678848, 0.002813887083902955, 0.017199497669935226, 0.0203342717140913, 0.0009909605141729116, 0.06210340932011604, -0.010571234859526157, 0.03326069191098213, 0.0626152902841568, -0.043765049427747726, -0.026721417903900146, -0.08414372056722641, -0.06541198492050171, -0.02627212554216385, -0.053905975073575974, 0.05859008803963661, -0.04483456537127495, 0.07329703122377396, 0.038301486521959305, -0.04167994111776352, -0.07915065437555313, 0.008805738762021065, 0.04242588207125664, -0.10057968646287918, 0.016514785587787628, -0.01540157850831747, -0.05984281003475189, 0.011399920098483562, 0.0460997000336647, -0.027085846289992332, 0.008032076060771942, 0.08775517344474792, -4.446594914497837e-08, 0.02154056541621685, 0.03246024623513222, 0.01341172307729721, 0.03471505269408226, 0.03564492240548134, -0.02393369749188423, 0.01717269793152809, -0.01761574111878872, 0.05353941768407822, -0.11786098778247833, 0.015695618465542793, -0.0247571412473917, 0.05504526197910309, -0.042382366955280304, 0.05748623609542847, 0.0706646665930748, 0.04284749925136566, -0.06955263763666153, -0.05636104196310043, 0.0507948063313961, -0.0006621287320740521, -0.042219605296850204, -0.018768003210425377, 0.03801421448588371, -0.009617948904633522, 0.04408654570579529, -0.02909301593899727, 0.009535904042422771, 0.08885783702135086, -0.029633192345499992, 0.01592547632753849, -0.01583719626069069, -0.009059209376573563, 0.05614893510937691, 0.021647775545716286, -0.020001471042633057, -0.10282710194587708, -0.018918581306934357, -0.051032088696956635, 0.019964896142482758, -0.08122696727514267, -0.028348518535494804, -0.05923096463084221, -0.027031321078538895, 0.020123962312936783, -0.024161389097571373, 0.02145027182996273, -0.0603070929646492, 0.0126713952049613, 0.07196678221225739, -0.06640762835741043, -0.04602352902293205, -0.008963748812675476, 0.020731361582875252, -0.0441826656460762, 0.08842740952968597, 0.01922784186899662, -0.057770226150751114, -0.08103001862764359, 0.031085625290870667, 0.04245627671480179, 0.10049297660589218, -0.09988880902528763, 0.01139324251562357]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AMassivelyParallelSelfTuningContextFreeParser.pdf,Deep Learning,"ANALOG IMPLEMENTATION OF SHUNTING NEURAL NETWORKS Bahram Nabet, Robert B. Darling, and Robert B. Pinter Department of Electrical Engineering, FT-lO University of Washington Seattle, WA 98195 ABSTRACT An extremely compact, all analog and fully parallel implementa- tion of a class of shunting recurrent neural networks that is ap- plicable to a wide variety of FET-based integration technologies is proposed. While the contrast enhancement, data compression, and adaptation to mean input intensity capabilities of the network are well suited for processing of sensory information or feature extrac- tion for a content addressable memory (CAM) system, the network also admits a global Liapunov function and can thus achieve stable CAM storage itself. In addition the model can readily function as a front-end processor to an analog adaptive resonance circuit. INTRODUCTION Shunting neural networks are networks in which multiplicative, or shunting, terms of the form Xi Lj f;(Xj) or Xi Lj Ij appear in the short term memory equations, where Xi is activity of a cell or a cell population or an iso-potential portion of a cell and Ii are external inputs arriving at each site. The first case shows recurrent activity, while the second case is non-recurrent or feed forward. The polarity of these terms signify excitatory or inhibitory interactions. Shunting network equations can be derived from various sources such as the passive membrane equation with synaptic interaction (Grossberg 1973, Pinter 1983), models of dendritic interaction (RaIl 1977), or experiments on motoneurons (Ellias and Grossberg 1975). While the exact mechanisms of synaptic interactions are not known in every in- dividual case, neurobiological evidence of shunting interactions appear in several 695 696 Nabet, Darling and Pinter areas such as sensory systems, cerebellum, neocortex, and hippocampus (Grossberg 1973, Pinter 1987). In addition to neurobiology, these networks have been used to successfully explain data from disciplines ranging from population biology (Lotka 1956) to psychophysics and behavioral psychology (Grossberg 1983). Shunting nets have important advantages over additive models which lack the ex- tra nonlinearity introduced by the multiplicative terms. For example, the total activity of the network, shown by Li Xi, approaches a constant even as the input strength grows without bound. This normalization in addition to being computa- tionally desirable has interesting ramifications in visual psychophysics (Grossberg 1983). Introduction of multiplicative terms also provides a negative feedback loop which automatically controls the gain of each cell, contributes to the stability of the network, and allows for large dynamic range of the input to be processed by the network. The automatic gain control property in conjunction with properly chosen nonlinearities in the feedback loop makes the network sensitive to small input values by suppressing noise while not saturating at high input values (Grossberg 1973). Finally, shunting nets have been shown to account for short term adaptation to input properties, such as adaptation level tuning and the shift of sensitivity with background strength (Grossberg 1983), dependence of visual size preference and latency of response on contrast and mean luminance, and dependence of temporal and spatial frequency tuning on contrast and mean luminance (Pinter 1985). IMPLEMENTATION The advantages, generality, and applicability of shunting nets as cited in the previ- ous section make their implementation very desirable, but digital implementation of these networks is very inefficient due to the need for analog to digital conver- sion, multiplication and addition instructions, and implementation of iterative al- gorithms. A linear feedback class of these networks (Xi Lj !; (Xj) = Xi Li J{ijXj), however, can be implemented very efficiently with simple, completely parallel and all analog circuits. FRAMEWORK Figure 1 shows the design framework for analog implementation of a class of shunt- ing nets. In this design addition (subtraction) is achieved, via Kirchoff's current law by placing transistors in upper (lower) rails, and through the choice of deple- tion or enhancement mode devices. Multiplicative, or shunting, interconnections are done by one transistor per interconnect, using a field-effect transistor (FET) in the voltage-variable conductance region. Temporal properties are characterized by cell membrane capacitance C, which can be removed, or in effect replaced by the parasitic device capacitances, if higher speed is desired. A buffer stage is necessary for correct polarity of interconnections and the large fan-out associated with high connectivity of neural networks. Analog Implementation of Shunting Neural Networks 697 + x. , I. , c Vdd -t"" '.J . x· J Vss Figure 1. Design framework for implementation of one cell in a shunting network. Voltage output of other cells is connected to the gate of transistors Qi,i' Such a circuit is capable of implementing the general network equation: (1) Excitatory and inhibitory input current sources can also be shunted, with extra circuitry, to implement non-recurrent shunting networks. NMOS, CMOS and GALLIUM ARSENIDE Since the basic cell of Fig. 1 is very similar to a standard logic gate inverter, but with the transistors sized by gate width-to-Iength ratio to operate in the nonsaturated current region, this design is applicable to a variety of FET technologies including NMOS, CMOS, and gallium arsenide (GaAs). A circuit made of all depletion-mode devices such as GaAs MESFET buffered FET logic, can implement all the terms of Eq. (1) except shunting excitatory terms and requires a level shifter in the buffer stage. A design with all enhancement mode devices such as silicon NMOS can do the same but without a level shifter. With the addition of p-channel devices, e.g. Si CMOS, all polarities and all terms of Eq. (1) can be realized. As mentioned previously a buffer stage is necessary for correct polarity of interconnections and fan out/fan in capacity. Figure 2 shows a GaAs MESFET implementation with only depletion mode devices which employs a level shifter as the buffer stage. 698 Nabet, Darling and Pinter VDD-------------r------~--------------~-- INPUTS: EXTERNAL OR FROM PREVJOUS LAYER EXCITATORY CONNECTIONS INHIBITORY CONNECTIONS GN~-L--~------~------~-- TUNABLE SELF-RELAXATION CONNECTION OUTPUT TO NEXT LAYER VSS--~--....L....­ LEVEL SHIFT AND BUFFER STAGE Figure 2. Gallium arsenide MESFET implementation with level shifter and depletion mode devices. Lower rail transistors produce shunting off-surround terms. Upper transistors can produce addi- tive excitatory connections. SPECIFIC IMPLEMENTATION The simplest shunting network that can be implemented by the general framework of Fig.1 is Fig. 2 with only inhibitory connections (lower rail transistors). This circuit implements the network model dX· "" d/ = Ii - a,X, + Xi(J(iXi) - Xi(L....J J(ijXj) j#i (2), The simplicity of the implementation is notable; a linear array with nearest neighbor interconnects consists of only 5 transistors, 1-3 diodes, and if required 1 capacitor per cell. A discrete element version of this implementation has been constructed and shows good agreement with expected properties. Steady state output is proportional to the square root of a uniform input thereby compressing the input data and showing adaptation to mean input intensity (figure 3). The network exhibits contrast en- hancement of spatial edges which increases with higher mean input strength (figure 4). A point source input elicits an on-center off-surround response, similar to the difference-of-Gaussians receptive field of many excitable cells. This 'receptive field' becomes more pronounced as the input intensity increases, showing the dependence of spatial frequency tuning on mean input level (figure 5). The temporal response of the network is also input dependent since the time constant of the exponential Analog Implementation of Shunting Neural Networks 699 decay of the impulse response decreases with input intensity. Finally, the depen- dence of the above properties on mean input strength can be tuned by varying the conductance of the central FET. 700.0 --- > E 11.1 600.0 500.0 · II 400.0 !:i ~ 5 300.0 ~ o 200.0 100.0 0.1 0.3 0.5 0 .7 0.9 1.1 1.3 1.5 1.7 1.11 2.1 INPUT CURRENT rnA Figure 3. Response of network to uniform input. Output is pro- portional to the square root of the input. DEPENDENCE OF ENHANCEMENT ON MEAN INPUT 1.5 1.4 ~ :J 1.3 0 ... a 1.2 N i 1.1 II: 0 z 5 1.0 ~ 0 0 .11 a f ... 0.8 ::l IL ~ 0 .7 0.11 2 3 5 7 CELL NUMBER Figure 4. Response of network to spatial edge patterns with the same contrast but increasing mean input level. 700 Nabet, Darling and Pinter Imox ~ 2.0J rnA, llmox - J75.74 mil 1.0 0.11 ~ g 0.11 :> 0 Q ~ 0 .7 !5 II. I iii 0.11 i 0.5 II: 0 Z 0.4 O.J 2 J 4 5 II 7 cEll NUMBER INPUT -x- OUTPUT Figure 5. Response of network to a point source input. Inset shows the receptive field of fly's lamina monopolar cells (LMC of Lucilia sericata). Horizontal axis of inset in visual angle, vertical a.xis relative voltage units of hyperpolarization. Inset from Pinter et al. (in preparation) CONTENT ADDRESSABILITY AND RELATION TO ART Using a theorem by Cohen and Grossberg (1983), it can be shown that the network equa.tion (2) a.dmits the global Liapunov function n n V - - ""'(l·ln(xi) - a'x' + K'x~) +.! '"" K··x 'XL - ~ 1 >. 1 1 1 1 2 ~ IJ J .. , ;=1 j,k=l (3) where>. is a constant, under the constraints Kij = Kji and Xi > O. This shows that in response to an arbitrary input the network always approaches an equilibrium point. The equilibria represent stored patterns and this is Content Addressable Memory (CAM) property. In addition, Eq. (2) is a special case of the feature representation field of an analog adaptive resonance theory ART-2 circuit, (Carpenter and Grossberg 1987), and hence this design can operate as a module in a learning multilayer ART architecture. Analog Implementation of Shunting Neural Networks 701 FUTURE PLANS Due to the very small number of circuit components required to construct a cell, this implementation is quite adaptable to very high integration densities. A solid state implementation of the circuit of figure (2) on a gallium arsenide substrate, chosen for its superiority for opto-electronics applications, is in progress. The chip includes monolithically fabricated photosensors for processing of visual information. All of the basic components of the circuit have been fabricated and tested. With standard 2 micron GaAs BFL design rules, a chip could contain over 1000 cells per cm2 , assuming an average of 20 inputs per cell. CONCLUSIONS The present work has the following distinguishing features: • Implements a mathematically well described and stable model. • Proposes a framework for implementation of shunting nets which are biologically feasible, explain variety of psychophysical and psychological data and have many desirable computational properties. • Has self-sufficient computational capabilities; especially suited for processing of sen- sory information in general and visual information in particular (N abet and Darling 1988). • Produces a 'good representation' of the input data which is also compatible with the self-organizing multilayer neural network architecture ART-2. • Is suitable for implementation in variety of technologies. • Is parallel, analog, and has very little overhead circuitry . ..-. 702 N abet, Darling and Pinter REFERENCES Carpenter, G.A. and Grossberg, S. (1987) ""ART 2: self organization of stable cat- egory recognition codes for analog input patterns,"". Applied Optics 26, pp. 4919- 4930. Cohen,M.A. and Grossberg, S. (1983) ""Absolute stability of global pattern forma- tion and parallel memory storage by competitive neural networks"" , IEEE Transac- tions on Systems Man and Cybernetics SMC-13, pp. 815-826. Ellias, S.A. and Grossberg, S. (1975) ""Pattern formation, contrast control, and oscillations in the short term memory of shunting on-center off-surround networks"" Biological Cybernetics, 20, pp. 69-98. Grossberg, S. (1973), ""Contour enhancement, Short term memory and constancies in reverberating neural networks,"" Studies in Applied Mathematics, 52, pp. 217- 257. Grossberg, S. (1983), ""The quantized geometry of visual space: the coherent com- putation of depth, form, and lightness."" The behavioral and brain sciences, 6, pp. 625-692. Lotka, A.J. (1956). Elements of mathematical biology. New York: Dover. Nabet, B. and Darling, R.B. (1988). ""Implementation of optical sensory neural net- works with simple discrete and monolithic circuits,"" (Abstract) Neural Networks, Vol.l, Suppl. 1, 1988, pp. 396. Pinter, R.B., (1983). ""The electrophysiological bases for linear and nonlinear prod- uct term lateral inhibition and the consequences for wide-field textured stimuli"" J. Theor. Bioi. 105 pp. 233-243. Pinter, R.B. (1985) "" Adaptation of spatial modulation transfer functions via non- linear lateral inhibition"" Bioi. Cybernetics 51, pp. 285-291. Pinter, R.B. (1987) ""Visual system neural networks: Feedback and feedforward lat- eral inhibition"" Systems and Control Encyclopedia (ed.M.G. Singh) Oxford: Perg- amon Press. pp. 5060-5065. Pinter, R.B., Osorio, D., and Srinivasan, M.V., (in preperation) ""Shift of edge preference to scototaxis depends on mean luminance and is predicted by a matched filter hypothesis in fly lamina cells"" RaIl, W. (1977). ""Core conductor theory and cable properties of neurons"" in Hand- book of Physiology: The Nervous System vol. I, part I, Ed. E.R. Kandel pp. 39-97. Bethesda, MD: American Physiological Society.","[-0.08212707936763763, -0.08683919906616211, 0.01025491114705801, -0.013398578390479088, -0.028336215764284134, 0.03502146899700165, 0.05872241407632828, 0.02718559466302395, 0.03566858172416687, -0.02034703455865383, 0.0056400420144200325, 0.037633176892995834, 0.0646413266658783, -0.029631400480866432, -0.03648717328906059, 0.05238223820924759, -0.017017723992466927, 0.09339884668588638, -0.051945917308330536, -0.0353679433465004, 0.05674411356449127, 0.005646520294249058, -0.01933743804693222, 0.008555583655834198, -0.016930000856518745, 0.009925858117640018, -0.05540145933628082, -0.02395184338092804, 0.03855857625603676, -0.05564553290605545, 0.07986404746770859, -0.008756554685533047, -0.025942528620362282, 0.014706142246723175, -0.09958915412425995, 0.06791351735591888, -0.08289825171232224, -0.0796472430229187, -0.009459581226110458, 0.009391924366354942, 0.0024259481579065323, 0.06900137662887573, -0.02202531322836876, -0.044908005744218826, 0.08475830405950546, -0.006086291279643774, 0.045510560274124146, -0.09899955242872238, -0.0026629434432834387, -0.04172901436686516, -0.02184225805103779, 0.03887562081217766, -0.056348614394664764, 0.09323109686374664, 0.0014091989723965526, 0.032011691480875015, -0.06812514364719391, 0.05450749024748802, -0.06706307083368301, -0.02079942636191845, 0.023599375039339066, -0.04294344782829285, 0.08487996459007263, -0.0027314918115735054, -0.05643844231963158, -0.01273334864526987, 0.03522609919309616, 0.06434737145900726, 0.06241645663976669, -0.012688476592302322, -0.008552971296012402, 0.04986966401338577, -0.0600535124540329, -0.02558232471346855, 0.006115229334682226, 0.047041766345500946, 0.11495111137628555, 0.009160397574305534, 0.08393408358097076, -0.00617213686928153, 0.03688552975654602, 0.018126580864191055, -0.020121201872825623, -0.04646480828523636, 0.07154801487922668, 0.014244993217289448, -0.023395318537950516, 0.030787648633122444, -0.028568310663104057, -0.06479175388813019, -0.029489658772945404, -0.06458583474159241, -0.003142746165394783, -0.025197017937898636, 0.007222427520900965, -0.0565473772585392, 0.08183769881725311, 0.014570528641343117, -0.05036596953868866, 0.11203694343566895, 0.04840916395187378, 0.05934829264879227, 0.003222124418243766, 0.006456104107201099, 0.06517279893159866, -0.05995837226510048, 0.06540927290916443, 0.06760917603969574, -0.032492853701114655, -0.08872910588979721, -0.003575589507818222, 0.04447011277079582, 0.015688279643654823, -0.010576814413070679, -0.00698004150763154, -0.02950107865035534, -0.002575821476057172, -0.08723841607570648, 0.02175481803715229, -0.010127323679625988, -0.010046450421214104, -0.08138827234506607, -0.1309322565793991, 0.03994407504796982, 0.01573534868657589, -0.047665610909461975, -0.04354235529899597, 4.1923275011527466e-33, -0.041899047791957855, 0.03721782937645912, 0.04873809590935707, -0.08086176216602325, 0.08960753679275513, -0.005566886626183987, 0.013026989996433258, 0.028234224766492844, 0.0017500178655609488, -0.024188127368688583, 0.0066321901977062225, 0.04386153072118759, -0.043454814702272415, 0.09425349533557892, -0.006993662565946579, -0.08912787586450577, -0.044075001031160355, -0.0647817999124527, 0.04025749862194061, -0.1322801411151886, 0.049350474029779434, -0.09470202773809433, 0.011882543563842773, 0.03221263736486435, -0.00034188598510809243, -0.046563565731048584, 0.06048008054494858, 0.048923980444669724, -0.02657918445765972, 0.009587438777089119, 0.00594755494967103, 0.03255308419466019, -0.06071235239505768, 0.021590502932667732, 0.03197476640343666, -0.09403280168771744, 0.07444994896650314, -0.020022355020046234, 0.06677042692899704, -0.04058081656694412, 0.01271363440901041, 0.03819656744599342, -0.019230924546718597, 0.025898631662130356, -0.06847894191741943, -0.10863323509693146, 0.005926717072725296, 0.072393037378788, -0.00397114735096693, -0.006782036740332842, -0.037971049547195435, -0.06142725422978401, -0.059072427451610565, -0.009444176219403744, -0.007440926972776651, -0.027835391461849213, 0.010912892408668995, 0.034070439636707306, 0.021297814324498177, 0.16458335518836975, -0.08233118057250977, 0.0036976714618504047, 0.01719358004629612, 0.05054718255996704, -0.03220697119832039, 0.07538503408432007, -0.0037490713875740767, 0.02946135215461254, 0.09833279997110367, -0.030897540971636772, 0.07427658885717392, 0.026094943284988403, -0.02357330173254013, -0.04985051602125168, 0.049260854721069336, -0.006461279932409525, -0.03659965470433235, -0.03668041527271271, -0.09864531457424164, 0.09854629635810852, -0.039122309535741806, 0.030996862798929214, -0.0577857531607151, 0.0942850187420845, -0.025257809087634087, -0.0333898663520813, 0.06996550410985947, -0.0464957058429718, -0.024437397718429565, -0.06304474174976349, -0.04386894032359123, 0.0638541579246521, 0.13736899197101593, -0.046353019773960114, -0.0677865743637085, -4.224446414921027e-33, -0.024300629273056984, 0.07967229932546616, -0.03888155147433281, 0.018034061416983604, -0.06131633743643761, -0.01937265135347843, -0.005152676720172167, 0.0677424967288971, 0.032386504113674164, -0.020523538812994957, -0.06758404523134232, -0.030881516635417938, 0.04011649265885353, -0.022538723424077034, 0.007748208474367857, -0.019877735525369644, -0.04905126243829727, -0.07590603828430176, 0.00015815430379007012, 0.024264726787805557, 0.014688660390675068, 0.08334321528673172, -0.03690796718001366, -0.035685520619153976, -0.02952784299850464, 0.04223361983895302, -0.13890671730041504, 0.05223750323057175, 0.0121805714443326, -0.011857408098876476, -0.03023281879723072, -0.057594772428274155, -0.03690088540315628, -0.01715635508298874, 0.009478721767663956, -0.04102162644267082, 0.09690123796463013, 0.016214197501540184, -0.03969067335128784, 0.05135257542133331, 0.09936705976724625, -0.04012908786535263, 0.030849557369947433, 0.06886567175388336, 0.08523313701152802, -0.02892123907804489, -0.09282047301530838, 0.015769079327583313, 0.02049695886671543, 0.06622186303138733, -0.017970766872167587, -0.0459006242454052, -0.07317861914634705, -0.002988790161907673, -0.009050891734659672, 0.035187944769859314, -0.04794662445783615, 0.040302395820617676, -0.016677647829055786, -0.013475906103849411, 0.018667077645659447, -0.13880577683448792, -0.05216941237449646, -0.06503032892942429, -0.018976006656885147, 0.01422237977385521, 0.015769850462675095, -0.06356124579906464, 0.05880391597747803, -0.06564291566610336, 0.04656146839261055, 0.010083976201713085, 0.06828965991735458, -0.054773081094026566, 0.004362855106592178, -0.07206501066684723, -0.08256030082702637, -0.029991064220666885, -0.0015676487237215042, 0.021396644413471222, -0.03531680628657341, -0.004467864520847797, 0.03452279791235924, -0.013720492832362652, 0.0011551373172551394, 0.01712927781045437, -0.03104267455637455, -0.02565108612179756, 0.02129143849015236, -0.06520067155361176, -0.02842218056321144, 0.0679439827799797, 0.0035140111576765776, 0.04891311004757881, -0.040933482348918915, -5.431657257304323e-08, -0.004419014323502779, 0.028325943276286125, -0.03483859822154045, 0.006219587288796902, 0.07127915322780609, -0.142426997423172, 0.017253940925002098, -0.06765381991863251, 0.045260991901159286, 0.020384224131703377, 0.11737702786922455, -0.039814867079257965, -0.059354983270168304, -0.04590744897723198, 0.022324729710817337, 0.07994981110095978, 0.04221336543560028, 0.025554263964295387, 0.02140580117702484, -0.023644037544727325, -0.0026133358478546143, 0.015823880210518837, 0.028030412271618843, 0.10164463520050049, 0.08771342039108276, -0.0257228072732687, -0.012026800774037838, -0.019859792664647102, 0.029508009552955627, -0.014598040841519833, 0.009357892908155918, 0.0268847793340683, 0.09372887015342712, 0.03437310457229614, 0.04138124734163284, 0.014687420800328255, 0.030631236732006073, 0.05285310372710228, 0.025402260944247246, 0.0017321587074548006, -0.028517013415694237, -0.03988228365778923, 0.008311830461025238, 0.04158563166856766, 0.014636430889368057, -0.06031601130962372, 0.08110890537500381, -0.06312902271747589, 0.01381057221442461, 0.006001770496368408, 0.05488726496696472, 0.01635058782994747, 0.03979656845331192, -0.032346028834581375, 0.03921040892601013, -0.031879451125860214, 0.07556169480085373, -0.052230048924684525, 0.015013454481959343, 0.07388587296009064, 0.005888148210942745, 0.06427250057458878, -0.04396633058786392, -0.020052475854754448]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AModelofNeuralOscillatorforaUnifiedSubmodule.pdf,Computer Vision,"614 Gish and Blanz Comparing the Performance of Connectionist and Statistical Classifiers on an Image Segmentation Problem Sheri L. Gish w. E. Blanz IBM Almaden Research Center 650 Harry Road San Jose, CA 95120 ABSTRACT In the development of an image segmentation system for real time image processing applications, we apply the classical decision anal- ysis paradigm by viewing image segmentation as a pixel classifica.- tion task. We use supervised training to derive a classifier for our system from a set of examples of a particular pixel classification problem. In this study, we test the suitability of a connection- ist method against two statistical methods, Gaussian maximum likelihood classifier and first, second, and third degree polynomial classifiers, for the solution of a ""real world"" image segmentation problem taken from combustion research. Classifiers are derived using all three methods, and the performance of all of the classi- fiers on the training data set as well as on 3 separate entire test images is measured. 1 Introduction We are applying the trainable machine paradigm in our development of an image segmentation system to be used in real time image processing applications. We view image segmentation as a classical decision analysis task; each pixel in a scene is described by a set of measurements, and we use that set of measurements with a classifier of our choice to determine the region or object within a scene to which that pixel belongs. Performing image segmentation as a decision analysis task pro- vides several advantages. We can exploit the inherent trainability found in decision Comparing the Performance of Connectionist and Statistical Classifiers 615 analysis systems [1 J and use supervised training to derive a classifier from a set of examples of a particular pixel classification problem. Classifiers derived using the trainable machine paradigm will exhibit the property of generalization, and thus can be applied to data representing a set of problems similar to the example problem. In our pixel classification scheme, the classifier can be derived solely from the qU8J1ti- tative characteristics of the problem data. Our approach eliminates the dependency on qualitative characteristics of the problem data which often is characteristic of explicitly derived classification algorithms [2,3J. Classical decision 8J1alysis methods employ statistical techniques. We have com- pared a connectionist system to a set of alternative statistical methods on classifi- cation problems in which the classifier is derived using supervised training, 8J1d have found that the connectionist alternative is comparable, and in some cases prefer- able, to the statistical alternatives in terms of performance on problems of varying complexity [4J. That comparison study also 8J.lruyzed the alternative methods in terms of cost of implementation of the solution architecture in digital LSI. hl terms of our cost analysis, the connectionist architectures were much simpler to implement than the statistical architectures for the more complex classification problems; this property of the connectionist methods makes them very attractive implementation choices for systems requiring hardware implementations for difficult applications. In this study, we evaluate the perform8J.lce of a connectionist method and several statisticrumethods as the classifier component of our real time image segmentation system. The classification problem we use is a ""real world"" pixel classification task using images of the size (200 pixels by 200 pixels) and variable data quality typical of the problems a production system would be used to solve. We thus test the suitability of the connectionist method for incorporation in a system with the per- formance requirements of our system, as well as the feasibility of our exploiting the adv8J.ttages the simple connectionist architectures provide for systems implemented in hardware. 2 Methods 2.1 The Image Segmentation System The image segmentation system we use is described in [5J, and summarized in Figure 1. The system is designed to perform low level image segmentation in real time; for production, the feature extraction and classifier system components are implemented in hardware. The classifier par8J.neters are derived during the Training Phase. A user at a workstation outlines the regions or objects of interest in a training image. The system performs low level feature extraction on the training image, and the results of the feature extraction plus the input from the user are combined automatically by the system to form a training data set. The system then applies a supervised training method making use of the training data set in order to derive the coefficients for the classifier which can perform the pixel classification task. The feature extraction process is capable of computing 14 classes of features for each pixel; up to 10 features with the highest discriminatory power are used to 616 Gish and Blanz describe all of the pixels in the image. TIns selection of features is based only on an analysis of the results of the feattue extraction process and is independent of the supervised learning paradigm being used to derive the classifier [6]. The identical feature extraction process is applied in both the Training and Running Phases for a particular image segmentation problem. Training Images Test Image Coefficients for Classifier Segmented Image TRAINING PHASE RUNNING PHASE Figure 1: Diagram of the real time image segmentation system. 2.2 The Image Segmentation Problem The image segmentation problem used in this study is from combustion research and is described in [3]. The images are from a series of images of a combustion chamber taken by a high speed camera during the inflammation process of a gas/air mix- hue. The segmentation task is to determine the area of inflamed gas in the image; therefore, the pixels in the image are classified into 3 different classes: cylinder, uninflamed gas, and flamed gas (See Figure 2). Exact determination of the area of flamed gas is not possible using pixel classification alone, but the greater the success of the pixel classification step, the greater the likelihood that a real time image segmentation system could be used successfully on this problem. 2.3 The Classifiers The set of classifiers used in tIns study is composed of a connectionist classifier based on the Parallel Distributed Processing (PDP) model described in [7] and two statistical methods: a Gaussian maximum likelihood classifier (a Bayes classifier), and a polynomial classifier based on first, second, and third degree polynomials. Tlus set of classifiers was used in a general study comparing the performance of Comparing the Performance of Connectionist and Statistical Classifiers 617 Figure 2: The imnge segmentntion problem is to classify each imllge pixel into 1 of 3 regions. the alternatives on a set of classification problems; all of the classifiers as well as adaptation procedures are described in detnil in that study [4]. Implementation and adaptation of nll classifiers in this study was performed as software simulation. The connectionist classifier was implemented in eMU Common Lisp rmming on an IBM RT workstation. The connectionist classifier nrchitecture is a multi-Inyer feedforwnrd network with one hidden layer. The network is fully connected, but there nre only connections between ndjacent layers. The number of units in the input and output layers are determined by the number of features in the fenture vector describing ench pixel and a binary encoding scheme for the class to which the pixel belongs, respectively. The number of units in the hidden layer is an architectural ""free parnmeter."" The network used in this study has 10 units in the input layer, 12 units in the hidden layer, and 3 units in the outPllt layer. Network activation is achieved by using the continuous, nonlinear logistic function defined in [8]. The connectionist adaptation procedure is the applicntion of the backpropagation learning rule also defined in [8]. For this problem, the learning rnte TJ = 0.01 and the momentum a = 0.9; both terms were held conshmt throughout adaptntion. The presentation of all of the patterns ill the training data set is termed a trial; network weights nnd unit binses were updated after the presentation of each pattern during a trial. The training data set for this problem was generated automatically by the image segmentation system. This training data set consists of approximately 4,000 ten element (feature) vectors (each vector describes one pixel); each vector is labeled as belonging to one of the 3 regions of interest in the imnge. The training data set was constructed from one entire training image, and is composed of vectors stntistically representative of the pixels in each of the 3 regions of interest in that image. 618 Gish and Dlanz All of the classifiers tested in this study were adapted from the same training data set. The connectionist classifier was defined to be converged for tlus problem before it was tested. Network convergence is determined from the results of two separate tests. III the first test, the difference between the network output and the target output averaged over the entire training data set has to reach a minimum. In the second test, the performance of the network in classifying the training data set is measured, and the number of misclassifications made by the network has to reach a minimum. Actual network performance in classifying a pattern is measured after post-processing of the output vector. The real outputs of each unit in the output layer are assigned the values of 0 or 1 by application of a 0.5 decision threshold. In our binary encoding scheme, the output vector should have only one element with the value 1; that element corresponds to one of the 3 classes. H the network produces an output vector with either more than one element with the value 1 or all elements with the value 0, the pattern generating that output is considered rejected. For the test problem in this study, all of the classifiers were set to reject patterns in the test data samples. All of the statistical classifiers had a rejection threshold set to 0.03. 3 Results The performance of each of the classifiers (connectionist, Gaussian maximum like- lihood, and linear, quadratic, and cubic polynomial) was measured on the training data set and test data representing 3 entire images taken from the series of com- bustion chamber images. One of those images, labeled Inlage 1, is the image from which the training data set was constructed. The performance of all of the classifiers is summarized in Table 1. Althollgh all of the classifiers were able to classify the training data set with com- parably few misclassifications, the Gaussian maximum likelihood classifier and the quadratic polynomial classifier were unable to perform on any of the 3 entire test images. The connectionist classifier was the only alternative tested in this study to deliver acceptable performance on all 3 test images; the connectionist classifier had lower error rates on the test images than it delivered on the training data sample. Both the linear polynomial and cubic polynomial classifiers performed acceptably on the test Image 2, but then both exhibited high error rates on the other two test images. For this image segmentation problem, only the connectionist method generalized from the training data set to a solution with acceptable performance. In Figure 3, the results from pixel classification performed by the connectionist and polynonual classifiers on all 3 test images are portrayed as segmented images. The actual test images are included at the left of the figure. 4 Conclusions Our results demonstrate the feasibility of the application of a connectionist decision analysis method to the solution of a ureal world"" image segmentation problem. The ~ata Sel ,----T . . raInIng Data Image 1 C Comparing the Performance of Connectionist and Statistical Classifiers 619 Conne;;l;on;sl II - Polynom;al -- -~auss;an --] Classifier Classifier Classifier Errorfl I Rejectb Degree I Error I Reject Error I Reject 1O.40%-~-.64% 1 '1l.25% 1.62% 12.84% ---0.12%- 2 9.61% 1.41% 3 8.13% 1.05% 8.84% 1.72% 1 41.70% 4.63% 94.27% 0.00% 2 57.55% 3.66% 3 25.86% 0.28% r-~------~~--~-r~~~~~r---- ----~~----~~----~ Image 2 5.82% 1.53% 1 12.01% 2.00% 69.09% 0.01% 2 68.01 % 0.58% 3 4.68% 0.26% Image 3 6.31 % - -::-1.-=-6-=3 %=o- tf---- 1=---- 19.68 % 5.43 % 88.35% 0.00% 2 45.89% 1.41% 3 25.75% 0.28% , _______ ~ ______ L_ _ _ ____ ~ ______ L_ _____ _ ~ ______ ~ ___ ___ ~ ______ __ flPercent misclauificatioDi for all patterns. bpercent of all patterns rejected. Clmage from which training data let was taken. Table 1: A sununary of the performance of the c16Ssifiers. inclusion of a connectionist classifier in our supervised segmentation system will al- low us to meet our performance requirements under real world problem constraints. Although the application of connectionism to the solution of real time machine vision problems represents a new processing method, our solution strategy h6S re- mained consistent with the decision analysis paradigm. Our connectionist cl6Ssifiers are derived solely from the quantitative characteristics of the problem data; our con- nectionist architecture thus remains simple and need not be re-designed according to qualitative characteristics of each specific problem to which it will be applied. Our connectionist architecture is independent of the image size; we have applied the identical architecture successfully to images which range in size from 200 pixels by 200 pixels to 512 pixels by 512 pixels [9). In most research to date in which neural networks are applied to machine vision, entire images explicitly are mapped to net- works by making each pixel in an image correspond to a different unit in a network layer (see [10,11) for examples). This ""pixel map"" representation makes scaling up to larger image sizes from the idealized ""toy"" research images a significant problem. Most statistical pattern classification methods require that problem data satisfy tIle assumptions of statistical models; unfortunately, real world problem data are complex and of variable quality and thus rarely can be used to guide the choice of an appropriate method for the solution of a particular problem a priori. For the image segmentation problem reported in this study, our cI6Ssifier performance results show that the problem data actually did not satisfy the assumptions behind the statistical models underlying the Gaussian maximum likelihood classifier or the polynomial 620 Gish and Blanz Figure 3: The grey levels assigned to each region nre: Black - cylinder, Light Grey - uninflamed gas, Grey - fhnned gas. Original images nre at the left of the figure. classifiers. It appenrs that the Gaussian model least fits our problem data, the polynomial classifiers provide a slightly better fi t, and the connect.ionist method provides the fit required for the solution of the problem. It is also notable that all the alternative m.ethods in this study could be aflapted to perform acceptably on the training data set, but extensive testing on several different entire images was required in order to demonstrate the true performance of the n1t.ernntive lllethods on the actual problem., rather than just on the trnining data set. These results show that a connectionist method is a viable choice for n system. such as ours which requires a simple nrchitecture readily implemented in hardware, the flexibility to handle cOl1lpi('x problems described by large amounts of data, and the robustness to not require problem data to meet, many model assnmptions 11 priori. Comparing the Performance of Connectionist and Statistical Classifiers 621 References [lJ R. O. Duda a.nd P. E. H6I't. Pattern Cla$$ification and Scene Analy,i$. Wiley, New York, 1973. [2J W. E. Blanz, J. L. C. Sanz, and D. Petkovic. Control-free low-level image segmentation: Theory, architecture,a.nd experimentation. In J. L. C. Sanz, editor, Advance$ of Machine Vi$ion, Application$ and Architect-ure"" Springer- Verlag, 1988. [3] B. Straub and W. E. Blanz. Combined decision theoretic and syntactic ap- proach to image segmentation. Machine Vi,ion and Application$, 2(1 ):17-30, 1989. [4J Sheri L. Gish and W. E. Blallz. Comparing a Connectioni$t Trainable Clauifier with Clauical Stati$tical Deci,ion AnalY$i$ Method$. Research Report RJ 6891 (65717), IBM, Jtme 1989. [5] W. E. Bla.nz, B. Slmng, C. Cox, W. Greiner, B. Dom, a.nd D. Petkovic. De$ign and implementation of a low level image ,egmentation architecture - LISA. Research Report RJ 7194 (67673), IBM, December 1989. [6] W. E. BI611z. Non-p6I'ametric feature selection for multiple class processes. In Proc. 9th Int. Con/. Pattern Recognition, Rome, Italy, Nov. 14-17 1988. [7J David E. Rumelhart, J61ues L. McClelland, et a1. Parallel Di$tributed Proceu- ing. MIT Press, C61ubridge, Massachusetts, 1986. [8] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Willia.ms. Le6I'nillg in- ternal representations by error propagation. In David E. Rumelllart, James L. McClell611d, et aI., editors, Parallel Di,tributed Proce66ing, chapter 8, MIT Press, Cambridge, Massachusetts, 1986. [9] W. E. Bla.nz 6l1d Sheri L. Gish. A Connectioni,t Clauifier Architecture Applied To Image Segmentation. Rese6I'ch Report RJ 7193 (67672), IBM, December 1989. [10} K. Fukushima, S. Miyake, and T. Ito. Neocognitron: a neura.lnetwork model for a mechanism of visual pattern recognition. IEEE Tran$actio1t$ on Sy,tem"" Man, and Cybernetic$, SMC-13(5):826-834, 1983. [U} Y. Hirai. A model of humau associative processor. IEEE Tran$action$ on Sy,tem$, Man, and Cybernetic$, SMC-13(5):851-857, 1983.","[-0.0994347482919693, -0.04453004524111748, -0.028971238061785698, -0.014635053463280201, 0.039394672960042953, -0.04256364703178406, 0.05120473355054855, -0.0067733535543084145, -0.04814677685499191, -0.007979677058756351, -0.0011303207138553262, -0.009763707406818867, 0.037725090980529785, 0.035939354449510574, -0.06467115879058838, -0.020869992673397064, -0.005215134471654892, 0.035135552287101746, -0.09732367098331451, -0.012896187603473663, 0.02094661258161068, -0.033876385539770126, -0.04130040854215622, 0.00646940479055047, 0.07331738620996475, 0.01442918460816145, 0.07297540456056595, -0.015827052295207977, -0.016102416440844536, 0.042290471494197845, -0.025683190673589706, -0.021724101155996323, 0.06415914744138718, 0.0165407732129097, -0.05011066049337387, 0.031218528747558594, 0.061690591275691986, 0.056174445897340775, -0.026103923097252846, 0.05804872512817383, -0.04383550211787224, -0.09259294718503952, 0.004117397591471672, -0.06009283661842346, 0.0945153534412384, 0.1273881494998932, 0.009937481954693794, -0.09426870942115784, 0.012811334803700447, -0.0426117442548275, -0.10797929018735886, 0.02395964041352272, -0.11728165298700333, 0.027589241042733192, -0.03983350470662117, -0.04678189381957054, 0.0977945551276207, 0.04143555462360382, 0.001427053241059184, 0.10158097743988037, 0.004640247207134962, -0.06005837023258209, -0.050824135541915894, 0.01762567274272442, -0.0057337018661201, -0.03406122326850891, 0.006506838370114565, -0.025839215144515038, 0.030284779146313667, -0.04458187147974968, -0.0024941822048276663, 0.11981505900621414, -0.03789616376161575, -0.013081458397209644, -0.09469614177942276, -0.011488375253975391, 0.07868068665266037, 0.025868650525808334, 0.046107396483421326, -0.05404982715845108, 0.03763540834188461, -0.029891736805438995, 0.06200327351689339, -0.00128080858848989, 0.11511074751615524, 0.0873110368847847, -0.06163233891129494, 0.06273382157087326, -0.022470854222774506, 0.06949128955602646, -0.02649153769016266, 0.016617344692349434, -0.06078946217894554, 0.025944476947188377, 0.005070220213383436, -0.03115599788725376, 0.019900891929864883, -0.039369646459817886, 0.06876756995916367, 0.05804010480642319, -0.08770965039730072, -0.049460068345069885, 0.004734591115266085, -0.007581243757158518, 0.043840065598487854, 0.059467583894729614, 0.06329607218503952, 0.010589228011667728, 0.09749038517475128, -0.08870429545640945, 0.035597722977399826, -0.02515135519206524, -0.08619870990514755, -0.05653223395347595, -0.010850225575268269, -0.01202526967972517, -0.005278267897665501, 0.021619077771902084, 0.02934889681637287, 0.05954602360725403, -0.024521537125110626, 0.030651813372969627, -0.008859970606863499, 0.0029145802836865187, 0.057444557547569275, -0.06924378871917725, -0.08515265583992004, 3.637038296161699e-33, -0.0200393944978714, -0.006400566082447767, 0.04778466373682022, -0.0819433331489563, -0.0028296448290348053, 0.028983350843191147, -0.06631983816623688, -0.047750696539878845, -0.005447016097605228, 0.035671040415763855, -0.0039026287849992514, -0.03177432715892792, -0.024249497801065445, 0.032080572098493576, 0.14186429977416992, 0.019653242081403732, -0.008433623239398003, -0.006633197423070669, -0.057257719337940216, -0.025463564321398735, 0.02939625270664692, -0.08568789809942245, 0.022083284333348274, -0.037137553095817566, 0.010649430565536022, 0.001620298600755632, 0.0976453423500061, -0.051306869834661484, -0.004706456325948238, 0.007320431061089039, -0.014443166553974152, 0.0030676990281790495, 0.029193328693509102, 0.025245608761906624, -0.03465757146477699, -0.049918320029973984, -0.03614296019077301, 0.03453899174928665, -0.052879124879837036, -0.023155758157372475, -0.009302916936576366, -0.010813510045409203, 0.05599229410290718, 0.02307637594640255, -0.06825460493564606, 0.018814589828252792, -0.00828429777175188, 0.11549932509660721, -0.005624229088425636, 0.02357853576540947, 0.037443194538354874, -0.022503025829792023, 0.028823502361774445, -0.027598321437835693, -0.04541914165019989, 0.06884948164224625, -0.015437198802828789, 0.04277215525507927, 0.01642100140452385, 0.03211873024702072, -0.04212412238121033, 0.03530431166291237, 0.003507361514493823, 0.08364581316709518, -0.012472936883568764, -0.01579534076154232, -0.026744170114398003, -0.016132323071360588, -0.012351279146969318, 0.048683036118745804, -0.06924755871295929, 0.04259635880589485, 0.035682979971170425, -0.04110053554177284, 0.04461628198623657, -0.00395614979788661, 0.012460253201425076, 0.05585590749979019, -0.07952499389648438, 0.031525757163763046, -0.06211220473051071, 0.0296928733587265, 0.0019056516466662288, -0.10580119490623474, -0.008929762989282608, 0.057587504386901855, 0.03666304051876068, -0.04878745973110199, -0.05807039514183998, 0.020892787724733353, -0.04356963559985161, 0.011112415231764317, -0.0059954016469419, 0.06611674278974533, 0.0014803349040448666, -3.854360018321877e-33, -0.012845700606703758, 0.07598619163036346, -0.024270255118608475, 0.06324288994073868, -0.013966129161417484, -0.01761229895055294, 0.009579094126820564, 0.00014929819735698402, -0.018907664343714714, -0.020412670448422432, 0.0733613595366478, 0.10247974097728729, -0.020674247294664383, 0.001556027797050774, -0.051989972591400146, -0.05999486520886421, -0.0630873516201973, -0.08130008727312088, -0.012991491705179214, 0.033573731780052185, 0.04703738912940025, 0.10892240703105927, -0.05659148469567299, -0.09604088217020035, -0.12081164121627808, 0.021201681345701218, -0.04297424107789993, 0.06896387040615082, -0.034400470554828644, 0.009424344636499882, -0.009051436558365822, -0.030945967882871628, -0.09323682636022568, 0.010691574774682522, 0.005452095530927181, 0.04658535495400429, 0.021880196407437325, -0.04472161456942558, -0.006572340149432421, 0.08897782862186432, 0.037094589322805405, 0.002056853612884879, -0.08834093064069748, -0.020708557218313217, 0.010953660123050213, -0.0426814928650856, 0.0004254968371242285, 0.03940417990088463, -0.05047732591629028, 0.06227146461606026, -0.06296123564243317, 0.03539752960205078, -0.14021557569503784, 0.06793689727783203, -0.007139271125197411, 0.028507990762591362, -0.08492204546928406, -0.013214613310992718, -0.028738830238580704, 0.029930345714092255, 0.04332958906888962, -0.050253916531801224, -0.004511144477874041, 0.08592484146356583, 0.02017742395401001, -0.07147818058729172, -0.035149939358234406, 0.02262476645410061, 0.04660416394472122, 0.06392775475978851, 0.011227509938180447, 0.022093387320637703, 0.058747950941324234, 0.048807401210069656, -0.05812777206301689, -0.00514924107119441, -0.03188428282737732, 0.01964377425611019, 0.032464418560266495, 0.024068299680948257, 0.036237601190805435, 0.01926344819366932, 0.023422861471772194, 0.08259259909391403, 0.08507031202316284, 0.06580344587564468, 0.03605547174811363, -0.027391521260142326, 0.16956597566604614, -0.11941412091255188, 0.043878182768821716, 0.013200799934566021, 0.055644441395998, 0.047175515443086624, -0.08858252316713333, -4.908066131292799e-08, 0.013990751467645168, -0.07556987553834915, 0.01748458296060562, -0.0536702498793602, 0.07056819647550583, -0.0134638836607337, -0.04279879853129387, 0.079827681183815, -0.05996045097708702, -0.052211444824934006, 0.0525067038834095, -0.011444885283708572, -0.0942026674747467, -0.005608327221125364, 0.0006435441318899393, -0.05218648910522461, 0.03249844163656235, 0.02265303023159504, 0.020810605958104134, 0.019682863727211952, -0.026329362764954567, -0.09206222742795944, 0.01550857163965702, 0.04908972606062889, -0.0317545160651207, -0.0742868110537529, -0.03970327600836754, 0.043846964836120605, -0.005760306492447853, 0.046902045607566833, -0.02822326309978962, 0.032449860125780106, 0.01813199743628502, 0.06683909147977829, 0.11129117757081985, -0.0034308978356420994, -0.07147151231765747, 0.039929021149873734, -0.0717470645904541, -0.0034766928292810917, 0.04701249673962593, -0.04870780184864998, 0.0177653469145298, 0.017212972044944763, 0.07712700963020325, 0.02370484359562397, 0.09189360588788986, -0.09682510793209076, 0.010780323296785355, 0.07693144679069519, 0.038915377110242844, 0.04357539862394333, -0.0019687835592776537, 0.005663952324539423, 0.06128079444169998, -0.04512578621506691, 0.02243380807340145, -0.05132492259144783, -0.01056359801441431, 0.03685606271028519, -0.06557613611221313, 0.030338754877448082, -0.07934233546257019, -0.040237586945295334]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AnalogImplementationofShuntingNeuralNetworks.pdf,Deep Learning,"626 ANALYZING THE ENERGY LANDSCAPES OF DISTRIBUTED WINNER-TAKE-ALL NETWORKS David S. Touretzky School of Computer Science Carnegie Mellon University Pittsburgh, P A 15213 ABSTRACT DCPS (the Distributed Connectionist Production System) is a neural network with complex dynamical properties. Visualizing the energy landscapes of some of its component modules leads to a better intuitive understanding of the model, and suggests ways in which its dynamics can be controlled in order to improve performance on difficult cases. INTRODUCTION Competition through mutual inhibition appears in a wide variety of network designs. This paper discusses a system with unusually complex competitive dynamics. The system is DCPS, the Distributed Connectionist Production System of Touretzky and Hinton (1988). DCPS is a Boltzmann machine composed of five modules, two of which, labeled ""Rule Space"" and ""Bind Space,"" are winner-take-all (WTA) networks. These modules interact via their effects on two attentional mod ules called clause spaces. Clause spaces are another type of competitive architecture based on mutual inhibition, but they do not produce WTA behavior. Both clause spaces provide evidential input to both WTA nets, but since connections are symmetric they also receive top-down ""guidance"" from the WTA nets. Thus, unlike most other competitive architectures, in DCPS the external input to a WTA net does not remain constant as its state evolves. Rather, the present output of the WTA net helps to determine which evidence will become visible in the clause spaces in the future. This dynamic attentional mechanism allows rule and bind spaces to work together even though they are not directly connected. DCPS actually uses a distributed version of winner-take-all networks whose oper- ating characteristics differ slightly from the non-distributed version. Analyzing the energy landscapes of DWTA networks has led to a better intuitive understanding of their dynamics. For a complete discussion of the role of DWTA nets in DCPS, and the ways in which insights gained from visualization led to improvements in the system's stochastic search behavior, see [Touretzky, 1989]. Energy Landscapes of Distributed Winner-Take-All Networks 627 DISTRIBUTED WINNER-TAKE-ALL NETWORKS In classical WTA nets [Feldman & Ballard, 1982], a unit's output value is a continu- ous quantity that reflects its activation level. In this paper we analyze a stochastic, distributed version of winner-take-all dynamics using Boltzmann machines, whose units have only binary outputs [Hinton & Sejnowski, 1986]. The amount of eviden- tial input to these units determines its energy gap [Hopfield, 1982], which in turn determines its probability of being active. The network's degree of confidence in a hypothesis is thus reflected in the amount of time the unit spends in the active state. A good instantaneous approximation to strength of support can be obtained by representing each hypothesis with a clique of k independent units looking at a common evidence pool. The number of active units in a clique reflects the strength of that hypothesis. DCPS uses cliques of size 40. Units in rival cliques compete via inhibitory connections If all units in a clique have identical receptive fields, the result is an ""ensemble"" Boltzmann machine [Derthick & Tebelskis, 1988]. In DCPS the units have only moderately sized, but highly overlapped, receptive fields, so the amount of evidence individual units perceive is distributed binomially. Small excitatory weights between sibling units help make up for variations in external evidence. They also make states where all the units in a single clique are active be powerful attractors. Energy tours in a DWTA take one of four basic shapes. Examples may be seen in Figure 1a. Let e be the amount of external evidence available to each unit, 0 the unit's threshold, k the clique size, and W, the excitatory weight between siblings. The four shapes are: Eager vee: the evidence is above threshold (e > 0). The system is eager to turn units on; energy decreases as the number of active units goes up. We have a broad, deep energy well, which the system will naturally fall into given the chance. Reluctant vee: the evidence is below threshold, but a little bit of sibling influence (fewer than k/2 siblings) is enough to make up the difference and put the system over the energy barrier. We have e < 0 < e +w,(k-1)/2. The system is initially reluctant to turn units on because that causes the energy to go up, but once over the hump it willingly turns on more units. With all units in the clique active, the system is in an energy well whose energy is below zero. Dimpled peak: with higher thresholds the total energy of the network may remain above zero even when all units are on. This happens when more than half of the siblings must be active to boost each unit above threshold, i.e., e + w,(k - 1) > 0 > e + w,(k - 1)/2. The system can still be trapped in the small energy well that remains, but only at low temperatures. The well is hard to reach since the system must first cross a large energy barrier by traveling far uphill in energy space. Even if it does visit the well, the system may easily bounce out of it again if the well is shallow. 628 Touretzky Smooth peak: when () > e + w.(k - 1), units will be below threshold even with full sibling support. In this case there is no energy well, only a peak. The system wants to turn all units off. VISUALIZING ENERGY LANDSCAPES Let's examine the energy landscape of one WTA space when there is ample evidence in the clause spaces for the winning hypothesis. We select three hypotheses, A, B, and C, with disjoint evidence populations. Let hypothesis B be the best supported one with evidence 100, and let A have evidence 40 and C have evidence 5. We will simplify the situation slightly by assuming that all units in a clique perceive exactly the same evidence. In the left half of Figure 1 b we show the energy curves for A, B, and C, using a value of 69 for the unit thresholds.1 Each curve is generated by starting with all units turned off; units for a particular hypothesis are turned on one at a time until all 40 are on; then they are turned off again one at a time, making the curve symmetric. Since the evidence for hypothesis A is a bit below threshold, its curve is of the ""reluctant vee"" type. The evidence for hypothesis B is well above threshold, so its curve is an ""eager vee."" Hypothesis C has almost no evidence; its ""dimpled peak"" shape is due almost entirely to sibling support. (Sibling weights have a value of +2; rival weights a value of -2.) Note that the energy well for B is considerably deeper than for A. This means at moderate temperature the model can pop out of A's energy well, but it is more likely to remain in B's well. The well for B is also somewhat broader than the well for A, making it easier for the B attractor to capture the model; its attract or region spans a larger portion of state space. The energy tours for hypotheses A, B, and C correspond to traversing three or- thogonal edges extending from a corner of a 40 x 40 x 40 cube. A point at location (x, y, z) in this cube corresponds to x A units, y B units, and z C units being active. During the stochastic search, A and B units will be flickering on and off simultaneously, so the model will also visit internal points of the cube not covered in the energy tour diagram. To see these points we will use two additional graphic representations of energy landscapes. First, note that hypothesis C gets so little support that we safely can ignore it and concentrate on A and B. This allows us to focus on just the front face of the state space cube. In Figure 2a, the number of active A units runs from zero to forty along the vertical axis, and the number of active B units runs from zero to forty along the horizontal axis. The arrows at each point on the graph show legal state transitions at zero temperature. For example, at the point where there are are 38 active B units and 3 active A units there are two arrows, pointing down and to the right. This means there are two states the model could enter next: it could either turn off one of the active A units, or turn on one more B unit, respectively. At nonzero temperatures other state transitions 1 All the weights and thresholds used in this paper are actual DCPS values taken from [Touretzky & Hinton, 1988]. Energy Landscapes of Distributed Winner-Take-All Networks 629 are possible, corresponding to uphill moves in energy space, but these two remain the most probable. The points in the upper left and lower right corners of Figure 2a are marked by ""Y"" shapes. These represent point attractors at the bottoms of energy wells; the model will not move out of these states unless the temperature is greater than zero. Other points in state space are said to be within the region of a particular attractor if all legal transition sequences (at T = 0) from those points lead eventually to the attractor. The attractor regions of A and B are outlined in the figure. Note that the B attractor covers more area than A, as predicted by its greater breadth in the energy tour diagram. Note also that there is a small ridge between the two attractor regions. From starting points on the ridge the model can end up in either final state. Figure 2b shows the depths of the two attractors. The energy well for B is substan- tially deeper than the well for A. Starting at the point in the lower left corner where there are zero A units and zero B units active, the energy falls off immediately when moving in the B direction (right), but rises initially in the A direction (left) before dropping into a modest energy well when most of the A units are on. Points in the interior of the diagram, representing a combination of A and B units active, have higher energies than points along the edges due to the inhibitory connections between units in rival cliques. We can see from Figures lb and 2 that the attractor for A, although narrower and shallower than the one for B, is still sizable. This is likely to mislead the model, so that some of the time it will get trapped in the wrong energy well. The fact that there is an attractor for A at all is due largely to sibling support, since the raw evidence for A is less than the rule unit threshold. We can eliminate the unwanted energy well for A by choosing thresholds that exceed the maximum sibling support of 2 x 39 = 78. DCPS uses a value of 119. However, early in the stochastic search the evidence visible in the clause spaces will be lower than at the conclusion of the search; high thresholds combined with low evidence would make the B attractor small and very hard to find. (See the right half of Figure Ie, and Figure 3.) Under these conditions the largest attractor is the one with all units turned off: the null hypothesis. ' DISCUSSION Our analysis of energy landscapes pulls us in two directions: we need low thresholds so the correct attractor is broad and easy to find, but we need high thresholds to eliminate unwanted at tractors associated with local energy minima. Two solutions have been investigated. The first is to start out with low thresholds and raise them gradually during the stochastic search. This ""pulls the rug out from under"" poorly- supported hypotheses while giving the model time to find the desired winner. The second solution involves clipping a corner from the state space hypercube so that the model may never have fewer than 40 units active at a time. This prevents the 630 Touretzky model from falling into the null attractor. When it attempts to drop the number of active units below 40 it is kicked away from the clipped edge by forcing it to turn on a few inactive units at random. Although DCPS is a Boltzmann machine it does not search the state space by simulated annealing in the usual sense. True annealing implies a slow reduction in temperature over many update cycles. Stochastic search in DCPS takes place at a single temperature that has been empirically determined to be the model's approximate ""melting point."" The search is only allowed to take a few cycles; typically it takes less than 10. Therefore the shapes of energy wells and the dynamics of the search are particularly important, as they determine how likely the model is to wander into particular attractor regions. The work reported here suggests that stochastic search dynamics may be improved by manipulating parameters other than just absolute temperature and cooling rate. Threshold growing and corner clipping appear useful in the case of DWTA nets. Additional details are available in [Touretzky, 1989]. Acknowledgments This research was supported by the Office of Naval Research under contract N00014- 86-K-0678, and by National Science Foundation grant EET-8716324. I thank Dean Pomerleau, Roni Rosenfeld, Paul Gleichauf, and Lokendra Shastri for helpful com- ments, and Geoff Hinton for his collaboration in the development of DCPS. References [1] Derthick, M. A., & Tebelskis, J. M. (1988) ""Ensemble"" Boltzmann machines have collective computational properties like those of Hopfield and Tank neu- rons. In D. Z. Anderson (ed.), Neural Information Processing Systems. New York: American Institute of Physics. [2] Feldman, J. A., & Ballard, D. H. (1982) Connectionist models and their prop- erties. Cognitive Science 6:205-254. [3] Hinton, G. E., & Sejnowski, T. J. (1986) Learning and relearning in Boltzmann machines. In D. E. Rumelhart and J. L. McClelland (eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1. Cam- bridge, MA: Bradford Books/The MIT Press. [4] Hopfield, J. J. (1982) Neural networks and physical systems with emergent col- lective computational abilities. Proceedings of the National Academy of Sciences USA, 79:2554-2558. [5] Touretzky, D. S., & Hinton, G. E. (1988) A distributed connectionist product.ion system. Cognitive Science 12(3):423-466. [6] Touretzky, D. S. (1989) Controlling search dynamics by manipulating energy landscapes. Technical report CMU-CS-89-113, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA. Energy Landscapes of Distributed Winner-Take-All Networks 631 \ ( ~ ! ~ ! Evldlncl: A&4O. ""100. C:5. \ , \ \ llnIhold • 69 Evldlncl: A&4O. 1060, C:5. j ! : : : ! \( \ ! , . '= nr.hold = 69 /\ I \ AJ'\ ! . , ! \ · . · . · . · . · . · . Evldlncl: A&4O. ""100. C:5. /\ ! ~ ! \ l \ · . · . · . · . · . · . -. 0. \ ! "":. : \f llnIhold = 119 Evldlncl: Aa4O. 1060. C:5. r\ ! \ ! \ ! \ · . · . · . f\\ f \ · . . . nr.hold = 119 1\ I , ; ; .f \ : 1\ 1\ ! \ · . · . · . · . · . · . · . · . · . · . · . · . · . · . · . !A'. : ! ! \ ! \ · . · . · . · . · . · . · . · . · . · . Figure 1: (a) four basic shapes for DWTA energy tours; (b) comparison of low vs. high thresholds in energy tours where there is a high degree of evidence for hypothesis B; (c) corresponding tours with low evidence for B. '~~'eJms A~J~U~ ~u!puods~JJo~ ~q'l (q) !~m'l'eJ~dw~'l OJ~Z 'l'e SUO!'l!su'eJ'l ~'l'e'ls I'e~~1 ('e) 'q 1 ~m~!d JO Jl'eq U~l ~q'l U! S'e '~~U~P!A~ q~!q pU'e sPloqs~Jq'l MOl :~ ~JI1~!d .... \~t'> ••• ~':J. fb e"" '69 = PT o4sa.J41 ""QIlI1 r •1Ik». '001=8 'O~=~ :aouapT A3 A~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 14444444444444444 44444444444444444 1444444444444444444444444444444444~44444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 144444444444444444444444444~444444444444 1444444~4444444444444444444~444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 '4444444444444444~4444444444444444444 14444444444444444 44444444444444444444 14444444444444444 4 44444444444444444444 14444444444444444 4444444444444444444 14444444444444444 4444444444444444 ~ 144444444444444444 444444444444444 ~~ 14444444444444444444444444444444444 ~~~ 1444444444444444444444444444444444 ~~~~ 144444444444444444444444444444444 ~~~~~ 14444444444444444444444444444444 ~~~~~~ 1444444444444444444444444444444 ~~~~~~~ 144444444444444444444444444444 ~~~~~~~~ 14444444444444444444444444444 ~~~~~~~~~ 1444444444444444444444444444 ~~~~~~~~~~ 144444444444444444444444444 ~~~~~~~~~~~ 14444444444444444444444444 ~~~~~~~~~~~~ 1444444444444444444444444 ~~~~~~~~~~~~~ 144444444444444444444444 ~~~~~~~~~~~~~~ 14444444444444444444444 ~~~~~~~~~~~~~~~ 1444444444444444444444 ~~~~~~~~~~~~~~~~ 144444444444444444444 ~~~~~~~~~~ ~~~~ 14444444444444444444 ~~~~~~~~~ ~~ ~~~~~ 1444444444444444444 ~~~~~~~~~~ ~~~~~ 144444444444444444 ~~~~~~~~~~~~ ~~~~~ 14444444444444444 ~~~~~~~~~~~~~~ ~~~~~~ 1444444444444444 ~~~~~~~~~~~~~~~~~~~~~~ 144444444444444 ~~~~~~~~~~~~~~~~~~~~~~~ 144 ~~~~~~~~~~~~~~~~~~~~~~~ AlIZla,m0J, ~f!9 t-t:;.! ~~ ~ ... -('I) ~ ~~ ~ .. ~ ('I) == ~ ..... ... oq ~ ::r ; ~ ..... ::r C-. ... o ('I) ~ ~ ~ I:T' o ~ - ~o.. N fI) ('I) ~ ... ~ o 0.. ~- ('I) 0 S == ""'C:j ('I) ~ < ~ ..... ~o.. ~ ('I) ... ~ ('I) ("") _. ('I) ..-- O""'~ ""-""~ ~ ..... ::r~ ('I) ~ ("") I:T' o ('I) ... ... ... ('I) ..... ~oq ""t:I::r o ~ 5..;- ..... - ~ ...., oq 0 ('I) ...., ~ ~ ('I) ..... ""'oq oq ~ '< ... ~ ('I) ~ .... ... ("") ~. ("")..- ~~ tTl. 4 I '\ • ., ~ • ,.., < .... a. III :J 0 III l> II A 0 • tIl II m 0 · -i ~ .., III Ul ~ 0 .... 0.. II ..... ..... \0 · ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 14444444444444444444~44444444444444 14444444444444444444444 444444444444444 t44444444444444444444 4 44444444444444 t44444444444444444444 44444444444444 t444444444444444444444444444444444444444 t44444444444~4444444~4444444444444444 t4444444444444444444 4444444444444444 t44444444444444444444 44444444444444444 t44444444444444444441 44444444444444 t44444444444444444444. 1144444444444444 1444444444444444444444444444444444444444 t444444444444444444444444444444444444444 t444444444444444444444444444444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 t444444444444444444444444444444444444444 t444444444444444444444444444444444444444 t444444444444444444444444444444444444444 t444444444444444444444444444444444444444 1444444444444444444444444444444444444444 1444444444444444444444444444444444444444 t444444444444444444444444444444444444444 14 4444444 444 4 4 , 444444444444444444444444444444444444444 ,~ 44444444444444444444444444444444444444 ,~ 4444444444444444444444444444444444444 ,~~ 444444444444444444444444444444444444 ,~~~ 44444444444444444444444444444444444 , 4444444444444444444444444444444444 , ~ 444444444444444444444444444444444 l ~~ 44444444444444444444444444444444 l~~~~~~~ 4444444444444444444444444444444 444444444444444444444444444444 tE.1 ~ r ~ (D fI) ~ ~ fI) &: 0'"" ~ ~ j;l.. ~ ~ ~ ~ (D ~ - z (D i a- 0) to c.o","[-0.10074921697378159, -0.062179084867239, -0.0430888868868351, 0.014650906436145306, 0.04224604740738869, 0.03581932559609413, -0.0008014561026357114, 0.009001534432172775, 0.09728266298770905, -0.017644956707954407, -0.05978761240839958, 0.00243827304802835, 0.037591829895973206, -0.03869355097413063, -0.07337343692779541, 0.021386483684182167, 0.05711425468325615, 0.03217080235481262, -0.07211633026599884, -0.011089423671364784, -0.009979302063584328, -0.0021534126717597246, -0.06338629871606827, -0.06471052020788193, -0.018378572538495064, 0.08222340047359467, -0.008830731734633446, 0.016719264909625053, -0.013406136073172092, -0.005923604127019644, -0.007887765765190125, -0.08167049288749695, -0.013044875115156174, 0.09391085058450699, 0.0025292031932622194, 0.06693362444639206, -0.08532025665044785, 0.0009695572080090642, -0.022052006796002388, 0.017759433016180992, 0.07220380753278732, 0.019328514114022255, -0.016192225739359856, -0.02362455427646637, 0.04250840097665787, 0.028802990913391113, -0.06534750759601593, 0.045695558190345764, -0.07903704047203064, -0.12137619405984879, -0.09089132398366928, -0.021451115608215332, 0.036883991211652756, 0.03621070459485054, 0.022730832919478416, 0.025940952822566032, 0.018544740974903107, 0.053169991821050644, -0.04119717702269554, -0.04024149477481842, 0.06050201132893562, -0.07736717164516449, -0.09103244543075562, 0.014821919612586498, 0.05986366048455238, 0.015186653472483158, 0.036808378994464874, 0.15074950456619263, 0.06081414595246315, -0.04470938444137573, 0.02309989370405674, 0.05031236261129379, -0.10093700885772705, 0.01856372132897377, 0.035052649676799774, 0.06408173590898514, 0.03177722170948982, -0.026057370007038116, 0.07744011282920837, -0.01959327422082424, 0.018366990610957146, -0.02015378698706627, -0.013488772325217724, 0.006435128394514322, 0.013979890383780003, -0.043027400970458984, -0.020355571061372757, 0.02038649097084999, 0.07786685973405838, -0.05685285106301308, -0.10977590084075928, 0.004789122845977545, 0.016426876187324524, -0.015013592317700386, 0.007018352393060923, 0.06344079226255417, 0.009436918422579765, -0.03951166570186615, -0.002134394831955433, 0.10207051038742065, 0.02105703204870224, -0.010108144953846931, 0.047872453927993774, -0.03235357999801636, 0.08010958135128021, -0.02173234522342682, 0.02637372352182865, 0.06637619435787201, 0.011792341247200966, -0.04347722604870796, -0.010639348067343235, 0.0779414102435112, -0.0014910815516486764, 0.014659043401479721, 0.01100060436874628, -0.03952110558748245, 0.026658453047275543, 0.05923471972346306, 0.06220334395766258, 0.06787536293268204, -0.05719607695937157, -0.04260360077023506, -0.05966278165578842, -0.04670461639761925, 0.04109679162502289, -0.03966045752167702, -0.09263373911380768, 2.9966056476176923e-33, -0.04795517772436142, -0.11136192083358765, 0.032477810978889465, -0.012665401212871075, 0.06428269296884537, 0.010678560473024845, -0.044840916991233826, -0.04147586226463318, -0.057815637439489365, 0.02678452804684639, -0.08087706565856934, 0.041599757969379425, 0.02281741425395012, 0.11323773115873337, 0.06124483048915863, -0.046866755932569504, 0.06044647470116615, -0.07437290251255035, 0.07902511954307556, -0.06685905903577805, 0.03943948075175285, 0.026286952197551727, 0.0072799003683030605, 0.0592668354511261, 0.0281020849943161, -0.025797821581363678, -0.0003851907385978848, -0.024936586618423462, -0.018916092813014984, 0.020227989181876183, -0.05765768140554428, 0.058743253350257874, -0.07717357575893402, 0.06464693695306778, -0.00011518874089233577, 0.026748813688755035, 0.000785097770858556, -0.11065641790628433, 0.07096488773822784, -0.042634572833776474, -0.0219317264854908, 0.010606971569359303, -0.05346956104040146, -0.007870553061366081, -0.13910920917987823, -0.026163799688220024, -0.0016740638529881835, 0.028541577979922295, -0.025755641981959343, -0.07177714258432388, 0.037144750356674194, -0.011780263856053352, 0.08883382380008698, -0.06866224110126495, 0.03795366734266281, -0.03857447952032089, 0.021347137168049812, 0.05392127111554146, 0.037866685539484024, 0.1655994951725006, -0.03681056201457977, 0.00574100948870182, -0.04831846058368683, 0.10162129998207092, 0.0650690570473671, 0.06832488626241684, -0.10549163073301315, -0.04018852859735489, 0.12223643064498901, -0.05225810781121254, -0.021871130913496017, 0.03181135281920433, -0.01234253216534853, -0.0025085313245654106, 0.10700377076864243, -0.03658970072865486, -0.001921636750921607, -0.011733250692486763, -0.04942748323082924, 0.11400724947452545, -0.04520867392420769, -0.017782289534807205, -0.08123542368412018, 0.0005728642572648823, -0.10389117896556854, -0.06988807767629623, -0.048133596777915955, -0.020989051088690758, -0.009086535312235355, -0.007642727345228195, -0.0029800347983837128, -0.03685293719172478, 0.06827126443386078, 0.10587618499994278, -0.04704234376549721, -4.0749202274185705e-33, -0.06468337029218674, -0.017775535583496094, -0.0693134292960167, 0.026698414236307144, 0.04926963523030281, 0.04514393210411072, -0.07224884629249573, -0.10220220685005188, -0.03106224536895752, -0.023737085983157158, 0.05613604933023453, -0.015175383538007736, 0.07737593352794647, 0.008458961732685566, 0.05177827924489975, -0.05052102357149124, 0.04213865473866463, -0.008235616609454155, 0.010408606380224228, -0.05044575035572052, 0.04155301675200462, 0.08844213932752609, -0.08350177854299545, -0.09286977350711823, 0.028223969042301178, -0.02707716077566147, -0.06174752488732338, 0.01844697631895542, -0.01514309924095869, 0.08794806152582169, -0.023251330479979515, -0.028724048286676407, -0.06207520514726639, -0.007030012086033821, 0.0013050729176029563, 0.10132299363613129, 0.017870159819722176, 0.038129668682813644, -0.031582482159137726, 0.012107876129448414, 0.022011304274201393, -0.08247298002243042, -0.08447738736867905, 0.08005498349666595, 0.032663021236658096, 0.03109889291226864, -0.015956848859786987, -0.031062886118888855, -0.03607887029647827, 0.024828407913446426, 0.0037670175079256296, 0.037024714052677155, -0.03317581117153168, -0.012825562618672848, -0.04035947844386101, -0.02731606923043728, 0.01795508898794651, 0.03961822763085365, 0.03629901632666588, 0.012539108283817768, 0.0075233313255012035, -0.10675963759422302, -0.014386761002242565, 0.042352110147476196, 0.020246421918272972, 0.0020763000939041376, -0.00805667694658041, 0.01564168557524681, 0.10494855046272278, -0.06549521535634995, 0.011520473286509514, 0.060431141406297684, 0.011759818531572819, 0.01652466505765915, 0.014316056855022907, 0.050440702587366104, 0.025793787091970444, 0.036662861704826355, -0.0032387219835072756, 0.010687733069062233, -0.07292074710130692, 0.0392073392868042, 0.0520036518573761, -0.014272096566855907, 0.05888283625245094, 0.060480747371912, 0.050478171557188034, -0.04670470580458641, 0.0275461133569479, 0.004755005706101656, 0.06285128742456436, -0.0400964692234993, 0.014096437022089958, 0.049462199211120605, -0.056128162890672684, -5.303339278839303e-08, 0.002321518724784255, 0.017863444983959198, 0.04186885058879852, -0.0032629133202135563, 0.06220196560025215, -0.02389451302587986, 0.08745366334915161, 0.011405767872929573, -0.019913818687200546, 0.0808546170592308, 0.10958016663789749, 0.024030324071645737, -0.0324869304895401, -0.05785011500120163, 0.03133286535739899, 0.007997616194188595, -0.006297441199421883, 0.022228993475437164, -0.04338777810335159, -0.04893447086215019, 0.05675654485821724, -0.036266207695007324, -0.0347234271466732, 0.07526595890522003, 0.038662225008010864, -0.09471733123064041, -0.033685266971588135, -0.024992207065224648, -0.0003685540286824107, 0.02455204725265503, -0.008992216549813747, 0.02211478166282177, 0.02689608745276928, -0.014147334732115269, -0.006419806275516748, 0.10217998921871185, -0.03840571269392967, -0.02648005075752735, -0.015744023025035858, 0.023341059684753418, -0.07975702732801437, 0.0018225049134343863, -0.03398717939853668, 0.05663411691784859, 0.04368490353226662, 0.007475478108972311, -0.004820934031158686, -0.05460290610790253, 0.02963610738515854, -0.028065385296940804, 0.013073764741420746, 0.014798782765865326, -0.02607543207705021, -0.01245156954973936, 0.02928689867258072, -0.005976858548820019, 0.020282959565520287, -0.08552423119544983, 0.003873682115226984, 0.03274693340063095, -0.02222837507724762, -0.004317830316722393, -0.08238548785448074, -0.05534781515598297]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AnalyzingtheEnergyLandscapesofDistributedWinnerTakeAllNetworks.pdf,Computer Vision,"NEURAL ANALOG DIFFUSION-ENHANCEMENT LAYER AND SPATIO-TEMPORAL GROUPING IN EARLY VISION Allen M. Waxman·,t, Michael Seibert·,t,RobertCunninghamt and I ian Wu· • Laboratory for Sensory Robotics Boston University Boston, MA 02215 t Machine Intelligence Group MIT Lincoln Laboratory Lexington, MA 02173 ABSTRACT A new class of neural network aimed at early visual processing is described; we call it a Neural Analog Diffusion-Enhancement Layer or ""NADEL."" The network consists of two levels which are coupled through feedfoward and shunted feedback connections. The lower level is a two-dimensional diffusion map which accepts visual features as input, and spreads activity over larger scales as a function of time. The upper layer is periodically fed the activity from the diffusion layer and locates local maxima in it (an extreme form of contrast enhancement) using a network of local comparators. These local maxima are fed back to the diffusion layer using an on-center/off-surround shunting anatomy. The maxima are also available as output of the network. The network dynamics serves to cluster features on multiple scales as a function of time, and can be used in a variety of early visual processing tasks such as: extraction of comers and high curvature points along edge contours, line end detection, gap filling in contours, generation of fixation points, perceptual grouping on multiple scales, correspondence and path impletion in long-range apparent motion, and building 2-D shape representations that are invariant to location, orientation, scale, and small deformation on the visual field. INTRODUCTION Computer vision is often divided into two main stages, ""early vision"" and ""late vision"", which correspond to image processing and knowledge-based recognition/interpretation, respectively. Image processing for early vision involves algorithms for feature enhancement and extraction (e.g. edges and comers), feature grouping (i.e., perceptual We acknowledge support from the Machine Intelligence Group of MIT Lincoln Laboratory. The views expressed are those of the authors and do not reflect the official policy or position of the U.S. Government 289 290 Waxman, Seibert, Cunningham and Wu organization), and the extraction of physical properties for object surfaces that comprise a scene (e.g. reflectance, depth, surface slopes and curvatures, discontinuities). The computer vision literature is characterized by a plethora of algorithms to achieve many of these computations, though they are hardly robust in performance. Biological neural network processing does, of course, achieve all of these early vision tasks, as evidenced by psychological studies of the preattentive phase of human visual processing. Often, such studies provide motivation for new algorithms in computer vision. In contrast to this algorithmic approach, computational neural network processing tries to glean organizational and functional insights from the biological realizations, in order to emulate their information processing capabilities. This is desirable. mainly because of the adaptive and real-time nature of the neural network architecture. Here, we shall demonstrate that a single neural architecture based on dynamical diffusion- enhancement networks can realize a large variety of early vision tasks that deal mainly with perceptual grouping. The ability to group image features on multiple scales as a function of time, follows from ""attractive forces"" that emerge from the network dynamics. We have already implemented the NADEL (in 16-bit arithmetic) on a video- rate parallel computer, the PIPE [Kent et al .• 1985], as well as a SUN-3 workstation. THE NADEL The Neural Analog Diffusion-Enhancement Layer was recently introduced by Seibert & Waxman [1989]. and is illustrated in Figure 1; it consists primarily of two levels which are coupled via feedforward and shunted feedback connections. Low-level features extracted from the imagery provide input to the lower level (a 2-D map) which spreads input activity over larger scales as time progresses via diffusion, allowing for passive decay of activity. The diffused activity is periodically sampled and passed upward to a contrast-enhancing level (another 2-D map) which locates local maxima in the terrain of diffuse activity. However, this forward pathway is masked by receptive fields which pass only regions of activity with positive Gaussian curvature and negative mean curvature; that is these receptive fields play the role of inhibitory dendro-dendritic modulatory gates. This masking fascilitates the local maxima detection in the upper level. These local maxima detected by the upper level are fed back to the lower diffusion level using a shunting dynamics with on-center/off-surround anatomy (cf. [Grossberg, 1973] on the importance of shunting for automatic gain control, and the role of center/surround anatomies in competitive networks). The local maxima are also available as outputs of the network, and take on different interpretations as a function of the input. A number of examples of spatio-temporal grouping will be illustrated in the next section. The primary result of diffusion-enhancement network dynamics is to create a long- range attractive force between isolated featural inputs. This force manifests itself by shifting the local maxima of activity toward one another. leading to a featural grouping over mUltiple scales as a function of time. This is shown in Figure 1, where two featural inputs spread their initial excitations over time. The individual activities superpose. with the tail of one gaussian crossing the maximum of the other gaussian at an angle. This Neural Analog Diffusion-Enhancement Layer 291 biases the superposition of activities, adding more activity to one side of a maximum than another, causing a shift in the local maxima toward one another. Eventually, the local maxima merge into a single maximum at the centroid of the individual inputs. If we keep track of the local maxima as diffusion progresses (by connecting the output of the enhancement layer to another layer which stores activity in short term memory), then the two initial inputs will become connected by a line. In Figure 1 we also illustrate the grouping of five features in two clusters, a configuration possessing two spatial scales. After little diffusion the local maxima are located where the initial inputs were. Further diffusion causes each cluster to form a single local maximum at the cluster centroid. Eventually, both clusters merge into a single hump of activity with one maximum at the centroid of the five initial inputs. Thus, multi scale grouping over time emerges. The examples of Figure 1 use only diffusion without any feedback, yet they illustrate the importance of localizing the local maxima through a kind of contrast-enhancement on another layer. The local maxima of activity serve as ""place tokens"" representing grouped features at a particular scale. The feedback pathway re-activates the diffusion layer, thereby allowing the grouping process to proceed to still larger scales, even across featureless areas of imagery. The dynamical evolution of activity in the NADEL can be modeled using a modified diffusion equation [Seibert & Waxman, 1989]. However, in our simulations of the NADEL we don't actually solve this differential equation directly. Instead, each iteration of the NADEL consists of a spreading of activity using gaussian convolution, allowing for passive decay, then sampling the diffusion layer, masking out areas which are not positive Gaussian curvature and negative mean curvature activity surfaces, detecting one local maximum in each of these convex areas, and feeding this back to the diffusion layer with a shunted on-center/off-surround excitation at the local maxima. In the biological system, diffusion can be accomplished via a recurrent network of cells with off- center/on-surround lateral connectivity, or more directly using electrotonic coupling across gap junctions as in the horizontal cell layer of the retina [Dowling, 1987]. Curvature masking of the activity surface can be accomplished using oriented off- center/on-surround receptive fields that modulate the connections between the two primary layers of the NADEL. SPATIO-TEMPORAL GROUPING We give several examples of grouping phenomena in early vision, utilizing the NADEL. In all cases its parameters correspond to gaussian spreading with 0'=3 and passive decay of 1 % per iteration, and on-center/off-surround feedback with 0'+=11'12 and 0'_=1. Grouping of Two Points: The simple case of two instantaneous point stimuli input simultaneously to the NADEL is summarized in Figure 2. We plot the time (N network iterations) it takes to merge the two inputs, as a function of their initial separation (S pixels). For S:S;6 the points merge in one iteration; for S>24 activity equilibrates and shifting of local maxima never begins. 292 Waxman, Seibert, Cunningham and Wu Grouping on Multiple Scales: Figure 3 illusttates the hierarchy of groupings generated by a square outline (31 pixels on a side) with gaps (9 pixels wide). Comer and line-end features are first enhanced using complementary center-surround receptive fields (modeled as a rectified response to a difference-of-gaussians), and located at the local maxima of activity. These features are shown superimposed on the shape in 3a; they serve as input to the NADEL. Figure 3b shows the loci of local maxima determined up to the second stable grouping, superimposed over the shape. Boundary completion fills the gaps in the square. In Figure 3c we show the loci of local maxima on the image plane, after the final grouping has occured (N=I00 iterations). The trajectory of local maxima through space-time (x,y,t) is shown in Figure 3d after the fourth grouping. It reveals a hierarchical organization similar to the ""scale-space diagrams"" of Witkin [1983]. It can be seen from Figure 3d that successive groupings form stable entities in that the place tokens remain stationary for several iterations of the NADEL. It isn't until activity has diffused farther out to the next representative scale that these local maxima start moving once again, and eventually merge. This relates stable perceptual groupings to place tokens (i.e., local maxima of activity) that are not in motion on the diffusion layer. The motion of place tokens can be measured in the same fashion as feature point motion across the visual field. Real-time receptive fields for measuring the motion of image edge and point features have recently been developed by Waxman et ale [1988]. Grouping of Time-Varying Inputs: The simplest example in this case corresponds to the grouping of two lights that are flashed at different locations at different times. When the time interval between flashes (Stimulus Onset Asynchrony SOA) is set appropriately, one perceives a smooth motion _or ""path impletion"" between the stimuli. This percept of ""long-range apparent motion"" is the cornerstone of the Gestalt Psychology moverment, and has remained unexplained for one-hundred years now [Kolers, 1972]. We have applied the NADEL to a variety of classical problems in apparent motion including the ""split motion"" percept and the mUlti-point Ternus configuration [Waxman et al., 1989]. Here we consider only the case of motion between two stimuli, where we interpret the locus of local maxima as the impleted path in apparent motion. However, the direction of perceived motion is not determined by the grouping process itself; only the path. We make the additional assumption that grouping generates a motion percept only if the second stimulus begins to shift immediately upon input to the NADEL. We suggest that the motion percept occurs only after path impletion is complete. That is, while grouping is active, its outputs are suppressed from our perception (a form of ""ttansient-on- sustained inhibition"" analogous to saccadic suppression). By varying the separation between the two stimuli, and the time (SOA) between their inputs, we can plot regimes for which the NADEL predicts apparent motion. This is shown in Figure 4, which compares favorably with the psychophysical results summarized in Figure 3.2 of [Kolers, 1972]. We find regimes in which complete paths are formed (""smooth motion""), partial paths are formed (""jumpy motion""), and no immediate shifting occurs (""no motion""). The maximum allowable SOA between stimuli (upper curves) is determined by the passive decay rate. Increasing this decay from 1 % to 3% will decrease the maximum SOA by a Neural Analog Diffusion-Enhancement Layer 293 factor of five. The minimum allowable SOA (lower curves) increases with increasing separation, since it takes longer for activity from the first stimulus to influence a more distant second stimulus. The linearity of the lower boundary has been interpreted by [Waxman et aI .• 1989] as suggestive of Korte's ""third law"" [Kolers, 1972], when taken in combination with a logarithmic transformation of the visual field [Schwartz, 1980]. Attentional Cues and Invariant Representations: Place tokens which emerge as stable groupings over time can also provide attentional cues to a vision system. They would typically drive saccadic eye motions during scene inspection, with the relative activities of these maxima and their order of emergence determining the sequence of rapid eye motions. Such eye motions are known to play a key role in human visual perception [Yarbus, 1967]; they are influenced by both bottom-up perceptual cues as well as top- down expectations. The neuromorphic vision system developed by Seibert & Waxman [1989], shown in Figure 5, utilizes the NADEL to drive ""eye motions"", and thereby achieve translational invariance in 2-D object learning and recognition. This is followed by a log-polar transform (which emulates the geniculo-cortical connections [Schwartz, 1980]) and another NADEL to achieve rotation and scale in variance as well. Further coding of the transformed feature points by overlapping receptive fields provides invariance to small deformation. Pattern learning and recognition is then achieved using an Adaptive Resonance Theory (ART-2) network [Carpenter & Grossberg, 1987]. REFERENCES G. Carpenter & S. Grossberg (1987). ART-2: Self-organization of stable category recognition codes for analog input patterns. Applied Optics 26, pp. 4919-4930. I.E. Dowling (1987). The RETINA: An approachable part or the brain. Cambridge, MA: Harvard University Press. S. Grossberg (1973). Contour enhancement, short term memory, and constancies in reverberating neural networks. Studies in Applied Mathematics 52. pp.217-257. E.W. Kent, M.O. Shneier & R. Lumia (1985). PIPE: Pipelined Image Processing Engine. Journal of Parallel and Distributed Computing 2, pp. 50-78. P.A. Kolers (1972). Aspects or Motion Perception. New York: Pergamon Press. E.L. Schwartz (1980). Computational anatomy and functional architecture of striate cortex: A spatial mapping approach to perceptual coding. Vision Research 20, pp. 645- 669. M. Seibert & A.M. Waxman (1989). Spreading activation layers, visual saccades and invariant representations for neural pattern recognition systems. N~ural Networks 2, pp. 9-27. 294 Waxman, Seibert, Cunningham and Wu A.M. Waxman, J. Wu & F. Bergholm (1988). Convected activation profiles and the measurement of visual motion. Proceeds. 1988 IEEE Conference on Computer Vision and Pattern Recognition. Ann Arbor, MI, pp. 717-723. A.M. Waxman. J. Wu & M. Seibert (1989). Computing visual motion in the short and the long: From receptive fields to neural networks. Proceeds. IEEE 1989 Workshop on Visual Motion. Irvine, CA. A.P. Witkin (1983). Scale space filtering. Proceeds. of the International Joint Conference on Artificial Intelligence. Karlsruhe, pp. 1019-1021. AL. Yarbus (1967). Eye Movements and Vision. New York: Plenum Press. ON·CENTER! OFF·SURROUND o OFF·CENTER! a.l-SURAOUNQ o SHUNTNG FEE08AO< o ON-CENTER! OFF·SURAOUND Figure 1 • (left) The NADEL takes featural input and diffuses it over a 2-D map as a function of time. Local maxima of activity are detected by the upper layer and fed back to the diffusion layer using an on-center/off-surround shunting anatomy. (right. top) Features spread their activities which superpose to generate an attractive force, causing distant features to group. (right, bottom) Grouping progresses in time over multiple scales. with small clusters emerging before extended clusters. Clusters are represented by their local ~axima of activity, which serve as place tokens. <! ~ .9 ~ .x ~ ~ 1000 800 600 .00 200 lOa so 60 ~o 20 10 6 I / / I / I / / I / I I I / Separalion 5 (pIXelS) 1 /, I I; Figure 2 _ The time (network iterations N) to merge two points input simultaneously to the NADEL, as a function of their initial separation (S pixels). lmilllillUml:l • • • ~~ .. ~ (a) y .--- .... ,-_ ..... (e) 1ii~Ill'J!I!IIIl!lll!l1llllllm= !,~ ~l"" 1 _· 1~~'l .' F~ ;h~ ' ~h ~~!:Ji. t~5~ :··'Id ':'.~ .J\llt! I~~ !"" ~ Ii' ~ ~,I tt~i'><'! k;'~ ""~~~""'r..;.'(!S.1-:'~-' ~7'~~':;' ~,~o.~_, ... ~~~~ . (b) Cd) Figure 3 - Perceptual grouping of a square outline with gaps. z ~ e. ~ e. t2 ~ ~ r!3. g ~ g. I g (""t- ~ ~ ~ ~ '-0 c:.n 200 100 80 60 ~ 40 <II c: .2 ~ 20 a> -l: 0 ~ iii 10 .s 8 C 6 C/) 4 2 1 Smooth Motion ./ ,/ ./ ./ ./' ./' Jumpy Motion ,/ ./ / 6 / / / / ./ ./ ./ ./ Separation S (pixels) ,/ Figure 4 - Apparent motion between two flashed lights: Stimulus Onset Asynchrony SOA (network iterations N) vs. Separation S (pixels). Solid curves indicate boundaries between which introduction of the second light yields immediate shifting of local maxima; dashed curves (above solid curves) indicate when final merge occurs yielding the impleted path. No Motion E_pectat Ions Spatial relations VIA NADEL FEATURE MAP EYE MOTIONS Figure 5 - The neuromorphic vision system for invariant learning and recognition of 2-D object~ utilizes three NADEL networks. ~ co (J') J ~ .... g- ,;+ g ::s e. ::s I§. a ~ ~ ~","[0.046193234622478485, -0.11516871303319931, 0.07223118841648102, 0.0040339999832212925, -0.010655459016561508, 0.024156438186764717, -0.034015487879514694, -0.009992347098886967, 0.05196892097592354, -0.04294504225254059, 0.008830786682665348, -0.00011230302334297448, -0.004369137343019247, 0.036268312484025955, -0.06958301365375519, 0.002271215431392193, 0.09146353602409363, 0.06374300271272659, -0.04567908123135567, -0.037902966141700745, 0.03242829069495201, -0.07304751127958298, -0.04733822122216225, 0.001888607395812869, 0.03194060176610947, 0.049961913377046585, -0.002080043312162161, -0.0490117147564888, 0.02010682038962841, -0.034726765006780624, 0.0637822225689888, -0.06462905555963516, 0.017951657995581627, 0.02963317185640335, -0.06890031695365906, 0.04739527031779289, 0.007114808075129986, 0.030140705406665802, -0.03303031995892525, -0.029252154752612114, 0.0181563850492239, 0.02603512443602085, -0.04943612962961197, -0.03267721086740494, 0.07152403891086578, 0.032351937144994736, 9.943734767148271e-05, -0.016104578971862793, -0.06755039840936661, -0.026843857020139694, -0.05306380242109299, -0.028333565220236778, -0.06411319971084595, 0.0733475536108017, 0.018919385969638824, 0.16410204768180847, -0.015978053212165833, -0.012585340067744255, -0.022174326702952385, -0.06533609330654144, -0.005147945135831833, -0.028460483998060226, 0.027488885447382927, 0.0046748328022658825, 0.0028984847012907267, 0.007906612940132618, 0.03163044899702072, -0.022875027731060982, 0.0672643706202507, -0.023395437747240067, 0.0590372234582901, 0.038605205714702606, -0.0004907906986773014, -0.08299768716096878, 0.00570807745680213, -0.0064653921872377396, 0.040038906037807465, -0.019370390102267265, 0.053182415664196014, -0.05303392931818962, 0.09075338393449783, 0.025939686223864555, -0.0343855582177639, -0.025624850764870644, 0.019586972892284393, 0.053723808377981186, -0.07194595038890839, -0.024125533178448677, -0.04979321360588074, -0.02838178351521492, -0.030002744868397713, 0.03336598724126816, -0.14538152515888214, -0.032722458243370056, 0.047093719244003296, -0.10979023575782776, 0.019017590209841728, 0.00393689563497901, 0.04716269671916962, 0.07933732122182846, -0.013322822749614716, -0.06989938765764236, -0.009622648358345032, 0.0038662669248878956, 0.07610887289047241, -0.02914546988904476, 0.06980791687965393, 0.04268661513924599, 0.013291988521814346, -0.03770272061228752, -0.007759597152471542, -0.00969748105853796, -0.05325686186552048, -0.03189206123352051, 0.006811911705881357, -0.016025621443986893, 0.06578418612480164, 0.0008477619267068803, 0.04246213287115097, -0.016343137249350548, 0.034586433321237564, -0.09002040326595306, -0.13757678866386414, -0.048035938292741776, 0.08584153652191162, -0.018629487603902817, -0.041632410138845444, 2.71537101190636e-33, -0.023821134120225906, -0.06902126222848892, -0.004650515969842672, -0.02932215854525566, 0.06879377365112305, -0.015266945585608482, 0.026925206184387207, -0.07829371094703674, 0.03550771251320839, -0.05392942577600479, -0.09819705039262772, 0.03804733231663704, -0.08663146942853928, 0.16216516494750977, -0.024290919303894043, -0.11798372119665146, 0.039021220058202744, 0.010676386766135693, -0.014578549191355705, -0.07083777338266373, -0.004950449801981449, -0.02493063174188137, -0.024161288514733315, -0.015403085388243198, -0.0577213317155838, -0.043322108685970306, -0.003927526529878378, -0.0325457900762558, 0.008318166248500347, 0.03113730065524578, -0.028386270627379417, 0.032235272228717804, -0.12997600436210632, -0.0019131931476294994, 0.03022877313196659, 0.036695800721645355, 0.05420822650194168, 0.013850853778421879, 0.06547462940216064, 0.04281921312212944, -0.027760248631238937, 0.042276039719581604, -0.029725929722189903, -0.02739959955215454, -0.03945380821824074, 0.03510896861553192, 0.0161678995937109, 0.0863649845123291, -0.015803925693035126, -0.049311500042676926, -0.006413315422832966, -0.08192448318004608, 0.06283379346132278, -0.13140133023262024, -0.029685040935873985, 0.03459744155406952, 0.042364731431007385, -0.01903877779841423, 0.008657466620206833, 0.05789659917354584, 0.05257832258939743, 5.079957554698922e-05, -0.006335776764899492, 0.046455830335617065, 0.027062121778726578, -0.005009915214031935, 0.03542323783040047, 0.04097333922982216, 0.1034407988190651, -0.019323555752635002, -0.041195936501026154, 0.11791122704744339, -0.02173493057489395, -0.058664627373218536, 0.08206963539123535, 0.014408786781132221, 0.05356639623641968, 0.039308812469244, -0.048604775220155716, 0.04670711234211922, -0.06550268828868866, -0.05881034955382347, -0.05733146890997887, -0.004430668894201517, 0.013213925994932652, 0.06647714972496033, 0.0014951126649975777, -0.015293793752789497, -0.03307941183447838, -0.06683854013681412, -0.023114722222089767, 0.001314871129579842, 0.03673096373677254, 0.04308057948946953, -0.03459833562374115, -3.693997609614762e-33, 0.02215811051428318, 0.10185620188713074, 0.01895890012383461, 0.06493204087018967, -0.04676911607384682, 0.025448061525821686, 0.008172657340765, 0.0740317553281784, 0.00807025097310543, -0.015983596444129944, -0.014661815948784351, 0.01861751638352871, 0.01777503825724125, -0.03994673490524292, 0.047142643481492996, -0.04412814602255821, 0.006016680970788002, -0.04345330595970154, 0.04639725387096405, -0.011089547537267208, -3.613741500885226e-05, 0.10247892886400223, -0.07896661758422852, -0.01901547983288765, -0.02794651873409748, 0.025960709899663925, -0.07683403044939041, 0.04838321730494499, -0.02076835371553898, -0.011241963133215904, -0.033963579684495926, -0.019743558019399643, 0.03827406466007233, -0.03741434961557388, -0.01748381368815899, 0.11855213344097137, 0.011814811266958714, -0.09178056567907333, -0.06740041077136993, -0.018115537241101265, 0.02847883850336075, -0.04720260575413704, 0.010533226653933525, 0.012531379237771034, 0.05074722319841385, -0.03043186105787754, -0.006059100851416588, 0.006677946075797081, -0.09710253030061722, 0.07775570452213287, 0.0038309500087052584, 0.08574288338422775, -0.0574367456138134, -0.0389755442738533, 0.002373176161199808, 0.08755066990852356, -0.0025843665935099125, -0.02742466703057289, 0.10065717250108719, 0.002629786031320691, 0.0451950766146183, -0.09664171189069748, -0.08404841274023056, -0.0328010655939579, -0.040600139647722244, 0.02469179593026638, -0.028620228171348572, 0.0052885133773088455, 0.05551672726869583, 0.010294381529092789, -0.011069308035075665, 0.06451763957738876, 0.0541704036295414, -0.06355560570955276, 0.010820911265909672, -0.13098593056201935, -0.008818733505904675, 0.038090407848358154, 0.030414100736379623, 0.06202495098114014, -0.08512773364782333, -0.040067657828330994, 0.03251668065786362, 0.025344759225845337, 0.051563385874032974, 0.05225628241896629, 0.003651742357760668, -0.036994580179452896, 0.0424363948404789, -0.007991141639649868, -0.015947340056300163, -0.02424941025674343, -0.03830040246248245, 0.02219153568148613, -0.12311149388551712, -5.49603385024966e-08, -0.055056434124708176, 0.01585216447710991, 0.034475818276405334, -0.04203929379582405, 0.053597837686538696, -0.07976073771715164, 0.0921880379319191, 0.09398072957992554, -0.01602739840745926, -0.03356446325778961, 0.03808300569653511, 0.011060305871069431, -0.028480540961027145, -0.08496583253145218, 0.05381667613983154, 0.08274296671152115, -0.032486945390701294, 0.00038314081029966474, -0.0195937417447567, -0.046932119876146317, 0.014181412756443024, -0.020049892365932465, 0.07568760961294174, 0.04641607776284218, 0.11524482071399689, -0.01910947449505329, -0.08918821811676025, 0.042247474193573, 0.00034512707497924566, -0.033177781850099564, 0.02603129856288433, 0.04242919757962227, 0.04317857697606087, 0.0573330819606781, 0.003697246080264449, 0.0631842166185379, -0.07782258838415146, 0.05566936731338501, -0.0834670439362526, 0.05243528261780739, -0.07239512354135513, -0.06359139084815979, 0.05168459564447403, 0.030651908367872238, 0.007119255606085062, -0.020416049286723137, 0.08025359362363815, -0.07055436819791794, 0.046860672533512115, 0.03295877203345299, 0.010625185444951057, 0.04061165452003479, 0.0036135544069111347, 0.06731602549552917, -0.016183745115995407, -0.025500252842903137, 0.0384165458381176, -0.08638258278369904, -0.019882170483469963, 0.0342378243803978, -0.017173204571008682, 0.09091930836439133, -0.03583076223731041, -0.021497031673789024]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AnAnalogSelfOrganizingNeuralNetworkChip.pdf,Computer Vision,"436 SIMULATION AND MEASUREMENT OF THE ELECTRIC FIELDS GENERATED BY WEAKLY ELECTRIC FISH Brian Rasnow1, Christopher Assad2, Mark E. Nelson3 and James M. Bow~ Divisions of Physics1 ,Elecbical Engineerini, and Biolo~ Caltech, Pasadena, 91125 ABSTRACT The weakly electric fish, Gnathonemus peters;;, explores its environment by gener- ating pulsed elecbic fields and detecting small pertwbations in the fields resulting from nearby objects. Accordingly, the fISh detects and discriminates objects on the basis of a sequence of elecbic ""images"" whose temporal and spatial properties depend on the tim- ing of the fish's electric organ discharge and its body position relative to objects in its en- vironmenl We are interested in investigating how these fish utilize timing and body-po- sition during exploration to aid in object discrimination. We have developed a fmite-ele- ment simulation of the fish's self-generated electric fields so as to reconstruct the elec- trosensory consequences of body position and electric organ discharge timing in the fish. This paper describes this finite-element simulation system and presents preliminary elec- tric field measurements which are being used to tune the simulation. INTRODUCTION The active positioning of sensory structures (i.e. eyes, ears, whiskers, nostrils, etc.) is characteristic of the information seeking behavior of all exploratory animals. Yet, in most existing computational models and in many standard experimental paradigms, the active aspects of sensory processing are either eliminated or controlled (e.g. by stimulat- ing fIXed groups of receptors or by stabilizing images). However, it is clear that the ac- tive positioning of receptor surfaces directly affects the content and quality of the sensory infonnation received by the nervous system. Thus. controlling the position of sensors during sensory exploration constitutes an important feature of an animals strategy for making sensory discriminations. Quantitative study of this process could very well shed light on the algorithms and internal representations used by the nervous system in dis- criminating peripheral objects. Studies of the active use of sensory surfaces generally can be expected to pose a number of experimental challenges. This is because, in many animals, the sensory surfac- es involved are themselves structurally complicated, making it difficult to reconstruct p0- sition sequences or the consequences of any repositioning. For example, while the sen- Simulation and Measurement of the Weakly Electric Fish 437 sory systems of rats have been the subjects of a great deal of behavioral (Welker, 1964) and neurophysiological study (Gibson & Welker, 1983), it is extremely difficult to even monitor the movements of the perioral surfaces (lips, snout, whiskers) used by these ani- mals in their exploration of the world let alone reconstruct the sensory consequences. For these reasons we have sought an experimental animal with a sensory system in which these sensory-motor interactions can be more readily quantified. The experimental animal which we have selected for studying the control of sensory surface position during exploration is a member of a family of African freshwater fish (Monniridae) that use self-generated electric fields to detect and discriminate objects in their environment (Bullock & Heiligenberg, 1986). The electrosensory system in these fish relies on an ""electric organ"" in their tails which produces a weak pulsed electric field in the surrounding environment (significant within 1-2 body lengths) that is then detected with an array of electrosensors that are extremely sensitive to voltage drops across the skin. These ""electroreceptors"" allow the fISh to respond to the perturbations in the elec- tric field resulting from objects in the environment which differ in conductivity from the surrounding water (Fig. 1). .. conducting • object IIID electric organ § electroreceptors electric field lines Figure 1. The peripheral electrosensory system of Gnathonemus petersii consists of an ""electric organ"" current source at the base of the tail and sev- eral thousand ""electroreceptor"" cells distributed non uniformly over the fish's body. A conducting object near the fish causes a local increase in the current through the skin. These fISh are nocturnal, and rely more on their electric sense than on any other sensory system in perfonning a wide range of behaviors (eg. detecting and localizing objects such as food). It is also known that these fish execute exploratory movements, changing their body position actively as they attempt an electrosensory discrimination (Toerring & Belbenoit, 1979). Our objective is to understand how these movements change the distri- bution of the electric field on the animals skin, and to determine what, if any, relationship this has to the discrimination process. There are several clear advantages of this system for our studies. First, the electrore- 438 Rasnow, Assad, Nelson and Bower ceptors are in a fixed position with respect to each other on the surface of the animal. Therefore, by knowing the overall body position of the animal it is possible to know the exact spatial relationship of electroreceptors with respect to objects in the environment. Second, the physical equations governing the self-generated electric fIeld in the fish's en- vironment are well understood. As a consequence, it is relatively straightforward to re- construct perturbations in the electric field resulting from objects of different shape and conductance. Third, the electric potential can be readily measured, providing a direct measure of the electric field at a distance from the fish which can be used to reconstruct the potential difference across the animals skin. And finally, in the particular species of fish we have chosen to work with, Gnathonemus petersii, individual animals execute a brief (100 J.1Sec) electric organ discharge (BOD) at intervals of 30 msec to a few seconds. Modification of the firing pattern is 1cnown to be correlated with changes in the electrical environment (Lissmann, 1958). Thus, when the electric organ discharges, it is probable that the animal is interested in ""taking a look"" at its surroundings. In few other sensory systems is there as direct an indication of the attentional state of the subject. Having stated the advantages of this system for the study we have undertaken, it is also the case that considerable effort will still be necessary to answer the questions we have posed. For example, as described in this paper, in order to use electric field mea- surements made at a distance to infer the voltages across the surface of the animal's skin, it is necessary to develop a computer model of the fish and its environment. This will allow us to predict the field on the animal's skin surface given different body poSitions relative to objects in the environment. This paper describes our first steps in constructing this simulation system. Experimental Approach and Methods Simulations of Fish Electric Fields The electric potential, cll(x), generated by the EOD of a weakly electric fish in a fish tank is a solution ofPoisson's equation: Ve(pVell) = f where p(x)and f(x) are the impedance magnitude and source density at each point x in- side and surrounding the fish. Our goal is to solve this equation for ell given the current source density, f, generated by the electric organ and the impedances, p, corresponding to the properties of the fish and external objects (rocks, worms, etc.). Given p and f. this equation can be solved for the potential ell using a variety of iterative approximation schemes. Iterative methods, in general, first discretize the spatial domain of the problem into a set of ""node"" points, and convert Poisson's equation into a set of algebraic equa- tions with the nodal potentials as the unknown parameters. The node values, in this case, each represent an independent degree of freedom of the system and, as a consequence, there are as many equations as there are nodes. This very large system of equations can Simulation and Measurement of the Weakly Electric Fish 439 then be solved using a variety of standard techniques, including relaxation methods, con- jugate gradient minimization, domain decomposition and multi-grid methods. To simulate the electric fields generated by a fish, we currently use a 2-dimensional fmite element domain discretization (Hughes, 1987) and conjugate gradient solver. We chose the finite element method because it allows us to simulate the electric fields at much higher resolution in the area of interest close to the animal's body where the elec- tric field is largest and where errors due to the discretization would be most severe. The fmite element method is based on minimizing a global function that corresponds to the potential energy of the electric field. To compute this energy, the domain is decomposed into a large number of elements, each with uniform impedance (see Fig. 2). The global energy is expressed as a sum of the contributions from each element, where the potential within each element is assumed to be a linear interpolation of the potentials at the nodes or vertices of each element The conjugate gradient solver determines the values of the node potentials which minimize the global energy function. 1\ IVrv'V 1\ 1\ rv:V J J 1\/1\ .J 1\/ 1\11\/1\/1\11\/1\/1\ 1\11\/[\ 1\1 IV V r--.. v 7' v [7 If\ If\ '\ '\ V\ V If\ J\ 1'\ 'w l/\ V 11'\ '\ 1/ :1'\ '\ '\ '\ V '\ 1'\"" '\ 11\ 1/\ \ '\ V Figure 2. The inner region of a fmite element grid constructed for simulat- ing in 2-dimensions the electric field generated by an electric fish. Measurement of Fish Electric Fields Another aspect of our experimental approach involves the direct measurement of the potential generated by a fish's EOD in a fish tank using arrays of small electrodes and differential amplifiers. The electrodes and electronics have a high impedance which min- imizes their influence on the electric fields they are designed to measure. The electrodes are made by pulling a 1mm glass capillary tube across a heated tungsten filament, result- ing in a fine tapered tip through which a 1~ silver wire is run. The end of this wire is melted in a flame leaving a 200J,un ball below the glass insulation. Several electrodes are then mounted as an array on a microdrive attached to a modified X-Yplotter under com- puter control and giving better than 1mm positioning accuracy. Generated potentials are amplified by a factor of 10 - 100, and digitized at a rate of 100kHz per channel with a 12 bit AID converter using a Masscomp 5700 computer. An array processor searches this 440 Rasnow, Assad, Nelson and Bower continuous stream of data for EOD wavefonns. which are extracted and saved along with the position of the electrode array. Results Calibration of the Simulator In order to have confidence in the overall system, it was fD'St necessary to calibrate both the recording and the simulation procedures. To do this we set up relatively simple geometrical arrangements of sources and conductors in a fish tank for which the potential could be found analytically. The calibration source was an electronic ""fake fish"" circuit that generated signals resembling the discharge of Gnathonemus. Point current source A point source in a 2-dimensional box is perhaps the simplest configuration with which to initially test our electric field reconstruction system. The analytic solution for the potential from a point current source centered in a grounded. conducting 2-dimen- sional box is: . (.n7t). (n7tx). h (.n7ty ) 00 sm(""2 sm L sm \L 4>(x. y) = L ri1t n =1 n L cosh(T) Our fmite element simulation. based on a regular 80 x 80 node grid differs from the above expression by less than 1 %. except in the elements adjacent to the source. where the potential change across these elements is large and is not as accurately reconstructed by a linear interpolation (Fig. 3). Smaller elements surrounding the source would im- prove the accuracy. however. one should note the analytic solution is infmite at the loca- tion of the ""point"" source whereas the measured and simulated sources (and real fish) have finite current densities. To measure the real equivalent of a point source in a 2-dimensional box. we used a linear current source (a wire) which ran the full depth of a real 3-dimensional tank. Measurements made in the midplane of the tank agree with the simulation and analytic solution to better than 5% (Fig. 3.). Uncertainty in the positions of the ClUTent source and recording sites relative to the position of the conducting walls probably accounts for much of this difference. Simulation and Measurement of the Weakly Electric Fish 441 1----~~--~--~----~------- o - measured x - simulated 00 2 4 6 8 10 12 14 16 dislaDce from source Figure 3. Electric potential of a point current source centered in a grounded 2-dimensional box. Measurements of Fish Fields and 2-Dimensional Simulations Calibration of our fmite element model of an electric fish requires direct measure- ments of the electric potential close to a discharging fish. Fig. 4 shows a recording of a single EOD sampled with 5 colinear electrodes near a restrained fish. The wavefonn is bipolar, with the fIrst phase positive if recorded near the animals head and negative if re- corded near the tail (relative to a remote reference). We used the peak amplitude of the larger second phase of the wavefonn to quantify the electric potential recorded at each location. Note that the potential reverses sign at a point approximately midway along the tail. This location corresponds to the location of the null potential shown in Fig. 5. 1500 1000 $' 5500 I 0 -1r- -soo -1000 o 200 ~sec Figure 4. EOD waveform sampled simultaneously from 5 electrodes. 442 Rasnow, Assad, Nelson and Bower Measurements of EODs from a restrained fish exhibited an extraordinarily small vari- ance in amplitude and waveform over long periods of time. In fact, the peak-peak ampli- tude of the EOD varied by less than 0.4% in a sample of 40 EOD's randomly chosen dur- ing a 30 minute period. Thus we are able to directly compare waveforms sampled se- quentially without renonnalizing for fluctuations in EOD amplitude. Figure 5 shows equipotential lines reconstructed from a set of 360 measurements made in the midplane of a restrained Gnathonemus. Although the observed potential re- sembles that from a purely dipolar source (Fig. 6), careful inspection reveals an asymme- try between the head and tail of the fISh. This asymmetry can be reproduced in our simu- lations by adjusting the electrical properties of the fish. Qualitatively, the measured fields can be reproduced by assigning a low impedance to the internal body cavity and a high impedance to the skin. However, in order to match the location of the null potential, the skin impedance must vary over the length of the body. We are currently quantifying these parameters, as described in the next section. !!!!m 1'!I!fl!IPf!~m II •• 1 ... ...... 1 ••• !~ ....... . Figure 5. Measured potentials (at peak of second phase of EOD) recorded from a restrained Gnathonemus petersii in the midplane of the fish. Equipotential lines are 20 m V apart. Inset shows relative location of fish and sampling points in the fISh tank. Figure 6. Equipotential lines from a 2-dimensional finite element simula- tion of a dipole using the grid of Fig. 2. The resistivity of the fish was set equal to that of the sWToundings in this simulation. Simulation and Measurement of the Weakly Electric Fish 443 Future Directions There is still a substantial amount of work that remains to be done before we achieve our goal of being able to fully reconstruct the pattern of electroreceptor activa- tion for any arbitrary body position in any particular environment. First. it is clear that we require more information about the electrical structure of the fISh itself. We need an accurate representation of the internal impedance distribution p(x) of the body and skin as well as of the source density f(x) of the electric organ. To some extent this can be ad- dressed as an inverse problem, namely given the measured potential cl>(x), what choice of p(x) and f(x) best reproduces the data. Unfortunately, in the absence of further con- straints, there are many equally valid solution, thus we will need to directly measure the skin and body impedance of the fish. Second, we need to extend our finite-element sim- ulations of the fish to 3-dimensions which, although conceptually straight forward, re- quires substantial technical developments to be able to (a) specify and visualize the space-filling set of 3-dimensional finite-elements (eg. tetrahedrons) for arbitrary configu- rations, (b) compute the solution to the much larger set of equations (typically a factor of 100-1(00) in a reasonable time, and (c) visualize and analyze the resulting solutions for the 3-dimensional electrical fields. As a possible solution to (b), we are developing and testing a parallel processor implementation of the simulator. References Bullock, T. H. & Heiligenberg, W. (Eds.) (1986). ""Electroreception"", Wiley & Sons, New York. Gibson, J. M. & Welker. W. I. (1983). Quantitative Studies of Stimulus Coding in First- Order Vibrissa Afferents of Rats. 1. Receptive Field Properties and Threshold Distributions. Somatosensory Res. 1:51-67. Hughes, T. J. (1987). The Finite Element Method: Linear Static and Dynamic Finite Element Analysis. Prentice-Hall, New Jersey. Lissmann. H.W. (1958). On the function and evolution of electric organs in fish. J. Exp. Bioi. 35:156-191. Toening, M. J. and Belbenoit. P. (1979). Motor Programmes and Electroreception in Monnyrid Fish. Behav. Ecol. Sociobiol. 4:369-379. Welker, W. I. (1964). Analysis of Sniffing of the Albino Rat Behaviour 22:223-244.","[-0.02451806329190731, 0.05599317327141762, 0.019651364535093307, -0.04766228422522545, -0.0006440066499635577, -0.08238308131694794, 0.011632177978754044, -0.0070169284008443356, 0.0023001807276159525, -0.022267378866672516, 0.01742655038833618, -0.15240508317947388, -0.07137980312108994, 0.017006507143378258, -0.01761353388428688, -0.061307791620492935, 0.01661645993590355, 0.035772405564785004, -0.055579472333192825, 0.06527233123779297, -0.004705599043518305, 0.021054139360785484, -0.018309449777007103, -0.07268040627241135, -0.06924200057983398, -0.025437701493501663, -0.05776891112327576, 0.009472227655351162, -0.04283561930060387, -0.07622719556093216, 0.01765650138258934, -0.039934951812028885, 0.0257160272449255, -0.024009451270103455, -0.025818441063165665, 0.014398511499166489, 0.055242713540792465, -0.05514848604798317, -0.009918833151459694, 0.09235165268182755, 0.031805939972400665, 0.02772134728729725, -0.0035533078480511904, 0.019397135823965073, 0.025389328598976135, 0.019308816641569138, 0.08643189072608948, -0.11987052112817764, -0.057702165096998215, -0.015426977537572384, -0.06000939756631851, -0.03160177916288376, 0.03427277132868767, 0.062278542667627335, 0.007686965633183718, 0.047255177050828934, -0.0013470478588715196, -0.0997631773352623, 0.012134670279920101, -0.09038980305194855, 0.0064821140840649605, 0.012162061408162117, 0.044886235147714615, -0.028951143845915794, -0.004928449168801308, -0.04913013428449631, 0.05776459723711014, -0.024853799492120743, 0.009911355562508106, -0.091115303337574, 0.04280421510338783, 0.01868920773267746, -0.0003213222371414304, -0.07850328087806702, -0.04527122154831886, 0.0389414057135582, -0.021210720762610435, 0.011628401465713978, 0.012625490315258503, -0.06258721649646759, 0.017323138192296028, -0.04967673867940903, -0.08908761292695999, -0.016767026856541634, 0.009803076274693012, 0.08566631376743317, 0.024635132402181625, 0.02956601232290268, -0.14986027777194977, 0.008720505982637405, 0.022918296977877617, -0.05814928188920021, -0.08582691103219986, -0.035501524806022644, 0.0020674250554293394, -0.03661999851465225, 0.07655621320009232, -0.013496591709554195, 0.04993867129087448, -0.0016672683414071798, 0.0172629002481699, -0.051687322556972504, 0.001019213581457734, 0.04413604363799095, 0.011565365828573704, -0.03159964084625244, 0.03512580320239067, 0.114957295358181, 0.06853833049535751, -0.04347464069724083, 0.01260798703879118, -0.01853102631866932, -0.04145453870296478, 0.08192592859268188, 0.030161157250404358, 0.11720326542854309, -0.11477980017662048, 0.026281453669071198, 0.14097334444522858, -0.022048577666282654, 0.03919953107833862, -0.07507770508527756, 0.003926796838641167, 0.037202902138233185, 0.08103243261575699, 0.03959694504737854, 0.04667971283197403, 2.4447389678974603e-33, -0.019279686734080315, -0.014156505465507507, -0.058502405881881714, -0.033867381513118744, 0.032165877521038055, -0.0359562523663044, -0.04036001116037369, 0.1261761337518692, 0.04108314588665962, 0.06933789700269699, -0.0030120816081762314, 0.11523351818323135, 0.054406676441431046, -0.01813642494380474, 0.05706414580345154, -0.0578916110098362, -0.035040732473134995, -0.04230985790491104, -0.03940131887793541, 0.017963647842407227, 0.10297145694494247, -0.06345733255147934, 0.0033471796195954084, -0.05033586546778679, -0.00882644858211279, 0.02673269994556904, -0.07654453068971634, -0.04306050017476082, -0.05431686341762543, 0.04994804412126541, -0.03842338174581528, 0.03395041078329086, 0.023331845179200172, 0.04326525703072548, 0.11151382327079773, 0.0058527663350105286, 0.09164945781230927, 0.020523883402347565, -0.05376425012946129, -0.016766797751188278, 0.04117894172668457, 0.009128565900027752, 0.05616400018334389, -0.03344534710049629, -0.004423632752150297, -0.07084182649850845, 0.042822785675525665, 0.09320679306983948, 0.07115485519170761, 0.06883840262889862, 0.04268792271614075, -0.05206459388136864, 0.0336868092417717, -0.0325106605887413, 0.046691905707120895, 0.11886727809906006, -0.10070174932479858, -0.009642666205763817, -0.03168434649705887, 0.0192818995565176, -0.031146405264735222, 0.06989806890487671, 0.02478143200278282, 0.00529085285961628, 0.047296732664108276, 0.035157687962055206, -0.0744357481598854, -0.05892910808324814, 0.043500546365976334, 0.017588108777999878, 0.004851567093282938, -0.019127925857901573, 0.00829752255231142, -0.08941277116537094, 0.010722954757511616, 0.05774719640612602, 0.012025045230984688, 0.04317134991288185, -0.05737321451306343, -0.023843029513955116, -0.007453043479472399, 0.05955183506011963, 0.002581084379926324, -0.0448458306491375, -0.0707135796546936, 0.025290189310908318, 0.09503649175167084, 0.024300159886479378, -0.03170882165431976, -0.0034501655027270317, 0.05538178235292435, 0.020184583961963654, 0.02428768388926983, -0.1220698356628418, 0.004358219914138317, -2.704066429671296e-33, 0.004500557668507099, 0.018331004306674004, -0.09015285968780518, 0.02948964573442936, 0.043071623891592026, 0.007604231126606464, 0.05262654274702072, 0.10494178533554077, 0.014337314292788506, -0.03543882444500923, -0.027360424399375916, 0.012719991616904736, -0.053013671189546585, 0.004964010324329138, 0.02961895242333412, -0.02628423273563385, -0.028503669425845146, -0.04357405751943588, 0.07275591790676117, -0.04347394034266472, 0.0010678964899852872, -0.006625200621783733, 0.05536224693059921, 0.04968280345201492, 0.01691175438463688, 0.07335338741540909, 0.006365921813994646, -0.04873378574848175, -0.03680773824453354, -0.0866350457072258, -0.07950339466333389, 0.0924551784992218, 0.024804910644888878, 0.024102186784148216, -0.0635717362165451, 0.016566628590226173, 0.024772806093096733, 0.08649863302707672, 0.011500365100800991, -0.06372197717428207, 0.0678359717130661, 0.04280482605099678, 0.011829174123704433, 0.03405284136533737, -0.03405104577541351, 0.07244890928268433, -0.03610822185873985, 0.0499068908393383, -0.024727895855903625, 0.048910606652498245, -0.0668000876903534, -0.015689093619585037, -0.0281489510089159, -0.05830519273877144, 0.027674976736307144, 0.0393013060092926, 0.02415260672569275, -0.0758642926812172, -0.01766095869243145, -0.03604445233941078, -0.03620860353112221, -0.048198677599430084, 0.046803075820207596, 0.05623982101678848, 0.002813887083902955, 0.017199497669935226, 0.0203342717140913, 0.0009909605141729116, 0.06210340932011604, -0.010571234859526157, 0.03326069191098213, 0.0626152902841568, -0.043765049427747726, -0.026721417903900146, -0.08414372056722641, -0.06541198492050171, -0.02627212554216385, -0.053905975073575974, 0.05859008803963661, -0.04483456537127495, 0.07329703122377396, 0.038301486521959305, -0.04167994111776352, -0.07915065437555313, 0.008805738762021065, 0.04242588207125664, -0.10057968646287918, 0.016514785587787628, -0.01540157850831747, -0.05984281003475189, 0.011399920098483562, 0.0460997000336647, -0.027085846289992332, 0.008032076060771942, 0.08775517344474792, -4.446594914497837e-08, 0.02154056541621685, 0.03246024623513222, 0.01341172307729721, 0.03471505269408226, 0.03564492240548134, -0.02393369749188423, 0.01717269793152809, -0.01761574111878872, 0.05353941768407822, -0.11786098778247833, 0.015695618465542793, -0.0247571412473917, 0.05504526197910309, -0.042382366955280304, 0.05748623609542847, 0.0706646665930748, 0.04284749925136566, -0.06955263763666153, -0.05636104196310043, 0.0507948063313961, -0.0006621287320740521, -0.042219605296850204, -0.018768003210425377, 0.03801421448588371, -0.009617948904633522, 0.04408654570579529, -0.02909301593899727, 0.009535904042422771, 0.08885783702135086, -0.029633192345499992, 0.01592547632753849, -0.01583719626069069, -0.009059209376573563, 0.05614893510937691, 0.021647775545716286, -0.020001471042633057, -0.10282710194587708, -0.018918581306934357, -0.051032088696956635, 0.019964896142482758, -0.08122696727514267, -0.028348518535494804, -0.05923096463084221, -0.027031321078538895, 0.020123962312936783, -0.024161389097571373, 0.02145027182996273, -0.0603070929646492, 0.0126713952049613, 0.07196678221225739, -0.06640762835741043, -0.04602352902293205, -0.008963748812675476, 0.020731361582875252, -0.0441826656460762, 0.08842740952968597, 0.01922784186899662, -0.057770226150751114, -0.08103001862764359, 0.031085625290870667, 0.04245627671480179, 0.10049297660589218, -0.09988880902528763, 0.01139324251562357]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AnAnalogVLSIChipforThinPlateSurfaceInterpolation.pdf,Computer Vision,"468 LEARNING THE SOLUTION TO THE APERTURE PROBLEM FOR PATTERN MOTION WITH A HEBB RULE Martin I. Sereno Cognitive Science C-015 University of California, San Diego La Jolla, CA 92093-0115 ABSTRACT The primate visual system learns to recognize the true direction of pattern motion using local detectors only capable of detecting the component of motion perpendicular to the orientation of the moving edge. A multilayer feedforward network model similar to Linsker's model was presented with input patterns each consisting of randomly oriented contours moving in a particular direction. Input layer units are granted component direction and speed tuning curves similar to those recorded from neurons in primate visual area VI that project to area MT. The network is trained on many such patterns until most weights saturate. A proportion of the units in the second layer solve the aperture problem (e.g., show the same direction-tuning curve peak to plaids as to gratings), resembling pattern-direction selective neurons, which ftrst appear inareaMT. INTRODUCTION Supervised learning schemes have been successfully used to learn a variety of input- output mappings. Explicit neuron-by-neuron error signals and the apparatus for propagating them across layers, however, are not realistic in a neurobiological context. On the other hand, there is ample evidence in real neural networks for conductances sensitive to correlation of pre- and post-synaptic activity, as well as multiple areas connected by topographic, somewhat divergent feedforward projections. The present project was to try to learn the solution to the aperture problem for pattern motion using a simple hebb rule and a layered feedforward network. Some of the connections responsible for the selectivity of cortical neurons to local stimulus features develop in the absence of pattered visual experience. For example, newborn cats and primates already have orientation-selective neurons in primary visual cortex (area 17 or VI), before they open their eyes. The prenatally generated orientation selectivity is sharpened by subsequent visual experience. Linsker (1986) Learning the Solution to the Aperture Problem 469 has shown that feedforward networks with somewhat divergent, topOgraphic interlayer connections, linear summation, and simple hebb rules develop units in tertiary and higher layers that have parallel, elongated excitatory and inhibitory sub fields when trained solely on random inputs to the frrst layer. By contrast, the development of the circuitry in secondary and tertirary visual cortical areas necessary for processing more complex, non-local features of visual arrays--e.g., orientation gradients, shape from shading, pattern translation, dilation, rotation--is probably much more dependent on patterned visual experience. Parietal direction noise horizontal direction noise 2-D rotation 3-D cylinder visual cortical areas, for example, are almost totally unresponsive in dark-reared monkeys, despite the fact that these monkeys have a normal- appearing VI (Hyvarinen, 1984). Behavioral indices suggest that development of some perceptual abilities may require months of experience. Human babies, for example, only evidence seeing the transition between randomly moving dots and circular 2-D motion at 6 months, while the transition from horizontally moving dots with random x -axis velocities to dots with sinusoidally varying X-axIS velocities (the latter gives the percept of a rotating 3-D cylinder) is only detected after 7 months (Spitz, Stiles-Davis, & Siegel, 1988) (see Fig. 1). Figure 1. Motion field transitions During the first 6 months of its life, a human baby typically makes approximately 30 million saccades, experiencing in the process many views which contain large moving fields and smaller moving objects. The importance of these millions of glances for the development of the ability to recognize complex visual objects has often been acknowledged. Brute visual experience may. however. be just as important in developing a solution to the simpler problem of detecting pattern motion using local cues. NETWORK ARCHITECTURE Moving visual stimuli are processed in several stages in the primate visual system. The first cortical stage is layer 4C-alpha of area VI, which receives its main ascending input from the magnocellular layers of the lateral geniculate nucleus. Layer 4C-alpha projects to layer 4B, which contains many tightly-tuned direction- selective neurons (Movshon et aI., 1985). These neurons, however, respond to 470 Sereno moving contours as if these contours were moving perpendicular their local orientation--Le .• they fire in proportion to the difference between the orthogonal component of motion and their best direction (for a bar). An orientation series run for a layer 4B nemon using a plaid (2 orthogonal moving gratings) thus results in two peaks in the direction tuning curve. displaced 45 degrees to either side of the peak for a single grating (Movshon et al.. 1985). The aperture problem for pattern motion (see e.g .• Horn & Schunck. 1981) thus exists for cells in area VI of the adult (and presumably infant) primate. Layer 4B neurons project topographically via direct and indirect pathways to area MT. a small exttastriate area specialized for processing moving stimuli. A subset Input Layer (=Vl, layer 4B) Second Layer (=MT) I \ i \ / \ / Figure 2. Network Architecture of neurons in MT show a single peak in their direction tuning curves for a plaid that is lined up with the peak for a single grating--Le., they fire in proportion to the difference between the true pattern direction and their best direction (for a bar). These neurons therefore solve the aperture problem presented to them by the local translational motion detectors in layer 4B of VI. The excitatory receptive fields of all MT neurons are much larger than those in VI as a result of divergence in the VI- MT projection as well as the smaller areal extent of MT compared to VI. M.E. Sereno (1987) showed using a supervised learning rule that a linear t two layer network can satisfactorily solve the aperture problem characterized above. The present task was to see if unsupervised learning might suffice. A simple caraicature of the Vl-to-MT projection was constructed. At each x-y location in the first layer of the network. there are a set of units tuned to a range of local directions and speeds. The input layer thus has four dimensions. The sample network illustrated above (Fig. 2) has 5 different directions and 3 speeds at each x-y location. Input units are granted tuning curves resembling those found for neurons in layer 4B of Learning the Solution to the Aperture Problem 471 ~:~ o X 2X velocity component orthogonal to contour =pl L>\t)(\ o I I o 0.5 speed component orthogonal to contour Figure 3. Excitatory Tuning Curves (1st layer) 1 area VI. The tuning curves are linear. with half-height overlap for both direction and speed (see Fig. 3--for 12 directions and 4 speeds). and direction and speed tuning interact linearly. Inhibition is either tuned or untuned (see Fig. 4). and scaled to balance excitation. Since direction tuning wraps around. there is a trough in the tuned inhibition condition. Speed tuning does not wrap around. The relative effect of direction and speed tuning in the output of ftrst layer units is set by a parameter. As with Linsker. the probability that a unit in the fust layer will connect with a unit in the second layer falls off as a gaussian centered on the retinotopically resp o resp o untuned 1\ tuned ~[""""""""""""'"" ~""""'"""""""""". \,. u: ........................ . o X velocity component orthogonal to contour ............... ........................ o 0.5 speed component orthogonal to contour (no wrap-around) 2X 1 Figure 4. Tuned vs. Untuned Inhibition 472 Sereno equivalent point in the second layer (see Fig. 2). New random numbers are drawn to generate the divergent gaussian projection pattern for each first layer unit (Le., all of the units at a single x-y location have different. overlapping projection patterns). There are no local connections within a layer. The network update rule is similar to that of Linsker except that there is no term like a decay of synaptic weights (k1) and no offset parameter for the correlations (k,J. Also, all of the units in each layer are modeled explicitly. The activation, Yj' for each unit is a linear weighted sum of its Ui inputs, scaled by a, and clipped to a maximum or minimum value: { a I. u· w·· I I) y. = ) Ymax.min Weights are also clipped to maximum and minimum values. The change in each weight. .1wij, is a simple fraction, a, of the product of the pre- and post-synaptic values: .1w·· = au·y· I) I ) RESULTS The network is tr:ained with a set of fullfield texture movements. Each stimulus consists of a set of randomly oriented contours--one at each x-y point--all moving in the same, randomly chosen pattern direction. A typical stimulus is drawn in figure 5 as the set of component motions visible to neurons in VI (i.e .• direction components perpendicular to the local contour); the local speed component varies as the cosine of the angle between the pattern direction and the perpendicular to the local contour. The single component motion at each point is run through the first layer tuning curves. The response of the input layer to such a pattern is shown in Figure 6. Each rectangular box represents a single x-y location, containing 48 units -- ./' -- ........ ~ '- ........ v t- v , .... ....... "" ~ ~ I' "" ~ "" ~ t- ...... .-. '. ? '"" .-. -+ ...... ? -- I' /' -.. , ~ ...... ""- ""- ....... .-. ....... .. '- --+ ./' I' ~ -- ~ --+ "" 1 -.. ""- ""- I' '- , ./' , A ""- ""- ...... II' "" -+ /' ""- v 1 ., .-. "" ~ ""- A .- /' ~ ./' -+ Figure 5. Local Compqnent Motions from a Training Stimulus (pattern direction is toward right) .. -. f ., , /' Learning the Solution to the Aperture Problem 473 p. . . . • • • . [J • • • o • • • • • ••• 0 • •• • • • . . . • • • [J D· •• . . . . · Q. • •••• c •••••• ·0····· ·00·· . . . . . . . • • • • • • • 0 • • • • · .. ·0· · . 00 .. • • • • • a a • • • · . . . . • 0 a • · . . . · ... 00· .. -0-.· -ac- .... ·Da .•••• aa •••••• -0- •••••• 0 ••••••••••••• 0 .••••••• aa ••••. ••••• · . . . . • • • • • • • • • • • • • 0 • • • • • • • • D • • • • • • • • • • a a • • • • • • • • J • • • • • • .•. 00· .• • • •• C ••• · 00 · · · · • ., • • • • • • • • • • C • • • • • ~ • • •••••••• 0 •• 0-. • • • •• • • •• • • • c·· • • • • 0 • • • • • • • • • • • • • • 0 • • • • • • • • • • • • • • 0 • • • • • • • • • • • ••. 00 . · ... aD· . . . . . . . ·0· . . · ald· • 0 . · · • • • • • • • • • • C • • • • • • • • • • • D a • • • • • . 0 a . . .•• • . a 0 ••••• Figure 6. Output of Portion of First Layer to a Training Slimulus (untuned inhibition) tuned to different combinations of direction and speed (12 directions run horizontally and 4 speeds run vertically). Open and filled squares indicate positive and negative outputs. Inhibition is untuned here. The hebb sensitivity, a, was set so that 1,000 such patterns could be presented before most weights saturated at maximum values. Weights intially had small random values drawn from a flat distribution centered around zero. The scale parameter for the weighted sum, a, was set low enough to prevent second layer units from saturating all the time. In Figure 6, direction tuning is 2.5 times as important as speed tuning in detennining the output of a unit Selectivity of second layer units for pattern direction was examined both before and after training using four stimulus conditions: 1) grating--contours perpendicular to pattern direction, 2) random grating--contours randomly oriented with respect to pattern direction (same as the training condition), 3) plaid--contours oriented 45 or 67 degrees from perpendicular to pattern direction, 4) random plaid--contours randomly oriented, but avoiding angles nearly perpendicular to pattern direction. The pre-training direction tuning curves for the grating conditions usually showed some weak direction selectivity. Pre-training direction tuning curves for the plaid conditions, however, were often twin-peaked, exhibiting pattern component responses displaced to either side of the grating peak. Mter training, by contrast, the direction tuning peaks in all test conditions were single and sharp, and the plaid condition peaks were usually aligned with the grating peaks. An example of the weights onto a mature pattern direction selective unit is shown in Figure 7. As before, each rectangular box contains 48 units representing one point in x-y space of the input layer (the tails of the 2-D gaussian are cropped in this illustration), except that the black and white boxes now represent negative and positive weights onto a single second layer unit. Within each box, 12 directions run horizontally and 4 speeds run vertically. The peaks in the direction tuning curves for gratings and 135 degree plaids for this unit were sharp and aligned. 474 Sereno .. :: :: :: "" ...... .. .. .. .. .. ...... .. .. .. :: . :: :: Figure 7. Mature Weights Onto Pattern Direction-Selective Unit Pattern direction selective units such as this comprised a significant fraction of the second layer when direction tuning was set to be 2 to 4 times as important as speed tuning in determining the output of fU'St layer units. Post-training weight structures under these conditions actually formed a continuum--from units with component direction selectivity, to units with pattern direction selectivity, to units with component speed selectivity. Not surprisingly, varying the relative effects of direction and speed in the VI tuning curves generated more direction-tuned-only or speed-tuned-only units. In all conditions, units showed clear boundaries between maximum and minimum weights in the direction-speed subspace each x-y point, and a single best direction. The location of these boundaries was always correlated across different x-y input points. Most units showing unambiguous pattern direction selectivity were characterized by two oppositely sloping diagonal boundaries between maximum and minimum weights in direction-speed subspace (see e.g., Fig. 7). The stimuli used to train the network above--fullfield movrnents of a rigid texture field of randomly oriented contours--are unnatural; generally, there may be one or more objects in the field moving in different directions and at different speeds than the surround. Weight distributions needed to solve the aperture problem appear when the network. is trained on occluding moving objects against moving backgrounds (object and background velocities chosen randomly on each trial), as long as the object is made small or large relative to the receptive field size. The solution breaks down when the moving objects occupy a significant fraction of the area of a second layer receptive field. Learning the Solution to the Aperture Problem 475 For comparison, the network was also trained using two different kinds of noise stimuli. In the fIrst condition (unit noise), each new stimulus consisted of random input values on each input unit With other network parameters held the same, the typical mature weight pattern onto a second layer unit showed an intimate intermixture of maximum and minimum weights in the direction-speed subspace at each x-y location. In the second condition (direction noise), each new stimulus consisted of a random direction at each x-y location. The mature weight patterns now showed continuous regions of all-maximum or all-minimum weights in the speed-direction supspace at each x-y point. In contrast to the situation with fullfieid texture movement stimuli, however, the best directions at each of the x-y points providing input to a given unit were uncorrelated. In addition, multiple best directions at a single x-y point sometimes appeared. DISCUSSION This simple model suggests that it may be possible to learn the solution to the aperture problem for pattern motion using only biologically realistic unsupervised learning and minimally structured motion fields. Using a similar network architecture, M.E. Sereno had previously shown that supervised learning on the problem of detecting pattern motion direction from local cues leads to the emergence of chevron shaped weight structures in direction-speed space (M.E. Sereno, 1986). The weight structures generated here are similar except that the inside or outside of the chevron is filled in, and upside-down chevrons are more common. This results in decreased selectivity to pattern speed in the second layer. The model needs to be extended to more complex motion correlations in the input-- e.g., rotation, dilation, shear, multiple objects, flexible objects. MT in primates does not respond selectively to rotation or dilation, while its target area MST does. Thus, biological estimates of rotation and dilation are made in two stages--rotation and dilation are not detected locally, but instead constructed from estimates of local translation. Higher layers in the present model may be able to learn interesting 'second-order' things about rotation, dilation, segmentation, and transparency. The real primate visual system, of course, has a great many more parts than this model. There are a large number of interconnected cortical visual areas--perbaps as many as 25. A substantial portion of the 600 possible between-area connections may be present (for review, see M.I. Sereno, 1988). There are at least 6 map-like visual structures, and several more non-retinotopic visual structures in the thalamus (beyond the dLGN) that interconnect with the cortical visual areas. Each visual cortical area then has its own set of layers and intedayer connections. The most unbiological aspect of this model is the lack of time and the crude methods of gain control (clipped synaptic weights and input/output functions). Future models should employ within-area connections and time-dependent hebb rules. Making a biologically realistic model of intermediate and higher level visual processing is difficult since it ostensibly requires making a biologically realistic model of earlier, yet often not less complex stations in the system--e.g., the retina, 476 Sereno dLGN, and layer 4C of primary visual cortex in the present case. One way to avoid having to model all of the stations up to the one of interest is to use physiological data about how the earlier stations respond to various stimuli, as was done in the present model. This shortcut is applicable to many other problems in modeling the visual system. In order for this to be most effective, physiologists and modelers need to cooperate in generating useful libraries of response profiles to arbitrary stimuli. Many stimulus parameters interact, often nonlinearly, to produce the final output of a cell. In the case of simple moving stimuli in VI and MT, we minimally need to know the interaction between stimulus size, stimulus speed, stimulus direction, surround speed, surround direction, and x-y starting point of the movement relative to the classical excitatory receptive field. Collecting this many response combinations from single cells requires faster serial presentation of stimuli is customary in visual physiology experiments. There is no obvious reason, however, why the rate of stimulus presentation need be any less than the rate at which the visual system nonnally operates--namely, 3-5 new views per second. Also, we need to get a better understanding of the 'stimulus set'. The very large set of stimuli on which the real visual system is trained (millions of views) is still very poorly characterized. It would be worthwhile and practical, nevertheless, to collect a naturalistic corpus of perhaps 1000 views (several hours of viewing). Acknowledgements I thank M.E. Sereno and U. Wehmeier for discussions and comments. Supported by NIH grant F32 EY05887. Networks and displays were constructed on the Rochester Connectionist Simulator. References B.K.P. Hom & B.G. Schunck. Determining optical flow. Artiflntell., 17, 185-203 (1981). J. Hyvarinen. The Parietal Cortex. Springer-Verlag (1984). R. Linsker. From basic network principles to neural architecture: emergence of orientation-selective cells. Proc. Nat. Acad. Sci. 83,8390-8394 (1986). J.A. Movshon, E.H. Adelson, M.S. Gizzi & W.T. Newsome. Analysis of moving visual patterns. In Pattern Recognition Mechanisms. Springer-Verlag, pp. 117- 151 (1985). M.E. Sereno. Modeling stages of motion processing in neural networks. Proc. 9th Ann. Con! Cog. Sci. Soc. pp. 405416 (1987). M.I. Sereno. The visual system. In I.W.v. Seelen, U.M. Leinhos, & G. Shaw (eds.), Organization of Neural Networks. VCH, pp. 176-184 (1988). R.V. Spitz, J. Stiles-Daves & R.M. Siegel. Infant perception of rotation from rigid structure-from-motion displays. Neurosci. Abstr. 14, 1244 (1988).","[0.001791353221051395, -0.11062062531709671, 0.061671581119298935, -0.049435749650001526, 0.022789224982261658, 0.029189910739660263, 0.022223155945539474, -0.06606823205947876, 0.019732963293790817, 0.04168064147233963, -0.03387758135795593, -0.03414681553840637, 0.04971478879451752, 0.06777726113796234, -0.0663733258843422, -0.04729151353240013, -0.010884277522563934, 0.1042109951376915, 0.0017720788018777966, -0.013501354493200779, 0.06414337456226349, -0.0091860955581069, -0.05361993983387947, 0.0049017854034900665, -0.02776562236249447, 0.05113191157579422, 0.007122345268726349, -0.013398276641964912, 0.01895301043987274, -0.0581517294049263, 0.07225852459669113, -0.03714951127767563, -0.06937434524297714, 0.06939608603715897, -0.1036001443862915, -0.02302989549934864, -0.04112623259425163, 0.007036647759377956, -0.03769509494304657, 0.022682631388306618, 0.004434429109096527, 0.03979058936238289, -0.029917310923337936, 0.004840787034481764, 0.1262371689081192, 0.10072996467351913, 0.005565399769693613, -0.02652830071747303, -0.06939735263586044, 0.019154153764247894, -0.059345461428165436, -0.06044955551624298, -0.027384644374251366, 0.06989648193120956, 0.06210850551724434, 0.0693434402346611, 0.033530186861753464, 0.00728279072791338, 0.009954552166163921, 0.025294972583651543, 0.03526679053902626, -0.0924096554517746, -0.03974118083715439, -0.07930834591388702, -0.0003638150228653103, 0.008392715826630592, -0.027870604768395424, -0.013174107298254967, 0.029835417866706848, -0.041595831513404846, 0.04004271700978279, 0.044774334877729416, -0.00028557071345858276, -0.005828940775245428, 0.021019378677010536, -0.001590014435350895, 0.04925156012177467, 0.09278128296136856, 0.032578807324171066, -0.11704184114933014, -0.015589207410812378, 0.005431600380688906, 0.043131448328495026, 0.02816144935786724, 0.1024257242679596, 0.006308624986559153, -0.1157236099243164, 0.0750802606344223, 0.024476323276758194, 0.010783674195408821, -0.009885049425065517, -0.03618106618523598, -0.09645088016986847, -0.11309932172298431, 0.026899538934230804, -0.021938692778348923, -0.008054121397435665, -0.03679802641272545, -0.0415801964700222, 0.034132570028305054, -0.011766482144594193, -0.05129425972700119, -0.017528148368000984, 0.022262662649154663, 0.08916991204023361, 0.027690734714269638, 0.040691543370485306, 0.09117510914802551, 0.08843415975570679, -0.0824505090713501, -0.0844133049249649, 0.010703046806156635, -0.044993091374635696, 0.02325744740664959, 0.014654314145445824, -0.06925904005765915, -0.006255663000047207, 0.008080308325588703, 0.027609432116150856, 0.056197863072156906, -0.04654167219996452, 0.03918850049376488, -0.06503218412399292, 0.008180951699614525, 0.07860176265239716, -0.057837486267089844, -0.04891641065478325, 4.0964475697439575e-33, -0.020597266033291817, 0.022252436727285385, -0.00702294334769249, -0.07417749613523483, -0.028876520693302155, 0.02300027385354042, -0.003114312421530485, 0.02708256058394909, 0.1162787452340126, 0.04517124593257904, -0.07940388470888138, 0.012644598260521889, -0.027324793860316277, 0.1271868348121643, 0.061865486204624176, -0.05698247253894806, 0.024529073387384415, -0.018826942890882492, -0.02370709367096424, -0.025159867480397224, -0.007863339968025684, -0.012306745164096355, 0.052884336560964584, -0.0810949057340622, -0.008912315592169762, 0.025648120790719986, -0.02706594206392765, -0.021083982661366463, -0.005472284276038408, 0.02305571548640728, 0.019622651860117912, 0.020746013149619102, 0.002664340427145362, -0.061220452189445496, -0.001864964491687715, -0.02603641338646412, 0.0647474154829979, 0.024904407560825348, 0.039704449474811554, -0.0067751966416835785, 0.020193764939904213, 0.01959654502570629, 0.0021276038605719805, -0.016261033713817596, -0.05404452607035637, 0.058039091527462006, 0.02698567323386669, 0.09446592628955841, -0.0477357916533947, -0.012339696288108826, 0.01482508797198534, -0.012530364096164703, -0.03181431069970131, -0.09313996136188507, 0.07690626382827759, 0.09137207269668579, -0.01831294409930706, 0.05105873942375183, 0.01222650334239006, 0.03366802632808685, 0.02604813314974308, 0.014841080643236637, 0.012930631637573242, 0.07700365036725998, 0.040959492325782776, -0.04024377092719078, -0.01267210952937603, 0.0377173013985157, 0.020336678251624107, -0.020996276289224625, -0.0006273332401178777, 0.08902226388454437, -0.03131904453039169, -0.06185172125697136, 0.052104391157627106, -0.04677436873316765, -0.04173816367983818, 0.009129596874117851, -0.0627819076180458, -0.020165814086794853, -0.07225892692804337, -0.0012520591262727976, -0.05858051776885986, -0.06814032047986984, -0.022738363593816757, 0.10217868536710739, 0.10787840932607651, -0.012128946371376514, -0.034294404089450836, -0.035639047622680664, 0.01484464667737484, -0.019450126215815544, 0.01116105169057846, -0.0045045348815619946, -0.02338421531021595, -4.444551854640746e-33, -0.00824347697198391, 0.05854208394885063, -0.04929709807038307, 0.002644134918227792, -0.0434466116130352, 0.020643731579184532, -0.01881473883986473, 0.011706201359629631, -0.049930389970541, -0.014412971213459969, -0.007619528099894524, 0.00895767007023096, -0.028222789987921715, 0.020996367558836937, -0.06329336017370224, 0.00013446072989609092, -0.0867626890540123, 0.03599027544260025, 0.11585086584091187, -0.07995827496051788, -0.06197551265358925, 0.07987561076879501, -0.08874085545539856, -0.010232366621494293, -0.042178649455308914, 0.04782016575336456, 0.005614768713712692, 0.15755368769168854, -0.07726846635341644, 0.07658874243497849, -0.06445372849702835, 0.0037159870844334364, 0.027380112558603287, -0.06139640882611275, -0.018026215955615044, 0.05981048196554184, 0.011393269523978233, -0.05802767351269722, -0.0546906441450119, -0.0055305673740804195, 0.007883689366281033, 0.049416422843933105, 0.016055654734373093, -0.006320981774479151, -0.0018462743610143661, -0.053616784512996674, -0.03212873265147209, 0.10776582360267639, -0.06167730316519737, 0.0519506074488163, -0.04339192062616348, -0.02504950761795044, -0.0993640199303627, -0.06505187600851059, -0.0626949816942215, 0.08936569094657898, -0.053981147706508636, 0.009790774434804916, 0.06332091242074966, -0.030207015573978424, -0.04253251850605011, -0.030070433393120766, -0.03675517439842224, -0.049163684248924255, 0.0450308695435524, 0.042654912918806076, -5.197558130021207e-05, -0.017716899514198303, 0.14859454333782196, 0.01871379464864731, 0.011127053759992123, 0.017566539347171783, 0.06620165705680847, -0.0046646264381706715, 0.035532981157302856, -0.01499030739068985, 0.028113270178437233, 0.025468412786722183, -0.054492514580488205, -0.02241882123053074, -0.03277532383799553, -0.033061329275369644, 0.020674223080277443, 0.09714136272668839, 0.05918938294053078, 0.05630256608128548, -0.011940163560211658, -0.02249113842844963, 0.05055456608533859, 0.004096620250493288, 0.009854977019131184, 0.06636010855436325, 0.0004987658467143774, -0.029120653867721558, 0.0019214691128581762, -5.422508664310044e-08, -0.1325087994337082, -0.0056649767793715, 0.03877587243914604, -0.05234171450138092, 0.08392707258462906, -0.001886557205580175, 0.05019472911953926, 0.0252433679997921, -0.020451612770557404, -0.088275246322155, 0.04356776550412178, 0.02236960642039776, -0.06696270406246185, -0.018929405137896538, 0.04800978675484657, 0.01570866070687771, 0.04702772572636604, 0.01742006093263626, -0.034927964210510254, 0.04067251458764076, 0.031907208263874054, -0.032338567078113556, 0.021383751183748245, 0.02139282040297985, 0.05245731770992279, -0.07248373329639435, -0.08574103564023972, 0.08583664149045944, -0.0170966237783432, 0.07897991687059402, 0.02441856823861599, -0.007533933036029339, 0.11767208576202393, -0.018604204058647156, 0.06625444442033768, 0.042393047362565994, 0.012435087002813816, -0.029002880677580833, 0.0230039581656456, -0.005853656213730574, -0.0567253977060318, 0.042064350098371506, 0.024834275245666504, -0.02596198581159115, -0.011330639012157917, -0.016296330839395523, 0.08134711533784866, -0.11242019385099411, -0.015474209561944008, -0.03359384462237358, -0.026783455163240433, 0.014577854424715042, -0.031318943947553635, 0.11121103167533875, 0.029095252975821495, -0.042002540081739426, 0.002849193988367915, -0.1584043651819229, -0.029559843242168427, 0.06032871454954147, -0.0217728354036808, 0.05416198447346687, -0.028017591685056686, -0.06286393105983734]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AnElectronicPhotoreceptorSensitivetoSmallChangesinIntensity.pdf,Deep Learning,"340 BACKPROPAGATION AND ITS APPLICATION TO HANDWRITTEN SIGNATURE VERIFICATION Dorothy A. Mighell Electrical Eng. Dept. Info. Systems Lab Stanford University Stanford, CA 94305 Timothy S. Wilkinson Electrical Eng. Dept. Info. Systems Lab Stanford University Stanford, CA 94305 ABSTRACT Joseph W. Goodman Electrical Eng. Dept. Info. Systems Lab Stanford University Stanford, CA 94305 A pool of handwritten signatures is used to train a neural net- work for the task of deciding whether or not a given signature is a forgery. The network is a feedforward net, with a binary image as input. There is a hidden layer, with a single unit output layer. The weights are adjusted according to the backpropagation algorithm. The signatures are entered into a C software program through the use of a Datacopy Electronic Digitizing Camera. The binary signa- tures are normalized and centered. The performance is examined as a function of the training set and network structure. The best scores are on the order of 2% true signature rejection with 2-4% false signature acceptance. INTRODUCTION Signatures are used everyday to authorize the transfer of funds for millions of people. We use our signature as a form of identity, consent, and authorization. Bank checks, credit cards, legal documents and waivers all require the everchanging personalized signature. Forgeries on such transactions amount to millions of dollars lost each year. A trained eye can spot most forgeries, but it is not cost effective to handcheck all signatures due to the massive number of daily transactions. Consequently, only disputed claims and checks written for large amounts are verified. The consumer would certainly benefit from the added protection of automated verification. Neural networks lend themselves very well to signature verification. Previously, they have proven applicable to other signal processing tasks, such as character recognition {Fukishima, 1986} {Jackel, 1988}, sonar target classification {Gorman, 1986}, and control- as in the broom balancer {Tolat, 1988}. HANDWRITING ANALYSIS Signature verification is only one aspect of the study of handwriting analysis. Recognition is the objective, whether it be of the writer or the characters. Writer recognition can be further broken down into identification and verification. Identi- Backpropagation and Handwritten Signature Verification 341 fication selects the author of a sample from among a group of writers. Verification confirms or rejects a written sample for a single author. In both cases, it is the style of writing that is important. Deciphering written text is the basis of character recognition. In this task, linguistic information such as the individual characters or words are extracted from the text. Style must be eliminated to get at the content. A very important application of character recognition is automated reading of zip-codes in the post office {Jackel, 1988}. Data for handwriting analysis may be either dynamic or static. Dynamic data requires special devices for capturing the temporal characteristics of the sample. Features such as pressure, velocity, and position are examined in the dynamic framework. Such analysis is usually performed on-line in real time. Static analysis uses the final trace of the writing, as it appears on paper. Static analysis does not require any special processing devices while the signature is being produced. Centralized verification becomes possible, and the processing may be done off-line. Work has been done in both static and dynamic analysis {Sato, 1982} {Nemcek, 1974}. Generally, signature verification efforts have been more successful using the dynamic information. It would be extremely useful though, to perform the verification using only the written signature. This would eliminate the need for costly machinery at every place of business. Personal checks may also be verified through a static signature analysis. TASK The handwriting analysis task with which this paper is concerned is that of signa- ture verification using an off-line method to detect casual forgeries. Casual forgeries are non-professional forgeries, in which the writer does not practice reproducing the signature. The writer may not even have a copy of the true signature. Casual forgeries are very important to detect. They are far more abundant, and involve greater monetary losses than professional forgeries. This signature verification task falls into the writer recognition category, in which the style of writing is the im- portant variable. The off-line analysis allows centralized verification at a lower cost and broader use. HANDWRITTEN SIGNATURES The signatures for this project were gathered from individuals to produce a pool of 80 true signatures and 66 forgeries. These are signatures, true and false, for one person. There is a further collection of signatures, both true and false, for other persons, but the majority of the results presented will be for the one individual. It will be clear when other individuals are included in the demonstration. The signatures are collected on 3x5 index cards which have a small blue box as 342 Wilkinson, Mighell and Goodman a guideline. The cards are scanned with a CCD array camera from Datacopy, and thresholded to produce binary images. These binary images are centered and normalized to fit into a 128x64 matrix. Either the entire 128x64 image is presented as input, or a 90x64 image of the three initials alone is presented. It is also possible to present preprocessed inputs to the network. SOFTWARE SIMULATION The type of learning algorithm employed is that of backpropagation. Both dwell and momentum are included. Dwell is the type of scheduling employed, in which an image is presented to the network, and the network is allowed to ""dwell"" on that input for a few iterations while updating its weights. C. Rosenberg and T. Sejnowski have done a few studies on the effects of scheduling on learning {Rosenberg, 1986}. Momentum is a term included in the change of weights equation to speed up learning {Rumelhart, 1986}. The software is written in Microsoft C, and run on an IBM PC/AT with an 80287 math co-processor chip. Included in the simulation is a piece-wise linear approximation to the sigmoid trans- fer function as shown in Figure 1. This greatly improves the speed of calculation, because an exponential is not calculated. The non-linearity is kept to allow for layering of the network. Most of the details of initialization and update are the same as that reported in NetTalk {Sejnowski, 1986}. OUT ~-111111::::+~----'. IN Figure 1. Piece-wise linear transfer function. Many different nets were trained in this signature verification project, all of which were feed-forward. The output layer most often consisted of a single output neuron, but 5 output neurons have been used as well. If a hidden layer was used, then the number of hidden units ranged from 2 to 53. The networks were both fully- connected and partially-connected. SAMPLE RUN The simplest network is that of a single neuron taking all 128x64 pixels as input, plus one bias. Each pixel has a weight associated with it, so that the total number of weights is 128x64 + 1 = 8193. Each white pixel is assigned an input value of + 1, each black pixel has a value of -1. The training set consists of 10 true signatures Backpropagation and Handwritten Signature Verification 343 with 10 forgeries. Figure 2a depicts the network structure of this sample run. OUT - 1 c:: 0 - u CD - CD "" 0.5 f-.. CD :J ""--- Q. 0 0 0.5 P(false acceptance) ~1~111J 111. ""~~~mlla (b) 1 1/ (a) ~ en LL. C 0.5 0 f 0 0 0.5 (e) Output Values (d) Figure 2. Sample run. a) Network = one output neuron, one weight per pixel, fully con- nected. Training set = 10 true signatures + 10 forgeries. b) ROC plot for the sample run. (Probability of fa1se acceptance vs probability of true detection). Test set = 70 true signatures + 56 forgeries. c) Clipped picture of the weights for the sample run. White = positive weight, black = negative weight. d) Cumulative distribution function for the true signatures (+) and for the forgeries (0) of the sample run. - 1 1 The network is trained on these 20 signatures until all signatures are classified 344 Wilkinson, Mighell and Goodman correctly. The trained network is then tested on the remaining 70 true signatures and 56 forgeries. The results are depicted in Figures 2b and 2d. Figure 2b is a radar operating characteristic curve, or roc plot for short. In this presentation of data, the proba- bility of detecting a true signature is plotted against the probability of accepting a forgery. Roc plots have been used for some time in the radar sciences as a means for visualizing performance {Marcum, 1960}. A perfect roc plot has a right angle in the upper left-hand corner which would show perfect separation of true signa- tures from forgeries. The curve is plotted by varying the threshold for classification. Everything above the threshold is labeled a true signature, everything below the threshold is labeled a forgery. The roc plot in Figure 2b is close to perfect, but there is some overlap in the output values of the true signatures and forgeries. The overlap can be seen in the cumulative distribution functions (cdfs) for the true and false signatures as shown in Figure 2d. As seen in the cdfs, there is fairly good separation of the output values. For a given threshold of 0.5, the network produces 1% rejection of true signatures as false, with 4% acceptance of forgeries as being true. IT one lowers the threshold for classification down to 0.43, the true rejection becomes nil, with a false acceptance of 7% . A simplified picture of the weights is shown in Figure 2c, with white pixels designating positive weights, and black pixels negative weights. OTHER NETWORKS The sample run above was expanded to include 2 and 3 hidden neurons with the single output neuron. The results were similar to the single unit network, implying that the separation is linear. The 128x64 input image was also divided into regions, with each region feeding into a single neuron. In one network structure, the input was sectioned into 32 equally sized regions of 16x16 pixels. The hidden layer thus has 32 neurons, each neuron receiving 16x16 + 1 inputs. The output neuron had 33 inputs. Likewise, the input image was divided into 53 regions of 16x16 pixels, this time overlapping. Finally, only the initials were presented to the network. (Handwriting experts have noted that leading strokes and separate capital letters are very significant in classification {Osborn, 1929}.) In this case, two types of networks were devised. The first had a single output neuron, the second had three hidden neurons plus one output neuron. Each of the hidden neurons received inputs from only one initial, rather than from all three. The network with the single output neuron produced the best results of all, with 2% true rejection and 2% false acceptance. IMPORTANCE OF FORGERIES IN THE TRAINING SET In all cases, the networks performed much better when forgeries were included in the training set. When an all-white image is presented as the only forgery, performance deteriorates significantly. When no forgeries are present, the network decides that Backpropagation and Handwritten Signature Verification 345 all signatures are true signatures. It is therefore desirable to include actual forgeries in the training set, yet they may be impractical to obtain. One possibility for avoiding·the collection of forgeries is to use computer generated forgeries. Another is to distort the true signatures. A third is to use true signatures of other people as forgeries for the person in question. The attraction of this last option is that the masquerading forgeries are already available for use. NETWORK WITHOUT FORGERIES To test the use of true signatures of other people for forgeries, the following network is devised. Once again, the input is the 128x64 pixel image. The output layer is comprised of five output neurons fully connected to the input image. The function of each output neuron is to be active when presented with a particular persons' signature. When a forgery is present, the output is to be low. Figure 3a depicts this network. The training set has 50 true signatures, ten for each of five people. Each signature has a desired output of true for one neuron, and false for the remaining four neurons. Once the network is trained, it is tested on 210 true signatures and 150 forgeries. Figures 3b and 3c record the results. At a threshold of 0.5, the true rejection is 3% and the false acceptance is 14%. Decreasing the threshold down to 0.41 gives 0% true rejection and 28% false acceptance. These results are similar to the sample run, though not as good. This is a simple demonstration of the use of other true signatures as forgeries. More sophisticated techniques could improve the discrimination. For instance, selecting names with similar lengths or spelling should improve the classification. CONCLUSION Automated signature verification systems would be extremely important in the business world for verifying monetary transactions. Countless dollars are lost each day to instances of casual forgeries. An artificial neural network employing the backpropagation learning algorithm has been trained on both true and false signa- tures for classification. The results have been very good: 2% rejection of genuine signatures with 2% acceptance of forgeries. The analysis requires only the static picture of the signature, there by offering widespread use through centralized ver- ification. True signatures of other people may substitute for the forgeries in the training set - eliminating the need for collecting non-genuine signatures. 346 Wilkinson, Mighell and Goodman JWG JTH TSW LDK ABH (a) - C 1 r--::iif1l---------.. lr-----------~~~--_=~ o - (,) CD - Q) ""t:S U.5 Q) ::J .. -- (f. 00.5 o I ~ ~ o~----------~--------~ o~~----~~~--------~ o 0.5 1 P(false acceptance) (b) o 0.5 Output Values (c) Figure 3. Network without forgeries for 5 individuals. a) Network = 5 output neurons, one for each individua~ as indi- cated by the initials. Training set = 10 true signatures for each individual. b) ROC plot for the network without forgeries. Test set = 210 true signatures + 150 forgeries. c) Cumulative distribution function for the true signatures (+) and for the forgeries (0) of the network without forgeries. Referenees 1 K. Fukishima and S. Miyake, ""Neocognitron: A biocybernetic approach to visual pattern recognitionJt , in NHK Laboratorie~ Note, Vol. 336, Sep 1986 (NHK Science and Technical Research Laboratories, Tokyo). Backpropagation and Handwritten Signature Verification 347 P. Gorman and T. J. Sejnowski, ""Learned classification of sonar targets using a massively parallel network"", in the proceedings of the IEEE ASSP Oct 21, 1986 DSP Workshop, Chatham, MA. L. D. Jackel, H. P. Graf, W. Hubbard, J. S. Denker, and D. Henderson, ""An application of neural net chips: handwritten digit recognition"", in IEEE In- ternational Oonference on Neural Networks 1988, II 107-115. J. T. Marcum, ""A statistical theory of target detection by pulsed radar"", in IRE Transactions in Information Theory, Vol. IT-6 (Apr.), pp 145-267, 1960. W. F. Nemcek and W. C. Lin, ""Experimental investigation of automatic signature verification"" in IEEE Transactions on Systems, Man, and Oybernetics, Jan. 1974, pp 121-126. A. S. Osborn, Questioned Documents, 2nd edition (Boyd Printing Co, Albany NY) 1929. C. R. Rosenberg and T. J. Sejnowski, ""The spacing effect on NETtalk, a mas- sively parallel network"", in Proceedings of the Eighth Annual Oonference of the Oognitive Science Society, (Hillsdale, New Jersey: Lawrence Erlbaum Associates, 1986) 72-89. D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ""Learning internal representa- tions by error propagation"", in Parallel Distributed Processing: Explorations in the Microstructures of Oognition. Vol. 1: Foundations, edited by D. E. Rumelhart & J. L. McClelland, (MIT Press, 1986). Y. Sato and K. Kogure, ""Online signature verification based on shape, motion, and writing pressure"", in Proceedings of the 6th International Oonference on Pattern Recognition, Vol. 2, pp 823-826 (IEEE NY) 1982. T. J. Sejnowski and C. R. Rosenberg, ""NETtalk: A Parallel Network that Learns to Read Aloud"", Johns Hopkins University Department of Electrical Engi- neering and Computer Science Technical Report JHU /EECS-86/01, (1986). V. V. Tolat and B. Widrow, ""An adaptive 'broom balancer' with visual inputs"" , in IEEE International Oonference on Neural Networks 1988, II 641-647.","[-0.07887984812259674, -0.029767265543341637, 0.013626647181808949, -0.025750137865543365, 0.012226799502968788, -0.007953532971441746, 0.0349656343460083, -0.00015553673438262194, -0.021386954933404922, -0.0776844248175621, -0.011881590820848942, 0.0571737177670002, 0.13290353119373322, -0.05628577619791031, -0.12014147639274597, -0.003790214192122221, -0.0016927181277424097, 0.06631924957036972, -0.03271004930138588, -0.027772866189479828, -0.06118618696928024, -0.04659019410610199, -0.031510137021541595, -0.08261212706565857, 0.04586904123425484, 0.029610712081193924, 0.011829869821667671, -0.029933756217360497, -0.020150132477283478, -0.060989636927843094, 0.021104414016008377, -0.02018626034259796, -0.0021456771064549685, 0.005297724157571793, -0.028423476964235306, 0.0506276860833168, 0.015337938442826271, -0.009063320234417915, 0.03130831941962242, -0.03525280952453613, -0.034418169409036636, -0.03689790889620781, -0.011854002252221107, 0.05527501925826073, 0.04689573124051094, 0.06354399025440216, 0.051905810832977295, 0.043750617653131485, -0.002839521737769246, -0.025728723034262657, 0.011616090312600136, -0.011298459954559803, -0.031088780611753464, 0.006533050909638405, -0.03248191252350807, -0.056668125092983246, 0.03478427603840828, 0.00773509219288826, -0.09293051809072495, 0.0013092199806123972, 0.003673373954370618, -0.0030943183228373528, -0.08220933377742767, -0.060419708490371704, 0.06874582171440125, 0.06608320027589798, -0.037374187260866165, -0.07113836705684662, 0.06384273618459702, -0.02448344975709915, 0.06342586874961853, 0.07244794070720673, -0.014821143820881844, 0.012213731184601784, 0.006098643410950899, 0.03635822609066963, 0.010519650764763355, 0.046892616897821426, 0.06429538875818253, -0.15765315294265747, 0.01336241327226162, -0.005898944102227688, 0.0023224467877298594, 0.008870178833603859, 0.09985468536615372, 0.002396591706201434, -0.11243388056755066, 0.04289846867322922, 0.005401299800723791, 0.04790507256984711, -0.0072586387395858765, 0.010518102906644344, -0.014279602095484734, -0.07803259789943695, 0.024855656549334526, 0.010134926065802574, 0.009176608175039291, -0.036894541233778, 0.019028950482606888, 0.08637871593236923, -0.0788521096110344, -0.00479997880756855, 0.04029833525419235, -0.0863787829875946, 0.14248153567314148, 0.045140378177165985, 0.03897165134549141, -0.0566304549574852, 0.10389801859855652, -0.15093500912189484, -0.023862726986408234, 0.020089883357286453, -0.06523922830820084, 0.030598120763897896, 0.08741294592618942, -0.056686751544475555, -0.009067093022167683, 0.01147020049393177, 0.006521632429212332, -0.01416833233088255, -0.06694914400577545, -0.02282898873090744, -0.043871648609638214, -0.005108751356601715, -0.009074789471924305, -0.07109149545431137, -0.08309119939804077, 4.401219826791575e-33, -0.041646189987659454, 0.15288977324962616, -0.016695434227585793, -0.052727408707141876, 0.03324592858552933, 0.034796375781297684, -0.018570158630609512, 0.014532080851495266, -0.054138652980327606, 0.05514483526349068, -0.09199707210063934, -0.03775453567504883, 0.03707472234964371, 0.1404099315404892, 0.07781334966421127, -0.005606466438621283, 0.0036530974321067333, 0.0023896575439721346, -0.012898527085781097, -0.034057632088661194, 0.04979609698057175, -0.06234443187713623, 0.06036779284477234, -0.005078552290797234, -0.044338516891002655, -0.05781674385070801, -0.008554407395422459, 0.03317295014858246, 0.044764917343854904, -0.003841794328764081, 0.01889769546687603, -0.027623973786830902, 0.042950063943862915, -0.030159732326865196, 0.012696480378508568, -0.027448417618870735, 0.049597788602113724, -0.038701754063367844, 0.061227548867464066, 0.00016455017612315714, 0.019346456974744797, 0.038469184190034866, -0.03472283110022545, -0.013760840520262718, -0.04526219889521599, 0.03215661272406578, 0.02318686433136463, 0.05946226045489311, 0.045261118561029434, -0.0023060140665620565, -0.012670977041125298, 0.005434032995253801, -0.04438949376344681, -0.07462691515684128, -0.05513257905840874, -0.043791793286800385, 0.007397049106657505, 0.06979978829622269, -0.009542618878185749, 0.04427708312869072, 0.05371669679880142, 0.04178714007139206, 0.0025854618288576603, 0.07216528803110123, -0.04678148403763771, 0.0010402991902083158, -0.013917912729084492, -0.029759269207715988, 0.06397401541471481, -0.018953531980514526, -0.00440601259469986, 0.04002563655376434, -0.03883710131049156, -0.07037988305091858, 0.016827518120408058, -0.044044848531484604, -0.006049049086868763, 0.019484708085656166, -0.01591571420431137, -0.008017689920961857, -0.10435884445905685, 0.09144899249076843, -0.028165427967905998, -0.02346722036600113, 0.02779698371887207, 0.05468301102519035, 0.02180751971900463, -0.08609412610530853, -0.05053291469812393, 0.004440971650183201, -0.05182582139968872, -0.035163454711437225, -0.008819377049803734, 0.05840373411774635, -0.03633313253521919, -3.243415931265194e-33, -0.03944266587495804, 0.06565126031637192, 0.0010142348473891616, 0.04847903177142143, -0.07043969631195068, -0.0441274493932724, -0.01369963027536869, 0.01348150335252285, -0.020036255940794945, 0.06335608661174774, 0.09236890822649002, -0.03366614133119583, 0.014824554324150085, -0.0379682295024395, -0.008184300735592842, -0.10918910801410675, -0.07609594613313675, 0.07817122340202332, 0.054817769676446915, -0.04386849328875542, 0.0038809170946478844, 0.09004789590835571, -0.05017658323049545, 0.09418633580207825, -0.030572108924388885, 0.03827016055583954, -0.020469259470701218, 0.0537152960896492, 0.01841050200164318, 0.03431004658341408, -0.05187023803591728, 0.05167349800467491, 0.003989484626799822, -0.0058684381656348705, -0.019972704350948334, -0.03252159431576729, 0.050797685980796814, 0.04582957178354263, 0.013581369072198868, 0.07343798130750656, 0.05688849091529846, 0.04998556897044182, -0.02771698124706745, -0.027950000017881393, -0.035015273839235306, -0.03637855872511864, -0.047399867326021194, 0.06115680932998657, 0.012320911511778831, 0.07026539742946625, 0.09294232726097107, -0.05463939905166626, -0.013488445430994034, 0.020566869527101517, -0.042243123054504395, 0.10279916226863861, 0.05616658553481102, 0.03971991315484047, 0.11836683750152588, 0.05123141035437584, -0.05853606387972832, -0.059258684515953064, 0.037692829966545105, -0.00032185844611376524, 0.056113820523023605, -0.013280435465276241, 0.02528209052979946, 0.0812334343791008, 0.0053986371494829655, 0.042947523295879364, 0.00408301642164588, -0.015966949984431267, 0.031200969591736794, 0.04949186369776726, 0.03403780981898308, -0.05710526183247566, -0.019485492259263992, -0.021707948297262192, -0.009430759586393833, -0.04143629968166351, -0.03775084763765335, -0.04899279773235321, -0.015895012766122818, 0.0920744240283966, 0.1240156963467598, 0.03792734444141388, -0.01593155413866043, -0.049717821180820465, 0.023758646100759506, 0.06772761046886444, 0.0009518617298454046, 0.09074410051107407, 0.10268635302782059, -0.04523751884698868, -0.029740232974290848, -4.9544723879080266e-08, -0.05987485870718956, 0.031759511679410934, -0.0032299079466611147, -0.0344354473054409, 0.011584945023059845, 0.003021651180461049, 0.0530085526406765, -0.04504188150167465, -0.01875307224690914, -0.12188778817653656, 0.09153562784194946, -0.0832957848906517, -0.1456020176410675, -0.13563846051692963, -0.03588014841079712, 0.0134982168674469, -0.020149661228060722, -0.05595288425683975, -0.053786180913448334, -0.030011530965566635, 0.02501973882317543, -0.00834699161350727, -0.06288593262434006, 0.04570658132433891, 0.03645355999469757, -0.03223443776369095, 0.01046258956193924, 0.11555242538452148, -0.008152741007506847, 0.023298848420381546, 0.0021153297275304794, 0.0012661630753427744, 0.10046658664941788, -0.011553258635103703, 0.01690908521413803, 0.0637972429394722, 0.039952509105205536, -0.004334358964115381, -0.01818716526031494, 0.070528045296669, -0.0012984994100406766, 0.03974677249789238, -0.03778703138232231, -0.04740734398365021, 0.028848297894001007, -0.03266962990164757, 0.03956590220332146, -0.055843938142061234, 0.010452657006680965, -0.005025768652558327, 0.052040114998817444, -0.05309959501028061, 0.02539811283349991, 0.05298107862472534, -0.03214837238192558, -0.05072134733200073, 0.0067283897660672665, -0.04690699279308319, 0.03866816312074661, 0.09368188679218292, 0.05774872004985809, 0.024227624759078026, -0.022269506007432938, -0.031122753396630287]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ANetworkforImageSegmentationUsingColor.pdf,Computer Vision,"A COMPUTATIONA.LLY ROBUST ANATOlVIICAL MODEL FOR RETIN.AL DIRECTIONAL SELECTI\lITY Norberto M. Grzywacz Center BioI. Inf. Processing MIT, E25-201 Cambridge, MA 02139 ABSTRACT Franklin R. Amthor Dept. Psychol. Univ. Alabama Birmingham Birmingham, AL 35294 We analyze a mathematical model for retinal directionally selective cells based on recent electrophysiological data, and show that its computation of motion direction is robust against noise and speed. INTROduCTION Directionally selective retinal ganglion cells discriminate direction of visual motion relatively independently of speed (Amthor and Grzywacz, 1988a) and with high contrast sensitivity (Grzywacz, Amthor, and Mistler, 1989). These cells respond well to motion in the ""preferred"" direction, but respond poorly to motion in the opposite, null, direction. There is an increasing amount of experimental work devoted to these cells. Three findings are particularly relevant for this paper: 1- An inhibitory process asymmetric to every point of the receptive field underlies the directional selectivity of ON-OFF ganglion cells of the rabbit retina (Barlow and Levick, 1965). This distributed inhibition allows small motions anywhere in the receptive field center to elicit directionally selective responses. 2- The dendritic tree of directionally selective ganglion cells is highly branched and most of its dendrites are very fine (Amthor, Oyster and Takahashi, 1984; Amthor, Takahashi, and Oyster, 1988). 3- The distributions of excitatory and inhibitory synapses along these cells' dendritic tree appear to overlap. (Famiglietti, 1985). Our own recent experiments elucidated some ofthe spatiotemporal properties of these cells' receptive field. In contrast to excitation, which is transient with stimulus, the inhibition is sustained and might arise from sustained amacrine cells (Amthor and Grzywacz, 1988a). Its spatial distribution is wide, extending to the borders of the receptive field center (Grzywacz and Amthor, 1988). Finally, the inhibition seems to be mediated by a high-gain shunting, not hyperpolarizing, synapse, that is, a synapse whose reversal potential is close to cell's resting potential (Amthor and Grzywacz, 1989). 477 478 Grzywacz and Amthor In spite of this large amount of experimental work, theoretical efforts to put these pieces of evidence into a single framework have been virtually inexistent. We propose a directional selectivity model based on our recent data on the inhibition's spatiotemporal and nonlinear properties. This model, which is an elab- oration of the model of Torre and Poggio (1978), seems to account for several phenomena related to retinal directional <;eledivity. THE ]\IIODEL Figure 1 illustrates the new model for retinal directional selectivity. In this modd, a stimulus moving in the null direction progressively activates receptive field regions linked to synapses feeding progressively more distal dendrites Of the ganglion cells. Every point in the receptive field center activates adjacent excitatory a~d inhibitory synapses. The inhibitory synapses are assumed to cause shunting inhibition. (""'""e also formulated a pre-ganglionic version of this model, which however, is outside the scope of this paper). NULL .-- FIGURE 1. The new model for retinal directional selectivity. This model is different than that of Poggio and Koch (1987), where the motion axis is represented as a sequence of activation of different dendrites. Furthermore, in their model, the inhibitory synapses must be closer to the soma than the excitatory ones. (However, our model is similar a model proposed, and argued against, elsewhere (Koc,h, Poggio, and Torre, 1982). An advantage of our model is that it accounts for the large inhibitory areas t.o most points of the receptive field (Grzywacz and Amthor, 1988). Also, in the new model, the distributions of the excitatory and inhibitory synapses overlap along the dendritic tree, as suggested (Famiglietti, 1985). Finally, the dendritic tree of ON- OFF directionally selective ganglion cells (inset- Amthor, Oyster, and Takahashi, A Computationally Robust Anatomical Model 479 1984) is consistent with our model. The tree's fine dendrites favor the multiplicity of directional selectivity and help to deal with noise (see below). In this paper, we make calculations with an unidimensional version of the model dealing with motions in the preferred and null directions. Its receptive field maps into one dendrite of the cell. Set the origin of coordinates of the receptive field to be the point where a dot moving in the null direction enters the receptive field. Let S be the size of the receptive field. Next, set the origin of coordinates in the dendri te to be the soma and let L be the length of the dendrite. The model postulates that a point z in the receptive field activates excitatory and inhibitory synapses in pain t z = zL/ S of the dendrite. The voltages in the presynaptic sites are assumed to be linearly related to the stimulus, [(z,t), that is, there are functions fe{t) and li(t) such that the excitatory, {3e(t), and inhibitory, {3i(t), presynaptic voltages of the synapses to position ;r in the dendrite are . . J = e, l, where ""*"" stands for convolution. We assume that the integral of Ie is zero, (the excitation is transient), and that the integral of Ii is positive. (In practice, gamma distribution functions for Ii and the derivatives of such functions for Ie were used in this paper's simulations.) The model postulates that the excitatory, ge, and inhibitory, gi, postsynaptic conductances are rectified functions of the presynaptic voltages. In some examples, we use the hyperbolic tangent as the rectification function: where Ij and T; are constants. In other examples, we use the rectification functions described in Grzywacz and Koch (1987), and their model of ON-OFF rectifications. For the postsynaptic site, we assume, without loss of generality, zero reversal potential and neglect the effect of capacitors (justified by the slow time-courses of excitation and inhibition). Also, we assume that the inhibitory synapse leads to shunting inhibition, that is, its conductance is not in series with a battery. Let Ee be the voltage of the excitatory battery. In general, the voltage, V, in different positions of the dendrite is described by the cable equation: d2~~:, t) = Ra (-Ee!}e (z, t) + V (z, t) (ge (z, t) + 9j (z, t) + 9,.)), where Ra is the axoplasm resistance per unit length, g,. is the resting membrane conductance, and the tilde indicates that in this equation the conductances are given per unit length. 480 Grzywacz and Amthor For the calculations illustrated in this paper, the stimuli are always delivered to the receptive field through two narrow slits. Thus, these stimuli activate synapses in two discrete positions of a dendrite. In this paper, we only show results for square wave and sinusoidal modulations of light, however, we also performed calculations for more general motions. The synaptic sites are small so that their resting conductances are negligible, and we assume that outside these sites the excitatory and inhibitory conductances are zero. In this case, the equation for outside the synapses is: d2U dy2 = U, where we defined A = 1/(Ra y,,)1/2 (the length constant), U = V/Ee, and y = Z/A. The boundary conditions used are where L = L/ A, and where if R, is the soma's input resistance, then p = R, /(RaA) (the dendritic-to-soma cond uctance ratio). The first condition means that currents do not flow through the tips of the dendrites. Finally, label by 1 the synapse proximal to the soma, and by 2 the distal one; the boundary conditions at the synapses are lim U = lim U, 1I~lIj 1I~lIj j = 1,2, II> 111 1I<lIj j = 1,2, (I ) where 7'e = geRa).. and 7'i = 9iRa)... It can be shown that the relative inhibitory strength for motions in the preferred direction decreases with L and increases with p. Thus, to favor conditions for multiplicity of direction selectivity in the receptive field, we perform calculations with L -+ 00 and p = 1. The strengths of the excitatory syna.pses are set such that their contribution to somatic voltage in the absence of inhibition is in variant with position. Finally, we ensure that the excitatory synapses never sat urate. Under these conditions, one can show that the voltage in the soma is: U (0) = {27'e,2 + (7'e,2 + 7'i,2 + 2) 7'e,d e2~y - (7'£,2 + 7'j,2) 7'e,1 , (2) ((7'i,l +2)7'i,2 + 27'j,1 +4)e26Y - 7'i,17'i,2 where 6y is the distance between the synapses. A fina.l quantity, which is used in this paper is the directional selectivity index It. Let Up and Un be the total responses to the second slit in the apparent motion in the preferred and null directions respectively. {Alternatively, for the sinusoidal A Computationally Robust Anatomical Model 481 motion, these quantities are the respective average responses.) \Ve follow Grzywacz and Koch (198i) and define (3) RESULTS This section presents the results of calcuhtions ba.""ed on the modd. \Ve address: the multiple computations of directional seir->divity in the {.·ells· receptive fields; the robustness of these computations again~t noise: I h"" robustness of these computa- tions against speed. Figure 2 plots the degree of directional selectivity for apparent lIlotions acti- vating two synapses as function of the synapses' distan,'e in a dendrite (computed from Equations 2 and 3). 1. ~--------------------------- 32 .. 16 .. "" c:: ... B >-.. - ~ '"" .5 .. u .. .. - ~ - 2 ., c: a '"" .. u .. ""-'"" 0.5 c .0 .00 .50 1.0 1.5 2.0 Dendritic Distance (>.1 l!~IG URE 2. Locality of lnkraction betwt't""'n synap""'t'~ a(I1\-'(1l,'"" hv apparent mo- tions. It can be shown that the critical parameter controlling whether a certain synap- tic distance produces a criterion directional selectivity is rj (Equation 1). As the parameter rj increases, the criterion distance decreases , Thus, ""ince in retinal di- rectionally selective cells the inhibition has high gain (Amthor and Grzywa<.""z, 1989) and the dendrites are fine (Amthor, Oyster and Takahashi, 1984; Amthor, Taka- hashi, and Oyster, 1988), then rj is high, and motions anywhere in receptive field should elicit directionally selective responses (Barlow and Levick, 1965). In other words, the model's receptive field computes motion direction multiple times. Next, we show that the high inhibitory gain and the cells' fine dendrites help to deal with noise, and thus. may explain the high contrast sensitivity (0.5% contrast- 482 Grzywacz and Amthor Grzywacz, Amthor, and Mistler, 1989) of the cells' directional selectivity. This conclusion is illustrated in Figure 3's multiple plots. .. - -Looo ~1 -NlgII ~I , "".., ...... a .. -""11 ~ 1.0 - OUTPUT NOISE VCCJ1ATDA'f 1IfI\/1 NOI. INIIIIT DAY 1IfI\/1 NOIS£ ! , .. .. .. N , , J .50 .. .. .. .. oo-===--e:;..........::.-....;::.,---==--..::.. 00 • . 0 10 •. 00 .0 10 .00 4.0 1.0 Response Response Aupan .. FIGURE 3. The model's computatifm of direction is robust against additive noise in the output, and in the excitatory and inhibitory inputs. To generate this figure, we used Equation 2 assuming that a Gaussian noise is added to the cell's output, excitatory input, or inhibitory input. {In the latter case, we used an approximation that assumes small standard deviation for the inhibitory input's noise.) Once again the critical parameter is the ri defined in Equation 1. The larger this parameter is, the better the model deals with noise. In the case of output noise, an increase of the parameter separatt's the preferred and null mean responses. For noise in the excitatory input, a parameter increase not only separates the means, but also reduces the standard deviation: Shunting inhibition SHUnTS down the noise. Finally, the most dramatic improvement occurs when the noise is in the inhibitory input. (In all these plots, the parameter increase is always by a factor of three.) Since for retinal direc tionally selective ganglion cells, ri is high (high inhibitory gain and fine dendrites), we conclude that the cells' mechanism are particularly well suited to deal with noise. For sinusoidal motions, the directional selectivity is robust for a large range of temporal frequencies provided that the frequencies are sufficiently low (Figure 4). (Nevertheless, the cell's preferred direction responses may be sharply tuned to either temporal frequency or speed- Amthor and Grzywacz, 1988). I ! i is I i :: ~ i , i i is A Computationally Robust Anatomical Model 483 -HI"", rl - 'llHl rl 2.00 -- 20.0 I I I --------------- 2.00 20.0 FrtQlltncy ~I I I I "" , I , I \ 200. IE-roo FIGURE 4. Directional selectivity is robust against speed modulation. To gener- ate this curve, we subtracted the average respons~ to a isolated flickering slit from the preferred and null average responses (from Equation 2). This robustness is due to the invariance with speed for low speeds of the relative temporal phase shift between inhibition and excitation. Since the excitation has band-pass characteristics, it leads the stimulus by a constant phase. On the other hand, the inhibition is delayed and advanced in the preferred and null directions respectively, due to the asymmdric spatial integration. The phase shifts due to this integration are also speed invariant. CONCLUSIONS We propose a new model for retinal directional selectivity. The shunting inhibi- tion of ganglion cells (Torre and Poggio, 1978), which is temporally sustained, is the main biophysical mechanism of the model. It postulates that for null direc- tion motion, the stimulus activates regions of the receptive field that are linked to excitatory and inhibitory synapses, which are progressively farther away from the soma. This models accounts for: 1- the distribution of inhibition around points of the receptive field (Grzywacz and Amthor, 1988); 2- the apparent full overlap of the distribution of excitatory and inhibitory synapses along the dendritic trees of directionally selective ganglion cells (Famiglietti, 1985); 3- the multiplicity of directionally selective regions (Barlow and Levick, 1965); 4- the high contrast sen- sitivity of the cells' directional selectivity (Grzywacz, Amthor, and Mistler, 1989); 5- the relative in variance of directional selectivity on stimulus speed (Amthor and Grzywacz, 1988). Two lessons of our model to neural network modeling are: Threshold is not the only neural mechanism, and the basic computational unit may not be a neuron 484 Grzywacz and Amthor but a piece of membrane (Grzywacz and Poggio, 1989). In our model, nonlinear interactions are relatively confined to specific dendritic tree branches (Torre and Poggio, 1978). This allows local computations by which single cells might generate receptive fields with multiple directionally selective regions, as observed by Barlow and Levick (1965). Such local computations could not occur if the inhibition only worked through a reduction in spike rate by somatic hyperpolarization. Thus, most neural network models may be biologically irrelevant, since they are built upon a too simple model of the neuron. The properties of a network depend strongly on its basic elements. Therefore, to understand the computations of biological networks. it may be essential to first understand the basic biophysical mechanisms of information processing before developing complex networks. ACKNOWLEDGMENTS We thank Lyle Borg-Graham and Tomaso Poggio for helpful discussions. Also, we thank Consuelita Correa for help with the figures. N .M.G. was supported by grant BNS-8809528 from the National Science Foundation, by the Sloan Foundation, and by a grant to Tomaso Poggio and Ellen Hildreth from the Office of Naval Research, Cognitive and Neural Systems Division. F.R.A. was supported by grants from the National Institute of Health (EY05070) and the Sloan Foundation. REFERENCES Amthor & Grzywacz (1988) Invest. Ophthalmol. Vi"". Sci. 29:225. Amthor & Grzywacz (1989) Retinal Directional Selectivity Is Accounted for by Shunting Inhibition. Submitted for Publication. Amthor, Oyster & Takahashi (1984) Brain Res. 298:187. Amthor, Takahashi & Oyster (1989) J. Compo Neurol. In Press. Barlow & Levick (1965) J. Physiol. 178:477. Famiglietti (1985) Neuro""ci. Abst. 11:337. Grzywacz & Amthor (1988) Neurosci. Ab""t. 14:603. Grzywacz, Amthor & Mistler (1989) Applicability of Quadratic and Threshold Mod- els to Motion Discrimination in the Rabbit Retina. Submitted for Publication. Grzywacz & Koch (1987) Synapse 1:417. Grzywacz & Poggio (1989) In An Introduction to Neural and Electronic Networks. Zornetzer, Davis & Lau, Eds. Academic Press, Orlando, USA. In Press. Koch, Poggio & Torre (1982) Philos. Tran"". R. Soc. B 298:227. Poggio & Koch (1987) Sci. Am. 256:46. Torre & Poggio (1978) Proc. R. Soc. B 202:409.","[0.009513751603662968, -0.1329958438873291, 0.08512945473194122, 0.009832296520471573, -0.03282250091433525, 0.01959175616502762, 0.04907946288585663, -0.03690936788916588, 0.0807427167892456, 0.09010830521583557, -0.008402939885854721, -0.010520609095692635, 0.014097158797085285, 0.03998934477567673, -0.06747006624937057, -0.03755510225892067, 0.14707300066947937, 0.0426349937915802, -0.03625169023871422, 0.04703439027070999, 0.02555963210761547, -0.06996127218008041, -0.0015133884735405445, -0.0019432370318099856, -0.011765574105083942, -0.06013111770153046, 0.021148037165403366, 0.05763708055019379, -0.07012703269720078, -0.13680756092071533, -0.055270515382289886, 0.044301588088274, -0.09084844589233398, 0.039232611656188965, -0.07022243738174438, 0.0045606717467308044, 0.018856795504689217, -0.016784952953457832, 0.017764000222086906, 0.03654124587774277, -0.005034624133259058, 0.018483515828847885, -0.05887375399470329, -0.0027282049413770437, 0.12121229618787766, -0.017775602638721466, 0.025521472096443176, -0.008376095443964005, -0.016289690509438515, -0.00670029129832983, -0.0655222237110138, 0.017731541767716408, -0.04018415883183479, 0.054197922348976135, 0.0190599225461483, 0.1219070553779602, -0.04416150972247124, 0.01574152708053589, 0.10782432556152344, -0.03350815922021866, 0.07157231867313385, -0.03238988667726517, 0.00672132708132267, -0.06693945080041885, -0.00421906216070056, -0.05019676685333252, -0.02983122691512108, -0.07956630736589432, 0.014522471465170383, 0.0021505423355847597, 0.049949195235967636, 0.0238809734582901, 0.017310092225670815, -0.06007992476224899, 4.794603955815546e-05, -0.008646317757666111, 0.08624114096164703, 0.03743179887533188, 0.03829845413565636, -0.07725779712200165, 0.05953124165534973, -0.019346216693520546, -0.012497037649154663, 0.03085431642830372, 0.04732969403266907, -0.01152859628200531, -0.04889510199427605, 0.07585374265909195, 0.0779573917388916, 0.04314710199832916, -0.00553693575784564, -0.06187938153743744, -0.025663141161203384, -0.11703303456306458, 0.01357835903763771, -0.04035651311278343, 0.0068989102728664875, -0.020992865785956383, 0.028575601056218147, -0.005433145444840193, 0.03659316897392273, -0.0879342257976532, -0.01172602642327547, 0.05666952580213547, 0.08294502645730972, -0.028618937358260155, 0.10127630084753036, 0.031110171228647232, -0.021306466311216354, -0.01986333727836609, -0.054932039231061935, -0.011631705798208714, 0.0029543498530983925, 0.09622704237699509, -0.0358104445040226, 0.0017288816161453724, -0.017111264169216156, 0.00468450179323554, 0.13719037175178528, -0.021058836951851845, 0.03835926577448845, -0.03663301095366478, -0.00452760886400938, -0.02666362188756466, 0.08054938167333603, -0.004018579609692097, -0.029722334817051888, 5.45492635071295e-33, -0.051225170493125916, 0.02738898992538452, 0.026367241516709328, -0.08668740093708038, 0.01034902036190033, -0.05806506797671318, -0.00013520068023353815, -0.03294500336050987, -0.010231244377791882, 0.020963748916983604, -0.08310113102197647, 0.038761917501688004, -0.005128670018166304, 0.10722275823354721, 0.05958665534853935, -0.03157823905348778, -0.007730386219918728, -0.04099749028682709, -0.04852408543229103, -0.0355948880314827, -0.008697960525751114, 0.034615229815244675, -0.0433245413005352, -0.07975015044212341, -0.0290200337767601, -0.043697431683540344, -0.07275832444429398, -0.02140793949365616, 0.011437691748142242, 0.03867565840482712, -0.00405374588444829, 0.07291211187839508, 0.03823137655854225, -0.030835416167974472, 0.016029594466090202, 0.03206914663314819, -0.016465291380882263, 0.010230368934571743, 0.11265543848276138, -0.026757128536701202, 0.003993898164480925, 0.11532486975193024, -0.029308581724762917, -0.05840839818120003, 0.015687845647335052, 0.08261611312627792, -0.03278772160410881, 0.07838286459445953, -0.04643658548593521, -0.0024695475585758686, 0.04810631275177002, -0.010108170099556446, -0.018239498138427734, -0.07666996121406555, -0.03079606033861637, 0.07059439271688461, -0.05186305567622185, 0.07962330430746078, -0.054405178874731064, 0.05201414227485657, -0.030882103368639946, 0.010366500355303288, 0.03230222687125206, -0.023154444992542267, 0.055829133838415146, 0.023883488029241562, -0.11444627493619919, -0.023023294284939766, -0.011509518139064312, -0.08719643205404282, 0.0342586524784565, 0.04207506403326988, 0.03462028130888939, -0.12989506125450134, 0.029388492926955223, -0.0062690372578799725, 0.03961798548698425, 0.0708579272031784, -0.04910682141780853, -0.10284925252199173, -0.096408411860466, -0.008899271488189697, -0.07611142098903656, 0.001853574300184846, 0.002101810649037361, 0.00817495584487915, 0.03396255522966385, -0.02747328020632267, -0.06692882627248764, -0.04986715316772461, 0.08862417936325073, 0.024423586204648018, -0.021644847467541695, -0.03256450593471527, -0.041449423879384995, -5.241166743777748e-33, -0.019999384880065918, -0.005470614414662123, 0.04112076014280319, 0.022540677338838577, -0.08072962611913681, 0.06820037215948105, 0.057994477450847626, -0.008026556111872196, 0.023702410981059074, -0.04854251444339752, 0.018380891531705856, 0.007926633581519127, -0.06849309056997299, 0.06627880036830902, -0.008001262322068214, -0.026680873706936836, -0.021591879427433014, 0.006428845692425966, 0.01789427176117897, -0.011704564094543457, -0.09161435812711716, 0.05462019518017769, -0.0023373914882540703, 0.03726530075073242, 0.023177048191428185, 0.10777046531438828, -0.05444544553756714, 0.08578226715326309, -0.0960235446691513, -0.07344892621040344, -0.051259543746709824, 0.06898818165063858, -0.051223669201135635, -0.053261470049619675, 0.04890362173318863, 0.0200016088783741, -0.10026346892118454, -0.017315734177827835, -0.03121205046772957, 0.045837417244911194, 0.03515847399830818, 0.062321651726961136, 0.04042922705411911, 0.0013643543934449553, 0.06641247123479843, 0.03213576227426529, 0.025881033390760422, 0.015763478353619576, -0.04097338020801544, 0.006774029228836298, -0.088216133415699, 0.02990666963160038, -0.06888467818498611, 0.026471668854355812, -0.03815792500972748, 0.054361749440431595, -0.008705918677151203, -0.016204899176955223, 0.09878987818956375, -0.008270274847745895, -0.023987727239727974, -0.013542520813643932, -0.03615978732705116, -0.026066357269883156, 0.04366767033934593, 0.057911284267902374, 0.009101152420043945, -0.0004975870251655579, 0.159633070230484, -0.07710447162389755, 0.055655039846897125, 0.03273118659853935, 0.055609796196222305, -0.01716742105782032, 0.017727317288517952, 0.01803039014339447, 0.029400162398815155, -0.015371463261544704, -0.026140056550502777, 0.02447550743818283, -0.01070693600922823, 0.05319100618362427, -0.04793379083275795, 0.00680885836482048, -0.026781601831316948, 0.02678178995847702, -0.019521361216902733, -0.0378592349588871, 0.050410449504852295, -0.031520549207925797, 0.048818189650774, -0.054409049451351166, 0.025420665740966797, 0.0012788634048774838, 0.055348608642816544, -5.029404803735815e-08, 0.0006809139740653336, -0.02090909518301487, 0.038927145302295685, -0.0365542508661747, 0.027829866856336594, -0.01813296601176262, 0.028248030692338943, 0.0370052196085453, -0.014821904711425304, -0.042452335357666016, 0.01047386322170496, 0.00862072966992855, 0.00533886207267642, 0.0021439315751194954, 0.01997959241271019, 0.09396818280220032, 0.00561954639852047, 0.0020664262119680643, -0.022723497822880745, 0.019506780430674553, -0.01865529827773571, -0.019699379801750183, -0.029377929866313934, 0.07898447662591934, 0.04944465681910515, 0.009955056011676788, -0.08585833758115768, -0.04103003069758415, -0.035376571118831635, 0.0003413556551095098, 0.09430614858865738, 0.044172219932079315, 0.10108157992362976, 0.056582991033792496, -0.050676360726356506, 0.02004329487681389, -0.007463167421519756, 0.07230251282453537, -0.01680217869579792, 0.015451138839125633, -0.0965852364897728, 0.014736546203494072, -0.05857796594500542, -0.019295597448945045, -0.03737418353557587, -0.06026628986001015, 0.11406461894512177, -0.13900738954544067, 0.016287116333842278, -0.014208905398845673, -0.07391540706157684, 0.019822832196950912, -0.006362915504723787, -0.005124527961015701, -0.029201442375779152, -0.026595303788781166, -0.046286460012197495, -0.04456109553575516, -0.06313339620828629, -0.0266446340829134, 0.05101630091667175, 0.05158022791147232, -0.09502435475587845, -0.007209330797195435]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AnInformationTheoreticApproachtoRuleBasedConnectionistExpertSystems.pdf,Deep Learning,"Mapping Classifier Systems Into Neural Networks Lawrence Davis BBN Laboratories BBN Systems and Technologies Corporation 10 Moulton Street Cambridge, MA 02238 January 16, 1989 Abstract Classifier systems are machine learning systems incotporating a genetic al- gorithm as the learning mechanism. Although they respond to inputs that neural networks can respond to, their internal structure, representation fonnalisms, and learning mechanisms differ marlcedly from those employed by neural network re- searchers in the same sorts of domains. As a result, one might conclude that these two types of machine learning fonnalisms are intrinsically different. This is one of two papers that, taken together, prove instead that classifier systems and neural networks are equivalent. In this paper, half of the equivalence is demonstrated through the description of a transfonnation procedure that will map classifier systems into neural networks that are isomotphic in behavior. Several alterations on the commonly-used paradigms employed by neural networlc researchers are required in order to make the transfonnation worlc. These alterations are noted and their appropriateness is discussed. The paper concludes with a discussion of the practical import of these results, and with comments on their extensibility. 1 Introd uction Classifier systems are machine learning systems that have been developed since the 1970s by 10hn Holland and, more recently, by other members of the genetic algorithm research community as welll . Classifier systems are varieties of genetic algorithms - algorithms for optimization and learning. Genetic algorithms employ techniques inspired by the process of biological evolution in order to ""evolve"" better and better IThis paper has benefited from discussions with Wayne Mesard, Rich Sutton, Ron Williams, Stewart Wilson, Craig Shaefer, David Montana, Gil Syswerda and other members of BARGAIN, the Boston Area Research Group in Genetic Algorithms and Inductive Networks. 49 50 Davis individuals that are taken to be solutions to problems such as optimizing a function, traversing a maze, etc. (For an explanation of genetic algorithms, the reader is referred to [Goldberg 1989].) Classifier systems receive messages from an external source as inputs and organize themselves using a genetic algorithm so that they will ""learn"" to produce responses for internal use and for interaction with the external source. This paper is one of two papers exploring the question of the fonnal relationship between classifier systems and neural networks. As normally employed, the two sorts of algorithms are probably distinct, although a procedure for translating the operation of neural networks into isomorphic classifier systems is given in [Belew and Gherrity 1988]. The technique Belew and Gherrity use does not include the conversion of the neural network learning procedure into the classifier system framework, and it appears that the technique will not support such a conversion. Thus, one might conjecture that the two sorts of machine learning systems employ learning techniques that cannot be reconciled, although if there were a subsumption relationship, Belew and Gherrity's result suggests that the set of classifier systems might be a superset of the set of neural networks. The reverse conclusion is suggested by consideration of the inputs that each sort of learning algorithm processes. When viewed as ""black boxes"", both mechanisms for learning receive inputs, carry out self-modifying procedures, and produce outputs. The class of inputs that are traditionally processed by classifier systems - the class of bit strings of a fixed length - is a subset of the class of inputs that have been traditionally processed by neural networks. Thus, it appears that classifier systems operate on a subset of the inputs that neural networks can process, when viewed as mechanisms that can modify their behavior. In fact, both these impressions are correct. One can translate classifier systems into neural networks, preserving their learning behavior, and one can translate neural networks into classifier systems, again preserving learning behavior. In order to do so, however, some specializations of each sort of algorithm must be made. This paper deals with the translation from classifier systems to neural networks and with those specializations of neural networks that are required in order for the translation to take place. The reverse translation uses quite different techniques, and is treated in [Davis 1989]. The following sections contain a description of classifier systems, a description of the transformation operator, discussions of the extensibility of the proof, comments on some issues raised in the course of the proof, and conclusions. 2 Classifier Systems A classifier system operates in the context of an environment that sends messages to the system and provides it with reinforcement based on the behavior it displays. A classifier system has two components - a message list and a population of rule-like entities called classifiers. Each message on the message list is composed of bits, and Mapping Classifier Systems Into Neural Networks 51 each has a pointer to its source (messages may be generated by the environment or by a classifier.) Each classifier in the population of classifiers has three components: a match string made up of the characters 0,1, and # (for ""don't care""); a message made up of the characters 0 and 1; and a strength. The top-level description of a classifier system is that it contains a population of production rules that attempt to match some condition on the message list (thus ""classifying"" some input) and post their message to the message list, thus potentially affecting the envirorunent or other classifiers. Reinforcement from the environment is used by the classifier system to modify the strengths of its classifiers. Periodically, a genetic algorithm is invoked to create new classifiers, which replace certain members of the classifier set. (For an explanation of classifier systems, their potential as machine learning systems, and their formal properties, the reader is referred to [Holland et al 1986].) Let us specify these processing stages more precisely. A classifier system operates by cycling through a fixed list of procedures. In order, these procedures are: Message List Processing. 1. Clear the message list. 2. Post the envirorunental messages to the message list. 3. Post messages to the message list from classifiers in the post set of the previous cycle. 4. Implement envirorunental reinforcement by analyzing the messages on the message list and altering the strength of classifiers in the post set of the previous cycle. Form the Bid Set. 1. Determine which classifiers match a message in the message list. A classifier matches a message if each bit in its match field matches its corresponding message bit. A 0 matches a 0, a 1 matches a I, and a # matches either bit. The set of all matching classifiers forms the current bid set. 2. Implement bid taxes by subtracting a portion of the strength of each classifier c in the bid set. Add the strength taken from c to the strength of the classifier or classifiers that posted messages matched by c in the prior step. Form the Post Set. 1. If the bid set is larger than the maximum post set size, choose classifiers stochastically to post from the bid set, weighting them in proportion to the magnitude of their bid taxes. The set of classifiers chosen is the post set. Reproduction Reproduction generally does not occur on every cycle. When it does occur, these steps are carried out: 1. Create n children from parents. Use crossover and/or mutation, chOOSing parents stochastically but favoring the strongest ones. (Crossover and mutation are two of the operators used in genetic algorithms.) 2. Set the strength of each child to equal the average of the strength of that child's parents. (Note: this is one of many ways to set the strength of a new classifier. The transformation will work in analogous ways for each of them.) 3. Remove n members of the classifier population and add the n new children to the classifier population. 3 Mapping Classifiers Into Classifier Networks The mapping operator that I shall describe maps each classifier into a classifier network. Each classifier network has links to environmental input units, links to 52 Davis other classifier networks, and match, post, and message units. The weights on the links leading to a match node and leaving a post node are related to the fields in the match and message lists in the classifier. An additional link is added to provide a bias term for the match node. (Note: it is assumed here that the environment posts at most one message per cycle. Modifications to the transfonnation operator to accommodate multiple environmental messages are described in the final comments of this paper.) Given a classifier system CS with n classifiers, each matching and sending mes- sages of length m, we can construct an isomorphic neural network composed of n classifier networks in the following way. For each classifier c in CS, we construct its corresponding classifier network, composed of n match nodes, I post node, and m message nodes. One match node (the environmental match node) has links to inputs from the environment. Each of the other match nodes is linked to the message and post node of another classifier network. The reader is referred to Figure 2 for an example of such a transformation. Each match node in a classifier network has m + 1 incoming links. The weights on the first m links are derived by applying the following transformation to the m elements of c's match field: 0 is associated with weight -1, 1 is associated with weight 1, and # is associated with weight O. The weight. of the final link is set to m + 1 - l, where l is the number of links with weight = 1. Thus, a classifier with match field (1 0 # 0 1) would have an associated network with weights on the links leading to its match nodes of 1, -1, 0, -I, 1, and 4. A classifier with match field (1 0#) would have weights of 1, -I, 0, and 3. The weights on the links to each message node in the classifier network are set to equal the corresponding element of the classifier's message field. Thus, if the message field of the classifier were (0 1 0), the weights on the links leading to the three message nodes in the corresponding classifier network would be 0, I, and O. The weights on all other links in the classifier network are set to 1. Each node in a classifier network uses a threshold function to determine its acti- vation level. Match nodes have thresholds = m + .9. All other nodes have thresholds = .9. If a node's threshold is exceeded, the node's activation level is set to 1. If not, it is set to O. Each classifier network has an associated quantity called strength that may be altered when the network is run, during the processing cycle described below. A cycle of processing of a classifier system CS maps onto the following cycle of processing in a set of classifier networks: Message List Processing. 1. Compute the activation level of each message node in CS. 2. If the environment supplies reinforcement on this cycle, divide that reinforcement by the number of post nodes that are currently active, plus 1 if the environment posted a message on the preceding cycle, and add the quotient to the strength of each active post node's classifier network. 3. If there is a message on this cycle from the environment, map it onto the first m environment nodes so that each node associated with a 0 is off and each node associated with a 1 is on. Tum the final environmental node on. If there is no environmental message, turn all environmental Mapping Classifier Systems Into Neural Networks 53 nodes off. Form the Bid Set. 1. Compute the activation level of each match node in each classifier network. 2. Compute the activation level of each bid node in each classifier network (the set of classifier networks with an active bid node is the bid set). 3. Subtract a fixed proportion of the strength of each classifier network cn in the bid set. Add this amount to the strength of those networks connected to an active match node in cn. (Strength given to the environment passes out of the system.) Form the Post Set. 1. If the bid set is larger than the maximum post set size, choose networks stochastically to post from the bid set, weighting them in proportion to the magnitude of their bid taxes. The set of networks chosen is the post set. (This might be viewed as a stochastic n-winners-take-all procedure). Reproduction. If this is a cycle on which reproduction would occur in the classifier system, carry out its analog in the neural network in the following way. 1. Create n children from parents. Use crossover and/or mutation, choosing parents stochastically but favoring the strongest ones. The ternary alphabet composed of -I, I, and 0 is used instead of the classifier alphabet of 0, 1, and #. After each operator is applied, the final member of the match list is set to m + 1 - l. 2. Write over the weights on the match links and the message links of n classifier networks to match the weights in the children. Choose networks to be re-weighted stochastically, so that the weakest ones are most likely to be chosen. Set the strength of each re-weighted classifier network to be the average of the strengths of its parents. It is simple to show that a classifier network match node will match a message in just those cases in which its associated classifier matched a message. There are three cases to consider. If the original match character was a #, then it matched any message bit. The corresponding link weight is set to 0, so the state of the node it comes from will not affect the activation of the match node it goes to. If the original match character was a 1, then its message bit had to be a 1 for the message to be matched. The corresponding link weight is set to 1, and we see by inspection of the weight on the final link, the match node threshold, and the fact that no other type of link has a positive weight, that every link with weight I must be connected to an active node for the match node to be activated. Finally, the link weight corresponding to a 0 is set to -1. If any of these links is connected to a node that is active, then the effect is that of turning off a node connected to a link with weight 1, and we have just seen that this will cause the match node to be inactive. Given this correspondence in matching behavior, one can verify that a set of classifier networks associated with a classifier system has the following properties: During each cycle of processing of the classifier system, a classifier is in the bid set in just those cases in which its associated networlc has an active bid node. Assuming that both systems use the same randomizing technique, initialized in the same way, the classifier is in the post set in just those cases when the network is in the post set. Finally, the parents that are chosen for reproduction are the transform as of those chosen in the classifier system, and the children produced are the transformations of the classifier system parents. The two systems are isomorphic in operation, assuming that they use the same random number generator. 54 Davis CLASSIFIER NETWORK 1 CLASSIFIER NETWORK 2 strength = 49.3 strength = 21.95 2 Figure 1: Result of mapping a classifier system witH two classifiers into a neural network. MESSAGE NODES TH = .9 POST NODES TH =.9 MATCH NODES TH = 3.9 ENVIRONMENT INPUT NODES Classifier 1 has match field (0 1 #), message field (1 1 0), and strength 49.3. Classifier 2 has match field (1 1 #), message field (0 1 1), and strength 21.95. Mapping Classifier Systems Into Neural Networks 55 4 Concluding Comments The transfonnation procedure described above will map a classifier system into a neural network that operates in the same way. There are several points raised by the techniques used to accomplish the mapping. In closing, let us consider four of them. First, there is some excess complexity in the classifier networks as they are shown here. In fact, one could eliminate all non-environmental match nodes and their links, since one can determine whenever a classifier network is reweigh ted whether it matches the message of each other classifier network in the system. If so, one could introduce a link directly from the post node of the other classifier networlc to the post node of the new networlc. The match nodes to the environment are necessary, as long as one cannot predict what messages the environment will post. Message nodes are necessary as long as messages must be sent out to the environment. If not, they and their incoming links could be eliminated as well. These simplifications have not been introduced here because the extensions discussed next require the complexity of the current architecture. Second, on the genetic algorithm side, the classifier system considered here is an extremely simple one. There are many extensions and refinements that have been used by classifier system researchers. I believe that such refinements can be handled by expanded mapping procedures and by modifications of the architecture of the classifier networks. To give an indication of the way such modifications would go, let us consider two sample cases. The first is the case of an environment that may produce multiple messages on each cycle. To handle multiple messages, an additional link must be added to each environmental match node with weight set to the match node's threshold. This link will latch the match node. An additional match node with links to the environment nodes must be added, and a latched counting node must be attached to it. Given these two architectural modifications, the cycle is modified as follows: During the message matching cycle, a series of subcycles is carried out, one for each message posted by the environment. In each subcycle, an environmental message is input and each environmental match node computes its activation. The environmental match nodes are latched., so that each will be active if it matched any environmental message. The count nodes will record how many were matched by each classifier network. When bid strength'is paid from a classifier network to the posters of messages that it matched, the divisor is the number of environmental messages matched as recorded by the count node, plus the number of other messages matched. Finally, when new weights are written onto a classifier network's links, they are written onto the match node connected to the count node as well. A second sort of complication is that of pass-through bits - bits that are passed from a message that is matched to the message that is posted. This sort of mechanism can be implemented in an obvious fashion by complicating the structure of the classifier networlc. Similar complications are produced by considering multiple-message matching, negation, messages to effectors, and so forth. It is an open question whether all such cases can be handled by modifying the architecture and the mapping operator, but I have not yet found one that cannot be so handled. 56 Davis Third, the classifier networks do not use the sigmoid activation functions that sup- port hill-c~bing techniques such as back-propagation. Further, they are recurrent networks rather than strict feed-forwanl networks. Thus, one might wonder whether the fact that one can carry out such transformations should affect the behavior of researchers in the field. This point is one that is taken up at greater length in the companion paper. My conclusion there is that several of the techniques imported into the neural network domain by the mapping appear to improve the performance of neu- ral networks. These include tracking strength in order to guide the learning process, using genetic operators to modify the network makeup. and using population-level measurements in order to determine what aspects of a network to use in reproduction. The reader is referred to [Montana and Davis 1989] for an example of the benefits to be gained by employing these techniques. Finally, one might wonder what the import of this proof is intended to be. In my view, this proof and the companion proof suggest some exciting ways in which one can hybridize the learning techniques of each field. One such approach and its successful application to a real-world problem is characterized in [Montana and Davis 1989]. References [1] Belew, Richard K. and Michael Gherrity, ""Back Propagation for the Classifier System"", in preparation. [2] Davis, Lawrence, ""Mapping Neural Networks into Classifier Systems"", submit- ted to the 1989 International Conference on Genetic Algorithms. [3] Goldberg, David E. Genetic Algorithms in Search, Optimization, and Machine Learning, Addison Wesley 1989. [4] Holland, John H, Keith J. Holyoak, Richard E. Nisbett, and Paul R. Thagard, Induction, MIT Press, 1986. [5] Montana, David J. and Lawrence Davis, ""Training Feedforward Neural Net- works Using Genetic Algorithms"", submitted to the 1989 International Joint Conference on Artificial Intelligence.","[-0.09341678768396378, -0.1371537744998932, -0.027282871305942535, -0.0003176205209456384, -0.0331018902361393, 0.02583346702158451, -0.011639897711575031, -0.028213776648044586, -0.033978041261434555, -0.032511066645383835, 0.0060005392879247665, -0.04179592430591583, 0.06650751084089279, -0.014099875465035439, -0.08411990851163864, -0.015357296913862228, -0.014804903417825699, 0.08713577687740326, -0.0554533451795578, 0.045282699167728424, 0.05218513682484627, 0.045934516936540604, -0.04114615544676781, 0.03944257274270058, -0.04674818739295006, 0.04425989091396332, -0.028752965852618217, 0.03775293752551079, -0.011291911825537682, -0.054537370800971985, 0.04161600396037102, 0.005812356248497963, -0.03835589066147804, 0.012861478142440319, -0.0819658637046814, 0.02465549297630787, -0.027296222746372223, 0.02280023694038391, 0.03734616935253143, -0.06291625648736954, -0.030836327001452446, -0.07063587009906769, -0.015761004760861397, 0.03398861363530159, 0.0854480117559433, 0.1142415702342987, 0.005187867674976587, -0.021167660132050514, -0.07069101184606552, -0.013280651532113552, -0.08992619812488556, -0.011102310381829739, -0.051477011293172836, 0.07921141386032104, 0.004291873425245285, 0.04022650793194771, 0.02876199223101139, 0.06000738590955734, -0.06992019712924957, 0.03754016011953354, -0.021486274898052216, -0.028102654963731766, 0.018948057666420937, -1.2734064512187615e-05, 0.009456791914999485, 0.07806061208248138, -0.02698570117354393, 0.008123260922729969, 0.06307472288608551, -0.08166451752185822, 0.006967207416892052, 0.045778192579746246, -0.009403555653989315, 0.0327565036714077, 0.04923428222537041, 0.06134021654725075, 0.04881109669804573, 0.10652615875005722, 0.025308096781373024, -0.025911683216691017, -0.027087921276688576, -0.01494301576167345, 0.01930944062769413, 0.03645925223827362, 0.07655299454927444, 0.013962591998279095, -0.10047866404056549, -0.008096729405224323, -0.009420245885848999, -0.029874589294195175, -0.018734049052000046, -0.0398002453148365, 0.011372667737305164, -0.0809771865606308, 0.0006001145229674876, -0.029040854424238205, -0.02838611602783203, -0.005815237294882536, 0.02921840362250805, 0.06079176813364029, -0.09900299459695816, -0.015300367958843708, 0.029270565137267113, -0.050288792699575424, 0.04666070640087128, 0.042897503823041916, 0.018147697672247887, 0.012114239856600761, 0.10523443669080734, -0.12142947316169739, -0.0667114183306694, -0.013796799816191196, -0.02286614663898945, -0.05895472317934036, -0.032001230865716934, -0.007328997366130352, -0.013335000723600388, -0.052943386137485504, 0.0009854575619101524, 0.048965148627758026, -0.045454639941453934, -0.005002487450838089, -0.039131421595811844, 0.030916793271899223, 0.011051267385482788, -0.0567682720720768, -0.11088480800390244, 6.077381833542045e-33, -0.048861537128686905, 0.03777538985013962, -0.029639266431331635, 0.009250825271010399, 0.0698404461145401, -0.09033186733722687, 0.02417285181581974, -0.0074503482319414616, 0.07699737697839737, -0.014123612083494663, -0.07266483455896378, 0.031094660982489586, 0.030676590278744698, 0.08119513839483261, 0.06429421156644821, 0.031859032809734344, -0.0005512009374797344, -0.060311201959848404, 0.018059279769659042, -0.1298392117023468, 0.01100066863000393, 0.08720334619283676, 0.0815662294626236, -0.03716723993420601, 0.01241912692785263, 0.028786007314920425, -0.007291995920240879, -0.0301812756806612, -0.06045181676745415, 0.014502034522593021, 0.04861218482255936, 0.01629277504980564, -0.010025114752352238, -0.07558370381593704, 0.04157447814941406, -0.019939713180065155, 0.044366516172885895, -0.0010247197933495045, 0.0014023405965417624, -0.02535846456885338, 0.03590778261423111, -0.013309687376022339, -0.02335534058511257, 0.032667335122823715, 0.02567996457219124, 0.011704543605446815, -0.025231460109353065, 0.04580949991941452, -0.012141360901296139, -0.00020075010252185166, 0.049747876822948456, -0.004617948550730944, -0.028765032067894936, -0.04728635400533676, 0.05675152689218521, 0.060256920754909515, -0.03376171737909317, 0.06118851155042648, -0.01454364974051714, 0.07668367773294449, -0.052173465490341187, 0.04424396902322769, 0.08806559443473816, 0.015704885125160217, 0.016676580533385277, -0.012752428650856018, 0.038273800164461136, 0.03147983178496361, 0.04988810792565346, -0.008643219247460365, 0.09213194251060486, -0.0122995525598526, -0.036474842578172684, 0.011775077320635319, 0.0647764801979065, -0.028132878243923187, 0.003957310691475868, -0.08924614638090134, -0.035316336899995804, 0.0037599310744553804, -0.03169974312186241, 0.05276558920741081, -0.04405134916305542, -0.06655240058898926, -0.08647047728300095, -0.02545139193534851, 0.0455397367477417, -0.022813186049461365, 0.035701412707567215, 0.05517001822590828, -0.039255961775779724, 0.008649064227938652, -0.033831946551799774, 0.02417425997555256, 0.014160383492708206, -7.534507365748075e-33, -0.08683449029922485, 0.025294672697782516, -0.13132329285144806, 0.02681383304297924, -0.08717548102140427, -0.008520686998963356, -0.06256583333015442, 0.030220234766602516, -0.08217731863260269, 0.06092645600438118, 0.01634434051811695, 0.019920561462640762, 0.030062668025493622, -0.02359042875468731, -0.09365768730640411, 0.011756807565689087, -0.04054201394319534, 0.04476577043533325, 0.05775731801986694, -0.022179674357175827, 0.04926592856645584, 0.1425911784172058, -0.1211850717663765, 0.051517389714717865, -0.02400856651365757, 0.03411169722676277, -0.07516731321811676, 0.1658104956150055, 0.03306739777326584, 0.0589243546128273, -0.02470875158905983, -0.0311606265604496, -0.0537729449570179, 0.027237851172685623, 0.03888680785894394, 0.050265733152627945, 0.03246118128299713, 0.010090912692248821, -0.05687214061617851, -0.002537015127018094, -0.020922744646668434, -0.0036332125309854746, -0.08595065027475357, -0.05897399038076401, 0.03897745534777641, -0.03681735694408417, -0.0183127261698246, 0.01067985501140356, 0.04516899213194847, -0.031113041564822197, 0.07076193392276764, 0.021655814722180367, -0.07084409147500992, -0.035523660480976105, -0.030256347730755806, 0.04309982806444168, 0.049639277160167694, -0.03039383515715599, -0.029414484277367592, 0.07625298202037811, -0.0279457475990057, -0.09980455785989761, 0.0502544566988945, -0.0009682109230197966, -0.00022931471175979823, 0.003048720769584179, 0.023636313155293465, 0.08379144221544266, 0.08163566142320633, -0.036522552371025085, 0.03418303653597832, 0.04153664782643318, 0.054101910442113876, -0.0023850782308727503, -0.037032026797533035, -0.043735817074775696, 0.00559235131368041, -0.002223936142399907, -0.08542955666780472, -0.050798844546079636, -0.03085111267864704, -0.07597681134939194, 0.021846719086170197, 0.06683681160211563, 0.048765260726213455, 0.028210124000906944, 0.056324832141399384, 0.011023654602468014, 0.062111176550388336, -0.0781707763671875, 0.0652039498090744, 0.05935470759868622, -0.05508234724402428, 0.01915646903216839, -0.012746361084282398, -5.691009619113174e-08, -0.058643195778131485, 0.030167123302817345, 0.0470544807612896, 0.024066943675279617, 0.02796139381825924, -0.017140023410320282, -0.001374104875139892, 0.023807300254702568, -0.08687042444944382, -0.021923022344708443, 0.003984389360994101, 0.03182476758956909, 0.011609025299549103, -0.003580093616619706, 0.025632154196500778, -0.0005990518257021904, -0.0037837717682123184, -0.011408383958041668, 0.01121127512305975, 0.0021305675618350506, 0.07498057186603546, -0.05667354539036751, 0.013439570553600788, 0.03315900266170502, 0.03782374784350395, -0.1280485987663269, -0.11113429069519043, 0.015278356149792671, -0.025787414982914925, 0.10798734426498413, -0.012990863062441349, 0.07518532127141953, -0.008989421650767326, -0.0034468614030629396, 0.10777321457862854, 0.03462051972746849, 0.02151268906891346, -0.055867962539196014, -0.13964888453483582, 0.05162880942225456, -0.033413227647542953, 0.017810214310884476, -0.0683053731918335, 0.01804535649716854, 0.006113202776759863, -0.08517300337553024, 0.08974918723106384, -0.10200252383947372, 0.0073121171444654465, 0.016778020188212395, 0.09034667909145355, 0.018046488985419273, -0.015836432576179504, 0.011777102015912533, -0.0014157509431242943, -0.045825421810150146, -0.012883063405752182, -0.1064564660191536, -0.05623815953731537, 0.0840328112244606, 0.012286413460969925, 0.05161222070455551, 0.02643594890832901, -0.09503620117902756]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ApplicationsofErrorBackPropagationtoPhoneticClassification.pdf,Deep Learning,"TRAINING MULTILAYER PERCEPTRONS WITH THE EXTENDED KALMAN ALGORITHM Sharad Singhal and Lance Wu Bell Communications Research, Inc. Morristown, NJ 07960 ABSTRACT A large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back-propagation algorithm described by Rumelhart et. a1. This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using two- dimensional examples. INTRODUCTION Multilayer perceptrons are one of the most popular artificial neural net structures being used today. In most applications, the ""back propagation"" algorithm [Rllmelhart et ai, 1986] is used to train these networks. Although this algorithm works well for small nets or simple problems, convergence is poor if the problem becomes complex or the number of nodes in the network become large [Waibel et ai, 1987]. In problems sllch as speech recognition, tens of thousands of iterations may be required for convergence even with relatively small elata-sets. Thus there is much interest [Prager anel Fallsiele, 1988; Irie and Miyake, 1988] in other ""training algorithms"" which can compute the parameters faster than back-propagation anel/or can handle much more complex problems. In this paper, we show that training multilayer perceptrons can be viewed as an identification problem for a nonlinear dynamic system. For linear dynamic Copyright 1989. Bell Communications Research. Inc. 133 134 Singhal and Wu systems with white input and observation noise, the Kalman algorithm [Kalman, 1960] is known to be an optimum algorithm. Extended versions of the Kalman algorithm can be applied to nonlinear dynamic systems by linearizing the system around the current estimate of the parameters. Although computationally complex, this algorithm updates parameters consistent with all previously seen data and usually converges in a few iterations. In the following sections, we describe how this algorithm can be applied to multilayer perceptrons and compare its performance with back- propagation using some two-dimensional examples. THE EXTENDED KALMAN FILTER In this section we briefly outline the Extended Kalman filter. Mathematical derivations for the Extended Kalman filter are widely available in the literature [Anderson and Moore, 1979; Gelb, 1974] and are beyond the scope of this paper. Consider a nonlinear finite dimensional discrete time system of the form: x(n+l) = In(x(n» + gn(x(n»w(n), den) = hn(x(n»+v(n). (1) Here the vector x (n) is the state of the system at time n, w (n) is the input, den) is the observation, v(n) is observation noise and In('), gn('), and hn(') are nonlinear vector functions of the state with the subscript denoting possible dependence on time. We assume that the initial state, x (0), and the sequences {v (n)} and {w (n)} are independent and gaussian with E [x (O)]=x(O), E {[x (O)-x (O)][x (O)-i(O»)I} = P(O), E [w (n)] = 0, E [w (n )w t (l)] = Q (n )Onl' E[v(n)] = 0, E[v(n)vt(l)] = R(n)onb (2) where Onl is the Kronecker delta. Our problem is to find an estimate i (n +1) of x (n +1) given d (j) , O<j <n. We denote this estimate by i (n +11 n). If the nonlinearities in (1) are sufficiently smooth, we can expand them llsing Taylor series about the state estimates i (n In) and i (n In -1) to obtain In(x(n» = I"" (i(n In» + F(n)[x(n)-i(n In)] + ... gn(x(n» = gil (i(n In» + ... = C(n) + ... hn(x(n» = hll(i(n In-I» + J-f1(n)[x(n)-i(n In-1)] + where C(ll) = gn(i(n Ill», din (x) dhll (x) F (ll ) = ---. -- , I-P (n ) = --., --- (3) ax x = .i (II III) Ox x=i(IIII1-1) i.e. G (n) is the value of the function g"" (.) at i (n In) and the ij th components of F (n) and H' (n) are the partial derivatives of the i th components of f II (.) and hll (-) respectively with respect to the j th component of x (n) at the points indicated. Neglecting higher order terms and assuming Training Multilayer Perceptrons 135 knowledge of i (n In) and i (n In-I), the system in (3) can be approximated as where x(n+l) = F(n)x(n) + G(n)w(n) + u(n) n>O z (n ) = HI (n )x (n )+ v (n) + y (n ), u(n) = /n(i(n In» - F(n)i(n In) y(n) = hn(i(n In-I» - H1(n)i(n In-1). (4) (5) It can be shown [Anderson and Moore, 1979] that the desired estimate i (n + 11 n) can be obtained by the recursion i(n+1In) =/n(i(n In» (6) i(n In) = i(n In-I) + K(n)[d(n) - hn(i(n In-1»] (7) K(n) = P(n In-I)H(n)[R(n)+HI(n)P(n In-I)H(n)tl (8) P(n+Iln) = F(n)P(n In)FI(n) + G(n)Q(n)G1(n) (9) P(n In) = P(n In-I) - K(n)HI(n)P(n In-I) (10) with P(11 0) = P (0). K (n) is known as the Kalman gain. In case of a linear system, it can be shown that P(n) is the conditional error covariance matrix associated with the state and the estimate i (n +1/ n) is optimal in the sense that it approaches the conditional mean E [x (n + 1) I d (0) ... d (n)] for large n . However, for nonlinear systems, the filter is not optimal and the estimates can only loosely be termed conditional means. TRAINING MULTILAYER PERCEPTRONS The network under consideration is a L layer perceptronl with the i th input of the k th weight layer labeled as :J-l(n), the jth output being zjk(n) and the weight connecting the i th input to the j th output being (}i~j' We assume that the net has m inputs and I outputs. Thresholds are implemented as weights connected from input nodes2 with fixed unit strength inputs. Thus, if there are N (k) nodes in the k th node layer, the total number of weights in the system is L M = ~N(k-l)[N(k)-l]. (11) k=1 Although the inputs and outputs are dependent on time 11, for notational brevity, we wil1 not show this dependence unless explicitly needed. l. We use the convention that the number of layers is equal to the number of weight layers . Thus we have L layers of Wl'iglrls labeled 1 · L and I ~ + I layers of /lodes (including the input and output nodes) labeled O · . . L . We will refer to the kth weight layer or the kth node layer unless the context is clear. 2. We adopt the convention that the 1st input node is the threshold. i.e. lit., is the threshold for the j th output node from the k th weight layer. 136 Singhal and Wu In order to cast the problem in a form for recursive estimation, we let the weights in the network constitute the state x of the nonlinear system, i.e. x = [Ob,Ot3 ... 0k(O),N(l)]t. (12) The vector x thus consists of all weights arranged in a linear array with dimension equal to the total number of weights M in the system. The system model thus is x(n+l)=x(n) n>O, den) = zL(n) + v(n) = hn(x(n),zO(n)) + v(n), (13) (14) where at time n, zO(n) is the input vector from the training set, d (n) is the corresponding desired output vector, and ZL (n) is the output vector produced by the net. The components of hn (.) define the nonlinear relationships between the inputs, weights and outputs of the net. If r(·) is the nonlinearity used, then ZL (n) = hn (x (n ),zO(n)) is given by zL(n) = r{(OL)tr{(OL-l)tr ... r{(OlyzO(n)}· .. }}.. (15) where r applies componentwise to vector arguments. Note that the input vectors appear only implicitly through the observation function hn ( . ) in (14). The initial state (before training) x (0) of the network is defined by populating the net with gaussian random variables with a N(x(O),P(O)) distribution where x (0) and P (0) reflect any apriori knowledge about the weights. In the absence of any such knowledge, a N (0,1/f. I) distribution can be used, where f. is a small number and I is the identity matrix. For the system in (13) and (14), the extended Kalman filter recursion simplifies to i(I1+1) = i(n) + K(n)[d(n) - hn(i(n),zO(n))] K (n) = P(n)H (n )[R (n )+H' (n )P(n )H(n )]-1 Pen +1) = P(n) - K (n )Ht (n)P (n) where P(n) is the (approximate) conditional error covariance matrix. (16) (17) (18) Note that (16) is similar to the weight update equation in back-propagation with the last term [ZL - hn (x ,ZO)] being the error at the output layer. However, unlike the delta rule used in back-propagation, this error is propagated to the weights through the Kalman gain K (n) which updates each weight through the entire gradient matrix H (n) and the conditional error covariance matrix P (n ). In this sense, the Kalman algorithm is not a local training algorithm . However, the inversion required in (17) has dimension equal to the llumber of outputs I, 110t the number of weights M, and thus does not grow as weights arc added to the problem. EXAMPLES AND RESULTS To evaluale the Olltpul and the convergence properties of the extended Kalman algorithm. we constructed mappings using two-dimensional inputs with two or four outputs as shown in Fig. 1. Limiting the input vector to 2 dimensions allows liS to visualize the decision regiolls ohtained by the net and Training Multilayer Perceptrons 137 to examine the outputs of any node in the net in a meaningful way. The x- and y-axes in Fig. 1 represent the two inputs, with the origin located at the center of the figures. The numbers in the figures represent the different output classes. 2 1 - - t------+-----I 1 2 I (a) REGIONS (b) XOR Figure 1. Output decision regions for two problems The training set for each example consisted of 1000 random vectors uniformly filling the region. The hyperbolic tangent nonlinearity was used as the nonlinear element in the networks. The output corresponding to a class was set to 0.9 when the input vector belonged to that class, and to -0.9 otherwise. During training, the weights were adjusted after each data vector was presented. Up to 2000 sweeps through the input data were used with the stopping criteria described below to examine the convergence properties. The order in which data vectors were presented was randomized for each sweep through the data. In case of back-propagation, a convergence constant of 0.1 was used with no ""momentum"" factor. In the Kalman algorithm R was set to I ·e-k / 50 , where k was the iteration number through the data. Within each iteration, R was held constant. The Stopping Criteria Training was considered complete if anyone of the following con~itions was satisfied: a. 2000 sweeps through the input data were used, h. the RMS (root mean squared) error at the output averaged over all training data during a sweep fell below a threshold 11' or c. the error reduction 8 after the i th sweep through the data fell below a threshold I::., where 8; = !3b;_1 + (l-,B) I ei-ei_l I. Here !3 is some positive constant less than unity, and ei is the error defined in b. In our simulations we set ;3 = 0.97, II = 10-2 and 12 = 10-5• 138 Singhal and Wu Example 1 - Meshed, Disconnected Regions: Figure l(a) shows the mapping with 2 disconnected, meshed regions surrounded by two regions that fill up the space. We used 3-layer perceptrons with 10 hidden nodes in each hidden layer to Figure 2 shows the RMS error obtained during training for the Kalman algorithm and back-propagation averaged over 10 different initial conditions. The number of sweeps through the data (x-axis) are plotted on a logarithmic scale to highlight the initial reduction for the Kalman algorithm. Typical solutions obtained by the algorithms at termination are shown in Fig. 3. It can be seen that the Kalman algorithm converges in fewer iterations than back-propagation and obtains better solutions. 1 0.8 Average 0.6 RMS Error 0.4 backprop 0.2 Kalman 0 1 2 5 10 20 50 100 200 500 10002000 No. of Iterations Figure 2. Average output error during training for Regions problem using the Kalman algorithm and backprop I I (a) (b) Figure 3. Typical solutions for Regions problem using (a) Kalman algorithm and (h) hackprop. Training Multilayer Perceptrons 139 Example 2 - 2 Input XOR: Figure 1(b) shows a generalized 2-input XOR with the first and third quadrants forming region 1 and the second and fourth quadrants forming region 2. We attempted the problem with two layer networks containing 2-4 nodes in the hidden layer. Figure 4 shows the results of training averaged over 10 different randomly chosen initial conditions. As the number of nodes in the hidden layer is increased, the net converges to smaller error values. When we examine the output decision regions, we found that none of the nets attempted with back-propagation reached the desired solution. The Kalman algorithm was also unable to find the desired solution with 2 hidden nodes in the network. However, it reached the desired solution with 6 out of 10 initial conditions with 3 hidden nodes in the network and 9 out of 10 initial conditions with 4 hidden nodes. Typical solutions reached by the two algorithms are shown in Fig. 5. In all cases, the Kalman algorithm converged in fewer iterations and in all but one case, the final average output error was smaller with the Kalman algorithm. 1 0.8 Average 0.6 RMS Error 0.4 Kalman 3 nodes 0.2 Kalman 4 nodes 0 1 2 5 10 20 50 100 200 500 10002000 No. of Iterations Figure 4. Average output error during training for XOR problem using the Kalman algorithm and backprop CONCLUSIONS In this paper, we showed that training feed-forward nets can be viewed as a system identification problem for a nonlinear dynamic system. For linear dynamic systems, the Kalman tllter is known to produce an optimal estimator. Extended versions of the Kalman algorithm can be used to train feed-forward networks. We examined the performance of the Kalman algorithm using artifkially constructed examples with two inputs and found that the algorithm typically converges in a few iterations. We also llsed back-propagation on the same examples and found that invariably, the Kalman algorithm converged in 140 Singhal and Wu l 2 1 ~ 1 2 I 2 I I Figure 5. (a) (b) Typical solutions for XOR problem using (a) Kalman algorithm and (b) backprop. fewer iterations. For the XOR problem, back-propagation failed to converge on any of the cases considered while the Kalman algorithm was able to find solutions with the same network configurations. References [1] B. D. O. Anderson and J. B. Moore, Optimal Filtering, Prentice Hall, 1979. [2] A. Gelb, Ed., Applied Optimal Estimation, MIT Press, 1974. [3] B. Irie, and S. Miyake, ""Capabilities of Three-layered Perceptrons,"" Proceedings of the IEEE International Conference on Neural Networks, San Diego, June 1988, Vol. I, pp. 641-648. [4] R. E. Kalman, ""A New Approach to Linear Filtering and Prediction Problems,"" 1. Basic Eng., Trans. ASME, Series D, Vol 82, No.1, 1960, pp.35-45. [5] R. W. Prager and F. Fallside, ""The Modified Kanerva Model for Automatic Speech Recognition,"" in 1988 IEEE Workshop on Speech Recognition, Arden House, Harriman NY, May 31-Jllne 3,1988. [6] D. E. Rumelharl, G. E. Hinton and R. J. Williams, ""Learning Internal Representations by Error Propagation,"" in D. E. Rllmelhart and J. L. McCelland (Eds.), Parallel Distributed Processing: Explorations in the Microstructure oj' Cognition. Vol 1: Foundations. MIT Press, 1986. [7J A. Waibel, T. Hanazawa, G. Hinton, K. Shikano and K. Lang ""Phoneme Recognition Using Time-Delay Neural Networks,"" A 1R internal Report TR-I-0006, October 30, 1987.","[-0.11151232570409775, -0.10688219964504242, 0.029065381735563278, -0.014007840305566788, -0.04017944633960724, 0.06551102548837662, 0.018138805404305458, -0.07061541825532913, -0.009073957800865173, -0.06792519986629486, -0.048751477152109146, 0.01924130506813526, -0.003321561962366104, -0.015953877940773964, -0.1355762779712677, 0.044837482273578644, 0.03583822399377823, 0.0446554571390152, 0.003584094112738967, -0.08203393965959549, 0.0012691353913396597, 0.05082281306385994, -0.03810788691043854, 0.045264407992362976, 0.03555017709732056, 0.10600725561380386, -0.045392461121082306, 0.013139801099896431, -0.005084332078695297, 0.06432382017374039, 0.02165418490767479, -0.07562660425901413, 0.011562533676624298, 0.032929256558418274, -0.10510282218456268, 0.044527094811201096, -0.1191510334610939, 0.06955011934041977, -0.08522314578294754, 0.028597982600331306, 0.007075524423271418, 0.0537981353700161, -0.05260254442691803, 0.04510194808244705, 0.10054552555084229, -0.02139165624976158, 0.007911481894552708, -0.026762621477246284, 0.022853920236229897, 0.009029540233314037, -0.024965768679976463, 0.005294330418109894, 0.029788905754685402, 0.022740736603736877, 0.002118277596309781, -0.05894237756729126, -0.05657516419887543, 0.10495542734861374, 0.0003979577450081706, 0.10582753270864487, 0.042196739464998245, -0.04779871925711632, 0.036914460361003876, -0.06359421461820602, 0.04190295562148094, 0.03989829123020172, -0.03710212558507919, 0.04792184382677078, -0.026355963200330734, -0.036613401025533676, 0.034536559134721756, 0.08720613270998001, -0.006178069394081831, 0.01509549468755722, 0.030354872345924377, -0.04260241985321045, 0.05272124707698822, 0.004989390727132559, 0.010641227476298809, 0.01888538897037506, 0.025874650105834007, -0.021768677979707718, 0.025563878938555717, -0.03421025723218918, 0.04182237386703491, -0.022668231278657913, -0.05845339596271515, -0.009824186563491821, 0.006124647334218025, -0.03336762264370918, -0.008840919472277164, -0.06378819048404694, -0.056669458746910095, -0.05663324519991875, 0.11809107661247253, 0.028108572587370872, -0.03758418560028076, 0.03505528345704079, -0.005153126548975706, 0.062210191041231155, -0.012147541157901287, -0.0064128912054002285, 0.01769421249628067, -0.013095155358314514, 0.05936312675476074, 0.010097671300172806, 0.02301839180290699, -0.04644293710589409, 0.06791488826274872, -0.11911225318908691, -0.037161506712436676, 0.06466807425022125, -0.013519348576664925, 0.02505352720618248, 0.06068006157875061, -0.0250154510140419, 0.07051608711481094, -0.006442710757255554, 0.015021061524748802, 0.1335708647966385, -0.10699161887168884, -0.06523287296295166, -0.0026957953814417124, 0.046948645263910294, 0.07754077762365341, 0.025307374075055122, -0.04789168760180473, 4.6683931623441726e-33, -0.12142070382833481, 0.07607197016477585, 0.06663383543491364, -0.06640844792127609, -0.03557318076491356, -0.043980248272418976, 0.020777730271220207, -0.06327261030673981, 0.01083375234156847, 0.01752324402332306, -0.038349736481904984, 0.04363632947206497, -0.033712148666381836, -0.031389690935611725, 0.0742860808968544, -0.06811849772930145, 0.036851927638053894, 0.0028033608105033636, 0.05710920691490173, -0.07246725261211395, 0.0721605122089386, -0.058989398181438446, 0.05647733435034752, -0.014683795161545277, 0.00059445237275213, 0.018950192257761955, 0.13075920939445496, -0.022316506132483482, -0.0012275633634999394, 0.01369563303887844, 0.06325451284646988, 0.04350961744785309, -0.05386078357696533, -0.03605443239212036, 0.014041795395314693, -0.01465946901589632, -0.044351134449243546, 0.015913547948002815, 0.04280000552535057, -0.039392318576574326, -0.02088860049843788, -0.015280419029295444, -0.003109289100393653, 0.013322098180651665, -0.06803932785987854, -0.08759720623493195, -0.08060897141695023, 0.009882700629532337, -0.03224069997668266, -0.08556374162435532, -0.0630795881152153, -0.038615886121988297, -0.05693574622273445, -0.07415471225976944, 0.03671626001596451, -0.04758409783244133, 0.026274889707565308, 0.07054765522480011, 0.042676154524087906, 0.051783375442028046, 0.03855018690228462, -0.027498964220285416, -0.030843771994113922, 0.07209581136703491, 0.05159984901547432, -0.019213948398828506, -0.06405385583639145, 0.07447139173746109, 0.06229567527770996, -0.006933783646672964, 0.0728253647685051, -0.03581070527434349, 0.019949896261096, -0.06055084988474846, 0.03856544941663742, -0.017633333802223206, -0.045192278921604156, -0.049956340342760086, -0.059992313385009766, 0.01696871966123581, -0.04577326029539108, 0.02439878322184086, 0.0017745980294421315, -0.0038908037822693586, -0.011848866008222103, -0.021250495687127113, -0.055839184671640396, -0.037601493299007416, 0.02051723375916481, 0.03897765278816223, -0.06311502307653427, 0.03290576487779617, -0.0006134689901955426, 0.03550078719854355, -0.02895265445113182, -4.8004272592224244e-33, -0.04753464087843895, 0.06681353598833084, -0.027292368933558464, 0.006590555422008038, -0.016203688457608223, -0.0010795786511152983, -0.0352766178548336, 0.1335155963897705, -0.028126606717705727, 0.02475423738360405, -0.001335601438768208, -0.03785563260316849, 0.010028823278844357, 0.01988350600004196, 0.025748584419488907, 0.04617787525057793, -0.02537025325000286, 0.05108243599534035, 0.09974422305822372, 0.012592854909598827, -0.03341497853398323, 0.020776871591806412, -0.04118814319372177, 0.04860834777355194, -0.033120863139629364, -0.06316755712032318, -0.0030857527162879705, 0.07368560135364532, -0.010972440242767334, -0.06277748942375183, -0.06301364302635193, -0.08482398837804794, -0.04004941135644913, 0.08935053646564484, -0.035710569471120834, 0.009048876352608204, 0.013761659152805805, -0.002156090922653675, 0.045678649097681046, 0.008604750037193298, 0.07235760986804962, 0.035734470933675766, -0.013559496030211449, -0.0293804369866848, 0.03263997286558151, -0.0747409462928772, -0.052302949130535126, 0.11082679033279419, -0.007858328521251678, -0.02767678163945675, 0.02746627666056156, -0.07866078615188599, 0.03604668006300926, -0.01804366149008274, -0.04340207576751709, 0.08178242295980453, 0.017080096527934074, 0.043879833072423935, 0.07117298245429993, -0.05155356973409653, -0.10697238892316818, -0.10676933079957962, 0.054515291005373, 0.050508566200733185, 0.018149176612496376, -0.013461882248520851, 0.0520792230963707, 0.04653564468026161, 0.03790585324168205, -0.034615982323884964, -0.034008242189884186, 0.049821887165308, -0.004380558151751757, 0.05100226029753685, -0.018215514719486237, 0.008606747724115849, -0.039298150688409805, -0.03183708339929581, -0.009012616239488125, -0.055218879133462906, 0.007095862179994583, -0.010918764397501945, 0.006692420691251755, -0.022112319245934486, 0.11215920001268387, 0.05700339749455452, 0.09380735456943512, 0.02147054858505726, -0.006112809292972088, -0.01326654851436615, 0.034670498222112656, 0.1052132099866867, 0.020244738087058067, -0.05956007540225983, -0.029994182288646698, -4.510579287853034e-08, -0.10014466196298599, -0.01160493865609169, 0.03940192237496376, -0.07434187829494476, 0.130378857254982, -0.08826044946908951, 0.05202652886509895, -0.017498960718512535, 0.001730649033561349, -0.08421406149864197, 0.03826126083731651, -0.013142404146492481, 0.034403275698423386, -0.042430195957422256, -0.030656427145004272, 0.024233702570199966, -0.013840696774423122, -0.03390803188085556, 0.06704220920801163, 0.043999094516038895, 0.032426051795482635, 0.04942631348967552, 0.02343159168958664, 0.10076244175434113, 0.043836161494255066, -0.01279255747795105, -0.07644864916801453, 0.039651401340961456, -0.02188829891383648, 0.04854092746973038, -0.01383382547646761, 0.02262968383729458, -0.02317981980741024, -0.01725883223116398, 0.03065825253725052, 0.052694592624902725, 0.03242415189743042, -0.02902771532535553, -0.03784466162323952, -0.05502428486943245, -0.030345004051923752, 0.035573314875364304, 0.022840294986963272, -0.025950737297534943, 0.06839786469936371, -0.0019480744376778603, -0.006926150061190128, -0.1417209655046463, 0.007934514433145523, -0.05226348713040352, 0.0796288326382637, 0.0022423628251999617, 0.030091986060142517, -0.009773659519851208, 0.05094951391220093, -0.018536239862442017, 0.03442968428134918, -0.11405979096889496, -0.0027586384676396847, 0.04835880547761917, -0.06076056510210037, 0.0450759083032608, -0.0691630020737648, -0.007115187589079142]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ASelfLearningNeuralNetwork.pdf,Deep Learning,"ANALOG IMPLEMENTATION OF SHUNTING NEURAL NETWORKS Bahram Nabet, Robert B. Darling, and Robert B. Pinter Department of Electrical Engineering, FT-lO University of Washington Seattle, WA 98195 ABSTRACT An extremely compact, all analog and fully parallel implementa- tion of a class of shunting recurrent neural networks that is ap- plicable to a wide variety of FET-based integration technologies is proposed. While the contrast enhancement, data compression, and adaptation to mean input intensity capabilities of the network are well suited for processing of sensory information or feature extrac- tion for a content addressable memory (CAM) system, the network also admits a global Liapunov function and can thus achieve stable CAM storage itself. In addition the model can readily function as a front-end processor to an analog adaptive resonance circuit. INTRODUCTION Shunting neural networks are networks in which multiplicative, or shunting, terms of the form Xi Lj f;(Xj) or Xi Lj Ij appear in the short term memory equations, where Xi is activity of a cell or a cell population or an iso-potential portion of a cell and Ii are external inputs arriving at each site. The first case shows recurrent activity, while the second case is non-recurrent or feed forward. The polarity of these terms signify excitatory or inhibitory interactions. Shunting network equations can be derived from various sources such as the passive membrane equation with synaptic interaction (Grossberg 1973, Pinter 1983), models of dendritic interaction (RaIl 1977), or experiments on motoneurons (Ellias and Grossberg 1975). While the exact mechanisms of synaptic interactions are not known in every in- dividual case, neurobiological evidence of shunting interactions appear in several 695 696 Nabet, Darling and Pinter areas such as sensory systems, cerebellum, neocortex, and hippocampus (Grossberg 1973, Pinter 1987). In addition to neurobiology, these networks have been used to successfully explain data from disciplines ranging from population biology (Lotka 1956) to psychophysics and behavioral psychology (Grossberg 1983). Shunting nets have important advantages over additive models which lack the ex- tra nonlinearity introduced by the multiplicative terms. For example, the total activity of the network, shown by Li Xi, approaches a constant even as the input strength grows without bound. This normalization in addition to being computa- tionally desirable has interesting ramifications in visual psychophysics (Grossberg 1983). Introduction of multiplicative terms also provides a negative feedback loop which automatically controls the gain of each cell, contributes to the stability of the network, and allows for large dynamic range of the input to be processed by the network. The automatic gain control property in conjunction with properly chosen nonlinearities in the feedback loop makes the network sensitive to small input values by suppressing noise while not saturating at high input values (Grossberg 1973). Finally, shunting nets have been shown to account for short term adaptation to input properties, such as adaptation level tuning and the shift of sensitivity with background strength (Grossberg 1983), dependence of visual size preference and latency of response on contrast and mean luminance, and dependence of temporal and spatial frequency tuning on contrast and mean luminance (Pinter 1985). IMPLEMENTATION The advantages, generality, and applicability of shunting nets as cited in the previ- ous section make their implementation very desirable, but digital implementation of these networks is very inefficient due to the need for analog to digital conver- sion, multiplication and addition instructions, and implementation of iterative al- gorithms. A linear feedback class of these networks (Xi Lj !; (Xj) = Xi Li J{ijXj), however, can be implemented very efficiently with simple, completely parallel and all analog circuits. FRAMEWORK Figure 1 shows the design framework for analog implementation of a class of shunt- ing nets. In this design addition (subtraction) is achieved, via Kirchoff's current law by placing transistors in upper (lower) rails, and through the choice of deple- tion or enhancement mode devices. Multiplicative, or shunting, interconnections are done by one transistor per interconnect, using a field-effect transistor (FET) in the voltage-variable conductance region. Temporal properties are characterized by cell membrane capacitance C, which can be removed, or in effect replaced by the parasitic device capacitances, if higher speed is desired. A buffer stage is necessary for correct polarity of interconnections and the large fan-out associated with high connectivity of neural networks. Analog Implementation of Shunting Neural Networks 697 + x. , I. , c Vdd -t"" '.J . x· J Vss Figure 1. Design framework for implementation of one cell in a shunting network. Voltage output of other cells is connected to the gate of transistors Qi,i' Such a circuit is capable of implementing the general network equation: (1) Excitatory and inhibitory input current sources can also be shunted, with extra circuitry, to implement non-recurrent shunting networks. NMOS, CMOS and GALLIUM ARSENIDE Since the basic cell of Fig. 1 is very similar to a standard logic gate inverter, but with the transistors sized by gate width-to-Iength ratio to operate in the nonsaturated current region, this design is applicable to a variety of FET technologies including NMOS, CMOS, and gallium arsenide (GaAs). A circuit made of all depletion-mode devices such as GaAs MESFET buffered FET logic, can implement all the terms of Eq. (1) except shunting excitatory terms and requires a level shifter in the buffer stage. A design with all enhancement mode devices such as silicon NMOS can do the same but without a level shifter. With the addition of p-channel devices, e.g. Si CMOS, all polarities and all terms of Eq. (1) can be realized. As mentioned previously a buffer stage is necessary for correct polarity of interconnections and fan out/fan in capacity. Figure 2 shows a GaAs MESFET implementation with only depletion mode devices which employs a level shifter as the buffer stage. 698 Nabet, Darling and Pinter VDD-------------r------~--------------~-- INPUTS: EXTERNAL OR FROM PREVJOUS LAYER EXCITATORY CONNECTIONS INHIBITORY CONNECTIONS GN~-L--~------~------~-- TUNABLE SELF-RELAXATION CONNECTION OUTPUT TO NEXT LAYER VSS--~--....L....­ LEVEL SHIFT AND BUFFER STAGE Figure 2. Gallium arsenide MESFET implementation with level shifter and depletion mode devices. Lower rail transistors produce shunting off-surround terms. Upper transistors can produce addi- tive excitatory connections. SPECIFIC IMPLEMENTATION The simplest shunting network that can be implemented by the general framework of Fig.1 is Fig. 2 with only inhibitory connections (lower rail transistors). This circuit implements the network model dX· "" d/ = Ii - a,X, + Xi(J(iXi) - Xi(L....J J(ijXj) j#i (2), The simplicity of the implementation is notable; a linear array with nearest neighbor interconnects consists of only 5 transistors, 1-3 diodes, and if required 1 capacitor per cell. A discrete element version of this implementation has been constructed and shows good agreement with expected properties. Steady state output is proportional to the square root of a uniform input thereby compressing the input data and showing adaptation to mean input intensity (figure 3). The network exhibits contrast en- hancement of spatial edges which increases with higher mean input strength (figure 4). A point source input elicits an on-center off-surround response, similar to the difference-of-Gaussians receptive field of many excitable cells. This 'receptive field' becomes more pronounced as the input intensity increases, showing the dependence of spatial frequency tuning on mean input level (figure 5). The temporal response of the network is also input dependent since the time constant of the exponential Analog Implementation of Shunting Neural Networks 699 decay of the impulse response decreases with input intensity. Finally, the depen- dence of the above properties on mean input strength can be tuned by varying the conductance of the central FET. 700.0 --- > E 11.1 600.0 500.0 · II 400.0 !:i ~ 5 300.0 ~ o 200.0 100.0 0.1 0.3 0.5 0 .7 0.9 1.1 1.3 1.5 1.7 1.11 2.1 INPUT CURRENT rnA Figure 3. Response of network to uniform input. Output is pro- portional to the square root of the input. DEPENDENCE OF ENHANCEMENT ON MEAN INPUT 1.5 1.4 ~ :J 1.3 0 ... a 1.2 N i 1.1 II: 0 z 5 1.0 ~ 0 0 .11 a f ... 0.8 ::l IL ~ 0 .7 0.11 2 3 5 7 CELL NUMBER Figure 4. Response of network to spatial edge patterns with the same contrast but increasing mean input level. 700 Nabet, Darling and Pinter Imox ~ 2.0J rnA, llmox - J75.74 mil 1.0 0.11 ~ g 0.11 :> 0 Q ~ 0 .7 !5 II. I iii 0.11 i 0.5 II: 0 Z 0.4 O.J 2 J 4 5 II 7 cEll NUMBER INPUT -x- OUTPUT Figure 5. Response of network to a point source input. Inset shows the receptive field of fly's lamina monopolar cells (LMC of Lucilia sericata). Horizontal axis of inset in visual angle, vertical a.xis relative voltage units of hyperpolarization. Inset from Pinter et al. (in preparation) CONTENT ADDRESSABILITY AND RELATION TO ART Using a theorem by Cohen and Grossberg (1983), it can be shown that the network equa.tion (2) a.dmits the global Liapunov function n n V - - ""'(l·ln(xi) - a'x' + K'x~) +.! '"" K··x 'XL - ~ 1 >. 1 1 1 1 2 ~ IJ J .. , ;=1 j,k=l (3) where>. is a constant, under the constraints Kij = Kji and Xi > O. This shows that in response to an arbitrary input the network always approaches an equilibrium point. The equilibria represent stored patterns and this is Content Addressable Memory (CAM) property. In addition, Eq. (2) is a special case of the feature representation field of an analog adaptive resonance theory ART-2 circuit, (Carpenter and Grossberg 1987), and hence this design can operate as a module in a learning multilayer ART architecture. Analog Implementation of Shunting Neural Networks 701 FUTURE PLANS Due to the very small number of circuit components required to construct a cell, this implementation is quite adaptable to very high integration densities. A solid state implementation of the circuit of figure (2) on a gallium arsenide substrate, chosen for its superiority for opto-electronics applications, is in progress. The chip includes monolithically fabricated photosensors for processing of visual information. All of the basic components of the circuit have been fabricated and tested. With standard 2 micron GaAs BFL design rules, a chip could contain over 1000 cells per cm2 , assuming an average of 20 inputs per cell. CONCLUSIONS The present work has the following distinguishing features: • Implements a mathematically well described and stable model. • Proposes a framework for implementation of shunting nets which are biologically feasible, explain variety of psychophysical and psychological data and have many desirable computational properties. • Has self-sufficient computational capabilities; especially suited for processing of sen- sory information in general and visual information in particular (N abet and Darling 1988). • Produces a 'good representation' of the input data which is also compatible with the self-organizing multilayer neural network architecture ART-2. • Is suitable for implementation in variety of technologies. • Is parallel, analog, and has very little overhead circuitry . ..-. 702 N abet, Darling and Pinter REFERENCES Carpenter, G.A. and Grossberg, S. (1987) ""ART 2: self organization of stable cat- egory recognition codes for analog input patterns,"". Applied Optics 26, pp. 4919- 4930. Cohen,M.A. and Grossberg, S. (1983) ""Absolute stability of global pattern forma- tion and parallel memory storage by competitive neural networks"" , IEEE Transac- tions on Systems Man and Cybernetics SMC-13, pp. 815-826. Ellias, S.A. and Grossberg, S. (1975) ""Pattern formation, contrast control, and oscillations in the short term memory of shunting on-center off-surround networks"" Biological Cybernetics, 20, pp. 69-98. Grossberg, S. (1973), ""Contour enhancement, Short term memory and constancies in reverberating neural networks,"" Studies in Applied Mathematics, 52, pp. 217- 257. Grossberg, S. (1983), ""The quantized geometry of visual space: the coherent com- putation of depth, form, and lightness."" The behavioral and brain sciences, 6, pp. 625-692. Lotka, A.J. (1956). Elements of mathematical biology. New York: Dover. Nabet, B. and Darling, R.B. (1988). ""Implementation of optical sensory neural net- works with simple discrete and monolithic circuits,"" (Abstract) Neural Networks, Vol.l, Suppl. 1, 1988, pp. 396. Pinter, R.B., (1983). ""The electrophysiological bases for linear and nonlinear prod- uct term lateral inhibition and the consequences for wide-field textured stimuli"" J. Theor. Bioi. 105 pp. 233-243. Pinter, R.B. (1985) "" Adaptation of spatial modulation transfer functions via non- linear lateral inhibition"" Bioi. Cybernetics 51, pp. 285-291. Pinter, R.B. (1987) ""Visual system neural networks: Feedback and feedforward lat- eral inhibition"" Systems and Control Encyclopedia (ed.M.G. Singh) Oxford: Perg- amon Press. pp. 5060-5065. Pinter, R.B., Osorio, D., and Srinivasan, M.V., (in preperation) ""Shift of edge preference to scototaxis depends on mean luminance and is predicted by a matched filter hypothesis in fly lamina cells"" RaIl, W. (1977). ""Core conductor theory and cable properties of neurons"" in Hand- book of Physiology: The Nervous System vol. I, part I, Ed. E.R. Kandel pp. 39-97. Bethesda, MD: American Physiological Society.","[-0.08212707936763763, -0.08683919906616211, 0.01025491114705801, -0.013398578390479088, -0.028336215764284134, 0.03502146899700165, 0.05872241407632828, 0.02718559466302395, 0.03566858172416687, -0.02034703455865383, 0.0056400420144200325, 0.037633176892995834, 0.0646413266658783, -0.029631400480866432, -0.03648717328906059, 0.05238223820924759, -0.017017723992466927, 0.09339884668588638, -0.051945917308330536, -0.0353679433465004, 0.05674411356449127, 0.005646520294249058, -0.01933743804693222, 0.008555583655834198, -0.016930000856518745, 0.009925858117640018, -0.05540145933628082, -0.02395184338092804, 0.03855857625603676, -0.05564553290605545, 0.07986404746770859, -0.008756554685533047, -0.025942528620362282, 0.014706142246723175, -0.09958915412425995, 0.06791351735591888, -0.08289825171232224, -0.0796472430229187, -0.009459581226110458, 0.009391924366354942, 0.0024259481579065323, 0.06900137662887573, -0.02202531322836876, -0.044908005744218826, 0.08475830405950546, -0.006086291279643774, 0.045510560274124146, -0.09899955242872238, -0.0026629434432834387, -0.04172901436686516, -0.02184225805103779, 0.03887562081217766, -0.056348614394664764, 0.09323109686374664, 0.0014091989723965526, 0.032011691480875015, -0.06812514364719391, 0.05450749024748802, -0.06706307083368301, -0.02079942636191845, 0.023599375039339066, -0.04294344782829285, 0.08487996459007263, -0.0027314918115735054, -0.05643844231963158, -0.01273334864526987, 0.03522609919309616, 0.06434737145900726, 0.06241645663976669, -0.012688476592302322, -0.008552971296012402, 0.04986966401338577, -0.0600535124540329, -0.02558232471346855, 0.006115229334682226, 0.047041766345500946, 0.11495111137628555, 0.009160397574305534, 0.08393408358097076, -0.00617213686928153, 0.03688552975654602, 0.018126580864191055, -0.020121201872825623, -0.04646480828523636, 0.07154801487922668, 0.014244993217289448, -0.023395318537950516, 0.030787648633122444, -0.028568310663104057, -0.06479175388813019, -0.029489658772945404, -0.06458583474159241, -0.003142746165394783, -0.025197017937898636, 0.007222427520900965, -0.0565473772585392, 0.08183769881725311, 0.014570528641343117, -0.05036596953868866, 0.11203694343566895, 0.04840916395187378, 0.05934829264879227, 0.003222124418243766, 0.006456104107201099, 0.06517279893159866, -0.05995837226510048, 0.06540927290916443, 0.06760917603969574, -0.032492853701114655, -0.08872910588979721, -0.003575589507818222, 0.04447011277079582, 0.015688279643654823, -0.010576814413070679, -0.00698004150763154, -0.02950107865035534, -0.002575821476057172, -0.08723841607570648, 0.02175481803715229, -0.010127323679625988, -0.010046450421214104, -0.08138827234506607, -0.1309322565793991, 0.03994407504796982, 0.01573534868657589, -0.047665610909461975, -0.04354235529899597, 4.1923275011527466e-33, -0.041899047791957855, 0.03721782937645912, 0.04873809590935707, -0.08086176216602325, 0.08960753679275513, -0.005566886626183987, 0.013026989996433258, 0.028234224766492844, 0.0017500178655609488, -0.024188127368688583, 0.0066321901977062225, 0.04386153072118759, -0.043454814702272415, 0.09425349533557892, -0.006993662565946579, -0.08912787586450577, -0.044075001031160355, -0.0647817999124527, 0.04025749862194061, -0.1322801411151886, 0.049350474029779434, -0.09470202773809433, 0.011882543563842773, 0.03221263736486435, -0.00034188598510809243, -0.046563565731048584, 0.06048008054494858, 0.048923980444669724, -0.02657918445765972, 0.009587438777089119, 0.00594755494967103, 0.03255308419466019, -0.06071235239505768, 0.021590502932667732, 0.03197476640343666, -0.09403280168771744, 0.07444994896650314, -0.020022355020046234, 0.06677042692899704, -0.04058081656694412, 0.01271363440901041, 0.03819656744599342, -0.019230924546718597, 0.025898631662130356, -0.06847894191741943, -0.10863323509693146, 0.005926717072725296, 0.072393037378788, -0.00397114735096693, -0.006782036740332842, -0.037971049547195435, -0.06142725422978401, -0.059072427451610565, -0.009444176219403744, -0.007440926972776651, -0.027835391461849213, 0.010912892408668995, 0.034070439636707306, 0.021297814324498177, 0.16458335518836975, -0.08233118057250977, 0.0036976714618504047, 0.01719358004629612, 0.05054718255996704, -0.03220697119832039, 0.07538503408432007, -0.0037490713875740767, 0.02946135215461254, 0.09833279997110367, -0.030897540971636772, 0.07427658885717392, 0.026094943284988403, -0.02357330173254013, -0.04985051602125168, 0.049260854721069336, -0.006461279932409525, -0.03659965470433235, -0.03668041527271271, -0.09864531457424164, 0.09854629635810852, -0.039122309535741806, 0.030996862798929214, -0.0577857531607151, 0.0942850187420845, -0.025257809087634087, -0.0333898663520813, 0.06996550410985947, -0.0464957058429718, -0.024437397718429565, -0.06304474174976349, -0.04386894032359123, 0.0638541579246521, 0.13736899197101593, -0.046353019773960114, -0.0677865743637085, -4.224446414921027e-33, -0.024300629273056984, 0.07967229932546616, -0.03888155147433281, 0.018034061416983604, -0.06131633743643761, -0.01937265135347843, -0.005152676720172167, 0.0677424967288971, 0.032386504113674164, -0.020523538812994957, -0.06758404523134232, -0.030881516635417938, 0.04011649265885353, -0.022538723424077034, 0.007748208474367857, -0.019877735525369644, -0.04905126243829727, -0.07590603828430176, 0.00015815430379007012, 0.024264726787805557, 0.014688660390675068, 0.08334321528673172, -0.03690796718001366, -0.035685520619153976, -0.02952784299850464, 0.04223361983895302, -0.13890671730041504, 0.05223750323057175, 0.0121805714443326, -0.011857408098876476, -0.03023281879723072, -0.057594772428274155, -0.03690088540315628, -0.01715635508298874, 0.009478721767663956, -0.04102162644267082, 0.09690123796463013, 0.016214197501540184, -0.03969067335128784, 0.05135257542133331, 0.09936705976724625, -0.04012908786535263, 0.030849557369947433, 0.06886567175388336, 0.08523313701152802, -0.02892123907804489, -0.09282047301530838, 0.015769079327583313, 0.02049695886671543, 0.06622186303138733, -0.017970766872167587, -0.0459006242454052, -0.07317861914634705, -0.002988790161907673, -0.009050891734659672, 0.035187944769859314, -0.04794662445783615, 0.040302395820617676, -0.016677647829055786, -0.013475906103849411, 0.018667077645659447, -0.13880577683448792, -0.05216941237449646, -0.06503032892942429, -0.018976006656885147, 0.01422237977385521, 0.015769850462675095, -0.06356124579906464, 0.05880391597747803, -0.06564291566610336, 0.04656146839261055, 0.010083976201713085, 0.06828965991735458, -0.054773081094026566, 0.004362855106592178, -0.07206501066684723, -0.08256030082702637, -0.029991064220666885, -0.0015676487237215042, 0.021396644413471222, -0.03531680628657341, -0.004467864520847797, 0.03452279791235924, -0.013720492832362652, 0.0011551373172551394, 0.01712927781045437, -0.03104267455637455, -0.02565108612179756, 0.02129143849015236, -0.06520067155361176, -0.02842218056321144, 0.0679439827799797, 0.0035140111576765776, 0.04891311004757881, -0.040933482348918915, -5.431657257304323e-08, -0.004419014323502779, 0.028325943276286125, -0.03483859822154045, 0.006219587288796902, 0.07127915322780609, -0.142426997423172, 0.017253940925002098, -0.06765381991863251, 0.045260991901159286, 0.020384224131703377, 0.11737702786922455, -0.039814867079257965, -0.059354983270168304, -0.04590744897723198, 0.022324729710817337, 0.07994981110095978, 0.04221336543560028, 0.025554263964295387, 0.02140580117702484, -0.023644037544727325, -0.0026133358478546143, 0.015823880210518837, 0.028030412271618843, 0.10164463520050049, 0.08771342039108276, -0.0257228072732687, -0.012026800774037838, -0.019859792664647102, 0.029508009552955627, -0.014598040841519833, 0.009357892908155918, 0.0268847793340683, 0.09372887015342712, 0.03437310457229614, 0.04138124734163284, 0.014687420800328255, 0.030631236732006073, 0.05285310372710228, 0.025402260944247246, 0.0017321587074548006, -0.028517013415694237, -0.03988228365778923, 0.008311830461025238, 0.04158563166856766, 0.014636430889368057, -0.06031601130962372, 0.08110890537500381, -0.06312902271747589, 0.01381057221442461, 0.006001770496368408, 0.05488726496696472, 0.01635058782994747, 0.03979656845331192, -0.032346028834581375, 0.03921040892601013, -0.031879451125860214, 0.07556169480085373, -0.052230048924684525, 0.015013454481959343, 0.07388587296009064, 0.005888148210942745, 0.06427250057458878, -0.04396633058786392, -0.020052475854754448]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AssociativeLearningviaInhibitorySearch.pdf,Optimization,"A BIFURCATION THEORY APPROACH TO THE PROGRAMMING OF PERIODIC A TTRACTORS IN NETWORK MODELS OF OLFACTORY CORTEX Bill Baird Department of Biophysics U.C. Berkeley ABSTRACT A new learning algorithm for the storage of static and periodic attractors in biologically inspired recurrent analog neural networks is introduced. For a network of n nodes, n static or n/2 periodic attractors may be stored. The algorithm allows programming of the network vector field indepen- dent of the patterns to be stored. Stability of patterns, basin geometry, and rates of convergence may be controlled. For orthonormal patterns, the l~grning operation reduces to a kind of periodic outer product rule that allows local, additive, commutative, incremental learning. Standing or traveling wave cycles may be stored to mimic the kind of oscillating spatial patterns that appear in the neural activity of the olfactory bulb and prepyriform cortex during inspiration and suffice, in the bulb, to predict the pattern recognition behavior of rabbits in classical conditioning ex- periments. These attractors arise, during simulat- ed inspiration, through a multiple Hopf bifurca- tion, which can act as a critical ""decision pOint"" for their selection by a very small input pattern. INTRODUCTION This approach allows the construction of biological models and the exploration of engineering or cognitive networks that employ the type of dynamics found in the brain. Patterns of 40 to 80 hz oscillation have been observed in the large scale ac- tivity of the olfactory bulb and cortex(Freeman and Baird 86) and even visual neocortex(Freeman 87,Grey and Singer 88), and found to predict the olfactory and visual pattern recognition responses of a trained animal. Here we use analytic methods of bifurcation theory to design algorithms for determining synap- tic weights in recurrent network architectures, like those 459 460 Baird found in olfactory cortex, for associative memory storage of these kinds of dynamic patterns. The ""projection algorithm"" introduced here employs higher order correlations, and is the most analytically transparent of the algorithms to come from the bifurcation theory ap- proach(Baird 88). Alternative numerical algorithms employing unused capacity or hidden units instead of higher order corr- elations are discussed in (Baird 89). All of these methods provide solutions to the problem of storing exact analog at- tractors, static or dynamic, in recurrent neural networks, and allow programming of the ambient vector field independent of the patterns to be stored. The stability of cycles or equi- libria, geometry of basins of attraction, rates of convergence to attractors, and the location in parameter space of primary and secondary bifurcations can be programmed in a prototype vector field - the normal form. To store cycles by the projection algorithm, we start with the amplitude equations of a polar coordinate normal form, with coupling coefficients chosen to give stable fixed points on the axes, and transform to Cartesian coordinates. The axes of this system of nonlinear ordinary differential equations are then linearly transformed into desired spatial or spatio-tem- poral patterns by projecting the system into network coordina- tes - the standard basis - using the desired vectors as colum- ns of the transformation matrix. This method of network syn- thesis is roughly the inverse of the usual procedure in bifur- cation theory for analysis of a given physical system. Proper choice of normal form couplings will ensure that the axis attractors are the only attractors in the system - there are no ""spurious attractors"". If symmetric normal form coef- ficients are chosen, then the normal form becomes a gradient vector field. It is exactly the gradient of an explicit poten- tial function which is therefore a strict Liapunov function for the system. Identical normal form coefficients make the normal form vector field equivariant under permutation of the axes, which forces identical scale and rotation invariant basins of attraction bounded by hyperplanes. Very complex periodic a~tractors may be established by a kind of Fourier synthesis as linear combinations of the simple cycles chosen for a subset of the axes, when those are programmed to be unstable, and a single ""mixed mode"" in the interior of that subspace is made stable. Proofs and details on vectorfield programming appear in (Baird 89). In the general case, the network resulting from the projection A Bifurcation Theory Approach to Programming 461 algorithm has fourth order correlations, but the use of restr- ictions on the detail of vector field programming and the types of patterns to be stored result in network architectures requiring only s~cond order correlations. For biological mod- eling, where possibly the patterns to be stored are sparse and nearly orthogonal, the learning rule for periodic patterns becomes a ""periodic"" outer product rule which is local, add- itive, commutative, and incremental. It reduces to the usual Hebb-like rule for static attractors. CYCLES The observed physiological activity may be idealized mathe- t . 11 ""1 "" 1 (ej + wt) • 1 2 S h ma 1ca y as a cyc e , r Xj e , J- , , ... ,n. uc a cycle is ~ ""periodic attractor"" if it is stable. The global amplitude r is just a scaling factor for the pattern ~ , and the global phase w in e1wt is a periodic scaling that scales x by a factor between ± 1 at frequency w as t varies. The same vector XS or ""pattern"" of relative amplitudes can appear in space as a standing wave, like that seen in the bulb, if the relative phase eS1 of each compartment (component) is the same, eS1+, - eS1, or as a traveling wave, like that seen in the ~repyriform cortex. if the relative phase components of ~s form a gradient in space, eS1+1 - 1/a e\. The traveling wave will ""sweep out"" the amplitude pattern XS in time, but the root-mean-square amplitude measured in an experiment will be the same ~s, regardless of the phase pattern. For an arbitrary phase vector, t~~se ""simple"" single frequency cycles can make very complicated looking spatio-temporal patterns. From the mathematical point of view, the relative phase pattern ~ is a degree of freedom in the kind patterns that can be stored. Patterns of uniform amplitude ~ which differed only in the phase locking pattern ~ could be stored as well. To store the kind of patterns seen in bulb, the amplitude vector ~ is assumed to be parsed into equal numbers of excita- tory and inhibitory components, where each class of component has identical phase. but there is a phase difference of 60 - 90 degrees between the classes. The traveling wave in the prepyriform cortex is modeled by introducing an additional phase g~adient into both excitatory and inhibitory classes. PROJECTION ALGORITHM The central result of this paper is most compactly stated as the following: 462 Baird THEOREM Any set S, s - 1,2, ... , n/2 , of cycles r S x.s e1(9js + wst) of linearly independent vectors of relative comJonent amplitudes xS E Rn and phases ~s E Sn, with frequencies wS E R and global amplitudes r S E R, may be established in the vector field of the analog fourth order network: by some variant of the projection operation : -1 Tij ... Emn Pim J mn P nj , T EPA p-1. p-1 p-1 ijk1· mn im mn mJ nk n1' where the n x n matrix P contains the real and imaginary com- ponents [~S cos ~s , ~s sin ~S] of the complex eigenvectors xS e19s as columns, J is an n x n matrix of complex conjugate eigenvalues in diagonal blocks, Amn is an n x n matrix of 2x2 blocks of repeated coefficients of the normal form equations, and the input bi &(t) is a delta function in time that establ- ishes an initial condition. The vector field of the dynamics of the global amplitudes rs and phases -s is then given exactly by the normal form equations : r s == Us r s In particular, for ask > 0 , and ass/akS < 1 , for all sand k, the cycles s - 1,2, ... ,n/2 are stable, and have amplitudes rs ;; (us/ass )1I2, where us· 1 - ""T • Note that there is a multiple Hopf bifurcation of codimension n/2 at ""T = 1. Since there are no approximations here, however, the theorem is not restricted to the neighborhood of this bifurcation, and can be discussed without further reference to bifurcation theory. The normal form equations for drs/dt and d_s/dt determine how r S and _s for pattern s evolve in time in interaction with all the other patterns of the set S. This could be thought of as the process of phase locking of the pattern that finally emerges. The unusual power of this al- gorithm lies in the ability to precisely specify these ~ linear interactions. In general, determination of the modes of the linearized system alone (li and Hopfield 89) is insuf- ficient to say what the attractors of the nonlinear system will be. A Bifurcation Theory Approach to Programming 463 PROOF The proof of the theorem is instructive since it is a constru- ctive proof, and we can use it to explain the learning algori- thm. We proceed by showing first that there are always fixed points on the axes of these amplitude equations, whose stabil- ity is given by the coefficients of the nonlinear terms. Then the network above is constructed from these equations by two coordinate transformations. The first is from polar to Car- tesian coordinates, and the second is a linear transformation from these canonical ""mode"" coordinates into the standard basis e1, e2, ... , eN' or ""network coordinates"". This second transformation constitutes the ""learning algorithm"", because it tra""nSfrirms the simple fixed points of the amplitude equa- tions into the specific spatio-temporal memory patterns desi- red for the network. Amplitude Fixed Points Because the amplitude equations are independent of the rota- tion _, the fixed points of the amplitude equations charact- erize the asymptotic states of the underlying oscillatory modes. The stability of these cycles is therefore given by the stability of the fixed points of the amplitude equations. On each axis r s' the other components rj are zero, by definition, rj = rj ( uj - Ek ajk r k2 ) • 0, for rj • 0, which leaves r s - rs ( Us - ass r s 2 ), and r s - 0 There is an equilibrium on each axis s, at r s.(us/ass )1I2, as claimed. Now the Jacobian of the amplitude equations at some fixed point r~ has elements J . . - - 2 a .. r~. r..... , J 11 = u. - :5 a .. r~.2 - ~ a .. r~.2 . lJ lJ 1 J 1 11 1 ]7-i lJ J For a fixed point r~s on axis s, J ij • 0 , since r~i or r~j • 0, making J a diagonal matrix whose entries are therefore its eigenvalues. Now J l1 • u1 - ais r~ s 2, for i /. s, and J ss • Us - :5 ass r~/. Since r~/ • us/ass' J ss • - 2 us' and J ii • ui - ais (us/ass). This gives aisfass > u1/us as the condition for nega- tive eigenvalues that assures the stability of r .... s. Choice of aji/aii ) uj/ui , for all i, j , therefore guarantees stability of all axis fixed points. Coordinate Transformations We now construct the neural network from these well behaved equations by the following transformations, First; polar to Cartesian, (rs'-s) to (v2s-1.v2s) : Using V2s-1 '"" r s cos -s v2s = r s sin -s ,and differentiating these 464 Baird gives: V2s-1 • r s cos ""s by the chain rule. Now substituting cos tis • v2s-1/r s ' and r s sin ""s • v2s, gives: v2s - v2s rs + (v2 l/r ) .. s- s s Entering the expressions of the normal form for rs and tis' gives: and since 222 rs = v2s-1 + v2s n/2 v2s-1 - Us v2s-1 - Ws v2s + E j [v2s-1 asj - v2s bsj] (v2j-/ + v2/) Similarly, n/2 v2s - Us v2s + Ws v2s-' + E j [v2s asj + v2s-1 bSj ] (v2j_/ + v2/)· Setting the bsj - 0 for simplicity, choosing Us - - T + 1 to get a standard network form, and reindexing i,j-l,2, ... ,n , we get the Cartesian equivalent of the polar normal form equa- tions. n n Here J is a matrix containing 2x2 blocks along the diagonal of the local couplings of the linear terms of each pair of the previous equations v2s-1 ' v2s • with - T separated out of the diagonal terms. The matrix A has 2x2 blocks of identical coef- ficients asj of the nonlinear terms from each pair. 1 - w, a'l a"" a12 a'2 w, 1 a"" a1, a'2 a'2 J = "" - 1 - w2 a 21 a 21 a 22 a22 w2 1 a 21 a 21 a22 a22 "" ., ~ A Bifurcation Theory Approach to Programming 465 Learning Transformation - Linear Term Second; J is the canonical form of a real matrix with complex conjugate eigenvalues, where the conjugate pairs appear in blocks along the diagonal as shown. The Cartesian normal form equations describe the interaction of these linearly uncoupled complex modes due to the coupling of the nonlinear terms. We can interpret the normal form equations as network equations in eigenvector (or ""memory"") coordinates, given by some diag- onalizing transformation P, containing those eigenvectors as its columns, so that J a p-1 T P. Then it is clear that T may instead be determined by the reverse projection T _ P J p-1 back into network coordinates, if we start with desired eigen- vectors and eigenvalues. We are free to choose as columns in P, the real and imaginary vectors [XS cos 9s , XS sin 9S] of the cycles ~s ei9s of any linearly independent- set -S of p~tterns to be learned. If we write the matrix expression for the proj- ection in component form, we recover the expression given in the theorem for Tij , Nonlinear Term Projection The nonlinear terms are transformed as well, but the expres- sion cannot be easily written in matrix form. Using the com- ponent form of the transformation, substituting into the Cartesian normal form, gives: Xi - (-'T+1) E j Pij (E k P-1jk xk) + E j Pij Ek J jk (E I P-\I xl) + E j Pij (Ek P-1jk xk) EI Ajl (Em p-\m xm) (En p-\n xn) Rearranging the orders of summation gives, Xi = (-'T+1) Ek (E j Pij P-1jk ) xk + EI (E k E j Pij J jk P-\l) xl ( -1 -1 p-1 ) + En Em Ek EI E j Pij P jk AjI P 1m In xk xm xn Finally, performing the bracketed summations and relabeling indices gives us the network of the theorem, xi = - 'T xi + E j T1j Xj + Ejkl Tijkl Xj Xk xl with the expression for the tensor of the nonlinear term, 466 Baird -1 -1 -1 Tijk1 - Emn Pim Amn P mj P nk P n1 Q.E.D. LEARNING RULE EXTENSIONS This is the core of the mathematical story, and it may be ex- tended in many ways. When the columns of P are orthonormal, then p-1 • pT, and the formula above for the linear network coupling becomes T = pJpT. Then, for complex eigenvectors, This is now a local, additive, incremental learning rule for synapse ij, and the system can be truly self-organizing be- cause the net can modify itself based on its own activity. Between units of equal phase, or when 9i s = 9j S - 0 for a static pattern, this reduces to the usual Hebb rule. In a similar fashion, the learning rule for the higher order nonlinear terms becomes a multiple periodic outer product rule when the matrix A is chosen to have a simple form. Given our present ignorance of the full biophysics of intracellular processing, it is not entirely impossible that some dimension- ality of the higher order weights in the mathematical network coul~ be implemented locally within the cells of a biological network, using the information available on the primary lines given by the linear connections discussed above. When the A matrix is chosen to have uniform entries Aij - c for all its off-diagonal 2 x 2 blocks, and uniform entries Aij - c - d for the diagonal blocks, then, T ijk1 • This reduces to the multiple outer product The network architecture generated by this learning rule is This reduces to an architecture without higher order correla- tions in the case that we choose a completely uniform A matrix (A1j - c , for all i,j). Then + + A Bifurcation Theory Approach to Programming 467 This network has fixed points on the axes of the normal form as always, but the stability condition is not satisfied since the diagonal normal form coefficients are equal, not less, than the remaining A matrix entries. In (Baird 89) we describe how clamped input (inspiration) can break this symmetry and make the nearest stored pattern be the only attractor. All of the above results hold as well for networks with sig- moids, provided their coupling is such that they have a Tayl- or's expansion which is equal to the above networks up to third order. The results then hold only in the neighborhood of the origin for which the truncated expansion is accurate. The expected performance of such systems has been verified in simulations. Acknowledgements Supported by AFOSR-87-0317. I am very grateful for the support of Walter Freeman and invaluable assistance of Morris Hirsch. References B. Baird. Bifurcation Theory Methods For Programming Static or Periodic Attractors and Their Bifurcations in Dynamic Neural Networks. Proc. IEEE Int. Conf. Neural Networks, San Diego, Ca.,pI-9, July(1988). B. Baird. Bifurcation Theory Approach to Vectorfield Program- ming for Periodic Attractors. Proc. INNS/IEEE Int. Conf. on Neural Networks. Washington D.C., June(1989). W. J. Freeman & B. Baird. Relation of Olfactory EEG to Be- havior: Spatial Analysis. Behavioral Neuroscience (1986). W. J. Freeman & B. W. van Dijk. Spatial Patterns of Visual Cortical EEG During Conditioned Reflex in a Rhesus Monkey. Brain Research, 422, p267(1987). C. M. Grey and W. Singer. Stimulus Specific Neuronal Oscillations in Orientation Columns of Cat Visual Cortex. PNAS. In Press(1988). Z. Li & J.J. Hopfield. Modeling The Olfactory Bulb. Biologi- cal Cybernetics. Submitted(1989}.","[-0.031085489317774773, -0.12302915006875992, 0.017084255814552307, -0.021223923191428185, -0.07315906882286072, 0.0004536091582849622, 0.019882041960954666, -0.04447168856859207, 0.07917424291372299, -0.02850765734910965, -0.008744800463318825, 0.028463050723075867, 0.04072701185941696, -0.015023505315184593, -0.014660894870758057, 0.03167348355054855, -0.11198829859495163, 0.019869489595294, 0.02274809405207634, -0.03615196794271469, 0.08214929699897766, 0.00777791254222393, -0.02712821774184704, -0.025322234258055687, -0.0821334645152092, 0.03053675778210163, -0.02205113135278225, 0.00773687195032835, 0.04825253784656525, 0.0019921432249248028, 0.0730898305773735, 0.0779964029788971, -0.04496525600552559, -0.0428180955350399, -0.07965070754289627, -0.01814873330295086, -0.054163143038749695, 0.02803870476782322, -0.0008885568240657449, 0.05345214158296585, 0.060547586530447006, -0.0003942716575693339, -0.026677589863538742, 0.03823620453476906, 0.054022930562496185, 0.012098328210413456, 0.04324065521359444, -0.022707950323820114, -0.012169448658823967, -0.06842576712369919, -0.02255348116159439, -0.023531418293714523, -0.030463136732578278, 0.02802574262022972, 0.043170347809791565, 0.017245227470993996, 0.01916597969830036, 0.042478546500205994, -0.02555355615913868, -0.061392154544591904, 0.03498531132936478, -0.005799967795610428, -0.005931949242949486, -0.05168360099196434, -0.019601140171289444, 0.009745453484356403, -0.07263771444559097, 0.04438132047653198, 0.1201215386390686, 0.015001478604972363, 0.025957005098462105, 0.05002790316939354, -0.04240712523460388, -0.01767646335065365, 0.09518274664878845, 0.10633532702922821, 0.0003437106206547469, 0.011207202449440956, 0.019828077405691147, -0.05382399633526802, -0.027493126690387726, 0.0398494228720665, -0.0028037240263074636, -0.059699248522520065, 0.039045609533786774, 0.0006464843172580004, 0.011148453690111637, 0.019594091922044754, 0.020880864933133125, -0.011053024791181087, -0.00828512106090784, 0.01348172314465046, 0.0055379560217261314, -0.14804618060588837, -0.04736318439245224, 0.06232684105634689, 0.06037862226366997, 0.06304064393043518, 0.014027786441147327, 0.045838601887226105, -0.019845500588417053, -0.05358041822910309, 9.345968283014372e-05, 0.07765218615531921, -0.011459246277809143, -0.030061127617955208, 0.016970504075288773, 0.019543157890439034, 0.082994244992733, -0.03908051922917366, -0.07769212871789932, 0.08999816328287125, 0.011145130731165409, 0.08850868791341782, 0.0005335230962373316, 0.020356999710202217, -0.04131188616156578, -0.04240883141756058, 0.1025911346077919, 0.12268081307411194, 0.05984053388237953, -0.039945088326931, -0.07596072554588318, 0.02430819533765316, 0.008713942021131516, 0.027734598144888878, -0.04374486580491066, 3.5257582867311837e-33, -0.027023235335946083, 0.03204462304711342, 0.008529956452548504, -0.0817968025803566, 0.03270377218723297, -0.03723612055182457, 0.01437970157712698, -0.004968974273651838, 0.08361772447824478, -0.005385400727391243, -0.11403543502092361, 0.06564883887767792, -0.03873305022716522, 0.1559351682662964, 0.03414331004023552, -0.04355579987168312, 0.010347836650907993, -0.07894176244735718, 0.039892036467790604, -0.1298799067735672, 0.0483560636639595, -0.05851336568593979, -0.054338373243808746, -0.04810123145580292, -0.0925237312912941, 0.029351430013775826, -0.020112240687012672, 0.04928082600235939, -0.08423776179552078, 0.013727225363254547, 0.048666175454854965, 0.05104675516486168, -0.036804407835006714, 0.017781797796487808, 0.045078933238983154, -0.056960515677928925, 0.10061142593622208, 0.059997834265232086, 0.050650786608457565, -0.06601710617542267, 0.0046302019618451595, -0.03082287684082985, 0.034406326711177826, 0.10010060667991638, 0.022237790748476982, -0.024967262521386147, 0.033177103847265244, 0.059352513402700424, -0.010549605824053288, -0.008622981607913971, 0.0029211193323135376, -0.03430980071425438, 0.012090817093849182, -0.041368577629327774, 0.028931772336363792, 0.05106906220316887, -0.022739732638001442, -0.04273226857185364, -0.017747484147548676, 0.08808668702840805, 0.00030648522078990936, 0.05060065910220146, 0.11791985481977463, 0.006946175824850798, -0.0022500306367874146, 0.03516020253300667, -0.015228472650051117, -0.08268820494413376, 0.09249810129404068, 0.06686854362487793, 0.06209677830338478, 0.0601966455578804, -0.003172877710312605, -0.12163427472114563, 0.022966766729950905, -0.055412955582141876, 0.0034301222767680883, 0.0053309490904212, -0.11757691949605942, 0.012933188118040562, -0.020813975483179092, -0.044789768755435944, -0.05079422518610954, 0.046096887439489365, -0.014979389496147633, -0.04497617855668068, 0.0729534924030304, -0.024955328553915024, -0.09037923067808151, -0.0077618807554244995, -0.007971257902681828, -0.0039009149186313152, 0.11948448419570923, 0.004504102282226086, -0.0115208076313138, -3.5121431234127845e-33, 0.0043514808639883995, 0.012113300152122974, -0.0023084699641913176, -0.012153650633990765, -0.022254258394241333, 0.03175472468137741, -0.08812689036130905, -0.022535249590873718, -0.047192852944135666, -0.04241416975855827, -0.020742861554026604, 0.0389503538608551, 0.09993480145931244, 0.04718082398176193, -0.001978581305593252, 0.017939994111657143, 0.021471675485372543, -0.030201269313693047, 0.0664709135890007, -0.016199497506022453, -0.08151627331972122, 0.05467734858393669, -0.18456807732582092, -0.04258599877357483, -0.030156373977661133, -0.0142222810536623, -0.0018352643819525838, 0.04217591881752014, -0.011419505812227726, 0.04508451744914055, -0.09939366579055786, -0.04684794694185257, 0.013520571403205395, 0.033918581902980804, 0.05144647881388664, 0.07013635337352753, 0.0015440295683220029, -0.04014600068330765, -0.018864862620830536, -0.022321408614516258, 0.010000583715736866, -0.004283628426492214, 0.03270144760608673, -0.02755492739379406, 0.06190931797027588, -0.03792647644877434, -0.030789362266659737, 0.05678075924515724, -0.00789665337651968, 0.10948865860700607, 0.026398835703730583, -0.006855988875031471, -0.06466798484325409, -0.06050405278801918, -0.013755157589912415, 0.10856763273477554, 0.04089385271072388, -0.0031539304181933403, 0.03703661635518074, -0.021351328119635582, -0.0501735620200634, -0.09361443668603897, 0.012544329278171062, -0.050992511212825775, -0.03514391928911209, -0.03656911477446556, -0.03325485438108444, 0.02556684985756874, 0.06369590759277344, -0.029808100312948227, -0.007982091046869755, 0.08403538912534714, 0.05447402969002724, -0.0070486715994775295, -0.04686693474650383, -0.029304830357432365, -0.06472238153219223, -0.032473817467689514, -0.05657593160867691, -0.016428286209702492, -0.10249229520559311, 0.03567800298333168, -0.04912581294775009, 0.01911642961204052, -0.06740576028823853, -0.00019767499179579318, -0.015129154548048973, 0.05154799297451973, 0.03480511158704758, -0.03339223191142082, 0.05115272477269173, 0.01906438358128071, -0.028903178870677948, 0.06692598015069962, 0.06344129145145416, -4.9118163758521405e-08, -0.08021079003810883, -0.001400741864927113, 0.04718497022986412, 0.03565814346075058, 0.09463319927453995, -0.03038145788013935, 0.05857931822538376, -0.07525472342967987, -0.018097801133990288, -0.02603244036436081, 0.09633640944957733, -0.010632642544806004, -0.026655107736587524, -0.03827626258134842, 0.038503773510456085, 0.10238369554281235, 0.05326513946056366, -0.005229336209595203, -0.023148320615291595, -0.03594609722495079, 0.0023955011274665594, 0.007071828003972769, -0.048494283109903336, 0.01425539143383503, 0.041846614331007004, -0.06648565083742142, -0.03580391779541969, 0.04381068795919418, -0.0038207368925213814, 0.03662152588367462, 0.02706347405910492, 0.012321177870035172, 0.02976772002875805, 0.041284456849098206, -0.01382969506084919, 0.017317840829491615, -0.048937149345874786, 0.0021119024604558945, -0.042671605944633484, -0.011966380290687084, -0.003541422076523304, 0.017944790422916412, -0.025499366223812103, -0.032232049852609634, 0.011956757865846157, -0.019373105838894844, 0.07999828457832336, -0.08722870796918869, 0.06700509041547775, 0.09314025193452835, -0.008529123850166798, 0.019898967817425728, 0.017154334113001823, -0.020997915416955948, -0.011399557814002037, -0.023223917931318283, -0.03222694247961044, -0.08148875832557678, -0.011864393018186092, 0.03780168294906616, -0.08949679136276245, 0.11344190686941147, -0.04181915521621704, -0.05180007219314575]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\AutomaticLocalAnnealing.pdf,Deep Learning,"340 BACKPROPAGATION AND ITS APPLICATION TO HANDWRITTEN SIGNATURE VERIFICATION Dorothy A. Mighell Electrical Eng. Dept. Info. Systems Lab Stanford University Stanford, CA 94305 Timothy S. Wilkinson Electrical Eng. Dept. Info. Systems Lab Stanford University Stanford, CA 94305 ABSTRACT Joseph W. Goodman Electrical Eng. Dept. Info. Systems Lab Stanford University Stanford, CA 94305 A pool of handwritten signatures is used to train a neural net- work for the task of deciding whether or not a given signature is a forgery. The network is a feedforward net, with a binary image as input. There is a hidden layer, with a single unit output layer. The weights are adjusted according to the backpropagation algorithm. The signatures are entered into a C software program through the use of a Datacopy Electronic Digitizing Camera. The binary signa- tures are normalized and centered. The performance is examined as a function of the training set and network structure. The best scores are on the order of 2% true signature rejection with 2-4% false signature acceptance. INTRODUCTION Signatures are used everyday to authorize the transfer of funds for millions of people. We use our signature as a form of identity, consent, and authorization. Bank checks, credit cards, legal documents and waivers all require the everchanging personalized signature. Forgeries on such transactions amount to millions of dollars lost each year. A trained eye can spot most forgeries, but it is not cost effective to handcheck all signatures due to the massive number of daily transactions. Consequently, only disputed claims and checks written for large amounts are verified. The consumer would certainly benefit from the added protection of automated verification. Neural networks lend themselves very well to signature verification. Previously, they have proven applicable to other signal processing tasks, such as character recognition {Fukishima, 1986} {Jackel, 1988}, sonar target classification {Gorman, 1986}, and control- as in the broom balancer {Tolat, 1988}. HANDWRITING ANALYSIS Signature verification is only one aspect of the study of handwriting analysis. Recognition is the objective, whether it be of the writer or the characters. Writer recognition can be further broken down into identification and verification. Identi- Backpropagation and Handwritten Signature Verification 341 fication selects the author of a sample from among a group of writers. Verification confirms or rejects a written sample for a single author. In both cases, it is the style of writing that is important. Deciphering written text is the basis of character recognition. In this task, linguistic information such as the individual characters or words are extracted from the text. Style must be eliminated to get at the content. A very important application of character recognition is automated reading of zip-codes in the post office {Jackel, 1988}. Data for handwriting analysis may be either dynamic or static. Dynamic data requires special devices for capturing the temporal characteristics of the sample. Features such as pressure, velocity, and position are examined in the dynamic framework. Such analysis is usually performed on-line in real time. Static analysis uses the final trace of the writing, as it appears on paper. Static analysis does not require any special processing devices while the signature is being produced. Centralized verification becomes possible, and the processing may be done off-line. Work has been done in both static and dynamic analysis {Sato, 1982} {Nemcek, 1974}. Generally, signature verification efforts have been more successful using the dynamic information. It would be extremely useful though, to perform the verification using only the written signature. This would eliminate the need for costly machinery at every place of business. Personal checks may also be verified through a static signature analysis. TASK The handwriting analysis task with which this paper is concerned is that of signa- ture verification using an off-line method to detect casual forgeries. Casual forgeries are non-professional forgeries, in which the writer does not practice reproducing the signature. The writer may not even have a copy of the true signature. Casual forgeries are very important to detect. They are far more abundant, and involve greater monetary losses than professional forgeries. This signature verification task falls into the writer recognition category, in which the style of writing is the im- portant variable. The off-line analysis allows centralized verification at a lower cost and broader use. HANDWRITTEN SIGNATURES The signatures for this project were gathered from individuals to produce a pool of 80 true signatures and 66 forgeries. These are signatures, true and false, for one person. There is a further collection of signatures, both true and false, for other persons, but the majority of the results presented will be for the one individual. It will be clear when other individuals are included in the demonstration. The signatures are collected on 3x5 index cards which have a small blue box as 342 Wilkinson, Mighell and Goodman a guideline. The cards are scanned with a CCD array camera from Datacopy, and thresholded to produce binary images. These binary images are centered and normalized to fit into a 128x64 matrix. Either the entire 128x64 image is presented as input, or a 90x64 image of the three initials alone is presented. It is also possible to present preprocessed inputs to the network. SOFTWARE SIMULATION The type of learning algorithm employed is that of backpropagation. Both dwell and momentum are included. Dwell is the type of scheduling employed, in which an image is presented to the network, and the network is allowed to ""dwell"" on that input for a few iterations while updating its weights. C. Rosenberg and T. Sejnowski have done a few studies on the effects of scheduling on learning {Rosenberg, 1986}. Momentum is a term included in the change of weights equation to speed up learning {Rumelhart, 1986}. The software is written in Microsoft C, and run on an IBM PC/AT with an 80287 math co-processor chip. Included in the simulation is a piece-wise linear approximation to the sigmoid trans- fer function as shown in Figure 1. This greatly improves the speed of calculation, because an exponential is not calculated. The non-linearity is kept to allow for layering of the network. Most of the details of initialization and update are the same as that reported in NetTalk {Sejnowski, 1986}. OUT ~-111111::::+~----'. IN Figure 1. Piece-wise linear transfer function. Many different nets were trained in this signature verification project, all of which were feed-forward. The output layer most often consisted of a single output neuron, but 5 output neurons have been used as well. If a hidden layer was used, then the number of hidden units ranged from 2 to 53. The networks were both fully- connected and partially-connected. SAMPLE RUN The simplest network is that of a single neuron taking all 128x64 pixels as input, plus one bias. Each pixel has a weight associated with it, so that the total number of weights is 128x64 + 1 = 8193. Each white pixel is assigned an input value of + 1, each black pixel has a value of -1. The training set consists of 10 true signatures Backpropagation and Handwritten Signature Verification 343 with 10 forgeries. Figure 2a depicts the network structure of this sample run. OUT - 1 c:: 0 - u CD - CD "" 0.5 f-.. CD :J ""--- Q. 0 0 0.5 P(false acceptance) ~1~111J 111. ""~~~mlla (b) 1 1/ (a) ~ en LL. C 0.5 0 f 0 0 0.5 (e) Output Values (d) Figure 2. Sample run. a) Network = one output neuron, one weight per pixel, fully con- nected. Training set = 10 true signatures + 10 forgeries. b) ROC plot for the sample run. (Probability of fa1se acceptance vs probability of true detection). Test set = 70 true signatures + 56 forgeries. c) Clipped picture of the weights for the sample run. White = positive weight, black = negative weight. d) Cumulative distribution function for the true signatures (+) and for the forgeries (0) of the sample run. - 1 1 The network is trained on these 20 signatures until all signatures are classified 344 Wilkinson, Mighell and Goodman correctly. The trained network is then tested on the remaining 70 true signatures and 56 forgeries. The results are depicted in Figures 2b and 2d. Figure 2b is a radar operating characteristic curve, or roc plot for short. In this presentation of data, the proba- bility of detecting a true signature is plotted against the probability of accepting a forgery. Roc plots have been used for some time in the radar sciences as a means for visualizing performance {Marcum, 1960}. A perfect roc plot has a right angle in the upper left-hand corner which would show perfect separation of true signa- tures from forgeries. The curve is plotted by varying the threshold for classification. Everything above the threshold is labeled a true signature, everything below the threshold is labeled a forgery. The roc plot in Figure 2b is close to perfect, but there is some overlap in the output values of the true signatures and forgeries. The overlap can be seen in the cumulative distribution functions (cdfs) for the true and false signatures as shown in Figure 2d. As seen in the cdfs, there is fairly good separation of the output values. For a given threshold of 0.5, the network produces 1% rejection of true signatures as false, with 4% acceptance of forgeries as being true. IT one lowers the threshold for classification down to 0.43, the true rejection becomes nil, with a false acceptance of 7% . A simplified picture of the weights is shown in Figure 2c, with white pixels designating positive weights, and black pixels negative weights. OTHER NETWORKS The sample run above was expanded to include 2 and 3 hidden neurons with the single output neuron. The results were similar to the single unit network, implying that the separation is linear. The 128x64 input image was also divided into regions, with each region feeding into a single neuron. In one network structure, the input was sectioned into 32 equally sized regions of 16x16 pixels. The hidden layer thus has 32 neurons, each neuron receiving 16x16 + 1 inputs. The output neuron had 33 inputs. Likewise, the input image was divided into 53 regions of 16x16 pixels, this time overlapping. Finally, only the initials were presented to the network. (Handwriting experts have noted that leading strokes and separate capital letters are very significant in classification {Osborn, 1929}.) In this case, two types of networks were devised. The first had a single output neuron, the second had three hidden neurons plus one output neuron. Each of the hidden neurons received inputs from only one initial, rather than from all three. The network with the single output neuron produced the best results of all, with 2% true rejection and 2% false acceptance. IMPORTANCE OF FORGERIES IN THE TRAINING SET In all cases, the networks performed much better when forgeries were included in the training set. When an all-white image is presented as the only forgery, performance deteriorates significantly. When no forgeries are present, the network decides that Backpropagation and Handwritten Signature Verification 345 all signatures are true signatures. It is therefore desirable to include actual forgeries in the training set, yet they may be impractical to obtain. One possibility for avoiding·the collection of forgeries is to use computer generated forgeries. Another is to distort the true signatures. A third is to use true signatures of other people as forgeries for the person in question. The attraction of this last option is that the masquerading forgeries are already available for use. NETWORK WITHOUT FORGERIES To test the use of true signatures of other people for forgeries, the following network is devised. Once again, the input is the 128x64 pixel image. The output layer is comprised of five output neurons fully connected to the input image. The function of each output neuron is to be active when presented with a particular persons' signature. When a forgery is present, the output is to be low. Figure 3a depicts this network. The training set has 50 true signatures, ten for each of five people. Each signature has a desired output of true for one neuron, and false for the remaining four neurons. Once the network is trained, it is tested on 210 true signatures and 150 forgeries. Figures 3b and 3c record the results. At a threshold of 0.5, the true rejection is 3% and the false acceptance is 14%. Decreasing the threshold down to 0.41 gives 0% true rejection and 28% false acceptance. These results are similar to the sample run, though not as good. This is a simple demonstration of the use of other true signatures as forgeries. More sophisticated techniques could improve the discrimination. For instance, selecting names with similar lengths or spelling should improve the classification. CONCLUSION Automated signature verification systems would be extremely important in the business world for verifying monetary transactions. Countless dollars are lost each day to instances of casual forgeries. An artificial neural network employing the backpropagation learning algorithm has been trained on both true and false signa- tures for classification. The results have been very good: 2% rejection of genuine signatures with 2% acceptance of forgeries. The analysis requires only the static picture of the signature, there by offering widespread use through centralized ver- ification. True signatures of other people may substitute for the forgeries in the training set - eliminating the need for collecting non-genuine signatures. 346 Wilkinson, Mighell and Goodman JWG JTH TSW LDK ABH (a) - C 1 r--::iif1l---------.. lr-----------~~~--_=~ o - (,) CD - Q) ""t:S U.5 Q) ::J .. -- (f. 00.5 o I ~ ~ o~----------~--------~ o~~----~~~--------~ o 0.5 1 P(false acceptance) (b) o 0.5 Output Values (c) Figure 3. Network without forgeries for 5 individuals. a) Network = 5 output neurons, one for each individua~ as indi- cated by the initials. Training set = 10 true signatures for each individual. b) ROC plot for the network without forgeries. Test set = 210 true signatures + 150 forgeries. c) Cumulative distribution function for the true signatures (+) and for the forgeries (0) of the network without forgeries. Referenees 1 K. Fukishima and S. Miyake, ""Neocognitron: A biocybernetic approach to visual pattern recognitionJt , in NHK Laboratorie~ Note, Vol. 336, Sep 1986 (NHK Science and Technical Research Laboratories, Tokyo). Backpropagation and Handwritten Signature Verification 347 P. Gorman and T. J. Sejnowski, ""Learned classification of sonar targets using a massively parallel network"", in the proceedings of the IEEE ASSP Oct 21, 1986 DSP Workshop, Chatham, MA. L. D. Jackel, H. P. Graf, W. Hubbard, J. S. Denker, and D. Henderson, ""An application of neural net chips: handwritten digit recognition"", in IEEE In- ternational Oonference on Neural Networks 1988, II 107-115. J. T. Marcum, ""A statistical theory of target detection by pulsed radar"", in IRE Transactions in Information Theory, Vol. IT-6 (Apr.), pp 145-267, 1960. W. F. Nemcek and W. C. Lin, ""Experimental investigation of automatic signature verification"" in IEEE Transactions on Systems, Man, and Oybernetics, Jan. 1974, pp 121-126. A. S. Osborn, Questioned Documents, 2nd edition (Boyd Printing Co, Albany NY) 1929. C. R. Rosenberg and T. J. Sejnowski, ""The spacing effect on NETtalk, a mas- sively parallel network"", in Proceedings of the Eighth Annual Oonference of the Oognitive Science Society, (Hillsdale, New Jersey: Lawrence Erlbaum Associates, 1986) 72-89. D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ""Learning internal representa- tions by error propagation"", in Parallel Distributed Processing: Explorations in the Microstructures of Oognition. Vol. 1: Foundations, edited by D. E. Rumelhart & J. L. McClelland, (MIT Press, 1986). Y. Sato and K. Kogure, ""Online signature verification based on shape, motion, and writing pressure"", in Proceedings of the 6th International Oonference on Pattern Recognition, Vol. 2, pp 823-826 (IEEE NY) 1982. T. J. Sejnowski and C. R. Rosenberg, ""NETtalk: A Parallel Network that Learns to Read Aloud"", Johns Hopkins University Department of Electrical Engi- neering and Computer Science Technical Report JHU /EECS-86/01, (1986). V. V. Tolat and B. Widrow, ""An adaptive 'broom balancer' with visual inputs"" , in IEEE International Oonference on Neural Networks 1988, II 641-647.","[-0.07887984812259674, -0.029767265543341637, 0.013626647181808949, -0.025750137865543365, 0.012226799502968788, -0.007953532971441746, 0.0349656343460083, -0.00015553673438262194, -0.021386954933404922, -0.0776844248175621, -0.011881590820848942, 0.0571737177670002, 0.13290353119373322, -0.05628577619791031, -0.12014147639274597, -0.003790214192122221, -0.0016927181277424097, 0.06631924957036972, -0.03271004930138588, -0.027772866189479828, -0.06118618696928024, -0.04659019410610199, -0.031510137021541595, -0.08261212706565857, 0.04586904123425484, 0.029610712081193924, 0.011829869821667671, -0.029933756217360497, -0.020150132477283478, -0.060989636927843094, 0.021104414016008377, -0.02018626034259796, -0.0021456771064549685, 0.005297724157571793, -0.028423476964235306, 0.0506276860833168, 0.015337938442826271, -0.009063320234417915, 0.03130831941962242, -0.03525280952453613, -0.034418169409036636, -0.03689790889620781, -0.011854002252221107, 0.05527501925826073, 0.04689573124051094, 0.06354399025440216, 0.051905810832977295, 0.043750617653131485, -0.002839521737769246, -0.025728723034262657, 0.011616090312600136, -0.011298459954559803, -0.031088780611753464, 0.006533050909638405, -0.03248191252350807, -0.056668125092983246, 0.03478427603840828, 0.00773509219288826, -0.09293051809072495, 0.0013092199806123972, 0.003673373954370618, -0.0030943183228373528, -0.08220933377742767, -0.060419708490371704, 0.06874582171440125, 0.06608320027589798, -0.037374187260866165, -0.07113836705684662, 0.06384273618459702, -0.02448344975709915, 0.06342586874961853, 0.07244794070720673, -0.014821143820881844, 0.012213731184601784, 0.006098643410950899, 0.03635822609066963, 0.010519650764763355, 0.046892616897821426, 0.06429538875818253, -0.15765315294265747, 0.01336241327226162, -0.005898944102227688, 0.0023224467877298594, 0.008870178833603859, 0.09985468536615372, 0.002396591706201434, -0.11243388056755066, 0.04289846867322922, 0.005401299800723791, 0.04790507256984711, -0.0072586387395858765, 0.010518102906644344, -0.014279602095484734, -0.07803259789943695, 0.024855656549334526, 0.010134926065802574, 0.009176608175039291, -0.036894541233778, 0.019028950482606888, 0.08637871593236923, -0.0788521096110344, -0.00479997880756855, 0.04029833525419235, -0.0863787829875946, 0.14248153567314148, 0.045140378177165985, 0.03897165134549141, -0.0566304549574852, 0.10389801859855652, -0.15093500912189484, -0.023862726986408234, 0.020089883357286453, -0.06523922830820084, 0.030598120763897896, 0.08741294592618942, -0.056686751544475555, -0.009067093022167683, 0.01147020049393177, 0.006521632429212332, -0.01416833233088255, -0.06694914400577545, -0.02282898873090744, -0.043871648609638214, -0.005108751356601715, -0.009074789471924305, -0.07109149545431137, -0.08309119939804077, 4.401219826791575e-33, -0.041646189987659454, 0.15288977324962616, -0.016695434227585793, -0.052727408707141876, 0.03324592858552933, 0.034796375781297684, -0.018570158630609512, 0.014532080851495266, -0.054138652980327606, 0.05514483526349068, -0.09199707210063934, -0.03775453567504883, 0.03707472234964371, 0.1404099315404892, 0.07781334966421127, -0.005606466438621283, 0.0036530974321067333, 0.0023896575439721346, -0.012898527085781097, -0.034057632088661194, 0.04979609698057175, -0.06234443187713623, 0.06036779284477234, -0.005078552290797234, -0.044338516891002655, -0.05781674385070801, -0.008554407395422459, 0.03317295014858246, 0.044764917343854904, -0.003841794328764081, 0.01889769546687603, -0.027623973786830902, 0.042950063943862915, -0.030159732326865196, 0.012696480378508568, -0.027448417618870735, 0.049597788602113724, -0.038701754063367844, 0.061227548867464066, 0.00016455017612315714, 0.019346456974744797, 0.038469184190034866, -0.03472283110022545, -0.013760840520262718, -0.04526219889521599, 0.03215661272406578, 0.02318686433136463, 0.05946226045489311, 0.045261118561029434, -0.0023060140665620565, -0.012670977041125298, 0.005434032995253801, -0.04438949376344681, -0.07462691515684128, -0.05513257905840874, -0.043791793286800385, 0.007397049106657505, 0.06979978829622269, -0.009542618878185749, 0.04427708312869072, 0.05371669679880142, 0.04178714007139206, 0.0025854618288576603, 0.07216528803110123, -0.04678148403763771, 0.0010402991902083158, -0.013917912729084492, -0.029759269207715988, 0.06397401541471481, -0.018953531980514526, -0.00440601259469986, 0.04002563655376434, -0.03883710131049156, -0.07037988305091858, 0.016827518120408058, -0.044044848531484604, -0.006049049086868763, 0.019484708085656166, -0.01591571420431137, -0.008017689920961857, -0.10435884445905685, 0.09144899249076843, -0.028165427967905998, -0.02346722036600113, 0.02779698371887207, 0.05468301102519035, 0.02180751971900463, -0.08609412610530853, -0.05053291469812393, 0.004440971650183201, -0.05182582139968872, -0.035163454711437225, -0.008819377049803734, 0.05840373411774635, -0.03633313253521919, -3.243415931265194e-33, -0.03944266587495804, 0.06565126031637192, 0.0010142348473891616, 0.04847903177142143, -0.07043969631195068, -0.0441274493932724, -0.01369963027536869, 0.01348150335252285, -0.020036255940794945, 0.06335608661174774, 0.09236890822649002, -0.03366614133119583, 0.014824554324150085, -0.0379682295024395, -0.008184300735592842, -0.10918910801410675, -0.07609594613313675, 0.07817122340202332, 0.054817769676446915, -0.04386849328875542, 0.0038809170946478844, 0.09004789590835571, -0.05017658323049545, 0.09418633580207825, -0.030572108924388885, 0.03827016055583954, -0.020469259470701218, 0.0537152960896492, 0.01841050200164318, 0.03431004658341408, -0.05187023803591728, 0.05167349800467491, 0.003989484626799822, -0.0058684381656348705, -0.019972704350948334, -0.03252159431576729, 0.050797685980796814, 0.04582957178354263, 0.013581369072198868, 0.07343798130750656, 0.05688849091529846, 0.04998556897044182, -0.02771698124706745, -0.027950000017881393, -0.035015273839235306, -0.03637855872511864, -0.047399867326021194, 0.06115680932998657, 0.012320911511778831, 0.07026539742946625, 0.09294232726097107, -0.05463939905166626, -0.013488445430994034, 0.020566869527101517, -0.042243123054504395, 0.10279916226863861, 0.05616658553481102, 0.03971991315484047, 0.11836683750152588, 0.05123141035437584, -0.05853606387972832, -0.059258684515953064, 0.037692829966545105, -0.00032185844611376524, 0.056113820523023605, -0.013280435465276241, 0.02528209052979946, 0.0812334343791008, 0.0053986371494829655, 0.042947523295879364, 0.00408301642164588, -0.015966949984431267, 0.031200969591736794, 0.04949186369776726, 0.03403780981898308, -0.05710526183247566, -0.019485492259263992, -0.021707948297262192, -0.009430759586393833, -0.04143629968166351, -0.03775084763765335, -0.04899279773235321, -0.015895012766122818, 0.0920744240283966, 0.1240156963467598, 0.03792734444141388, -0.01593155413866043, -0.049717821180820465, 0.023758646100759506, 0.06772761046886444, 0.0009518617298454046, 0.09074410051107407, 0.10268635302782059, -0.04523751884698868, -0.029740232974290848, -4.9544723879080266e-08, -0.05987485870718956, 0.031759511679410934, -0.0032299079466611147, -0.0344354473054409, 0.011584945023059845, 0.003021651180461049, 0.0530085526406765, -0.04504188150167465, -0.01875307224690914, -0.12188778817653656, 0.09153562784194946, -0.0832957848906517, -0.1456020176410675, -0.13563846051692963, -0.03588014841079712, 0.0134982168674469, -0.020149661228060722, -0.05595288425683975, -0.053786180913448334, -0.030011530965566635, 0.02501973882317543, -0.00834699161350727, -0.06288593262434006, 0.04570658132433891, 0.03645355999469757, -0.03223443776369095, 0.01046258956193924, 0.11555242538452148, -0.008152741007506847, 0.023298848420381546, 0.0021153297275304794, 0.0012661630753427744, 0.10046658664941788, -0.011553258635103703, 0.01690908521413803, 0.0637972429394722, 0.039952509105205536, -0.004334358964115381, -0.01818716526031494, 0.070528045296669, -0.0012984994100406766, 0.03974677249789238, -0.03778703138232231, -0.04740734398365021, 0.028848297894001007, -0.03266962990164757, 0.03956590220332146, -0.055843938142061234, 0.010452657006680965, -0.005025768652558327, 0.052040114998817444, -0.05309959501028061, 0.02539811283349991, 0.05298107862472534, -0.03214837238192558, -0.05072134733200073, 0.0067283897660672665, -0.04690699279308319, 0.03866816312074661, 0.09368188679218292, 0.05774872004985809, 0.024227624759078026, -0.022269506007432938, -0.031122753396630287]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\BackpropagationandItsApplicationtoHandwrittenSignatureVerification.pdf,NLP,"256 AN INFORMATION THEORETIC APPROACH TO RULE-BASED CONNECTIONIST EXPERT SYSTEMS Rodney M. Goodman, John W. Miller Department of Electrical Engineering C altech 116-81 Pasadena, CA 91125 Padhraic Smyth Communication Systems Research Jet Propulsion Laboratories 238-420 4800 Oak Grove Drive Pasadena, CA 91109 Abstract We discuss in this paper architectures for executing probabilistic rule-bases in a par- allel manner, using as a theoretical basis recently introduced information-theoretic models. We will begin by describing our (non-neural) learning algorithm and theory of quantitative rule modelling, followed by a discussion on the exact nature of two particular models. Finally we work through an example of our approach, going from database to rules to inference network, and compare the network's performance with the theoretical limits for specific problems. Introduction With the advent of relatively cheap mass storage devices it is common in many domains to maintain large databases or logs of data, e.g., in telecommunications, medicine, finance, etc. The question naturally arises as to whether we can extract models from the data in an automated manner and use these models as the basis for an autonomous rational agent in the given domain, i.e., automatically generate ""expert systems"" from data. There are really two aspects to this problem: firstly learning a model and, secondly, performing inference using this model. What we propose in this paper is a rather novel and hybrid approach to learning and in- ference. Essentially we combine the qu'alitative knowledge representation ideas of AI with the distributeq, computational advantages of connectionist models, using an underlying theoretical basis tied to information theory. The knowledge repre- sentation formalism we adopt is the rule-based representation, a scheme which is well supported by cognitive scientists and AI researchers for modeling higher level symbolic reasoning tasks. We have recently developed an information-theoretic al- gorithm called ITRULE which extracts an optimal set of probabilistic rules from a given data set [1, 2, 3]. It must be emphasised that we do not use any form of neural learning such as backpropagation in our approach. To put it simply, the ITRULE learning algorithm is far more computationally direct and better understood than (say) backpropagation for this particular learning task of finding the most infor- mative individual rules without reference to their collective properties. Performing useful inference with this model or set of rules, is quite a difficult problem. Exact theoretical schemes such as maximum entropy (ME) are intractable for real-time applications. An Infonnation Theoretic Approach to Expert Systems 257 We have been investigating schemes where the rules represent links on a directed graph and the nodes correspond to propositions, i.e., variable-value pairs. Our approach is characterised by loosely connected, multiple path (arbitrary topology) graph structures, with nodes performing local non-linear decisions as to their true state based on both supporting evidence and their a priori bias. What we have in fact is a recurrent neural network. What is different about this approach compared to a standard connectionist model as learned by a weight-adaptation algorithm such as BP? The difference lies in the semantics of the representation [4]. Weights such as log-odds ratios based on log transformations of probabilities possess a clear mean- ing to the user, as indeed do the nodes themselves. This explicit representation of knowledge is a key requirement for any system which purports to perform reasoning, probabilistic or otherwise. Conversely, the lack of explicit knowledge representation in most current connectionist approaches, i.e., the ""black box"" syndrome, is a ma- jor limitation to their application in critical domains where user-confidence and explanation facilities are key criteria for deployment in the field. Learning the model Consider that we have M observations or samples available, e.g., the number of items in a database. Each sample datum is described in terms of N attributes or features, which can assume values in a corresponding set of N discrete alpha- bets. For example our data might be described in the form of lO-component binary vectors. The requirement for discrete rather than continuous-valued attributes is dictated by the very nature of the rule-based representation. In addition it is impor- tant to note that we do not assume that the sample data is somehow exhaustive and ""correct."" There is a tendency in both the neural network and AI learning literature to analyse learning in terms of learning a Boolean function from a truth table. The implicit assumption is often made that given enough samples, and a good enough learning algorithm we can always learn the function exactly. This is a fallacy, since it depends on the feature representation. For any problem of interest there are always hidden causes with a consequent non-zero Bayes misclassification risk, i.e., the function is dependent on non-observable features (unseen columns of the truth table). Only in artificial problems such as game playing is ""perfect"" classification possible - in practical problems nature hides the real features. This phenomenon is well known in the statistical pattern recognition literature and renders invalid those schemes which simply try to perfectly classify or memorise the training data. We use the following simple model of a rule, i.e., IT Y = y then X = x with probability p where X and Yare two attributes (random variables) with ""x"" and ""y"" being values in their respective discrete alphabets. Given sample data as described earlier we pose the problem as follows: can we find the ""best"" rules from a given data set, say the K best rules? We will refer to this problem as that of generalised rule induction, in order to distinguish it from the special case of deriving classification 258 Goodman, Miller and Smyth rules. Clearly we require both a preference measure to rank the rules and a learning algorithm which uses the preference measure to find the K best rules. Let us define the information which the event y yields about the variable X, say !(Xj y). Based on the requirements that !(Xj y) is both non-negative and that its expectation with respect to Y equals the average mutual information J(Xj Y), Blachman [5] showed that the only such function is the j-measure, which is defined as i(Xj y) = p(x\y) log (p(x\y)) + p(x\y) log (p(x)~y)) p(x) p(x) More recently we have shown that i(Xj y) possesses unique properties as a rule information measure [6]. In general the j-measure is the average change in bits required to specify X between the a priori distribution (p(X)) and the a posteriori distribution (p(X\y)). It can also be interpreted as a special case of the cross-entropy or binary discrimination (Kullback [7]) between these two distributions. We further define J(Xj y) as the average information content where J(X; y) = p(Y)-i(Xj y). J(Xj y) simply weights the instantaneous rule information i(X; y) by the probability that the left-hand side will occur, i.e., that the rule will be fired. This definition is motivated by considerations of learning useful rules in a resource-constrained environment. A rule with high information content must be both a good predictor and have a reasonable probability of being fired, i.e., p(y) can not be too small. Interestingly enough our definition of J(Xj y) possesses a well-defined interpretation in terms of classical induction theory, trading off hypothesis simplicity with the goodness-of-fit of the hypothesis to the data [8]. The ITRULE algorithm [1, 2, 3] uses the J-measure to derive the most informative set of rules from an input data set. The algorithm produces a set of K probabilistic rules, ranked in order of decreasing information content. The parameter K may be user-defined or determined via some statistical significance test based on the size of the sample data set available. The algorithm searches the space of possible rules, trading off generality of the rules with their predictiveness, and using information- theoretic bounds to constrain the search space. Using the Model to Perform Inference Having learned the model we now have at our disposal a set of lower order con- straints on the N-th order joint distribution in the form of probabilistic rules. This is our a priori model. In a typical inference situation we are given some initial conditions (i.e., some nodes are clamped), we are allowed to measure the state of some other nodes (possibly at a cost), and we wish to infer the state or probability of one more goal propositions or nodes from the available evidence. It is important to note that this is a much more difficult and general problem than classification of a single, fixed, goal variable, since both the initial conditions and goal propositions may vary considerably from one problem instance to the next. This is the infer- ence problem, determining an a posteriori distribution in the face of incomplete and uncertain information. The exact maximum entropy solution to this problem is in- An Information Theoretic Approach to Expert Systems 259 tractable and, despite the elegance of the problem formulation, stochastic relaxation techniques (Geman [9]) are at present impractical for real-time robust applications. Our motivation then is to perform an approximation to exact Bayesian inference in a robust manner. With this in mind we have developed two particular models which we describe as the hypothesis testing network and the uncertainty network. Principles of the Hypothesis Testing Network In the first model under consideration each directed link from Y to x is assigned a weight corresponding to the weight of evidence of yon x. This idea is not necessarily new, although our interpretation and approach is different to previous work [10, 4]. Hence we have W -1 p{xIY) -1 p(:xIY) :r.y - og p(x) og p(x) and R = -log p(x) :r. p(x) and the node x is assigned a threshold term corresponding to a priori bias. We use a sigmoidal activation function, i.e., n 1 a ( x) = --~7'""""E=-t----;;R'--, l+e T where l:J.E:r. = I: W:r.y; . q(y,) - R:r. ,=1 based on multiple binary inputs Y1 ... Yn to x. Let 8 be the set of all Yi which are hypothesised true (Le., a{yd = 1), so that AE = I p(x) + '"" (1 p(xlYd _ 1 p(xIY,)) L.l:r. og p(x) L- og p(x) og p(x) y;ES If each y, is conditionally independent given x then we can write p(xIS) = p(x) II p(xIY,) p(xIS) p(x) y;ES p(xlYd Therefore the updating rule for conditionally independent y, is: T . log a(x) = log p(xI8) 1 - a(x) 1 - p(x/S) Hence a(x) > ~ iff p{xI8) > ~ and if T == 1, a(x) is exactly p(xIS). In terms of a hypothesis test, a(x) is chosen true iff: '"" I p(XIYi) > I p{x) L- og - og-- p(XIYi) - p(x) Since this describes the Neyman-Pearson decision region for independent measure- ments (evidence or yd with R:r. = -log :~~~ [11], this model can be interpreted as a distributed form of hypothesis testing. 260 Goodman, Miller and Smyth Principles of the Uncertainty Network For this model we defined the weight on a directed link from Yi to x as . ( p(XIYi) _ p(xIYi))) W XYi = si.1(XjYi) = Si· p(XIYi}log( p(x) ) + p(xly,)log( p(x) where Si = ±1 and the threshold is the same as the hypothesis model. We can interpret W:Zlli as the change in bits to specify the a posteriori distribution of x. H P(XIYi) > p{x), w:ZYi has positive support for x, i.e., Si = +1. H P{XIYi) < p(x), W:Zlli has negative support for x, Le., Si = -1. IT we interpret the activation a(Yi) as an estimator (p(y)) for p(Yi), then for multiple inputs, i ~ .. ( ) P(XIYi) (_ P(XIYi) ) - ~ p(Yi).Si. p(XIYi log( p{x) ) + P xly,) log( p(x) ) • This sum over input links weighted by activation functions can be interpreted as the total directional change in bits required to specify x, as calculated locally by the node x. One can normalise !:1Ex to obtain an average change in bits by dividing by a suitable temperature T. The node x can make a local decision by recovering p(x) from an inverse J-measure transformation of !:1E (the sigmoid is an approximation to this inverse function). Experimental Results and Conclusions In this section we show how rules can be generated from example data and auto- matically incorporated into a parallel inference network that takes the form of a multi-layer neural network. The network can then be ""run"" to perform parallel inference. The domain we consider is that of a financial database of mutual funds, using published statistical data [12]. The approach is, however, typical of many different real world domains. Figure 1 shows a portion of a set of typical raw data on no-load mutual funds. Each line is an instance of a fund (with name omitted), and each column represents an attribute (or feature) of the fund. Attributes can be numerical or categorical. Typical categorical attributes are the fund type which reflect the investment objec- tives of the fund (growth, growth and income, balanced, and agressive growth) and a typical numerical attribute is the five year return on investment expressed as a percentage. There are a total of 88 fund examples in this data set. From this raw data a second quantized set of the 88 examples is produced to serve as the input to ITRULE (Figure 2). In this example the attributes have been categorised to binary values so that they can be directly implemented as binary neurons. The ITRULE software then processes this table to produce a set of rules. The rules are ranked in order of decreasing information according to the J-measure. Figure 3 shows a An Infonnation Theoretic Approach to Expert Systems 261 portion (the top ten rules) of the ITRULE output for the mutual fund data set. The hypothesis test log-likelihood metric h(Xj y), the instantaneous j-measure j(Xj y), and the average J-measure J(Xj y), are all shown, together with the rule transition probability p{x/y). In order to perform inference with the ITRULE rules we need to map the rules into a neural inference net. This is automatically done by ITRULE which gener- ates a network file that can be loaded into a neural network simulator. Thus rule information metrics become connection weights. Figure 4 shows a typical network derived from the ITRULE rule output for the mutual funds data. For clarity not all the connections are shown. The architecture consists of two layers of neurons (or ""units""): an input layer and an output layer, both of which have an activation within the range {O,l}. There is one unit in the input layer (and a corresponding unit in the output layer) for each attribute in the mutual funds data. The output feeds back to the input layer, and each layer is synchronously updated. The output units can be considered to be the right hand sides of the rules and thus receive inputs from many rules, where the strength of the connection is the rule's metric. The output units implement a sigmoid activation function on the sum of the in- puts, and thus compute an activation which is an estimator of the right hand side posteriori attribute value. The input units simply pass this value on to the output layer and thus have a linear activation. To perform inference on the network, a probe vector of attribute values is loaded into the input and output layers. Known values are clamped and cannot change while unknown or desired attribute values are free to change. The network then relaxes and after several feedback cycles converges to a solution which can be read off the input or output units. To evaluate the models we setup fo~r standard clas- sification tests with varying number of nodes clamped as inPlits. Undamped nodes were set to their a priori probability. After relaxing the network, the activation of the ""target"" node was compared with the true attribute values for that sample in order to determine classification performance. The two models were each trained on 10 randomly selected sets of 44 samples. The performance results given in Table 1 are the average classification rate of the models on the other 44 unseen samples. The Bayes risk (for a uniform loss matrix) of each classification test was calculated from the 88 samples. The actual performance of the networks occasionally exceeded this value due to small sample variations on the 44/44 cross validations. Table 1 Units Cramped Uncertainty Test HYPOthesis Test 1 - Bayes' Risk 9 66.8% 70.4% 88.6% 5 70.1% 70.1% 80.6% 2 48.2% 63.0% 63.6% 1 51.4% 65.7% 64.8% 262 Goodman, Miller and Smyth We conclude from the performance of the networks as classifiers that they have indeed learned a model of the data using a rule-based representation. The hypoth- esis network performs slightly better than the uncertainty model, with both being quite close to the estimated optimal rate (the Bayes' risk). Given that we know that the independence assumptions in both models do not hold exactly, we coin the term robust inference to describe this kind of accurate behaviour in the presence of incomplete and uncertain information. Based on these encouraging initial results, our current research is focusing on higher-order rule networks and extending our theoretical understanding of models of this nature. Acknowledgments This work is supported in part by a grant from Pacific Bell, and by Caltech's program in Advanced Technologies sponsored by Aerojet General, General Motors and TRW. Part of the research described in this paper was carried out by the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration. John Miller is supported by NSF grant no. ENG-8711673. References 1. R. M. Goodman and P. Smyth, 'An information theoretic model for rule-based expert systems,' presented at the 1988 International Symposium on Information Theory, Kobe, Japan. 2. R. M. Goodman and P. Smyth, 'Information theoretic rule induction,' Proceed- ings of the 1988 European Conference on AI, Pitman Publishing: London. 3. R. M. Goodman and P. Smyth, 'Deriving rules from databases: the ITRULE algorithm,' submitted for publication. 4. H. Geffner and J. Pearl, 'On the probabilistic semantics of connectionist net- works,' Proceedings of the 1987 IEEE ICNN, vol. II, pp. 187-195. 5. N. M. Blachman, 'The amount of information that y gives about X,' IEEE Transactions on Information Theory, vol. IT-14 (1), 27-31, 1968. 6. P. Smyth and R. M. Goodman, 'The information content of a probabilistic rule,' submitted for publication. 7. S. Kullback, Information Theory and Statistics, New York: Wiley, 1959. 8. D. Angluin and C. Smith, 'Inductive inference: theory and methods,' ACM Computing Surveys, 15(9), pp. 237-270, 1984. 9. S. Geman, 'Stochastic relaxation methods for image restoration and expert sys- tems,' in Maximum Entropy and Bayesian Methods in Science and Engineering (Vol. 2), 265-311, Kluwer Academic Publishers, 1988. 10. G. Hinton and T. Sejnowski, 'Optimal perceptual inference,' Proceedings of the IEEE CVPR 1989. 11. R. E. Blahut, Principles and Practice of Information Theory, Addison-Wesley: Reading, MA, 1987. 12. American Association of Investors, The individual investor's guide to no-load mutual funds, International Publishing Corporation: Chicago, 1987. An Infonnation Theoretic Approach to Expert Systems 263 Fund Type 5 Year Diver- Beta Bull Bear Stocks Invest- Net Distri- Expense Turn- Total Return sity (Risk) Perf. Perf. 0/0 ment Asset butions Ratio % over Assets 0/0 Incm. $ Value $ (%NAV\ Rate %$M Balanced 136 C 0.8 B D 87 0.67 37 .3 17.63 0.79 34 415 Growth 32 .5 C 1.05 E B 81 -0.02 12.5 0.88 1.4 200 16 Growth& Income 88.3 A 0.96 C D 82 0.14 11.9 4.78 1.34 127 27 Agressive -24 A 1.23 E E 95 0.02 6.45 9.30 1.4 1 61 64 Growth&lncome 172 E 0.59 A B 73 0.53 13.6 9.97 1.09 31 113 Balanced 144 C 0.71 B B 51 0.72 13 10.44 0.98 239 190 Flgure1. Raw Mutual Funds Data Type Type Type Type 5 Year Beta Stocks Turn- Assets Distri- Diver- Bull Bear A B G (?J Return 0/0 >90% over butions sity Perf. Perf. S&P=1380/0 above S&P <100% <$100M <150/0NAV C.D.E C.D.E C.D.E below S&P >100% >$100M >150/0NAV A.B AB A,B no no yes no below under1 no low large high low high low no no yes no below over1 no high small low low low high no no no yes below under1 no high small low high low low no no no yes above under1 no low large low low high high no no no yes below under1 yes low small high high low high no no yes no above under1 no low large high high high low Figure 2. Quantized Mutual Funds Data ITRULE rule output: Mutual Funds p(x/y) j(X;y) J(X;y) h(X;y) 1 IF 2 IF 3 IF 4 IF 5 IF 6 IF 7 IF 8 IF 9 IF 10 IF D D 5yrRebS&P BullJ)erf Assets BullJ)erf typeA BullJ)erf typeGl BullJ)erf typeG Assets above lHEN BullJ)erf high 0.97 0.75 0.235 4.74 low lHEN 5yrRet>S&P below 0.98 0.41 0.201 4.31 large lHEN BullJ)erf high 0.81 0.28 0.127 2.02 high lHEN 5yrRet>s&P above 0.40 0.25 0.127 -1.71 yes lHEN typeG no 0 .04 0 .50 0.123 -3 .87 low lHEN Assets small 0.18 0.25 0.121 -1 .95 yes lHEN typeG no 0.05 0 .49 0.109 -3.74 high lHEN Assets large 0.72 0.21 0.109 1.64 yes lHEN typeA no 0.97 0.27 0.108 3.54 small lHEN Bull perf low 0 .26 0.19 0.103 -1.57 Figure 3. Top Ten Mutual Funds Rules nfo2atl~ 0 0 ~ ~ ~ Input layer - linear units metric connection weights I I one unit per attribute I I o DOD 0 D D 0 I Feedback connections weight = 1 o output layer - sigmoid units Figure 4. Rule Network","[-0.061647966504096985, -0.04922603815793991, -0.039226189255714417, -0.02166025899350643, -0.025506678968667984, -0.07055116444826126, 0.05815153941512108, 0.05147897079586983, 0.020639093592762947, 0.013051156885921955, -0.058414261788129807, 0.007773334160447121, 0.13259649276733398, 0.05947590991854668, 0.008290903642773628, 0.057738542556762695, 0.008356144651770592, 0.00832750927656889, -0.11585623770952225, -0.025694403797388077, -0.02631780505180359, -0.01815851777791977, -0.0901864618062973, -0.03573303297162056, -0.022054167464375496, 0.0044882274232804775, 0.04563967138528824, 0.008783387020230293, -0.04093340039253235, -0.03715633973479271, 0.0006204548990353942, -0.09059672802686691, 0.05250733718276024, 0.027839137241244316, 0.07740963250398636, 0.005036927759647369, -0.030318068340420723, 0.040526166558265686, -0.0019378067227080464, 0.04777083173394203, 0.015999436378479004, -0.04334848001599312, -0.05535325035452843, 0.0636344775557518, 0.10286290943622589, 0.08054520934820175, 0.012621208094060421, 0.05931125208735466, -0.07288990914821625, 0.011482816189527512, -0.13438983261585236, -0.0074708773754537106, 0.029955781996250153, 0.04345673322677612, 0.041699040681123734, 0.010914289392530918, 0.040280912071466446, -0.017140261828899384, -0.1263151913881302, -0.05224086716771126, -0.01553518045693636, -0.08939637243747711, -0.10453250259160995, -0.04633133485913277, -0.046529028564691544, 0.04095057025551796, -0.031607095152139664, 0.06180562451481819, 0.04221053421497345, 0.03775924816727638, -0.0518234483897686, 0.06770673394203186, -0.06765762716531754, 0.06677121669054031, -0.001166471396572888, -0.016327127814292908, 0.024246107786893845, 0.035967953503131866, -0.0016072455327957869, -0.040739160031080246, -0.054760128259658813, -0.04101550951600075, 0.0018092209938913584, 0.008610363118350506, 0.06457586586475372, -0.08745145052671432, -0.018963206559419632, 0.03857080265879631, 0.07420200854539871, 0.018779071047902107, -0.017044277861714363, -0.009020896628499031, 0.060792598873376846, -0.016222316771745682, 0.013113386929035187, 0.08797276765108109, 0.03208533301949501, -0.09055240452289581, 0.04644370079040527, 0.10303843021392822, -0.034621935337781906, 0.06101418286561966, -0.041505586355924606, -0.07307280600070953, 0.04103957489132881, 0.04693664610385895, 0.010725406929850578, 0.06981595605611801, 0.09228602796792984, -0.12052003294229507, -0.04799104481935501, 0.0573435015976429, -0.05210704356431961, -0.03043176606297493, 0.015359921380877495, 0.00045039525139145553, -0.029390254989266396, 0.04960092902183533, -0.004915053490549326, 0.026599733158946037, -0.04074856638908386, 0.03040897287428379, 0.026340415701270103, 0.04940175637602806, 0.06203753128647804, 0.049707502126693726, -0.11349356919527054, 2.778406161785236e-33, -0.0030742413364350796, 0.02586268074810505, 0.033273424953222275, -0.018270006403326988, 0.03358013555407524, 0.0038929441943764687, -0.08871911466121674, -0.007548499386757612, 0.024371614679694176, 0.05290554463863373, -0.07902493327856064, 0.013828175142407417, -0.027812479063868523, -0.03970278054475784, 0.09903686493635178, 0.025244422256946564, -0.04030046612024307, -0.008653257973492146, 0.025558458641171455, -0.05635605379939079, 0.062405191361904144, -0.04511558264493942, -0.027052640914916992, 0.001307717990130186, 0.03741830214858055, -0.008900959976017475, 0.028572095558047295, 0.03878554329276085, 0.09340212494134903, 0.014863369055092335, -0.07178732007741928, -0.010865927673876286, 0.012400479055941105, 0.039867956191301346, 0.01974140666425228, 0.012241069227457047, -0.046884458512067795, -0.02238091267645359, 0.0245034359395504, -0.020799975842237473, -0.042031534016132355, -0.02149292081594467, 0.009213373064994812, 0.039862047880887985, -0.1349625438451767, -0.006273508537560701, -0.03781991824507713, 0.014955943450331688, -0.03787624463438988, -0.1308690309524536, 0.06153915449976921, -0.004785686731338501, 0.03247546777129173, -0.07572248578071594, -0.008708607405424118, 0.01882297359406948, -0.030151236802339554, 0.05864723399281502, 0.021482350304722786, 0.053180597722530365, -0.0042635779827833176, 0.04878741502761841, 0.014829345047473907, 0.06155683472752571, 0.004317695740610361, 0.05064060166478157, -0.04631001502275467, 0.013407117687165737, 0.08753255009651184, -0.015301297418773174, -0.003924005199223757, 0.07693959772586823, -0.02901069074869156, -0.031228922307491302, 0.04577512666583061, -0.00665769400075078, -0.002686364809051156, -0.014187072403728962, -0.030851950868964195, 0.022687451913952827, -0.026866059750318527, 0.040171124041080475, 0.027447950094938278, 0.031661007553339005, 0.04707948863506317, -0.011919300071895123, -0.0021524352487176657, -0.010345003567636013, -0.047544609755277634, -0.03870643675327301, -0.060173917561769485, -0.02036173827946186, -0.0016203083796426654, 0.02228649891912937, -0.010668274946510792, -4.0201664352412376e-33, -0.07432587444782257, -0.05466112866997719, -0.044999830424785614, 0.05022452026605606, 0.00031194943585433066, -0.05028901249170303, -0.11862026900053024, -0.04639781266450882, 0.0487956628203392, -0.11669477820396423, -0.04774175211787224, 0.0005145246395841241, 0.06249669939279556, -0.04954111948609352, -0.046511851251125336, -0.014275524765253067, -0.07601732760667801, -0.05828819423913956, 0.027717823162674904, 0.06642164289951324, -0.025648916140198708, -0.02501089684665203, -0.11648716032505035, -0.011444238014519215, 0.028901448473334312, 0.01616361178457737, -0.06257113069295883, 0.10758987814188004, -0.003557677613571286, 0.0895407423377037, -0.08680380135774612, -0.07218343019485474, -0.013998341746628284, -0.05043778941035271, -0.06480709463357925, 0.045083001255989075, -0.024597495794296265, 0.044770773500204086, -0.0071534933522343636, 0.05825284868478775, -0.016666091978549957, -0.038999319076538086, -0.042920853942632675, -0.04934540018439293, -0.0015575794968754053, -0.024701818823814392, -0.06171911582350731, 0.036191366612911224, -0.025649210438132286, -0.005460164975374937, -0.0023798795882612467, -0.0020935842767357826, -0.020852485671639442, 0.006833234336227179, -0.06693737208843231, 0.07476752251386642, 0.02020237408578396, 0.027027253061532974, 0.03288325294852257, 0.07605639100074768, -0.055845197290182114, -0.05674906447529793, -0.006675001233816147, 0.05308492109179497, -0.004175118636339903, -0.029201645404100418, -0.0062429881654679775, 0.01733274571597576, 0.012988198548555374, 0.0031840254087001085, 0.01271276269108057, -0.01766805350780487, -0.0739118829369545, 0.10198645293712616, 0.06137963756918907, 1.95668799278792e-05, -0.03086812049150467, 0.011296287178993225, 0.019025739282369614, 0.015746992081403732, -0.00574703561142087, 0.028206557035446167, 0.0674377828836441, 0.013194561935961246, 0.03282242640852928, -0.017787115648388863, 0.1141563206911087, -0.007111954502761364, 0.0603909008204937, -0.037475623190402985, 0.033372629433870316, 0.020366430282592773, -0.0063119144178926945, 0.04917851835489273, -0.1635749191045761, -5.621932075428049e-08, -0.04865696281194687, 0.002224669326096773, 0.08184080570936203, 0.043739475309848785, 0.12245425581932068, -0.02754298225045204, 0.04195640608668327, 0.02128753438591957, -0.050005074590444565, -0.05828229710459709, 0.0302780382335186, 0.04303321614861488, -0.0352928526699543, -0.024095986038446426, 0.007636594120413065, 0.07792049646377563, 0.07808209955692291, 0.021868566051125526, -0.05335273593664169, 0.00777119304984808, 0.05727249011397362, -0.03282637894153595, 0.0028096288442611694, 0.05482311174273491, 0.13228926062583923, -0.07683917135000229, -0.05081038549542427, 0.0538950115442276, 0.005627317354083061, 0.12264413386583328, -0.028265248984098434, -0.011541307903826237, 0.04066965728998184, 0.022410644218325615, 0.09074895828962326, 0.07500550895929337, 0.02183029055595398, -0.09941017627716064, -0.054377153515815735, -0.07224269211292267, 0.0031494207214564085, -0.043882861733436584, -0.025150714442133904, 0.025006601586937904, 0.11613603681325912, -0.029257113113999367, -0.001605115132406354, -0.03743385896086693, 0.08242513239383698, 0.06541018187999725, 0.043495796620845795, -0.008026623167097569, 0.011931095272302628, 0.010740840807557106, 0.031664833426475525, -0.018125930801033974, 0.07219543308019638, -0.03283952549099922, -0.01700684241950512, -0.025162911042571068, -0.010555190965533257, 0.050348687916994095, 0.04803634062409401, -0.03646095469594002]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ComparingBiasesforMinimalNetworkConstructionwithBackPropagation.pdf,Deep Learning,"124 ADAPTIVE NEURAL NET PREPROCESSING FOR SIGNAL DETECTION IN NON-GAUSSIAN NOISE1 Richard P. Lippmann and Paul Beckman MIT Lincoln Laboratory Lexington, MA 02173 ABSTRACT A nonlinearity is required before matched filtering in mInimum error receivers when additive noise is present which is impulsive and highly non-Gaussian. Experiments were performed to determine whether the correct clipping nonlinearity could be provided by a single-input single- output multi-layer perceptron trained with back propagation. It was found that a multi-layer perceptron with one input and output node, 20 nodes in the first hidden layer, and 5 nodes in the second hidden layer could be trained to provide a clipping nonlinearity with fewer than 5,000 presentations of noiseless and corrupted waveform samples. A network trained at a relatively high signal-to-noise (SIN) ratio and then used as a front end for a linear matched filter detector greatly reduced the probability of error. The clipping nonlinearity formed by this network was similar to that used in current receivers designed for impulsive noise and provided similar substantial improvements in performance. INTRODUCTION The most widely used neural net, the adaptive linear combiner (ALe). is a single- layer perceptron with linear input and output nodes. It is typically trained using the LMS algorithm and forms one of the most common components of adaptive filters. ALes are used in high-speed modems to construct equalization filters, in telephone links as echo cancelers, and in many other signal processing applications where linear filtering is required [9]. The purpose of this study was to determine whether multi- layer perceptrons with linear input and output nodes but with sigmoidal hidden nodes could be as effective for adaptive nonlinear filtering as ALes are for linear filtering. 1 This work wa.s sponsored by the Defense Advanced Research Projects Agency and the Depart- ment of the Air Force. The views expressed are those of the authors and do not reflect the policy or position of the U . S. Government. Adaptive Neural Net Preprocessing for Signal Detection 125 The task explored in this paper is signal detection with impulsive noise where an adaptive nonlinearity is required for optimal performance. Impulsive noise occurs in underwater acoustics and in extremely low frequency communications channels where impulses caused by lightning strikes propagate many thousands of miles [2]. This task was selected because a nonlinearity is required in the optimal receiver, the structure of the optimal receiver is known, and the resulting signal detection error rate provides an objective measure of performance. The only other previous studies of the use of multi-layer perceptrons for adaptive nonlinear filtering that we are aware of [6,8] appear promising but provide no objective performance comparisons. In the following we first present examples which illustrate that multi-layer percep- trons trained with back-propagation can rapidly form clipping and other nonlinear- ities useful for signal processing with deterministic training. The signal detection task is then described and theory is presented which illustrates the need for nOll- linear processing with non-Gaussian noise. Nonlinearities formed when the input to a net is a corrupted signal and the desired output is the uncorrupted signal are then presented for no noise, impulsive noise, and Gaussian noise. Finally, signal detection performance results are presented that demonstrate large improvements in performance with an adaptive nonlinearity and impulsive noise. FORMING DETERMINISTIC NONLINEARITIES A theorem proven by Kohnogorov and described in [5] demonstrates that single- input single-output continuous nonlinearities can be formed by a multi-layer percep- tron with two layers of hidden nodes. This proof, however, requires complex nonlin- ear functions in the hidden nodes that are very sensitive to the desired input/output function and may be difficult to realize. ""More recently, Lapedes [4] presented an intuitive description of how multi-layer perceptrons with sigmoidal nonlinearities could produce continuous nonlinear mappings. A careful mathematical proof was recently developed by Cybenko [1] which demonstrated that continuous nonlinear mappings can be formed using sigmoidal nonlinearities and a multi-layer perceptron with one layer of hidden nodes. This proof, however, is not constructive and does not indicate how many nodes are required in the hidden layer. The purpose of our study was to determine whether multi-layer perceptrons with sigmoidal nonlineari- ties and trained using back-propagation could adaptively and rapidly form clipping nonlinearities. Initial experiments were performed to determine the difficulty of learning complex mappings using multi-layer perceptrons trained using back-propagation. Networks with 1 and 2 hidden layers and from 1 to 50 hidden nodes per layer were evalu- ated. Input and output nodes were linear and all other nodes included sigmoidal nonlinearities. Best overall performance was provjded by the three-layer perceptron shown in Fig. 1. It has 20 nodes in the first and 5 nodes in the second hidden layer. This network could form a wide variety of mappings and required only slightly more training than other networks. It was used in all experiments. 126 Lippmann and Beckman y - OUTPUT (LInear Sum) (20 Nodes) x - INPUT Figure 1: The multi-layer perceptron with linear input and output nodes that was used in all experiments. The three-layer network shown in Fig. 1 was used to form clipping and other deterministic nonlinearities. Results in Fig. 2 demonstrate that a clipping nonlinear- ity ('auld be formed with fewer than 1,000 input samples. Input/output point pairs were determined by selecting the input at random over the range plotted and using tlte deterministic clipping function shown as a solid line in Fig. 2. Back-propagation training [7] was used with the gain term (11) equal to 0.1 and the momentum term (0') equal to 0.5. These values provide good convergence rates for the clipping func- tion and all other functions tested. Initial connection weights were set to small random values. The multi-layer percept ron from Fig. 1 was also used to form the four nonlinear functions shown in Fig. 3. The ""Hole Punch"" is useful in nonlinear signal process- ing. It performs much the same function as the clipper but completely eliminates amplitudes above a certain threshold le\'el. Accurate approximation of this function required more than 50,000 input samples. The ""Step"" has one sharp edge and could be roughly approximated after 2,000 input samples. The ""Double Pulse"" requires approximation of two close ""pulses"" and is the nonlinear function analogy of the disjoint region problem studied in [3]. In this examplf>, back-propagation training approximated the rightmost pulse first after 5,000 input samples. Both pulses were then approximated fairly well after 50,000 input samples. The ""Gaussian Pulse"" is a smooth curve that could be approximated well after only 2,000 input samples. These results demonstrate that back-propagation training with sigmoidal 1I0nlin- earities can form many different nonlinear functions. Qualitative results on training times are similar to those reported in [.1]. In this previous study it was de mOll- Adaptive Neural Net Preprocessing for Signal Detection 127 BEFORE TRAINING DESIRED .... 1 ACTUAL .!: ••••••••. ~ 0 ~ ..... . 0-1 -2_2 -1 0 2 O. II: 0 II: II: u.I (f) :IE ex: -2 200 40 TRIALS 1000 TRIALS -1 0 t 2 -2 -1 0 2 INPUT (I() RMS ERROR 400 606 800 1000 TRIALS Figure 2: Clipping nonlinearities formed using back-propagation training and the multi-layer perceptron from Fig. 1 (top) and the rms error produced by these Ilon- linearities versus training time (bottom). strated that simple half-plane decision regions could be formed for classification problems with little training while complex disjoint decision regions required long training times. These new results suggest that complex nonlinearities with many sharp discontinuities require much more training time than simple smooth curves. THE SIGNAL DETECTION TASK The signal detection task was to discriminate between two equally likely input sig- nals as shown in Fig. 4. One signal (so(t)) corresponds to no input and the other signal (Sl(t)) was a sinewa\'c pulse with fixed duration and known amplitude, fre- quency, and phase. Noise was added to these inputs, the resultant signal was passed through a memoryless nonlinearity, and a matched filter was then used to select hy- pothesis Ho corresponding to no input or HI corresponding to the sinewave pulse. The matched filter multiplied the output of the nonlinearity by the known time- aligned signal waveform, integrated this product over time, and decided HI if the result was greater than a threshold and Ho otherwise. The threshold was selected to provide a minimum overall error rate. The optimum nonlinearity Ilsed in the de- tector depends on the noise distribu tion. If the signal levels are small relati\'e to the noise levels, then the optimum nonlinearity is approximated by f (J') = t;~ In{ (In (J')). where r .. (x) is the instantaneous probability density function of the noise (2]- This function is linear for Gaussian noise but has a clipping shape for impulsi\'e noise. 128 Lippmann and Beckman HOLE PUNCH STEP 2r---~---r--~---. I I f- -, .- l/ ""-. 0 • I f- / .......... I ····~·:·~5 ............... i.~j .............. ., N.5.ooo . N.5oo - ----- ----- Nco 50.000 ---_. ~ ·2 ·2 -, 0 2 Nco 2.000 1 1 -1--' ·2 o 2 ., I- :::l DOUBLE PULSE no GAUSSIAN PULSE I- 2 ::l 0 ·2 ·1 o 2 INPUT (xl Figure 3: Four deterministic nonlinearities formed using the multi-layer perceptron from Fig. 1. Desired functions are plotted as solid lines while functions formed using back-propagation with different numbers of input samples are plotted using dots and dashes. Examples of the signal, impulsive noise and Gaussian noise are presented in Fig. 5. The signal had a fixed duration of 250 samples and peak amplitude of 1.0. The impulsive noise was defined by its amplitude distribution and inter-arrival time. Amplit udes had a zero mean, Laplacian distribution with a standard de\'iation (IJ) of 14.1 in all experiments. The standard deviation was reduced to 2.8 in Fig. 5 for illustrative purposes. Inter-arrival times (L\T) between noise impulses had a Poisson distribution. The mean inter-arrival time was varied in experiments to obtain different SIN ratios after adding noise. For example varying inter-arrival times from 500 to 2 samples results in SIN ratios that vary from roughly 1 dB to - 24 dB. Additive Gaussian noise had zero mean and a standard oeviation (IJ) of 0.1 in all experiments. ADAPTIVE TRAINING WITH NOISE The three-layer perceptron was traineq as shown in Fig. 6 using the signal plus Iloist> as the input and the uncorrupted signal as the desired output. Network weights were adapted after every sample input using back-propagation training. Adaptive nonlinearitics formed during training are shown in Fig. 7. These are similar to those Adaptive Neural Net Preprocessing for Signal Detection 129 SO(II--- S,II)~ ~-...o{ MEMORYlESS NONLINEARITY NOISE 'I • I(x) N(tl MATCHED FILTER DETECTOR Figure 4: The signal detection task was to discriminate between a sinewa\·e pulse and a no-input condition with additive impulsive noise. UNCORRUPTED SIGNAL IMPULSE NOISE ct = 2.8 .H~ 12 GAUSSIAN NOISE maO (J • O. t o 50 100 150 200 250 0 50 100 150 200 2500 50 100 150 200 250 SAMPLES Figure 5: The input to the nonlinearity with no noise, additive impulsive noise, and additive Gaussian noise. required by theory. No noise results in nonlinearity that is linear over the range of the input sinewave (-1 to + 1) after fewer than 3,000 input samples. Impulsive noise at a high SIN ratio (6.T = 125 or SIN = -5 dB) results in a nonlinearity that clips above the signal level after roughly 5,000 input samples and then slowly forms a ""Hole Punch"" nonlinearity as the number of training samples increases. Gaussian noise noise results in a nonlinearity that is roughly linear over the range of the input sinewave after fewer than 5,000 input samples. SIGNAL DETECTION PERFORMANCE Signal detection performance was measured using a matched filter detector and the nonlinearity shown in the center of Fig. 7 for 10,000 input training samples. The error rate with a minimum-error matched filter is plotted in Fig. 8 for impulsive lIoise at SIN ratios ranging from roughly 5 dB to -24 dB. This error rate was estimated from 2,000 signal detection trials. Signal detection performance always improved with the nonlinearity and sometimes the improvement was dramatic. The error rate provided with the adaptively-formed nonlinearity is essentially identical 130 Lippmann and Beckman ~ ... ::;) Q. ... ::;) 0 5(1) X MULTI-LAYER Y )--.... --1 PERCEPTRON - I. NOISE BACK-PROPAGATION 1-_ ..... ALGORITHM + E DESIRED OUTPUT Figure 6: The procedure used for adaptive training. NO NOISE IMPULSE NOISE GAUSSIAN NOISE 2 0 N.1.000 -, N . 2.ooo N.3.ooo N. '00,000 N.5,000 ·2 -2 -, 0 2 ·2 ., 0 2 ·2 ., 0 2 INPUT (x) Figure 7: Nonlinearities formed with adaptive training with no additive noise, with additive impulsive noise at a SIN level of -5 dB, and with additive Gaussian noise. to that provided by a clipping nonlinearity that clips above the signal level. This error rate is roughly zero down to - 24 dB and then rises rapidly with higher levels of impulsive noise. This rapid increase in error rate below -24 dB is not shown in Fig. 8. The error rate with linear processing rises slowly as the SIN ratio drops and reaches roughly 36% when the SIN ratio is -24 dB. Further exploratory experiments demonstrated that the nonlinearity formed by back-propagation was not robust to the SIN ratio used during training. A clipping nonlinearity is only formed when the number of samples of uncorrupted sinewave input is high enough to form the linear function of the curve and the number of samples of noise pulses is low, but sufficient to form the non~ill('ar clipping section of the nonlinearity. At high noise levels the resulting nonlinearity is not linear Over the range of the input signal. It instead resembles a curve that interpolates between a flat horizontal input-output curve and the desired clipping curve. SUMMARY AND DISCUSSION In summary, it was first demonstrated that multi-layer perccptrons with linear w ~ < a: a: 40 o 20 a: a: w 10 Adaptive Neural Net Preprocessing for Signal Detection 131 ADAPTIVE NONLINEAR PROCESSING LINEAR ~ PROCESSING o ••• ..., •••••• .I ..... .I •••.• ~. ~ ____ -~. -Z5 ' -20 -15 -10 -5 o 5 SIN RATIO Figure 8: The signal detection error rate with impulsive noise when the SIN ratio after adding the noise ranges from 5 dB to - 24 dB. input and output nodes could approximate prespecified clipping nonlinearities re- quired for signal detection with impulsive noise with fewer than 1,000 trials of back-propagation training. More complex nonlinearities could also be formed but required longer training times. Clipping nonunearities were also formed adaptively using a multi-layer perceptron with the corrupted signal as the input and the noise- free signal as the desired output. Nonlinearities learned using this approach at high S / N ratios were similar to those required by theory and improved signal detection performance dramatically at low SIN ratios. Further work is necessary to further explore the utility of this technique for forming adaptive nonlinearities. This work should explore the robustness of the nonlinearity formed to variations in the input S / N ratio. It should also explore the use of multi-layer perccptrons and back- propagation training for other adaptive nonlinear signal processing tasks such as system identification, noise removal, and channel modeling. 132 Lippmann and Beckman References [1] G. Cybenko. Approximation by superpositions of a sigmoidal function. Re- search note, Department of Computer Science, Tufts University, October 1988. [2] J. E. Evans and A. S Griffiths. Design of a sanguine noise processor based upon world-wide extremely low frequency (elf) recordings. IEEE Transactions on Communications, COM-22:528-539, April 1974. [3] W. M. Huang and R. P. Lippmann. Neural net and traditional classifiers. In D. Anderson, editor, Neural Information Processing Systems, pages 387-396, New York, 1988. American Institute of Physics. [4] A. Lapedes and R. Farber. How neural nets work. In D. Anderson, editor, Neu- ral Information Processing Systems, pages 442-456, New York, 1988. American Institute of Physics. [5] G. G. Lorentz. The 13th problem of Hilbert. In F. E. Browder, editor, Afath- ematical Developments Arising from Hilbert Problems. American Mathematical Society, Providence, R.I., 1976. [6] D. Palmer and D. DeSieno. Removing random noise from ekg signals using a back propagation network, 1987. HNC Inc., San Diego, CA. [7] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal represen- tations by error propagation. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing, volume 1: Foundations, chapter 8. MIT Press, Cambridge, MA, 1986. [8] S. Tamura and A. Wailbel. Noise reduction using connectionist models. In Pro- ceedings IEEE International Conference on Acoustics, Speech and Signal Pro- cessing, volume 1: Speech Processing, pages 553-556, April 1988. [9] B. Widrow and S. D. Stearns. Adaptive Signal Processing. Prentice-Hall, NJ, 1985.","[-0.08866297453641891, -0.10901849716901779, 0.05439338833093643, 0.03469114005565643, 0.02175716869533062, -0.007409519981592894, -0.0011482039699330926, -0.12696561217308044, -0.00039291149005293846, -0.06392008811235428, 0.016685819253325462, -0.01064967643469572, 0.03204357251524925, -0.06153489649295807, -0.10148410499095917, 0.04985729977488518, 0.0456961989402771, 0.06716586649417877, 0.03583754226565361, 0.005641203839331865, -0.018985720351338387, -0.027066081762313843, -0.07706210017204285, 0.013820940628647804, 0.08498086780309677, 0.016161298379302025, -0.059031274169683456, -0.016238350421190262, -0.026301873847842216, 0.025339433923363686, 0.1358952671289444, -0.07047992199659348, 0.14198163151741028, 0.10239710658788681, -0.1306532323360443, -0.03516547381877899, -0.00376911205239594, 0.027231747284531593, 0.05892758071422577, 0.011228270828723907, -0.03347546607255936, -0.0065913125872612, -0.0665140151977539, -0.04253855347633362, 0.014058350585401058, -0.06262964010238647, -0.03463561460375786, -0.10539348423480988, 0.012958495877683163, -0.010679710656404495, 0.02300228737294674, 0.011541320942342281, -0.010283044539391994, 0.050739552825689316, 0.04216322302818298, 0.030903778970241547, -0.07001443207263947, 0.08145340532064438, -0.03618887811899185, 0.020049769431352615, -0.003922698553651571, -0.0020285467617213726, 0.044608063995838165, -0.03829339146614075, 0.03360619768500328, 0.04363332688808441, -0.009584834799170494, 0.06808508187532425, 0.05786217376589775, -0.029557131230831146, -0.004109050612896681, 0.07492782175540924, -0.002362919272854924, 0.013154399581253529, -0.007278453558683395, 0.031690455973148346, 0.021592039614915848, 0.024897435680031776, -0.01580641232430935, 0.03278111293911934, -0.03716583549976349, -0.04847557842731476, -0.02119048871099949, -0.05240278318524361, 0.06178176775574684, 0.04296546429395676, -0.05760573595762253, 0.01164579950273037, -0.0796307697892189, -0.07430603355169296, -0.07343956083059311, -0.013333724811673164, -0.04874315485358238, 0.01941673271358013, 0.04502818360924721, 0.00830911286175251, -0.03237094357609749, 0.014230464585125446, 0.06284454464912415, 0.06639797240495682, 0.023160094395279884, -0.025539176538586617, 0.031917452812194824, -0.0038364373613148928, -0.016097506508231163, 0.004000719636678696, 0.06499024480581284, -0.0005174702382646501, 0.06807848811149597, -0.0703204870223999, -0.010465729050338268, 0.04486234858632088, -0.025105204433202744, -0.01510386448353529, 0.0465301088988781, -0.0027538801077753305, 0.04281194880604744, -0.027218328788876534, 0.05771436169743538, -0.010310064069926739, -0.08670827001333237, -0.026380589231848717, -0.044610731303691864, 0.056765150278806686, 0.12105821818113327, 0.021764200180768967, -0.05274318531155586, 5.811076527099133e-33, -0.06076104938983917, 0.06522338092327118, -0.02199910208582878, -0.11754906922578812, -0.015101614408195019, 0.01189801562577486, 0.029811346903443336, -0.01793801598250866, 0.06668917089700699, 0.051893264055252075, -0.035013772547245026, 0.003222075756639242, -0.04381316527724266, -0.017072871327400208, 0.009841274470090866, -0.005166028626263142, 0.013448982499539852, -0.01672222651541233, 0.025890812277793884, -0.015562799759209156, -0.05147518590092659, -0.03664463758468628, 0.0013833706034347415, -0.03518019616603851, 0.02400793321430683, 0.013877687975764275, 0.021401721984148026, 0.023981768637895584, 0.009076387621462345, 0.0179479718208313, -0.007888508960604668, 0.1075974628329277, 0.005170954391360283, -0.04024544730782509, 0.014407754875719547, -0.05457313731312752, 0.00808365736156702, 0.05419367179274559, -0.005989282391965389, -0.029747148975729942, -0.014729594811797142, 0.025934195145964622, 0.03334684297442436, 0.012934721074998379, -0.010144350118935108, -0.1297835111618042, -0.02640940062701702, 0.09110529720783234, -0.0225943922996521, 0.001828487729653716, -0.01891351118683815, -0.049711424857378006, -0.04602721706032753, 0.004996717907488346, -0.04071030393242836, 0.018207572400569916, 0.018072713166475296, 0.0573529414832592, 0.06322769075632095, 0.10368922352790833, -0.07302918285131454, -0.04122009873390198, -0.029960384592413902, -0.017944656312465668, 0.04923945665359497, 0.04543672129511833, 0.01138988696038723, 0.04339621961116791, 0.035328082740306854, -0.05584743246436119, 0.010463525541126728, 0.036768123507499695, -0.07337183505296707, 0.014531882479786873, 0.05848517641425133, -0.061909399926662445, -0.022890198975801468, -0.031009089201688766, 0.011678501032292843, -0.013756928034126759, -0.03710002079606056, 0.020953847095370293, 0.057359371334314346, 0.01619253307580948, -0.08294562995433807, 0.1525035798549652, -0.003168449504300952, -0.031022770330309868, 0.004051272291690111, 0.03733601048588753, -0.03263618424534798, 0.01943245530128479, -0.023486262187361717, -0.016264894977211952, -0.029201839119195938, -3.1428002269905294e-33, 0.0525631420314312, 0.1319204866886139, -0.0704825296998024, 0.05090385675430298, -0.012698601931333542, 0.08561727404594421, -0.004508811514824629, -0.017487427219748497, 0.008587012067437172, -0.010767858475446701, 0.034191954880952835, -0.07784289866685867, 0.012843132950365543, 0.018039481714367867, 0.0030464986339211464, 0.030587555840611458, -0.06678122282028198, 0.005254726391285658, 0.056512653827667236, -0.0038567788433283567, 0.061805419623851776, -0.0016303316224366426, -0.002146681072190404, 0.04565194249153137, -0.06684403121471405, 0.008900110609829426, -0.07487968355417252, 0.12484796345233917, 0.04267746955156326, -0.03640719875693321, 0.005910125561058521, 0.03837187588214874, 0.024060433730483055, -0.04791269078850746, 0.042008571326732635, 0.03605327010154724, 0.022006064653396606, 0.05172264575958252, 0.049153272062540054, -0.04296077787876129, 0.02185976877808571, 0.06439529359340668, 0.01579873636364937, 0.004750349093228579, -0.03296744450926781, -0.03563821315765381, 0.019984500482678413, 0.0379972942173481, -0.024949485436081886, 0.06314535439014435, -0.04364011064171791, 0.024895109236240387, -0.04545240476727486, 0.027475401759147644, -0.011739388108253479, 0.07197242975234985, 0.007735139690339565, 0.07233668118715286, 0.059594713151454926, -0.018511846661567688, -0.06475228071212769, -0.00880555622279644, -0.02076825499534607, -0.0656251311302185, 0.05029674619436264, 0.023169374093413353, 0.01634109951555729, 0.1396096646785736, 0.03232942894101143, 0.0017319241305813193, -0.05674029514193535, 0.03792881593108177, 0.06186933442950249, 0.003009561914950609, 0.00237566651776433, 0.008294733241200447, -0.0774209201335907, -0.07281084358692169, -0.004381577949970961, 0.060500696301460266, -0.01569352112710476, -0.006374266464263201, -0.0382423959672451, 0.027219511568546295, 0.058306675404310226, 0.08661612868309021, 0.02188689261674881, -0.07334475964307785, -0.00859864056110382, -0.04860367625951767, 0.037509527057409286, 0.08556724339723587, -0.05973954126238823, -0.01882331818342209, -0.026797998696565628, -5.146806003608617e-08, -0.06347640603780746, 0.062394868582487106, -0.09541041404008865, -0.029187537729740143, 0.06822174042463303, -0.0929795354604721, 0.0727740079164505, 0.00958662386983633, -0.05785069987177849, -0.14481882750988007, 0.03975329175591469, -0.016664059832692146, -0.05335535481572151, -0.04328669235110283, 0.017863750457763672, -0.0557694286108017, -0.05615278333425522, -0.00030882455757819116, 0.01246373075991869, 0.024323489516973495, 0.04333406314253807, 0.0036506508477032185, -0.02430838905274868, -0.017076969146728516, 0.11066116392612457, -0.023591088131070137, -0.06729236245155334, 0.11614072322845459, -0.019600238651037216, -0.020419452339410782, 0.02892078086733818, 0.008710639551281929, 0.006028481759130955, -0.022994108498096466, 0.10961795598268509, 0.11093376576900482, 0.050278838723897934, -0.03723336383700371, -0.021330881863832474, 0.038817767053842545, -0.03971804305911064, 0.009260369464755058, 0.023281782865524292, -0.0338473841547966, -0.042239464819431305, -0.08681031316518784, 0.04014507308602333, -0.15808545053005219, 0.04527466744184494, -0.018089929595589638, 0.12652215361595154, 0.0037677634973078966, 0.03739084675908089, -0.041591886430978775, 0.03338983654975891, -0.06080436706542969, 0.026095038279891014, -0.10269339382648468, -0.041102949529886246, 0.12024460732936859, -0.04019319266080856, -0.020789775997400284, -0.03020959533751011, 0.021694811061024666]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ComputerModelingofAssociativeLearning.pdf,Deep Learning,"COMPARING BIASES FOR MINIMAL NETWORK CONSTRUCTION WITH BACK-PROPAGATION Lorien Y. Pratt Rutgers University Stephen Jo~ Hansont Bell Communications Research Morristown. New Jersey 07960 New Brunswick. New Jersey 08903 ABSTRACT Rumelhart (1987). has proposed a method for choosing minimal or ""simple"" representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units. (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart·s minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general. the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima. INTRODUCTION Many supervised connectionist models use gradient descent in error to solve various kinds of tasks (Rumelhart. Hinton & Williams. 1986). However. such gradient descent methods tend to be "".opportunistic"" and can solve problems in an arbitrary way dependent on starting point in weight space and peculiarities of the training set. For example. in Figure 1 we show a ""mesh"" problem which consists of a random distribution of exemplars from two categories. The spatial geometry of the categories impose a meshed or overlapping subset of the exemplars in the two dimensional feature space. As the meshed part of the categories increase the problem becomes more complex and must involve the combination of more linear cuts in feature space and consequently more nonlinear cuts for category separation. In the top left corner of Figure l(a). we show a mesh geometry requiring only three cuts for category separation. In the bottom center t Also member of Cognitive Science Laboratory, 221 Nassau Street, Princeton University, Princeton, New lersey,08S42 177 178 Hanson and Pratt I (b) is the projection of the three cut solution of the mesh in output space. In the top right of this Figure I(c) is a typical solution provided by back-propagation starting with 16 hidden units. This Figure shows the two dimensional featme space in which 9 of the lines cuts are projected (the other 7 are outside the [0.1] unit plane). ~ r-------------------------~ 0.0 0.2 U 0.' 0.8 '.0 Figure 1: Mesh problem (a). output space (b) and typical back-propagation solution (c) Examining the weights in the next layer of the network indicates that in fact. 7 of these 9 line segments are used in order to construct the output surface shown in Figure l(b). Consequently. the underlying feature relations determining the output surface and category separation are arbitrary. more complex then necessary and may result in anomalous generalizations. Rumelhart (1987). has proposed a way to increase the generalization capabilities of learning networks which use gradient descent methods and to automatically control the resources learning networks use-for example. in tenns of ""hidden"" units. His hypothesis concerns the nature of the 'representation in the network: "" ... the simplest most robust network which accounts/or a data set will, on awrage,lead to the best generalization to the population from which the training set has been drawn"". The basic approach involves adding penalty terms to the usual error function in order to constrain the search and cause weights to differentially decay. This is similar to many proposals in statistical regression where a ""simplicity"" measure is minimized along with the error term and is sometimes referred to as ""biased"" regression (Rawlings. 1988). Basically. the statistical concept of biased regression derives from parameter estimation approaches that attempt to achieve a best linear unbiased estimator (""BLUE""). By definition an unbiased estimator is one with the lowest possible variance and theoretically. unless there is significant collinearityl or nonlinearity amongst the 1. For example, Ridge regreuiolt is a special case of biased regression which attempts to make a singular correlation matrix non-lingular by adding a small arbitrary coostant to the diagonal of the matrix. This increase in the diagonal may lower the impact of the off-diagonal elements and thus reduce the effects of collinearity • Comparing Biases for Minimal Network Construction 179 variables. a least squares estimator (LSE) can be also shown to be a BLUE. If on the other hand. input variables are correlated or nonlinear with the output variables (as is the case in back-propagation) then there is no guarantee that the LSE will also be unbiased. Consequently. introducing a bias may actually reduce the variance of the estimator of that below the theoretically unbiased estimator. Since back-propagation is a special case of multivariate nonlinear regression methods we must immediately give up on achieving a BLUE. Worse ye4 the input variables are also very likely to be collinear in that input data are typically expected to be used for feature extraction. Consequently. the neural network framework leads naturally to the exploration of biased regression techniques. unfortunately. it is not obvious what sorts of biases ought to be introduced and whether they may be problem dependent Furthennore. the choice of particular biases probably determines the particular representation that is chosen and its nature in tenns of size. structure and ""simplicity"". This representation bias may in turn induce generalization behavior which is greater in accuracy with larger coverage over the domain. Nonetheless. since there is no particular motivation for minimizing a least squares estimator it is important to begin exploring possible biases that would lead to lower variance and more robust estimators. In this paper we explore two general type of bias which introduce explicit constraints on the hidden units. First we discuss the standard back-propagation method. various past methods of biasing which have been called ""weight decay"". the properties of our biases. and finally some simple benchmark tests using parity and a speech recognition task. BACK·PROPAGATION The Back-propagation method [2] is a supervised learning technique using a gradient descent in an error variable. The error is established by comparing an output value to a desired or expected value. These errors can be accumulated over the sample: E = LL (yu - ;ir)2 (1) • i Assuming the output function is differentiable then a gradient of the error can be found, and we require that this derivative be decreasing. dE --=0 dWij • (2) Over multiple layers we pass back a weighted sum of each derivative from units above. WEIGHT DECAY Past wo~ using biases have generally been based on ad hoc arguments that weights should differentially decay allowing large weights to persist and small weights to tend 180 Hanson and Pratt towards zero sooner. Apparently. this would tend to concentrate more of the input into a smaller number of weights. Generally. the intuitive notion is to somehow reduce the complexity of the network as defined by the nmnber of connections and number of hidden units. A simple but inefficient way of doing this is to include a weight decay tenn in the usual delta updating rule causing all weights to decay on each learning step (where W = Wjj throughout): (3) Solving this difference equation shows that for P < 1.0 weights are decaying exponentially over steps towards zero. "" . aE w"" = a 1: P""'"" (--)j + P"" Wo (4) ;=1 Ow This approach introduces the decay tenn in the derivative itself causing error tenns to also decrease over learning steps which may not be desirable. BIAS The sort of weight decay just discussed can also be derived from genezal consideration of ""costs"" on weights. For example it is possible to consider E with a bias tenn which in the simple decay case is quadratic with weight value (Le. w2). We now combine this bias with E producing an objective function that includes both the error term and this bias function: O=E+B (5) where. we now want to minimize ao = aE + aB (6) o~·· ow·· ow·· 'I 'I 'I In the quadratic case the updating rule becomes. aE W,,+1 = a (- :\.... - 2w,,) + w"" (7) ClW;J Solving this difference equation derives the updating rule from equation 4. "" . oE w"" = a I:(1-2a)""""'(- Ow ); + (l-2a)""wo (8) lal ij In this case. however without introduction of other parameters. a is both the learning rate 2. MOlt tX the wort discussed here has not been previously publilhed but nonetheless has entered into general use in many cormectionisl models and wu recently summarized on the COlIMctionist Bw/~tin Board by John Kruschke. Comparing Biases for Minimal Network Construction 181 and related to the decay tenn and must be strictly < ~ for weight decay. Unifonn weight decay has a disadvantage in that large weights are decaying at the same rate as small weights. It is possible to design biases that influence weights only when they are relatively small or even in a particular range of values. For example. Rumelhart has entertained a number of biases. one fonn in particular that we will also explore is based on a rectangular hyperbolic function. w1 B:: (1+w2) (9) It is infonnative to examine the derivative associated with this function in order to understand its effect on the weight updates. dB 2w - dwij ::- (1+w2)1 (10) This derivative is plotted in Figure 2 (indicated as Rumelhart) and is non-monotonic showing a strong differential effect on small weights (+ or -) pushing them towards zero. while near zero and large weight values are not significantly affected. BIAS PER UNIT It is possible to consider bias on each hidden unit weight group. This has the potentially desirable effect of isolating weight changes to hidden unit weight groups and could effectively eliminate hidden units. Consequently. the hidden units are directly determining the bias. In order to do this. first define w·::~lw··1 I If..I 'I' (11) j where i is the ith hidden unit. Hyperbolic Bias Now consider a function similar to Rumelhart's but this time with Wi, the ith hidden group as the variable. W· B- ' - 1 +AWi· The new gradient includes the term from the bias which is. aB Asgn(wij) - dWij = (1 +Wi)2 Exponential Bias A similar kind of bias would be to consider the negative exponential: (12) (13) 182 Hanson and Pratt (14) This bias is similar to the hyperbolic bias tenn as above but involves the exponential which potentially produce more unifonn and gradual rate changes towards zero, dB sgn(wij) --= (15) dWij ( e ~Wi) . The behavior of these two biases (hyperbolic, exponential) are shown as function of weight magnitudes in Figure 2. Notice that the exponential bias term is more similar in slope change to RumelharCs (even though his is non-monotonic) than the hyperbolic as weight magnitude to a hidden unit increases. q - II) i d - 0 ~ 0 'iI d 'i 'tJ an 9 q -. -3 -2 -1 o weightvalu8 1 2 3 Figure 2: Bias function behavior of Rumelharfs, Hyperbolic and Exponential Obviously there are many more kinds of bias that one can consider. These two were chosen in order to provide a systematic test of varying biases and exploring their differential effectiveness in minimizing network complexity. SOME COMPARISONS Parity These biased Back-propagation methods were applied to several counting problems and to a speech (digit) recognition problem. In the following graphs for example, we show the results of 100 runs of XOR and 4-bit parity at 11 =.1 (learning rate) and ex=.8 (moving average) starting with 10 hidden units. The parameter A. was optimized for the bias runs. Comparing Biases for Minimal Network Construction 183 II I , I: ......... ., .... ---...... oJ~~~~~~~=-~. • • \I 12 _._- J: J: ...------- -.- --- a 4 12 _.-- ~------ r- ....... - ---- r-- r- r- t-- - ~ , , , , , . • I •• .'1 _._- Figure 3: Exclusive OR runs for standard, hyperbolic and exponential biasing Shown are runs for the standard case without biases, the hyperbolic bias and the exponential bias. Once a solution was reached all hidden Wlits were tested individually by removing each of them one at a time from the network and then testing on the training set Any hidden unit which was unnecessary was removed for data analysis. Only the number of these ""functional units"" are reported in the histograms. Notice the number of hidden units decrease with bias runs. An analysis of variance (statistical test) verified this improvement for both the hyperbolic and exponential over the standard. Also note that the exponential is significandy better than the hyperbolic. This is also confinned for the 4-bit parity case as shown in Figure 4. - -_ .... _- ... --..... -- I' • I- I' -_. __ .... _- ........ ------ . . --- ..........-..-.. _- . , -_. Figure 4: four-bit parity runs for standard. hyperbolic and exponential biasing 184 Hanson and Pratt Speech Recognition Samples of 10 spoken digits (0-9) each were collected (same speaker throughout--DJ. BUlT kindly supplied data). Samples were then preprocessed using FFTs retaining the first 12 Cepstral coefficients. To avoid ceiling effects only two tokens each of the 10 digits were used for training (""0"", ""0"",""1"",""1"", .... ""9"",.,9., .. ) each network. Eight such 2 token samples were used for replications. Another set of 50 spoken digits (5 samples of each of the 10 digits) were collected for transfer. All runs were matched across methods for number ofleaming sweeps «300),11=.05, a=.2, and A=.01 which were optimized for the exponential bias. Shown in the following table is the results of the 8 replications for the standard and the exponential bias. doll COIII&I'IiIlecl{up 1 IIIIIDIe 1'rInIrer , HIdden Units TrlDafer 'HWcnUnill rl 5K II 64~ 10 r2 6K 11 76~ 13 r3 62~ 18 64"" 14 1'4 6A 14 74"" 14 d 62~ 16 56 .. 11 16 c569& 19 68 .. 14 t7 58"" 18 54"" 11 IS sa"" 18 64 .. 9 17%.56 65~ 12.%.71 Table 1: Eight replications with transfer for standard and exponential bias. In this case there appears to both an improvement in the average number of hidden units (functional ones) and transfer. A typical correlation of the improved transfer and reduced hidden unit usage for a single replication is plotted in the next graph. J! ~ y- -1.21+ 71.7. ,- -.trT l ~ I 2 I :I i I/) or ~ 10 12 14 18 11 I1IMnber of hidden unIIs Figure 5: Transfer as a function of hidden unit usage for a single replication We note that introduction of biases decrease the probability of convergence relative to the standard case (as many as 75% of the parity runs did not converge within criteria Comparing Biases for Minimal Network Construction 185 number of sweeps.) Since the search problem is made more difficult by introducing biases it now becomes even more important to explore methods for improving convergence similar for example. to simulated annealing (Kirkpatrick. Gelatt & Vecchi. 1983) CONCLUSIONS Minimal networks were defined and two types of bias were compared in a simple counting problem and a speech recognition problem. In the counting problems under biasing conditions the number hidden units tended to decrease towards the minimum required for the problem although with a concomitant decrease in convergence rate. In the speech problem also under biasing conditions the number of hidden units tended to decrease as the transfer rate tended to improve. Acknowledgements We thank Dave Rumelhart for discussions concerning the minimal network concept. the Bellcore connectionist group and members of the Princeton Cognitive Science Lab for a lively environment for the development of these ideas. References Kirkpalrick. S .• Gelatt. C. D .• & Vecchi. M. P .• Optimization by simulated annealing. Science. 220. 671-680. (1983). Rawlings. I. 0 •• Applied Regression Analysis. Wadsworth & Brooks/Cole, (1988). Rumelhart D. E .• Personal Communication, Princeton. (1987). Rumelhart D. E., Hinton G. E .• & Williams R .• Learning Internal Representations by error propagation. Nature. (1986).","[-0.018934477120637894, -0.07891710847616196, 0.016219688579440117, 0.02595979906618595, -0.012140593491494656, 0.08609502017498016, 0.03700637444853783, -0.03938480466604233, 0.018932152539491653, -0.08004513382911682, -0.040448371320962906, 0.025581911206245422, 0.09079334884881973, 0.044802989810705185, -0.08391445875167847, -0.011280246078968048, 0.08592335134744644, 0.09570050239562988, -0.06344014406204224, -0.0491609163582325, -0.0014177963603287935, 0.030958382412791252, -0.0017082851845771074, 0.02242429181933403, 0.06850578635931015, -0.03707936033606529, -0.02091226913034916, -0.02440371736884117, 0.03550367429852486, 0.032520025968551636, 0.02235995978116989, -0.031750183552503586, 0.0667072981595993, 0.0010639629326760769, -0.08062201738357544, 0.01709405891597271, -0.06389184296131134, 0.08632203191518784, -0.04162384197115898, 0.03617488592863083, -0.02527775801718235, 0.03205958008766174, 0.023001646623015404, 0.03673223406076431, 0.04293942451477051, -0.05111295357346535, 0.025798514485359192, -0.007750910706818104, -0.032128263264894485, -0.01837056688964367, 0.009990205988287926, 0.04959999769926071, -0.07437332719564438, 0.06242883577942848, -0.04209231585264206, -0.028508378192782402, -0.01302733737975359, 0.05697795748710632, -0.08152955770492554, 0.06802255660295486, 0.03923918306827545, -0.13904397189617157, -0.0661255493760109, -0.06001141294836998, -0.0017017761711031199, -0.015401345677673817, -0.016151009127497673, -0.0024268662091344595, -0.01865815557539463, 0.028762340545654297, 0.05854985490441322, 0.07223058491945267, -0.08502882719039917, 0.07424743473529816, 0.07601743936538696, 0.06016892194747925, 0.1470794826745987, -0.013020279817283154, 0.0445292629301548, -0.017204351723194122, 0.041786350309848785, -0.020416444167494774, 0.037669505923986435, -0.0005607765051536262, 0.11226781457662582, -0.03687185049057007, -0.08125194162130356, 0.027668597176671028, 0.004189277067780495, -0.057214681059122086, -0.013173679821193218, -0.03542834147810936, -0.06301283836364746, 0.03236674517393112, 0.04684381186962128, 0.0010883668437600136, -0.017543088644742966, -0.003856463823467493, -0.03979019820690155, 0.10536409169435501, -0.06526089459657669, -0.006252356804907322, 0.09367212653160095, -0.05226846784353256, 0.036006685346364975, 0.037640415132045746, -0.0007896373281255364, 0.02489611878991127, 0.07065393030643463, -0.10122409462928772, -0.027840374037623405, -0.0036632122937589884, -0.028674688190221786, 0.029751768335700035, 0.03970766067504883, -0.08467324078083038, 0.07387156039476395, -0.04595186188817024, -0.021255506202578545, 0.05666051432490349, -0.032723940908908844, 0.034843653440475464, 0.021562300622463226, -0.016614219173789024, 0.047667115926742554, 0.0031008950900286436, -0.013521479442715645, 4.1893013378833485e-33, 0.024976426735520363, 0.06315746903419495, 0.028429191559553146, -0.013695967383682728, 0.08544615656137466, -0.0043695722706615925, 0.007441970519721508, 0.001692265155725181, 0.059418585151433945, 0.0038720848970115185, -0.07828836143016815, -0.020187288522720337, -0.01640257053077221, 0.022784600034356117, 0.09651609510183334, -0.02079843170940876, -0.00011724367504939437, 0.05208813399076462, 0.0010477608302608132, -0.1259440779685974, -0.0076658171601593494, -0.04087551683187485, 0.0456811897456646, -0.05685943737626076, 0.013608070090413094, -0.0584886372089386, 0.09298919141292572, -0.06857728958129883, -0.05442372336983681, 0.002694458933547139, -0.08561238646507263, -0.06514004617929459, 0.040558815002441406, -0.012084337882697582, 0.06439736485481262, 0.003042582655325532, 0.017304856330156326, -0.0076754130423069, 0.0378822423517704, -0.08175124228000641, -0.0019326696638017893, 0.04428858682513237, 0.03478626906871796, 0.006384952459484339, -0.046470798552036285, -0.06499508768320084, 0.025232568383216858, 0.04256986081600189, 0.004347273614257574, 0.006663595791906118, -0.0245286263525486, 0.02652190439403057, -0.03558491915464401, -0.04418586939573288, 0.045287810266017914, -0.010930981487035751, 0.034363068640232086, 0.0799340158700943, 0.061315301805734634, 0.037397582083940506, 0.03668889403343201, 0.029217569157481194, -0.007861866615712643, 0.07641717046499252, 0.05035465955734253, 0.03821262717247009, -0.11419636011123657, -0.017043564468622208, 0.06466161459684372, -0.03039807640016079, 0.022655341774225235, 0.021157609298825264, -0.03649245575070381, -0.07476256787776947, 0.06970018148422241, 0.07834126055240631, 0.02751452475786209, -0.07446014881134033, -0.026859384030103683, 0.009481019340455532, -0.07499559223651886, 0.026863055303692818, -0.08074205368757248, 0.0073189022950828075, -0.08310867846012115, 0.014630163088440895, 0.06909524649381638, -0.07441718131303787, 0.052402667701244354, 0.02680613100528717, -0.04175674170255661, -0.007861204445362091, -0.065945565700531, 0.039082542061805725, 0.022963227704167366, -3.147915831468514e-33, -0.12269292771816254, 0.1201009601354599, -0.004792403429746628, -0.018696116283535957, -0.02167375385761261, -0.005390307866036892, 0.024173704907298088, 0.01959984377026558, -0.008455888368189335, 0.019273022189736366, -0.027218090370297432, -0.031608905643224716, 0.019321199506521225, -0.015221705660223961, -0.0075888303108513355, 0.028383802622556686, -0.02720518968999386, 0.028396068140864372, 0.034402262419462204, 0.022335436195135117, 0.09106560051441193, 0.10303474217653275, -0.18466579914093018, 0.039013057947158813, -0.027586163952946663, -0.021193059161305428, -0.01212973054498434, 0.03656061738729477, -0.022734031081199646, -0.004475824069231749, -0.03661326318979263, -0.057166751474142075, -0.04840567708015442, 0.00351602747105062, -0.06855517625808716, 0.07602334767580032, 0.008501756936311722, 0.032169654965400696, -0.03690417483448982, 0.039966702461242676, 0.010884772054851055, -0.024901321157813072, -0.05267815291881561, -0.05719836801290512, 0.01453894842416048, -0.07598312944173813, -0.06403997540473938, -0.037126149982213974, -0.02493469975888729, 0.004472735337913036, 0.01039034128189087, -0.002342991763725877, -0.04160325229167938, 0.06624345481395721, -0.0729190930724144, -0.0008034812053665519, 0.03621591627597809, 0.0528736338019371, 0.10735781490802765, -0.018170014023780823, -0.06559394299983978, -0.09969394654035568, -0.03351638838648796, -0.031623583287000656, -0.00769448559731245, -0.0030957828275859356, 0.009358597919344902, 0.023042991757392883, 0.051707372069358826, 0.015607970766723156, -0.044350359588861465, 0.03653815761208534, 0.05825747549533844, 0.02787765860557556, -0.045769013464450836, 0.039147134870290756, -0.046848494559526443, -0.012358802370727062, -0.08197581768035889, 0.0011372979497537017, 0.009158255532383919, -0.020706087350845337, 0.04385547339916229, 0.008189779706299305, 0.1068774163722992, 0.04763052240014076, 0.08429095149040222, 0.03498898074030876, 0.023076027631759644, -0.014868369325995445, -0.037219490855932236, 0.057097721844911575, 0.038268353790044785, -0.021291716024279594, -0.0277247354388237, -5.0749861202348256e-08, -0.18338359892368317, 0.012745905667543411, -0.029034869745373726, -0.013960499316453934, 0.06956975162029266, -0.058531101793050766, -0.004885384347289801, 0.03156852349638939, -0.03609378635883331, -0.01612953469157219, 0.05322917178273201, 0.015102226287126541, -0.028102947399020195, 0.03296076878905296, -0.04967190697789192, 0.029173316434025764, -0.009604689665138721, -0.017251670360565186, 0.01441141963005066, -0.019223053008317947, 0.015647470951080322, 0.0372079499065876, -0.00827040895819664, 0.08733007311820984, 0.06843190640211105, -0.09201174229383469, -0.014226884581148624, 0.07644909620285034, 0.033631615340709686, 0.011171916499733925, -0.017486562952399254, 0.0768875703215599, -0.008210343308746815, 0.016944242641329765, -0.009559042751789093, 0.09845808148384094, -0.02046995982527733, -0.00552216824144125, -0.1030622273683548, 0.045121192932128906, 0.005522033199667931, -0.006416154094040394, -0.022396104410290718, 0.009197156876325607, 0.0809023305773735, -0.013465306721627712, -0.03517734631896019, -0.06230355426669121, -0.04281356930732727, -0.11537947505712509, 0.07635045051574707, -0.052707966417074203, -0.015513534657657146, -0.035217754542827606, 0.07026004046201706, -0.019011471420526505, 0.018137721344828606, -0.045163583010435104, -0.0399068146944046, 0.06831493228673935, -0.04113723337650299, 0.09320171922445297, -0.08005543053150177, -0.054049234837293625]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ConnectionistLearningofExpertPreferencesbyComparisonTraining.pdf,Deep Learning,"264 NEURAL APPROACH FOR TV IMAGE COMPRESSION USING A HOPFIELD TYPE NETWORK Martine NAILLON Jean-Bernard THEETEN Laboratoire d'Electronique et de Physique Appliquee * 3 Avenue DESCARTES, BP 15 94451 LIMEIL BREVANNES Cedex FRANCE. ABSTRACT A self-organizing Hopfield network has been developed in the context of Vector Ouantiza- -tion, aiming at compression of television images. The metastable states of the spin glass-like network are used as an extra storage resource using the Minimal Overlap learning rule (Krauth and Mezard 1987) to optimize the organization of the attractors. The sel f-organi zi ng scheme that we have devised results in the generation of an adaptive codebook for any qiven TV image. I NTRODOCTI ON The ability of an Hopfield network (Little,1974; Hopfield, 1982,1986; Amit. and al., 1987; Personnaz and al. 1985; Hertz, 1988) to behave as an associative memory usua 11 y aSSlJ11es a pri ori knowl edge of the patterns to be stored. As in many applications they are unknown, the aim of this work is to develop a network capable to learn how to select its attractors. TV image compression using Vector Quantization (V.Q.)(Gray, 1984), a key issue for HOTV transmission, is a typical case, since the non neural algorithms which generate the list of codes (the codebookl are suboptimal. As an alternative to the prani si ng neural canpressi on techni ques (Jackel et al., 1987; Kohonen, 1988; Grossberg, 1987; Cottrel et al., 19B7) our idea is to use the metastability in a spin glass-like net as an additional storage resource and to derive after a ""cl assi cal II cl usteri nq a 1 gori thm a sel f-organi zi ng sheme for generatf ng adaptively the codebook. We present the illustrative case of 2D-vectors. * LEP : A member of the Philips Research Organization. Neural Approach for TV Image Compression 265 NON NEURAL APPROACH In V.O., the image is divided into blocks, named vectors, of N pixels (typically 4 x 4 pixels). Given the codebook, each vector is coded by associating it with the nearest element of the list (Nearest Neighbour Classifier) ( fi g ure 1). INPUT YEtTa"" EMCaD£"" COP1PARE CODE BOOK INDEX ~ ~ ftECDN- INDEX CODEBOOK STRUCTED VECTOR Figure 1 : Basic scheme of a vector quantizer. For designing an optimal codebook, a clustering algorithm is app1 ied to a training set of vectors (figure 2), the criterium of optimality being a distorsion measure between the training set and the codebook. The algorithm is actua 11 y subopt ima1, especi a 11 y for non connex training set, as it is based on an iterative computation of centers of grav i ty whi ch tends to overcode the dense regions of poi nts whereas the 1 ight ones are undercoded (figure 2). - - ----------------- ~r_---------~~-~---~ a 1::- .. .. a+--____ ~-------~--------~ 110.0 230.0 PIXEl.. 1 . . ---.------- . . -- --- - -- - - -- - . -- - --- --- Figure 2 : Training set of two pixels vectors and the associated codebook canputed by a non neural c1 ustering algorithm: overcoding of the dense regions (pixel 1 148) and subcoding of the light ones. 266 Naillon and Theeten NEURAL APPROACH In a Hopfield neural network, the code vectors are the attractors of the net and the neural dynamics (resolution phase) is substituted to the nearest neighbourg classification. ~en patterns - referred to as II prototypes"" and named here ""explicit memory"" are prescribed in a spin glass-like net, other attractors referred to as ""metastable states"" - are induced in the net (Sherrington and Kirkpatrick, 1975; Toulouse, 1977; Hopfield, 1982; Mezard and al., 1984). We consider those induced attractors as additional memory named here ""impl icit memory"" whi ch can be used by the network to code the previously mentioned light regions of points. This provides a higher flexibility to the net during the self-organization process, as it can choose in a large basis of explicit and implicit attractors the ones which will optimize the coding task. NEURAL NOTATION A vector of 2 pixels with 8 bits per pel is a vector of 2 dimensions in an Eucl idean space where each dimension corresponds to 256 grey levels. To preserve the Euclidean di stance, we use the well-known themometri c notati on : 256 neurons for 256 level s per dimens i on, the number of neurons set to one, wi th a reg ul ar orderi ng, g iv i ng the pixel luminance, e.g. 2 = 1 1-1-1-1-1 ••• For vectors of dimension 2, 512 neurons will be used, e.g. v=(2,3)= (1 1-1-1 •••••• -1,1 1 1-1-1 ••• ,-1) INDUCTION PROCESS The induced impl icit memory depends on the prescription rule. We have compared the Projection rule (Personnaz and al., 1985) and the Minimal Overlap rule (Krauth and Mezard, 1987). The metastable states are detected by relaxing any point of the training set of the figure 2, to its corresponding prescribe or induced attractor marked in figure 3 with a small diamond. For the two rules, the induction process is rather detenni ni stic, generati ng an orthogonal mesh : if two prototypes (P11,P12) and (P21,P22) are prescribed, a metastable state is induced at the cross-points, namely (P11,P22) and (P21,P12) (figure 3). Neural Approach for TV Image Compression 267 a+-__ ~ ____ ~ __ ~ ar---~----~--~ ... -. .. ..... .. ... ... PDIB. 1 PDCEI. 1 Figure 3 : Comparaison of the induction process for 2 prescription rules. The prescribed states are the full squares, the induced states the open diamonds. What differs between the two rul es ; s the number of induced attractors. For 50 prototypes and a training set of 2000 2d-vectors, the projection rule induces about 1000 metastable states (ratio 1000/50 = 20) whereas Min Over induces only 234 (ratio 4.6). This is due to the different stabil ity of the prescribed and the induced states in the case of Min Over (Naillon and Theeten, to be published). GENERALIZED ATTRACTORS Some attractors are induced out of the image space (Figure 4) as the 512 neurons space has 2512 configurations to be compared with the (28 )2= 216 image configurati ons. We extend the image space by defi n1 ng a ""genera 1 i ze d attractor"" as the class of patterns having the same number of neurons set to one for each pixel, whatever thei r orderi ng. Such a notati on corresponds to a random thermometri c neural representati on. The simul ati on has shown that the generalized attractors correspond to acceptable states (Figure 4) i.e. they are located at the place when one would like to obtain a normal attractor. 268 N siIlon and Theeten i NO GENERAUZATION WITI-I GENERAUZATJON i i· ~;m""""ING~ r' ~ , ~ CII!JeMIJZm AJ'TAACT'OR wrTHOVT AT / .... 1'-.(f;-6 ,~ --< ¥. ~ ~ \l'J!'ft 0-\ -,' .A '1ft 6.. .. [ '& i ~ ~!~~ . '~~6 i ~6 ... "",b. ..._ +6A-+ I't hjlt ~ t "" ~~~r.~ lf~ .. ~~ ~ 6+6 ~ I' .... ... ,J::.,..,.- ~ \ l!Jl!. -~ ..... ,,' - '- l!.4--6 \---J. ... 6""'6.-6 ~. -(·1.f~ll1"" '. -.,... 'fl •• f~· . 1 i ~~/ ~ .. ;;~ (J.,"". - j.J ~. • ~ ~J -'-t~~i &~ ~ f. t I ~ & -. ~, ,)t ~~'~ ~5\ :- 4- • ; .... ... .. .. -- .... .. .. .. .. to PIXEL! PIXEL ! Figure 4 : The induced bassins of attractions are represented with arrows. In the left plot, some training vectors have no attractor in the image space. After generalization (randon thermometric notation), the right ~ot shows their corresponding attractors. ADAPTIVE NEURAL CODEBOOK LEARNING An iterative sel f- organi zi ng process has been developed to optimi ze the codebook. For a given TV image, the codebook is defined, at each step of the process, as the set of prescribed and induced attractors, selected by the training set of vectors. The self-organizing scheme is controlled by a cost function, the distorsion measure between the training set and the codebook. Having a target of 50 code vectors, we have to prescri be at each step, as discussed above, typically 50/4.6 = 11 prototypes. As seen in figure Sa, we choose 11 initial prototypes uniformly distributed along the bisecting line. Using the training set of vectors of the figure 2, the induced metastable states are detected with their corresponding bassins of attraction. The 11 most frequent, prescribed or induced, attractors are selected and the 11 centers of gravi ty of thei r bassi ns of attracti on are taken as new prototypes (figure 5b ). After 3 iterations, the distorsion measure stabilizes (Table 1). Neural Approach for TV Image Compression 269 INmALIZATION n: • • i • • • "" • "" ~ • ~ • • s- • s • • i ..... - --- .... ... -- .... Fi gure 5a i i "" ~ • ! 8 , I , , i Figure 5b scheme. PlXB.1 PIXEl. 1 Initialization of the self-organizing scheme. ITERATION ·1 PROTOTYPES FAST ORGANIZATION • • • • • N • ~ •• • •• • · · First iteration of the self-organizinq 1001 itrrllioM 1 2 3 4 5 «Iobal codebook «eneralized dislofsion size aUraclors 1571 53 ! 0 1031 57 4 97 79 i 20 97 84 I 20 98 . 68 i 15 Table 1 : Evolution of the distorsion measure versus the iterations of the self-organizing scheme. It stabilizes in 3 iterations. 270 NOOllon and Theeten Fourty 1 i nes of a TV image (the port of Ba 1 timore) of 8 bits per pel, has been coded with an adaptive neural codebook of 50 20-vectors. The coherence of the coding is visible from the apparent continuity of the image (Figure 6). The coded image has 2.5 bits per pel. I - j • 1 Figure 6 : Neural coded image with 2.5 bits per pel. CONCLUSION Using a ""classical"" clusterinq algorithm, a self-organizing scheme has been developed in a Hopfield network f.or the adaptive design of a codebook of small d imensi on vectors ina Vector Quanti zati on techni Que. It has been shown that using the Minimal Overlap prescription rule, the metastable states induced in a spin gl ass-like network can be used as extra-codes. The optimal organization of the prescribed and induced attractors, has been defined as the limit organization obtained from the iterative learning process. It is an example of ""learning by selection"" as already proposed by physicists and biologists (Toulouse and ale 1986). Hard~re impl ementation on the neural VLSI ci rcuit curren~y designed at LEP should allow for on-line codebook computations. We woul d like to thank J.J. Hopfield who has inspired this study as well H. Bosma and W. Kreuwel s from Phil ips Research Laboratories, Eindhoven, who have allow to initialize this research. Neural Approach for TV Image Compression 271 REFERENCES 1 - J.J. Hopfield, Proc. Nat. Acad. Sci. USA, 79, 2554 - 2558 (1982); J.J. Hopfield and D.W. Tank, SC1ence 233 , 625 (1986) ; W.A. Little, Math. Biosi.,..!2., 101-120 :-T1974). 2 - D.J. ftrnit, H. Gutfreund, and H. Sanpolinslc.y, Phys.Rev. 32, Ann. Phys. 173, 30 (1987). - 3 - L. Personnaz, I. Guyon and G. Dreyfus, J. Phys. Lett. 46, L359 (1985). 4 - J.A. Hertz, 2nd International Conference on ""Vector and pa ra 11 e 1 canputi ng, Transo, Norway, June (1988). 5 - M.A. Virasoro, Disorder Systems and Biological Organization, ed. E. Bienenstoclc., Springer, Berlin (1985); H. Gutfreund (Racah Institute of Physics, Jerusalem) (1986); C. Cortes, A. Kro<;lh and J .A. Hertz, J. of Phys. A., (1986). 6 - R .M. Gray, IEEE ASSP Magazi ne 5 (Apr. 1984). 7 - L.D. Jackel, R.E. Howard, J.S. Denker, W. Hubbard and S.A. ~ol1a, ADpl ied Ootics, Vol. 26, Q, (1987). 8 - i. Kononen, Finland, Helsinky University of Technology, Tech. ~eo. No. iKK..;:""·A601; T. Kahanen, ~Jeural Networks, 1, ~jumoer :, (1988). - 9 - S. Grossoerg, Cognitive ScL,.!.!., 23-63 (1987). 10 - G.W. Cottrell, P. Murro and D.Z. Zioser, Institute of cognitive Science, Report 8702 (1987). 11 - D. Sherrington and S. Kirkpatrick, Phys. Rev. Lett. 35 t 1792 (1975); G. Toulouse, Commun. Phys. 2, 115-119 (lID); M. Mezard , G. Parisi, N. Sourlas , G. Toulouse and M. Virasoro, Phys. Dey. Lett., g, 1156-1159 (1984). 12 - W. Krauth and M. Mezard t J. Phys.A : Math. Gen. 20, L 745-L 752 (1987) 13 - M. ~Jaillon and J.B. Theeten, to be published. 14 - G. Toulouse, S. Dehaene and J.P. Changeux, Pro. Natl.Acad. Sci. USA,~, 1695, (1986).","[-0.08319021016359329, 0.0038389109540730715, -0.06928248703479767, -0.07867950201034546, -0.029932593926787376, 0.018339110538363457, 0.008700713515281677, -0.09300137311220169, -0.003120329463854432, -0.028795816004276276, -0.016614172607660294, 0.08589550852775574, 0.05667176470160484, -0.04435494542121887, -0.07031629234552383, -0.05357731133699417, -0.033214934170246124, 0.06210533156991005, -0.09667153656482697, -0.0403965525329113, 0.01649758405983448, -0.09202565252780914, 0.004360973834991455, -0.03807152807712555, 0.03670647740364075, 0.06023477762937546, -0.013761702924966812, 0.06248742714524269, 0.05149436369538307, -0.08622433245182037, 0.13898468017578125, 0.06334373354911804, 0.06019199267029762, 0.04954586923122406, -0.07696297019720078, 0.004542204551398754, -0.08606293052434921, 0.03808595612645149, -0.04436779022216797, 0.04770209267735481, 0.0218781977891922, 0.05672728642821312, -0.03414157032966614, -0.03239522874355316, 0.004603524226695299, 0.048860616981983185, 0.0022651096805930138, -0.023449474945664406, -0.025741806253790855, -0.06942161172628403, -0.09736848622560501, 0.09141416102647781, -0.0774986669421196, 0.06878913193941116, 0.045289140194654465, 0.017796354368329048, 0.008717654272913933, 0.055128589272499084, -0.05535140633583069, 0.018541600555181503, 0.06060842052102089, -0.05045609548687935, -0.04060426354408264, 0.07192137837409973, 0.017296314239501953, 0.036537062376737595, 0.04321734234690666, 0.06094986945390701, 0.03632758930325508, -0.0942072868347168, 0.008221362717449665, 0.023891042917966843, -0.026532955467700958, 0.04961853846907616, 0.03558802604675293, 0.0960797443985939, 0.09106261283159256, 0.01771477237343788, 0.0420929454267025, -0.05305405706167221, -0.02906152419745922, -0.03477779030799866, -0.010221556760370731, 0.022728629410266876, 0.02131560444831848, -0.03120010532438755, -0.05686682090163231, 0.040971871465444565, -0.017132965847849846, -0.01655029132962227, -0.029377002269029617, 0.037861090153455734, -0.00835504662245512, -0.07829438149929047, 0.09882903844118118, 0.013190897181630135, 0.07779602706432343, -0.060097917914390564, -0.017763596028089523, 0.0697585716843605, 0.03833393007516861, -0.06856322288513184, 0.030345918610692024, 0.011313436552882195, 0.04584197327494621, -0.047637004405260086, 0.09412721544504166, 0.0707237496972084, 0.04022195190191269, -0.04477081075310707, -0.07338223606348038, 0.058334629982709885, -0.0469534695148468, -0.057071976363658905, -0.01613243669271469, -0.03565359488129616, -0.03118347004055977, 0.036050453782081604, 0.05397342890501022, -0.026776321232318878, 0.02226189523935318, -0.06075355410575867, -0.07524967193603516, -0.01880629174411297, 0.0034132846631109715, -0.02497043088078499, -0.12421785295009613, 5.582747769711443e-33, -0.034458354115486145, 0.0642242506146431, -0.07468464970588684, -0.04656394571065903, 0.04742006957530975, 0.1281125843524933, -0.011778448708355427, 0.038186874240636826, 0.09368535876274109, 0.03539767488837242, -0.08133634179830551, 0.02707853354513645, -0.044561319053173065, 0.08072948455810547, 0.03166944161057472, -0.047430746257305145, -0.03091120347380638, -0.019281256943941116, -0.04367329180240631, -0.047448329627513885, 0.09186508506536484, -0.04030543193221092, -0.024200424551963806, -0.04778023809194565, -0.05695991963148117, 0.00593504449352622, 0.04129999503493309, -0.04737163707613945, -0.03268657997250557, 0.008050255477428436, 0.03340678662061691, 0.017104875296354294, -0.017709463834762573, -0.04206168279051781, 0.045038219541311264, -0.0507793053984642, 0.008886863477528095, -0.023691188544034958, 0.06187503784894943, -0.03869197890162468, 0.03299352899193764, 0.042738430202007294, 0.005108861718326807, 0.005869696848094463, -0.07992023229598999, -0.02005394548177719, 0.07639719545841217, 0.04870636388659477, -0.05591893196105957, -0.004441509954631329, 0.03378274291753769, -0.049262236803770065, -0.006056366488337517, -0.08237390220165253, 0.06767251342535019, 0.00011615896801231429, 0.03205711767077446, 0.015480387955904007, 0.08321098983287811, 0.06555821746587753, 0.012075896374881268, 0.06989935785531998, 0.04512025788426399, 0.09881149232387543, 0.05460617318749428, 0.009532005526125431, 0.03517327457666397, -0.019073765724897385, 0.005596495699137449, -0.024384858086705208, 0.0243422519415617, 0.09527997672557831, -0.03700966387987137, -0.07174895703792572, 0.05758814886212349, -0.0055479914881289005, -0.01845475286245346, -0.011037482880055904, -0.0733519196510315, -0.057282447814941406, -0.07546348124742508, -0.01455511711537838, -0.012989814393222332, -0.054128337651491165, -0.07748044282197952, -0.002384175080806017, 0.05420204624533653, -0.04251523315906525, -0.04600945860147476, -0.03376041725277901, 0.010391879826784134, -0.0465908944606781, 0.07419466227293015, 0.00902820099145174, -0.007077991031110287, -5.0052766189783935e-33, 0.025901010259985924, 0.053588103502988815, -0.03342452645301819, -0.02486114390194416, 0.019134612753987312, -0.02453765831887722, 0.035768695175647736, -0.0019641919061541557, -0.02576839178800583, -0.0688377320766449, -0.028012501075863838, 0.01346983015537262, 0.04353353753685951, -0.039049819111824036, -0.008526910096406937, 0.014730545692145824, -0.030193069949746132, -0.09900324791669846, 0.020939752459526062, -0.04490452632308006, -0.06975068151950836, 0.03061780147254467, -0.09146840125322342, 0.030496925115585327, 0.004983118735253811, -0.01394279208034277, 0.03099132888019085, 0.049348942935466766, -0.0006185885285958648, 0.0691373199224472, -0.003738105297088623, -0.08907350897789001, -0.06509926170110703, -0.0007660117116756737, 0.028066018596291542, 0.08489292114973068, 0.06573457270860672, 0.00471432926133275, -0.07063521444797516, -0.023291347548365593, 0.008722045458853245, 0.023714745417237282, -0.10376935452222824, 0.0064612748101353645, 0.0251791849732399, -0.08244668692350388, -0.06779064983129501, 0.06519055366516113, -0.026265207678079605, 0.09262380003929138, 0.001087958924472332, 0.030904503539204597, -0.03952699154615402, -0.016488151624798775, 0.037187494337558746, 0.08448703587055206, 0.013349653221666813, 0.022786863148212433, 0.024959491565823555, -0.04737124219536781, -0.007045519072562456, -0.14366893470287323, -0.0245149377733469, -0.0383673794567585, -0.023438431322574615, 0.018127597868442535, -0.01633535698056221, 0.045718465000391006, -0.012944287620484829, 0.04249178245663643, 0.010538670234382153, -0.023045601323246956, 0.051089316606521606, 0.016854681074619293, -0.00013984348333906382, 0.023969391360878944, 0.06092303246259689, 0.07439512014389038, 0.06504792720079422, -0.023513389751315117, -0.09141324460506439, 0.021126262843608856, -0.006828954443335533, 0.03923683613538742, 0.0952659398317337, 0.03809162229299545, 0.014188730157911777, -0.0020656113047152758, 0.04391034319996834, -0.024916982278227806, 0.03127385675907135, 0.01060742512345314, 0.02810417301952839, 0.053711697459220886, 0.038863006979227066, -5.393986768353898e-08, -0.08625303953886032, -0.045666273683309555, 0.06490372121334076, 0.017929432913661003, 0.0598030649125576, -0.1219501867890358, 0.07369699329137802, -0.013151826336979866, 0.05116705223917961, -0.09243909269571304, 0.08936762809753418, 0.039780281484127045, -0.09177134931087494, -0.0012750541791319847, 0.02646578848361969, 0.04794664680957794, -0.01978570967912674, -0.006807362195104361, 6.054717960068956e-05, 0.09845412522554398, 0.033601123839616776, -0.05969814956188202, -0.038158249109983444, 0.0404486358165741, 0.0313916839659214, -0.036623843014240265, -0.011911445297300816, -0.057232730090618134, 0.03338412567973137, 0.014005418866872787, 0.007800566963851452, 0.023829294368624687, 0.04901958629488945, -0.049154892563819885, 0.014967802911996841, 0.058069806545972824, -0.0167097095400095, 0.013999545946717262, -0.02629726566374302, -0.02416195347905159, -0.00734260119497776, -0.09590652585029602, 0.06590930372476578, -0.05907279625535011, 0.039978768676519394, -0.043891727924346924, 0.07863762229681015, -0.0276592168956995, 0.030965490266680717, 0.011275223456323147, 0.02048496901988983, 0.04876609146595001, -0.043415576219558716, 0.027707545086741447, 0.06064152345061302, -0.0758526474237442, 0.05397377163171768, -0.021795164793729782, 0.12055828422307968, 0.06953119486570358, -0.00892733782529831, 0.019336136057972908, -0.08800661563873291, -0.029082313179969788]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ConsonantRecognitionbyModularConstructionofLargePhonemicTimeDelayNeuralNetworks.pdf,NLP,"SPREADING ACTIVATION OVER DISTRIBUTED MICROFEATURES * James Hendler Depart.ment, of Computer Science University of Maryland College Park, MD 20742 ABSTRACT One att·empt at explaining human inferencing is that of spread- ing activat,ion, particularly in the st.ructured connectionist para- digm. This has resulted in t.he building of systems with semanti- cally nameable nodes which perform inferencing by examining t.he pat,t.erns of activation spread. In this paper we demonst.rate t.hat simple structured network infert'ncing can be p(>rformed by passing art.iva.t.ion over the weights learned by a distributed alga- rit,hm. Thus, an account, is provided which explains a well- behaved rela t ionship bet.ween structured and distri butt'd conn('c- t.ionist. a.pproachrs. INTRODUCTION A primar~· difference brtween t,he nPllral net.works of 20 years ago and t.he (""urrent genera Lion of connect,ionist models is t.he addit.ion of mechanisms whic h permit t.he s),st,em to create all internal represent,ation. These subsymbolic, semantica.lly unnameable, feat.urrs which a.re induced by connectionist. learning algorithms havr been discussed as bt,ing of import. bot,h in structured and distri- but.ed ronnl""ctionist nrtworks (cf. Feldman and Balla.rd , 1982; Rumelhart and McClelland, 198(j). The fact that network learning algorit.hms can creatr these rm·cro!eal'ure,s is not, however. enough in itself t.o aC('Qunt for how rognition works. Most. of what, we call int.elligent thought. dt'rives from being able t,o rea- :son about. t.he relatioll:::> I)t'tween object.s, to hypothesize about event.s a.nd things, etc. If we are to do cognit.ive modeling we must. complet.e the story by rxplainiJlg how networks can rea,-;oll in tht> wa.y that, humans (or other int,elligrnt beings) do. aile attempt at (-'xplaining such rea.<;oning is that of spreading activat.ion ill the structured cOllllect,ionist. and marker- passing (cf. Charniak, ]983; Hendler, 1987) • The aut.hor is also affiliatf' ci wit.h thp Instit.ut.e for Acivanced C'omputpr Studies a.nd t.he Sys- tems Research Center a.t the Universit.y of Ma.ryland. Funding for this work was provided in part by Office of Naval Resf'arch Grallt N00014-88--K - 0,560. 553 554 Hendler approaches. In t,hese syst,em::i semantically nanwable nodes permit an energy spread; and reasoning about tilt' world is accounted for by looking at, either stahh' configurations of tJw activation (the st.ruet.ured connectionist. approach) or at. t,he paths fOllnd by examining int.ersect,ions among t.he nodes (the marker- passing te('llnique). In this paper we will demonst.rate t.hat. simple Rt,ruct.lJred- network- like infE'rencing ('an be performed by pa.ssing act.ivat.ion over t,he wt'ightR lea.rned by a distribut.ed algorithm. Thus, an account. is provided which explains a well-behaved relation~hip bet.ween st.ruct,ured and dist.ributed connt'ct.ionist a pproa.ches. THE SPREADING ACTIVATION MODEL In this paper we will demonstrate that local connect.ionist.- Iike net.works can be built by spreading activat.ion ovel' t.he microfeaturcs learned b.\' a dist.ribut.ed net- work. To show t.his, we st,art wit.h a simple example which c1f'Jl1onst.rat.es the activation spreading mechanism used. ThE' part.icular net.work We will use ill this example is a 6- 3-8 three-layer net.work t rained by t.he hack-propagation learning algorithm. The training set used is shown in table 1. The \w·ights bet.ween the out.put node::; and hidden units which are learned by the Iletwork (after h'arning t.o tlw !)O% level for a typical rlln) are shown in figure 1. TABLE 1. Training Set. for Examph' 1. Inpll t Output. Pattern Pattern 000000 10000000 0000 J 1 01000000 001100 00100000 OOll}1 00010000 110000 00001000 lIOO]] 00000100 ] I I 100 00000010 111111 00000001 Spreading Activation over Distributed Microfeatures 555 Weights h1 h2 h3 n1 -4.98 4.40 -2.82 n2 -6.99 -4.99 -2.23 n3 -6.11 3.49 0.30 n4 -6.37 -4.68 2.53 n5 4.36 3.73 -5.09 n6 4.38 -5.97 -3.67 n7 0.89 1.07 3.32 nB 3.88 -6.95 1.88 Figure 1. Weights L~arnecl by Back Propagation To underst.and how t,he act,ivat.ioll spreads, let. us examine what. occurs when activation is started at node nl wit.h a weight of 1. This activation strength is divided by the outbranching of the nodf' and then mult.iplied by t.he weight of each link to t.he hidden units. Thus a.ctiva.tion flows from nl t.o hi with a strength of 1/ 3 1"" Weighl(tl1 ,hl}. A similar computat.ion is made t.o each of t.he other hidden units. This act.ivat.ion now spreads to each of the ot.her Ollt.pUt. nodes in turn. Thus, n2 would gain act,ivat.ion of Acti','ation(hl) I Wet""ght(n2,hl)/ 8 -1- Activat-ion(h2) 1"" lYe'ight(n2)d)/ 8 + Ad1'vation(h3) 1"" Weight(n2,h3)/ 8 or .80 from rli. Table 2 shows a graph of t.he act.ivat,ioll spread between the output units. The table, which is symmetric, can t.hus be read as showing t.he out,put at each of the other units when an activation st.rength of 1 is placed at t,he named node. Look- ing at. the table we see t,hat. t.il(' hight:'st, activat.ion OCClJrs among nodes which share t.he most feat,ures of tJlf' input (i .e. ~anlf' value and posit.ion) while t.he lowest is se('n among tho::;t:' pat.tt'fIls shariug t.he fewest feat.ures. 556 Hendler However, as well as having this property, t.able 2 can be seen as providing a matrix which specifies t.he weights bet.ween t.he out.put nodes if viewed as a st.ruc- t,ured net.work. That. is. ,,1 is conned·ed to rtf! by a strength of + .80, to nB by a st.rength of + 1.0:3, etc. Thus. by I\~ing this t.echniquE' distribut.ed r{'presentations can be turned into connectivity weight.s for st.ructured lIet.works. When non-· ort.hogonal wpights are lIsed. t he same act.ivat ion--spreading a.lgorithm produces a struct.ured network whi('h can be used for more complex inferenciug than can the dist.ributed net work alone. We demonst.rat.e t.his b)' a simple. and aga.in contrin·d. example. This example is mot.ivat.ed by Gary Cot.trell's st.ructured JI10dd for word sense disambiguation (CoUrt'll, Hl85). Cottrell, using weights clerived by hand, demonstrat.ed that. a struct.ured connect.ionis!. net.work could di:';t inguish both word- sense a.nd case- slot assignm('nts for amhiguous lexical items. Pre~enkd wit.h the sent{'nce ""John threw the fight."" f·he syst.em would ndivate a node for one meaning of ""throw/' presented wit.h ""John t.hrew t.he ball"" it would come up wit.h another. The nodes of Cottrell's network included worels (John. Threw , etc .). word RenseR (Johnl, Propp\. etc.) :lnd cClI'P- sloLs (TAGT (lIgt'llt of tl1(-' throw). PAGT (ag('nt of t,he Propel). etc.). TABLE 2. Adivat.jon Spn>ad in 6--:3·8 Network. ,,1 II::! oj 114 ·//5 uB Tl7 u8 nl • .80 1.03 .17 .38 -1.57 - .:38 - 2.3 n2 .80 • 1.02 2.6 -1.57 .:31 -.79 .14 n.'J 1.03 1.02 • .97 - .0:3 - 2.03 - .0:3 --1.97 n4 .17 2.60 .97 • - 2.42 -.:38 -.Of! .52 '115 .38 -1.57 - .63 -2.42 • .04 -.;38 -.77 116 -1.57 .31 -2 .03 -.38 .64 • -.6 2.14 n7 - .38 -.79 - .03 -.09 -.;38 - .f) • .09 uB - 2.3 -.14 -l.f!7 .52 - .77 2.1 ·1 .09 • To duplicate Gary's network via training, we presented a 3-layer backprop net ... work wit.h a training set in which distribut.ed pat.t.erns, H'ry loosely corresponding to a ""dictionary"" of word (-,Ileodingsl were associa.ted wit,h a vector representing each of the individual noell's which would he represented in Cottrt-ll's system, but. wit.h no struct.ure. Thus, each elemPIlt. in t.he training set. is }- Whieh in any realisti r system would sornp. day be rpplaeed by aduaJ signal pro.;pssing out- p""t.S Of Othpf rpprespnt.alinns of ai'liial worn pfonuneiat.ioll forms. Spreading Activation over Distributed Microfeatures 557 a 16 bit vretor (represent.ing a four word srnt.en('e. ea('h word as a. 4 bit. pattrrn), associated wit.h allot.he)· 16 bit. ve('tor repr<'spnt.ing t.he nodes Bobl Johnl propE'l t.hrfW IJght I ball J jlJgt pob) tagt. tobj bob John t.hrfw U1E' fight ball For t.his example, the system was t.rnineci on th(· E'1I('odings of t.he four srnt.encrs John t.hrew the ball John threw the fight. Bob t.1Hfw t be hall Bob t.hrfw Lllf 6ght wit.h the output set. high foJ' tho:-;e objects /II the second vedor whi('h \v('/,e a.ppropriatf·\y a.<.;so('iaj,rd . (IS shown in Tabh· :3. TABLE ;~. Tr<lining S(·t fa)' Example Z. Input. Pattern 011 0 0001 0101 OOlO 01 J 0 OOOJ 0101 J010 100/ OOOJ 010J 00/0 J 00 J 000 1 0101 10 10 Output. Patt.el'n JOO 1 1000 1J J 01 I 10 lOlOOlJI00101101 0/0/100011011110 01 JOOJ 1100011101 Upon complet.ion of t.he kaming, t.he aet.ivat.ioJl sprrading algorithm was used to derive a table of COlll1f'ctivity weights betw('ell the out.put. unit.s as shown in table 4. These weight.s W(')'e then t.ransferred into a local COllnect ionist simulat.or and a very simple act.ivation spn'adiJlg modrl was llsed to examine t.he result·s. \Vhpn Wt' run t.he simulator. u~ing tht' aeti\'atioIl spn·ading oY('r leaJ'lwd wpights. exactly t.he f('sult.s prodllced by Cott.reJl's n{'twork lIr{' seen . Thus: Act.ivation from tht, nodt's corresponding to john. tbroll', tllf-. alld fif/hl cam,e a posit.ive activation at. the node for ('Throw"" and a negative a('- t.ivat.ion at t.he node for ""Propr!."" while A('tivat.ion from joh"" throlt' the ball sprt'ad positiniy to "" Prope'"" ,lIlel not. t.o 'I t IIrow . ,. Furt.her, ot·her effrct.s which cHP also predict.ed by C'ou.rdl's model lire seeJl: Act.iYat.ioJl at. TAGT and TOB.! spreads posit.i\'t' activation to TIr.,.o/l' and not t.o Propel. and Activation at PAG?' and POB) causes a spread t.o Propel but. not to Thro'W. 558 Hendler TABLE 4. Connectivity Weights for Example 2. *** -·0.12 0.01 -0.01 -0.01 0.01 0.01 0.01 - 0.01 -0.01 0.12 - 0 .12 ···0.0:3 -0.0:3 -0.01 0.00 - 0.12 *** - 0.01 0.01 0.01 - 0.01 -0.01 -0.01 0.01 0.01 -·0.12 0 .12 0.0:3 O.O~ 0.01 ·-0.01 0.01 -OJ)} *u -0.04 -0.04 0.04 0.04 0.05 -0.05 - 0.04 0.01 - 0.01 - 0.02 - 0.02 - 0.04 0.04 -0.01 0.01 ·-0.04 *** 0.04 -0.04 - 0.04 -0.05 0.05 0.04 ·_·0.01 0.01 0.02 0.02 0.04 - 0 .04 -0.0] 0.01 -0.04 0.04 u* -0.04 -0.0,5 -0,05 0.05 0,04 -0,01 0,01 0.02 0.02 0.04 - 0.04 0,01 -0,01 0,04 -0.04 -0,04 *** 0.05 0,05 -0.0.5 -·0.04 0.01 - 0.01 - 0.02 -0.02 - 0.04 0.04 O.OJ -0.01 0.04 -0.04 -0,05 O.O!) *** 0.05 -0.05 -0.05 0.01 ··0.00 -0.02 -0.02 - 0.04 0.05 0.01 ·-0.01 0.05 -0.05 -0,05 0.05 O.OS u* -·0.05 -O.O!) 0.01 -O.OJ -0'()2 -0,02 ··0.04 0.05 -0.01 0.01 -0,05 0.05 0.05 -0.05 -0.05 - 0.05 *** 0.05 -0.01 O.()] 0 .02 0.0:3 0.04 - 0.05 -0.01 0.01 -0.04 0.04 0.04 -0.04 -0,05 -0.05 0.05 u* -0.01 0.01 0.02 0.02 0.04 -0.05 0.12 -0.12 0 .01 -0.01 - 0.01 0.01 0.01 0.01 -0.01 -0.01 *** ·-O.l:.? ·-0.0:3 -0.0:3 ·-0.01 0.0] - 0.12 0.12 ·-0,01 0.01 0.01 -·0.01 -0.00 -0.01 0.01 O.OJ ··0.12 *** O.O:~ 0.0:3 0 .01 ··0.00 -0.0:3 0.03 -0.02 0.02 0.02 -·0.02 -·0.02 -0.02 0 .20 0.02 -0.02 0.02 0.02 ·-0.0:3 0.0:3 *** ··0.03 0.0:3 ··0.02 0,02 0.02 -0.02 ·-0.02 -0.02 * ** 0.02 - 0.02 0.0;3 0.02 0.0:3 0.0:) 0.20 ··0.0] 0.01 -·0.04 0.04 0.04 - 0.04 -·0.04 -0.04 0.02 *** . ~.O .• O.O<J D.O·' · 0.01 O.OJ 0.02 0.00 O.OJ 0.04 -0.04 ·-0.04 0.04 0.0,5 0,05 0.02 ·0.04 *** O.O!) O.OG O.OJ ·0.00 0 .02 We believe that results like this one ma.y argue t.hat. :-;tl'uetllrt'd IId·works are int.pgrally linked to dist.ributed networks ill t,hat. distribllt.<·d Ilf'twork le:ullillg t.echuiqut's may provide a fundamental ba~i:-; for ('xplaining the ('ogllit.in' df'VelOP- ment. of structured networks . III addition, Wt' see that :,-imple illf{'J'f'llt ial rt'a~on­ ing ('an be produ('ed using purdy cOIlJledioni~t. mockl:,. CONCLUDING REMARKS Wt' have at.t.empt.ed t.o ;;;hO\ .... that a model u;;;ing an activation spn'ading va.rinnt ean be used t.o take learned connect.ionist models aJld lwrform SOllle limit ed forms of inferencing upon t.helll. Furt.her, WP have argllt""d t.hat t.his tf'chniqlll' Illay pro- ""idr a comput.ational modd in which strllctured network;;; C;}l) Iw leal'llPd and t.hat st,ruct.ured net.work::; provide the infeI't'ncing (·apnbilities missing in purely dist.rihut.ed models. However. befort' w(' can t.ruly furtht'r thi~ claim. ~ignit1('ant work remain;;; to be done. We must ext.end alld ('xplort' such models. particularly t'xamining whether t.hese t·ypes of t.echniqups ('an be extt'llded t.o handle tlw com- plt'xitr t·hat can be found in real·-world prol)JpIll~ and sl'rioll~ ('ogllitin models . In part ieula.r we are beginlling an examinat.ion of two cru('ial isslIP::;: First., will t.he technique described above work for realist.ic problem·;? In particular, can t.hl' infen·ncing be designed t.o impact on t.1l(' ]'Pwgnition by t 11(> di.stribut.ed net.- work? If so, one ('ould Sf'f', for l'xample, a speech recognit.ion program coupled to a system like Cottrell's natural languagt' sy:.;\.eJll, providing a handle for a t.ext. underst.anding system. Similarly such a t.e('hnique might. allow the int.('gration of top -down and bott.OIll'-IIP pro('('ssing for vision a.nd ot.ht'r such signal Pl'ocf'ssing tasks. Spreading Activation over Distributed Microfeatures 559 Secondly. we wish to see jf more complex spreading activation models could be hooked to this t.ype of model. Could networks such as t.hose proposed by Sha.st.ri (1985), Diedt'rich (HI8!»). and Polla.ck and 'Valt.z (1982) which provide complex inferencing bllt requirt' more ,structure than simply wt'ight.s bet.ween unit.s, be ab.stract.t'cI out. of the learned weight.s? Two partieular areas current.ly being pur- ~llf'd by t.JH' author. for t'xampk foclls on act.i\'e inhibit.ioll modeb for dt't.ermin- ing whetlwr port.jon~ of the nt'twork C(ln lw ~lIpPJ'f'ssed t.o provide mort' complex inferencing and t.he learning of structures given temporally ordered information. References Charniak, E. Passing markt'rs: A t.heory of contt'xtual influence lJl language comprehension Cognitive S6ence, 7(3), 198:~. 171-190. Cottrell, G.W. A Connection-ist Approach to Word- Sense DiMlmb1'gllation, Doe- t.oral Disst'rta.t.ion, Comput.t'r Scienc(' Dt'pt.. l fllivl'rsit.y of Hochesler, 1985. Diederich, J. Parallelverarbeil/l'flg in 1IetzlL'erk-ba.~ie1'fen Systemen PhD Disst'rta- tion, Dt'pt. of Linguistics, llni\'ersity of Rielt~f('ld. J985. Feldman, J.A. and Ballard , D.H. (J 982). Connect.ionist models a.nd t.heir propt:'r- t.ies. Cognit1've Science. 6. 205-·254- Hendler, J.A. JlItegrating Maf'l.·er -pali8illg aod Problem Solving: A .~p,.e(/dirtg artivatiou approach to 1'ml)ro'l'cd choice ill ploflJlillg Lawrence Erlbaulll As~ociate~, N .J., Novt'mbt'J', Hlg7. Pollack, J.B. and 'Yalt.7., D.L Nat·u!'al Language Processing lIsiug spr£'ading act.ivat.ion and lateral inhibit.ion Proceedings of the FO'llrth intema.i1'o'l1al Confer- ence of Ihe Cognitive S'o'eure S'ociety, 1982, .50-58. Rurnelhart .. D.E. and McClelland. J.L. ((,ds.) Parallel D';strib'llfed Computing Cambridge. ,Ma.: MJT Prel-is. Shastri, L. El'ideul1'al RWMuing ·in SefJ/(llItic /\.retll'{)rk.~: A formal theor:1J ((tid //8 parallel implementatioTl Doct.oral DiS8t'l't at.jon, Computt'r Scit'nce Depart ment, l Jnivefsit.y of Roc-hester, Sept .. J 985.","[-0.0947280079126358, -0.0627097487449646, -0.0024075189139693975, 0.016761211678385735, -0.0762338787317276, -0.0930061787366867, 0.04910493269562721, 0.07038877159357071, 0.05908111482858658, 0.036658890545368195, 0.018419723957777023, 0.0035596403758972883, 0.058379992842674255, -0.024311956018209457, -0.023028384894132614, -0.035437408834695816, -0.004256398882716894, 0.01575392670929432, -0.08874751627445221, -0.05266928672790527, -0.004529152996838093, -0.04731946438550949, -0.06011057272553444, -0.07448911666870117, 0.0011926178121939301, -0.019742926582694054, -0.06351125240325928, -0.030611727386713028, 0.03243071585893631, -0.094572052359581, 0.06506915390491486, 0.07036701589822769, 0.03656739369034767, 0.03291445970535278, -0.08358091115951538, 0.08348477631807327, -0.10325376689434052, -0.005753732286393642, -0.019177475944161415, 0.021020665764808655, 0.07465171068906784, -0.015980210155248642, -0.004873547237366438, 0.007472328841686249, -0.02518743835389614, 0.01682487688958645, 0.0431903675198555, 0.05463975667953491, -0.07810333371162415, -0.021393388509750366, -0.06995124369859695, -0.008477220311760902, 0.008009185083210468, 0.08842478692531586, 0.003328111255541444, 0.005709768272936344, 0.003608038416132331, 0.010133478790521622, -0.04074368625879288, -0.008816948160529137, 0.04304559901356697, -0.027028972283005714, -0.08543211221694946, 0.030261332169175148, 0.04079080745577812, 0.025996776297688484, 0.0735895112156868, 0.08501522988080978, 0.026689236983656883, -0.031430866569280624, 0.09104854613542557, -0.015971589833498, -0.09711945801973343, 0.04488483816385269, 0.09421978890895844, 0.06624972820281982, 0.07888201624155045, -0.026159897446632385, 0.007688656449317932, 0.0013014305150136352, -0.013370857574045658, 0.033161409199237823, 0.01710784249007702, -0.10017691552639008, -0.01854327693581581, -0.00511592673137784, -0.028633948415517807, -0.008072080090641975, -0.0037445761263370514, -0.013864053413271904, -0.04646822065114975, 0.00021287042181938887, -0.07081906497478485, 0.005624448414891958, 0.06519579142332077, 0.005078115966171026, -0.021943069994449615, -0.07327861338853836, -0.0033744664397090673, 0.12341328710317612, -0.000614170974586159, -0.03546076640486717, -0.01576511561870575, -0.04249081760644913, -0.009987183846533298, -0.04541805386543274, -0.010306560434401035, 0.024136405438184738, 0.02513897977769375, 0.06471925973892212, -0.026461901143193245, 0.0024970865342766047, -0.002054979559034109, 0.009211532771587372, -0.017918262630701065, -0.05106066167354584, 0.01696145348250866, 0.027524679899215698, 0.08069679886102676, -0.08919011056423187, -0.03218918666243553, -0.022637896239757538, -0.10409878194332123, -0.031109420582652092, 0.030452067032456398, 0.04983685910701752, -0.02386518567800522, 6.581050673484671e-33, -0.026843976229429245, 0.005423637572675943, -0.006960676982998848, 0.025901764631271362, 0.023807412013411522, -0.0051165795885026455, -0.008504624478518963, 0.007833465933799744, -0.05064748600125313, -0.050767749547958374, -0.07355555146932602, 0.004640847444534302, 0.013502832502126694, 0.045393653213977814, 0.0589555986225605, -0.055209919810295105, 0.04039519652724266, 0.0031499348115175962, 0.06484568119049072, 0.006896382663398981, 0.022281823679804802, -0.023021068423986435, -0.05110819265246391, -0.03243451565504074, 0.035309355705976486, -0.015122243203222752, -0.06165073812007904, -0.10594440251588821, -0.016060959547758102, 0.010363725945353508, 0.024654321372509003, 0.03673650696873665, -0.0451718270778656, 0.021857764571905136, 0.023118920624256134, -0.0008826098055578768, 0.003647079924121499, -0.09423300623893738, 0.0034859853330999613, -0.07854291796684265, 0.05313975363969803, 0.014507033862173557, -0.01620469056069851, -0.008988741785287857, -0.016438879072666168, -0.013333940878510475, -0.08854544162750244, 0.013820402324199677, -0.029511259868741035, -0.01337627787142992, 0.01780969835817814, 0.011763682588934898, 0.005690580699592829, -0.04537317529320717, 0.06360193341970444, 0.10374415665864944, -0.03424350544810295, 0.06432206928730011, 0.044232454150915146, 0.16082362830638885, 0.021302102133631706, -0.04143987596035004, 0.013074373826384544, 0.08123058080673218, 0.09170670062303543, 0.05152171850204468, -0.10822931677103043, -0.011273159645497799, 0.06754271686077118, 0.07125484943389893, -0.02200591005384922, 0.08085355907678604, -0.03662937879562378, -0.07338505983352661, 0.025292569771409035, -0.03745396435260773, -0.04105427861213684, -0.021636810153722763, 0.00887216068804264, 0.03146754950284958, -0.039979033172130585, -0.00415778299793601, -0.060094159096479416, 0.008982989005744457, -0.11037937551736832, 0.022990556433796883, 0.01203866396099329, -0.02550910972058773, -0.06319735199213028, 0.02799917757511139, 0.0802285373210907, -0.00161194137763232, -0.019804731011390686, -0.021877771243453026, -0.009329456835985184, -6.44277213020966e-33, -0.1384229063987732, 0.057578809559345245, -0.07608848810195923, 0.13264018297195435, -0.042117852717638016, -0.026907414197921753, -0.04251762852072716, -0.017588937655091286, -0.05864926800131798, 0.042877066880464554, 0.0033665960654616356, 0.03232087567448616, -0.004979348741471767, -0.10809600353240967, 0.03560715913772583, -0.04459432512521744, 0.028336303308606148, -0.012308605015277863, 0.08935556560754776, 0.004606295842677355, 0.0646226704120636, 0.0601436123251915, -0.0878630131483078, -0.04745686426758766, 0.08599667251110077, -0.05204075574874878, -0.052736494690179825, 0.10840533673763275, 0.015535558573901653, -0.007368754595518112, -0.020162198692560196, -0.0027600436005741358, -0.038258809596300125, 0.02211831696331501, 0.022809240967035294, 0.060385312885046005, 0.08417655527591705, 0.11415327340364456, 0.01308955904096365, -0.026770731434226036, 0.04484625905752182, -0.03135531023144722, -0.12280017137527466, 0.030353989452123642, 0.016369659453630447, -0.03152371942996979, -0.08330962806940079, 0.029115796089172363, 0.010710976086556911, -0.043044596910476685, 0.027375267818570137, -0.001244385726749897, 0.016983194276690483, -0.015518607571721077, -0.04310741275548935, 0.06320337951183319, 0.05386972054839134, 0.0321061909198761, 0.021839292719960213, -0.032068584114313126, -0.027116237208247185, -0.10478665679693222, 0.039414674043655396, -0.008880124427378178, 0.010299003683030605, -0.002103881211951375, 0.007319408468902111, -0.02667449787259102, 0.00904823001474142, 0.006072069983929396, 0.04833214730024338, 0.05515314266085625, 0.03937217965722084, -0.08386053889989853, -0.014856300316751003, -0.015008817426860332, -0.0199712123721838, -0.02602945640683174, -0.04190246760845184, 0.01683424971997738, -0.13602915406227112, 0.06526459008455276, 0.004277820698916912, -0.0556037575006485, 0.10364113748073578, -0.008248011581599712, 0.12590672075748444, 0.030168429017066956, -0.004043348599225283, 0.05457375943660736, -0.021012557670474052, 0.03950848802924156, 0.04472363740205765, -0.019616445526480675, -0.10338100045919418, -6.080553305309877e-08, 0.02139844372868538, -0.023606810718774796, -0.016687434166669846, 0.07068846374750137, 0.1199604943394661, 0.049741946160793304, 0.06853002309799194, 0.0071862367913126945, -0.03346472606062889, 0.04740922898054123, -0.011620607227087021, -0.04453488811850548, -0.03074914589524269, 0.03611116111278534, 0.14437781274318695, 0.062011174857616425, 0.009177039377391338, -0.08193076401948929, -0.1099412590265274, -0.009623103775084019, 0.005545387510210276, 0.015279446728527546, -0.04746381938457489, 0.05295636132359505, 0.01765032485127449, -0.01923421397805214, 0.008708411827683449, 0.04426238685846329, 0.01592424139380455, 0.04200224205851555, -0.059476349502801895, 0.01623942330479622, 0.03882558271288872, 0.06596443802118301, -0.02548784762620926, 0.0931265726685524, 0.0039362129755318165, 0.01002776063978672, -0.03588562458753586, -0.003321847878396511, -0.055657315999269485, -0.03106820024549961, 0.030033452436327934, 0.04660540074110031, 0.0547427162528038, 0.007205337751656771, -0.0482884980738163, -0.011271532624959946, 0.03714399039745331, 0.0014377878978848457, 0.036981139332056046, 0.06019787862896919, -0.023655805736780167, 0.029195476323366165, 0.0008628778159618378, 0.0017086903098970652, 0.0007993644685484469, -0.09011533111333847, 0.04185137525200844, 0.10505952686071396, -0.026315558701753616, -0.004068903159350157, -0.09593077003955841, -0.05882994458079338]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ConstraintsonAdaptiveNetworksforModelingHumanGeneralization.pdf,NLP,"Training Stochastic Model Recognition Algorithms 211 Training Stochastic Model Recognition Algorithms as Networks can lead to Maximum Mutual Information Estimation of Parameters John s. Bridle Royal Signals and Radar Establishment Great Malvern Worcs. UK WR143PS ABSTRACT One of the attractions of neural network approaches to pattern recognition is the use of a discrimination-based training method. We show that once we have modified the output layer of a multi- layer perceptron to provide mathematically correct probability dis- tributions, and replaced the usual squared error criterion with a probability-based score, the result is equivalent to Maximum Mu- tual Information training, which has been used successfully to im- prove the performance of hidden Markov models for speech recog- nition. If the network is specially constructed to perform the recog- nition computations of a given kind of stochastic model based clas- sifier then we obtain a method for discrimination-based training of the parameters of the models. Examples include an HMM-based word discriminator, which we call an 'Alphanet'. 1 INTRODUCTION It has often been suggested that one of the attractions of an adaptive neural network (NN) approach to pattern recognition is the availability of discrimination-based training (e.g. in Multilayer Perceptrons (MLPs) using Back-Propagation). Among the disadvantages of NN approaches are the lack of theory about what can be computed with any partir.ular structure, what can be learned, how to choose a network architecture for a given task, and how to deal with data (such as speech) in which an underlying sequential structure is ofthe essence. There have been attempts to build internal dynamics into neural networks, using recurrent connections, so that they might deal with sequences and temporal patterns [1, 2], but there is a lack of relevant theory to inform the choice of network type. Hidden Markov models (HMMs) are the basis of virtually all modern automatic speech recognition systems. They can be seen as an extension of the parametric statistical approach to pattern recognition, to deal (in a simple but principled way) witli temporal patterning. Like most parametric models, HMMs are usually trained using within-class maximum-likelihood (ML) methods, and an EM algorithm due to Baum and Welch is particularly attractive (see for instance [3]). However, recently 212 Bridle some success has been demonstrated using discrimination-based training methods, suc.h as the so-called Maximum Mutual Information criterion [4] and Corrective Training[5] . This paper addresses two important questions: • How can we design Neural Network architectures with at least the desirable properties of methods based on stochastic models (such as hidden Markov models)? • What is the relationship between the inherently discriminative neural network training and the analogous MMI training of stochastic models? We address the first question in two steps. Firstly, to make sure that the outputs of our network have the simple mathematical properties of conditional probability distributions over class labels we recommend a generalisation of the logistic nonlin- earity; this enables us (but does not require us) to replace the usual squared error criterion with a more appropriate one, based on relative entropy. Secondly, we also have the option of designing networks which exactly implement the recognition computations of a given stochastic model method. (The resulting 'network' may be rather odd, and not very 'neural', but this is engineering, not biology.) As a con- tribution to the investigation of the second question, we point out that optimising the relative entropy criterion is exactly equivalent to performing Maximum Mutual Information Estimation. By way of illustration we describe three 'networks' which implement stochastic model classifiers, and show how discrimination training can help. 2 TRAINABLE NETWORKS AS PARAMETERISED CON- DITIONAL DISTRIBUTION FUNCTIONS We consider a trainable network, when used for pattern classification, as a vector function Q( re, 8) from an input vt>ctor re to a set of indicators of class membership, {Qj}, j = 1, ... N. The parameters 8 modify the transfer function. In a multi- layer perceptron, for instance, the parameters would be values of weights. Typically, we have a training set of pairs (ret,ct), t = 1, ... T, of inputs and associated true class labels, and we have to find a value for 8 which specialises the function so that it is consistent with the training st't. A common procedure is to minimise E( 8), the sum of the squart's of the differt'nces hetwt'en the network outputs and true class indicators, or targets: '1' N E(8) =: L L(Qj(ret, 8) - bj,c,)2, t=l j==l where bj,c = 1 if j = c, otht'rwise O. E and Q will be written without the 8 argument where the meaning is clear, and wt' may drop the t subscript. It is well known that the value of F(~) which minimises the expected value of (F(~) - y)2 is the expected value of y given~. The expected value of bj,e, is P( C = j I X = red, the probability that the class associated with ret is the jth class. Training Stochastic Model Recognition Algorithms 213 From now on we shall assume that the desired output of a classifier network is this conditional probability distribution over classes, given the input. The outputs must satisfy certain simple constraints if they are to be interpretable as a probability distribution. For any input, the outputs must all be positive and they must sum to unity. The use of logistic nonlinearities at the outputs of the network ensures positivity, and also ensures that each output is less than unity. These constraints are appropriate for outputs that are to be interpreted as probabilities of Boolean events, but are not sufficient for I-from-N classifiers. Given a set of unconstrained values, Vj(:e), we can ensure both conditions by using a Normalised Exponential transformation: Qj(~) = eVj(a!) / L eVIe(~) Ie This transformation can be considered a multi-input generalisation of the logistic, operating on the whole output layer. It preserves the rank order of its input values, and is a differentiable generalisation of the 'winner-take-all' operation of picking the maximum value. For this reason we like to refer to it as soft max. Like the logistic, it has a simple implementation in transistor circuits [6]. If the network is such that we can be sure the values we have are all positive, it may be more appropriate just to normalise them. In particular, if we can treat them as likelihoods of the data given the possible classes, Lj(~) = P(X = ~ Ie =i), then normalisation produces the required conditional distribution (assuming equal prior probabilities for the classes). 3 RELATIVE ENTROPY SCORING FOR CLASSIFIERS In this section we introduce an information-theoretic criterion for training I-from- N classifier networks, to replace the squared error criterion, both for its intrinsic interest and because of the link to discriminative training of stochastic models. the class with highest likelihood. This is justified by if we assume equal priors P(c) (this can be generalised) and see that the denominator P(~) = Lc P(~ I c)P(c) is the same for all classes. It is also usual to train such classifiers by ma:¥:imising the data likelihood given the correct classes. Maximum Likelihood (ML) training is appropriate if we are choosing from a family of pdfs which includes the correct one. In most real-life applications of pattern classification we do not have knowledge of the form of the data distributions, although we may have some useful ideas. In tbat case ML may be a rather bad approach to pdf estimation for the purpose of pattern clauification, because what matters is the f'elalive densities. An alternative is to optimise a measure of success in pattern classification, and this can make a big difference to performance, particularly when the assumptions about the form of the class pdfs is badly wrong. 214 Bridle To make the likelihoods produced by a SM classifier look like NN outputs we can simply normalise them: Ie Then we can use Neural Network optimisation methods to adjust the parameters. a SUlll, weighted by the joint probability, of the MI of the joint events ,.... P(X=:r,Y=y) I(X, Y) = ,L; P(X:=::r, Y=y)log p{X -=:r)p-(Y~Yf (~,y) For discrimination training of sets of stochastic models, Bahl et.al. suggest max- imising the Mutual Information, I, between the training observations and the choice of the correspolluing correct class. ,"""" P(C =.: Ct,X=Zt) ,........... P(C=Ct IX=zt}P(X=zd I(X, C) = ,L; log = ,L; log . P(C=cdP(X=z) P(C=ct}P(X=z) t t P(C=Ct I X = zt} should be read as the probability that we choose the correct class for the tth training example. If we are choosing classes according to the conditional distribution computed using parameters (J then P(C=Ct IX = zd = QCt(z,(J), and If the second term involving the priors is fixed, we are left with maximising LlogQCt(:rt,6) = -J. t The RE-based score we use is J ..;; -- }:;:;;1 L;=l Pjtlog Qj{ zd, where Pjt is the probability of class j associated with input Zt 1ll the training set. If as usual the training set specifies only oue true class, Ct for each Zt then Pj,t = [)j,Ct and T J = -- LlogQCt(zt}, t=l the sum of the logs of the outputs for the correct classes. J can be derived from the Relative Entropy of distribution Q with respect to the true conditional distribution P, averaged over the input distribution: J d:r P(X = z)G(Q I P), where G(Q I P) = - L P(c I z)log ~~(Iz~)' C information, cross entropy, asymmetric divergence, directed divergence, I-divergence, and Kullback-Leibler number. RE scoring is the basis for the Boltzmann Machine learning algorithm [7] and has also been proposed and used for adaptive networks with continuous-valued outputs [8, 9, 10, 11], but usually in the form appropriate to separate logistics and independent Boolean targets. An exception is [12]. There is another way of thinking about this 'log-of correct-output' score. Assume that the way we would use the outputs of the network is that, rather than choosing Training Stochastic Model Recognition Algorithms 215 the class with the largest output, we choose randomly, picking from the distribution specified by the outputs. (Pick class j with probability Qj.) The probability of choosing the class Ct for training sample IBt is simply Qet (tee). The probability of choosing the correct class labels for all the training set is n;=1 Qet (1Bt). We simply seek to maximise this probability, or what is equivalent, to minimise minus its log: T J = - L log Qet(ted· t=l In order to compute the partial derivatives of J wrt to parameters of the network, we first need :gj -= -Pjt!Qj The details of the back-propagation depend on the form of the network, but if the final non-linearity is a normalised exponential (softmax), '""' 8Jt Qj(:l) = exp(Vj(:z:))/ Lt exp(V"" (:z:)), then [6] aV- -= (Qj(:z:t) - bj,et)' "" J We see that the derivative before the output nonlinearity is the difference between the corresponding output and a one-from-N target. We conclude that softmax output stages and I-from-N RE scoring are natural partners. 4 DISCRIMINATIVE TRAINING In stochastic model (probability-density) based pattern classification we usually compute likelihoods of the data given models for each class, P(IB I c), and choose. So minimising our J criterion is also maximising Bahl's mutual information. (Also see [13).) 5 STOCHASTIC MODEL CLASSIFIERS AS NETWORKS 5.1 EXAMPLE ONEs A PAIR OF MULTIVARIATE GAUSSIANS The conditional distribution for a pair of multivariate Gaussian densities with the same arbitrary covariance matrix is a logistic function of a weighted sum of the input coordinates (plus a constant). Therefore, even if we make such incorrect assumptions as equal priors and spherical unit covariances, it is still possible to find values for the parameters of the model (the positions of the means of the assumed distributions) for which the form of the conditional distribution is correct. (The means may be far from the means of the true distributions and from the data means.) Of course in this case we have the alternative of using a weighted-sum logistic, unit to compute the conditional probability: the parameters are then the weights. 5.2 EXAMPLE TWO: A MULTI-CLASS GAUSSIAN CLASSIFIER Consider a model in which the distributions for each class are multi-variate Gaus- sian, with equal isotropic unit variances, and different means, {mj}. The prob- ability distribution over class labels, given an observation IB I is P( c = j lIB) = e 1'; / L"" e V"", where V; = -IIIB - mj 112. This can be interpreted as a one-layer feed-forward non-linear network. The usual weighted sums are replaced by squared Euclidean distances, and the usual logistic output non-linearities are replaced by a normalised exponential. 216 Bridle For a particular two-dimensional10-class problem, derived from Peterson and Bar- ney's formant data, we have demonstrated [6] that training such a network can cause the ms to move from their ""natural"" positions at the data means (the in-class maximum likelihood estimates), and this can improve classification performance on unseen data (from 68% correct to 78%). 5.3 EXAMPLE THREE: ALPHANETS Consider a set of hidden Markov models (HMMs), one for each word, each param- eterised by a set of state transition probabilities, {a~j}' and observation likelihood functions {b~ ('"" H, where a~j is the probability that in model k state i will be fol- lowed by state j, and b~ ( ""') is the likelihood of model k emi tting observation '"" from state j. For simplicity we insist that the end of the word pattern corresponds to state N of a model. The likelihood, Lie (lett) of model k generating a given sequence "",tt ~ ""'1, •• "" ""'M is a sum, over all sequences of states, of the joint likelihood of that state sequence and the data: M LIe(ler) = L IT a!'_I""f b!I(""'d with 8M = N. 'I ... IM t=2 This can be r.omput.ed efficiently via the forward recursion [3J glvlllg which we can think of as a recurrent network. (Note that t is used as a time index here.) If the observation sequence ""':'"" could only have come from one of a set of known, equally likely models, then the posterior probability that it was from model k is p(r=k I "",f!) = QIe("",f!) = Llc("",f1 ) / L Lr("",r)· r These numbers are the output of our special ""recurrent neural network"" for isolated word discrimination, which we call an ""Alphanet"" [14J. Backpropagation of partial derivatives of the J score has the form of the backward recurrence used in the Baum-Welch algorithm, but they include discriminative terms, and we obtain the gradient of the relative entropy/mutual information. 6 CONCLUSIONS Discrimination-based training is different from within-class parameter estimation, and it may be useful. (Also see [15].) Discrimination-based training for stochastic models and for networks are not distinct, and in some cases can be mathematically identical. The notion of specially constructed 'network' architectures which implement stochas- tic model recognition algorithms provides a way to construct fertile hybrids. For instance, a Gaussian classifier (or a HMM classifier) can be preceeded by a nonlin- ear transformation (perhaps based on semilinear logistics) and all the parameters Training Stochastic Model Recognition Algorithms 217 of the system adjusted together. This seems a useful approach to automating the discovery of 'feature detectors'. © British Crown Copyright 1990 References [1] R P Lippmann. Review of neural networks for speech recognition. Neural Computation, 1(1), 1989. [2] It L Watrous. Connectionist speech recognition using the temporal flow model. In .Pl'Oc. IEEE W ol'kshop on Speech Recognition, June 1988. [3] A B Poritz. Hidden Markov models: a guided tour. In Proc. IEEE Int. Conf. Acouslics Speech and Signal P1'Ocessillg, pages 7-13, 1988. [4] L R Bahl, P F Brown, P V de Souza, and R L Mercer. Maximum mutual information estimation of hidden Markov model parameters. In Proc. IEEE Tnt. Conf. Acoustics Speech and Signal P,'ocessing, pages 49-52, 1986. [5] L R Bahl, P F Brown, P V de Souza, and R L r.fercer. A new algorithm for the estimation of HMM parameters. In P,'Vf. IEEE Int. Con!. Acoustics Speech and Signal Processmg, pages 493-496, 1988. [6] J S Bridle. Probabilistic interpretation of feedforward classification network output.s, with relationships to statistical pattern recognition. In F Fougelman- Soulie and J Herault, editors, Neuro-computing: algorithms, architectures and appfications, Springer-Verlag, 1989. [7] D HAckley, G E Hinton, and T J Sejnowski. A learning algorithm for Boltz- mann machines. Cognitive Science, 9:147-168,1985. [8] L Gillick. Probability scores for backpropagation networks. July 1987. Per- sonal communication. [9] G E Hinton. Connectionist LeaJ'ning Procedures. Technical Report CMU-CS- 87-115, Carnegie Mellon University Computer Science Department, June 1987. [10] E B Baum and F Wilczek. Supervised learning of probability distributions by neural networks. In D Anderson, editor, Neura,Z Infol'mation Processing Systems, pages 52""-6], Am. lnst. of Physics, 1988. [11] S SoHa, E Levin, and M Fleisher. Accelerated learning in layered neural net- works. Complex Systems, January 1989. [12] E Yair and A Gersho. The Boltzmann Perceptron Network: a soft classifier. III D Touretzky, editor, Advances in Neuml Information Processing Systems 1, San Mateo, CA: Morgan Kaufmann, 1989. [13] P S Gopalakrishnan, D Kanevsky, A Nadas, D Nahamoo, and M A Picheny. Decoder seledion based on cross-entropies. In Proc. IEEE Int. Conf. Acoustics Speech and Signal Pl'ocessing, pages 20-23, 1988. [14] J S Bridle. Alphanets: a recurrent 'lleural' network architecture with a hidden Markov model interpretation. Spee('h Communication, Special N eurospeech issue, February 1990. [15] ""L Niles, H Silverman, G Tajclllnan, and 1\'1 Bush. How limited training data can allow a neural network to out-perform an 'optimal' classifier. In Proc. IEEE in.t. Conf. Acoustics Speech and Signal Processing, 1989.","[-0.13850760459899902, -0.12741288542747498, -0.027008075267076492, -0.05085846781730652, 0.016332242637872696, 0.06715448200702667, 0.08219420909881592, -0.07482617348432541, 0.03837863355875015, -0.09923595935106277, -0.060768287628889084, -0.03141216188669205, 0.04491976276040077, 0.040906187146902084, -0.04706087335944176, -0.03625055402517319, 0.020645834505558014, 0.060673363506793976, -0.03384818509221077, -0.07205455005168915, 0.049844738095998764, 0.1368856281042099, 0.023637477308511734, 0.036375824362039566, -0.02504109963774681, -0.025731228291988373, 0.025982195511460304, -0.02705046720802784, -0.0744319036602974, 0.01906975917518139, 0.13294923305511475, -0.051723308861255646, 0.10994361340999603, 0.01527034118771553, 0.015788443386554718, -0.016690274700522423, -0.04061464965343475, 0.010185482911765575, -0.027288462966680527, 0.04459371417760849, -0.004927642177790403, -0.012488043867051601, -0.07399360090494156, 0.015478711575269699, 0.10281628370285034, 0.03136324882507324, 0.001541909878142178, -0.04754871502518654, -0.009164859540760517, 0.018903739750385284, 0.004129395354539156, 0.007503068074584007, -0.04529377073049545, 0.04149767756462097, -0.004784086253494024, 0.007682865485548973, -0.028127050027251244, 0.025684619322419167, -0.039897847920656204, 0.010442379862070084, -0.08311201632022858, -0.062100477516651154, 0.03013605624437332, -0.05111870542168617, 0.0018367852317169309, 0.030951786786317825, -0.02130051515996456, 0.06191379949450493, 0.03315150365233421, -0.02791525423526764, -0.034318774938583374, 0.10426008701324463, -0.03591404855251312, 0.027156241238117218, -0.023860296234488487, 0.0035981794353574514, 0.09245048463344574, 0.006200061179697514, 0.02370062842965126, -0.06096748262643814, -0.021059144288301468, 0.043982572853565216, 0.12480106204748154, -0.04655587673187256, 0.10772929340600967, -0.025002559646964073, -0.09164240211248398, -0.02940361015498638, 0.02814522199332714, -0.09138012677431107, -0.06905506551265717, -0.08192592114210129, -0.03047158382833004, -0.0020472039468586445, 0.019969483837485313, 0.08499646186828613, -0.049476850777864456, -0.029213489964604378, 0.012200343422591686, 0.08117510378360748, -0.02574944868683815, -6.576487794518471e-05, 0.007408409379422665, -0.03041975386440754, -0.018709421157836914, 0.03044387511909008, 0.02552514709532261, -0.012383107095956802, 0.08201687037944794, -0.06806481629610062, 0.01851285994052887, 0.050768882036209106, -0.024531254544854164, 0.015491172671318054, 0.049064770340919495, 0.020345313474535942, -0.050299759954214096, -0.028570154681801796, 0.03961488604545593, 0.06595828384160995, -0.08931859582662582, -0.0027444579172879457, -0.010233432985842228, 0.03160994127392769, 0.04474944621324539, 0.08619310706853867, -0.028888048604130745, 2.6360446123510412e-33, -0.005484722554683685, 0.08134586364030838, 0.029420414939522743, -0.05980027839541435, 0.0014158887788653374, -0.03352139890193939, -0.03720485046505928, -0.0005954355001449585, 0.052753712981939316, 0.05210433155298233, -0.052987318485975266, 0.020344989374279976, -0.06332787871360779, -0.04191592335700989, 0.04784195125102997, 0.058283157646656036, -0.02678091824054718, 0.01183555368334055, 0.007648217957466841, -0.06500379741191864, 0.030866170302033424, 0.02271036058664322, 0.0346379429101944, -0.018733365461230278, 0.026683902367949486, 0.037370748817920685, 0.06748570501804352, -0.08458492904901505, 0.047682058066129684, 0.023610353469848633, 0.048283375799655914, -0.007599080912768841, 0.02548893913626671, -0.046874623745679855, 0.024017060175538063, -0.05601762607693672, -0.08387627452611923, 0.054153650999069214, -0.014188556931912899, -0.06505180150270462, -0.015195267274975777, -0.029941827058792114, 0.035002220422029495, -0.0162650216370821, -0.044821906834840775, -0.07271038740873337, -0.01248693186789751, -0.0014920012326911092, 0.021298276260495186, -0.05055282637476921, 0.07435951381921768, -0.01760135032236576, -0.08332139998674393, -0.017023321241140366, 0.017485886812210083, -0.012209421955049038, 0.042936231940984726, 0.08612716943025589, 0.07975739985704422, 0.03459670767188072, -0.011365359649062157, 0.0008830358274281025, 0.07115744799375534, 0.028610816225409508, 0.09627246856689453, -0.008723462000489235, -0.09674645215272903, 0.07345126569271088, 0.04367683455348015, -0.010877509601414204, 0.05760088935494423, -0.005862790625542402, 0.026068920269608498, -0.04196925833821297, 0.05552629008889198, -0.03259829431772232, -0.04754159227013588, -0.04191054403781891, -0.036244746297597885, 0.07524404674768448, 0.01172712817788124, 0.04803570732474327, -0.04072519391775131, -0.08276460319757462, -0.06021269038319588, -0.011280986480414867, 0.0614791065454483, -0.019105566665530205, 0.0458686538040638, 0.024707093834877014, 0.002951359376311302, -0.014518174342811108, -0.009280391968786716, -0.027985693886876106, -0.0358583964407444, -2.7676367968217807e-33, -0.06929398328065872, 0.12356051057577133, -0.0492687001824379, 0.003567661624401808, -0.051111143082380295, -0.03340286388993263, 0.0049463738687336445, 0.03137105703353882, -0.05478913336992264, 0.012745600193738937, -0.017372429370880127, -0.04830857738852501, 0.05104180797934532, -0.04043009132146835, 0.011800435371696949, -0.013183793984353542, -0.035061076283454895, 0.11704789847135544, 0.025975635275244713, 0.03342094272375107, 0.04714270681142807, 0.03127984330058098, -0.11741439998149872, 0.028337804600596428, -0.06150761991739273, -0.02905230224132538, -0.013288725167512894, 0.10092877596616745, 0.07282716780900955, -0.02205217070877552, -0.10059229284524918, -0.013164541684091091, -0.07996989786624908, 0.04658006876707077, -0.04258948192000389, -0.043102774769067764, 0.041281409561634064, 0.00021974911214783788, -0.02721710316836834, 0.030814560130238533, 0.010948794893920422, 0.02986948750913143, -0.03767361119389534, -0.05019828677177429, 0.002264029812067747, -0.052735235542058945, -0.06987107545137405, 0.05829715356230736, 0.04033232480287552, -0.003859072457998991, 0.012365806847810745, -0.009025090374052525, -0.0250661913305521, 0.02767772041261196, -0.043038126081228256, 0.01947180926799774, -0.09157552570104599, -0.027228742837905884, -0.008973577059805393, 0.045758508145809174, -0.056458935141563416, -0.019874155521392822, 0.010209024883806705, 0.01070838887244463, 0.03684759512543678, 0.007177823688834906, -0.04617965593934059, 0.04610598087310791, 0.12246551364660263, 0.003392574144527316, -0.00742321228608489, 0.016482122242450714, 0.036065004765987396, 0.03705678507685661, -0.032911255955696106, -0.03926582261919975, 0.0022757730912417173, -0.04314997419714928, -0.058531202375888824, -0.061440374702215195, -0.030703045427799225, -0.06188266724348068, -0.005923119373619556, -0.010833404026925564, 0.09262686222791672, 0.0908205658197403, 0.09603922814130783, -0.05149901658296585, 0.07060582935810089, -0.06152467802166939, 0.010201911441981792, 0.06154075264930725, 0.01908925548195839, 0.06616857647895813, -0.07842197269201279, -4.868975267413589e-08, -0.07711151987314224, 0.0041468217968940735, 0.03673006221652031, -0.01940607652068138, 0.0963880866765976, -0.06161252036690712, 0.0413416251540184, -0.006629399489611387, -0.028989268466830254, -0.09960970282554626, 0.046625200659036636, -0.05401613935828209, -0.019133644178509712, -0.03837618604302406, 0.009022279642522335, 0.07618787884712219, 0.04209550470113754, 0.004084375686943531, 0.053877316415309906, -0.020566757768392563, 0.13010399043560028, 0.022506801411509514, -0.020354468375444412, 0.15257367491722107, 0.007426280528306961, -0.00497910613194108, -0.03592490032315254, 0.043111659586429596, -0.020184071734547615, 0.05378049612045288, 0.016173815354704857, 0.0037945618387311697, 0.005685378331691027, -0.0013548108981922269, 0.0807594433426857, 0.05118885636329651, 0.0889410525560379, -0.023014742881059647, -0.02084946818649769, -0.01749931462109089, 0.0022035257425159216, 0.029127486050128937, -0.048177506774663925, 0.012543066404759884, 0.05072254687547684, 0.0007799181039445102, -0.006173081696033478, -0.14431266486644745, -0.0018062356393784285, 0.005721937865018845, 0.06809281557798386, -0.011062010191380978, 0.014242076314985752, -0.00017165002645924687, -0.005224237684160471, 0.0021849132608622313, 0.0676635131239891, -0.15030254423618317, -0.003548803273588419, 0.055212754756212234, 0.0021010956261307, 0.03091348707675934, -0.04864657670259476, -0.023749738931655884]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ConvergenceandPatternStabilizationintheBoltzmannMachine.pdf,Optimization,"186 AN APPLICATION OF THE PRINCIPLE OF MAXIMUM INFORMATION PRESERVATION TO LINEAR SYSTEMS Ralph Linsker IBM T. J. Watson Research Center, Yorktown Heights, NY 10598 ABSTRACT This paper addresses the problem of determining the weights for a set of linear filters (model ""cells"") so as to maximize the ensemble-averaged information that the cells' output values jointly convey about their input values, given the statistical properties of the ensemble of input vectors. The quantity that is maximized is the Shannon information rate, or equivalently the average mutual information between input and output. Several models for the role of processing noise are analyzed, and the biological motivation for considering them is described. For simple models in which nearby input signal values (in space or time) are correlated, the cells resulting from this optimization process include center-surround cells and cells sensitive to temporal variations in input signal. INTRODUCTION I have previously proposed [Linsker, 1987, 1988] a principle of ""maximum information preservation,"" also called the ""infomax"" principle, that may account for certain aspects of the organization of a layered perceptual network. The principle applies to a layer L of cells (which may be the input layer or an intermediate layer of the network) that provides input to a next layer M. The mapping of the input signal vector L onto an output signal vector M, f:L ~ M, is characterized by a conditional probability density function (""pdf"") p(MI L). The set S of allowed mappings I is specified. The input pdf PL(L) is also given. (In the cases considered here, there is no feedback from M to L.) The infomax principle states that a mapping I should be chosen for which the Shannon information rate [Shannon, 1949] R(j) == f dL PL(L) f dM p(MI L) 10g[P(MI L)/PM(M)] (1) is a maximum (over allIin the set S). Here PM(M) == fdLPL(L)P(MIL) is the pdf of the output signal vector M. R is identical to the average mutual information between Land M. Maximum Infonnation Preservation to Linear Systems 187 To understand better how the info max principle may be applied to biological systems and complex synthetic networks, it is useful to solve the infomax optimization problem explicitly for simpler systems whose properties are nonetheless biologically motivated. This paper therefore deals with the practical computation of infomax solutions for cases in which the mappings! are constrained to be linear. INFOMAX SOLUTIONS FOR A SET OF LINEAR FILTERS We consider the case of linear model ""neurons"" with multivariate Gaussian input and additive Gaussian noise. There are N input (L) cells and N' output (M) cells. The input column vector L = (Lt,~, ... ,LNF is randomly selected from an N-dimensional Gaussian distribution having mean zero. That is, (2) where QL is the covariance matrix of the input activities, Q6 = J dL PL(L)LjLj • (Superscript T denotes the matrix transpose.) To specify the set S of allowed mappings !:L .... M, we define a processing model that includes a description of (i) how noise enters during processing, (ii) the independent variables over which we are to maximize R, and (iii) any constraints on their values. Figure 1 shows several such models. We shall analyze the simplest, then explain the motivation for the more complex models and analyze them in turn. Model A -- Additive noise of constant variance In Model A of Fig. 1 the output signal value of the nth M cell is: (3) The noise components ""11 are independently and identically distributed (fli.i.d. "") random variables drawn from a Gaussian distribution having a mean of zero and variance B. Each mapping !:L .... M is characterized by the values of the {Cnj } and the noise parameter B. The elements of the covariance matrix of the output activities are (using Eqn. 3) (4) where ~nm = 1 if n = m and 0 otherwise. Evaluating Eqn. 1 for this processing model gives the information rate: R(j) = (1/2) In Det W(j) (5) where ~m = Q:!'/ B. (R is the difference of two entropy terms. See [Shannon, 1949], p.57, for the entropy of a Gaussian distribution.) 188 Linsker If the components Cni of the C matrix are allowed to be arbitrarily large, then the information rate can be made arbitrarily large, and the effects of noise become arbitrarily small. One way to limit C is to impose a ""resource constraint"" on each M cell. An example of such a constraint is ~jqj = 1 for all n. One can then attempt directly, using numerical methods, to maximize Eqn. 5 over all allowed C for given B. However, when some additional conditions (below) are satisfied, further analytical progress can be made. Suppose the NL-cells are uniformly spaced along the line interval [0,1] with periodic boundary conditions, so that cell N is next to cell 1. [The analysis can be extended to a two- (or higher-) dimensional array in a straightforward manner.] Suppose also that (for given N) the covariance Q6 of the input values at cells i and j is a function QL(Sj) only of the displacement s'J from i to j. (We deal with the periodicity by defining Sab = b - a - Ya~ and choosing the integer Yab such that -N/2 S Sab < N/2.) Then QL is a Toeplitz matrix, and its eigenvalues {Ak} are the components of the discrete Fourier transform (""F.T."") of QL(S): Ak = ~sQL(s) exp( -2~ks/N), (-N/2) S k < N/2. (6) We now impose two more conditions: (1) N' = N. This simplifies the resulting expressions, but is otherwise inessential, as we shall discuss. (2) We constrain each M cell to have the same arrangement of C-values relative to the M cell's position. That is, Cnj is to be a function C(Sni) only of the displacement Sni from n to i. This constraint substantially reduces the computational demands. We would not expect Figure 1. L· I L· I (S,C) (D) Four processing models (A)-(D): Each diagram shows a single M cell (indexed by n) having output activity Mn. Inputs {LJ may be common to many M cells. All noise contributions (dotted lines) are uncorrelated with one another and with {LJ. GC = gain control (see text). Maximum Information Preservation to Linear Systems 189 it to hold in general in a biologically realistic model -- since different M cells should be allowed to develop different arrangements of weights -- although even then it could be used as an Ansatz to provide a lower bound on R. The section, ""Temporally-correlated input patterns,"" deals with a situation in which it is biologically plausible to impose this constraint. Under these conditions, (Q:!') is also a Toeplitz matrix. Its eigenvalues are the components of the F.T. of QM(snm). For N' = N these eigenvalues are (B + A~k) , where Zk = ICkl2 and Ck == ~sC(s) exp( -2'TT~ks/N) is the F.T. of C(s). [This expression for the eigenvalues is obtained by rewriting Eqn. 4 as: QM(snm) = B8n_m.o + ~j.jC(snJQL(Sj)C(sm) ,and taking the F.T. of both sides.] Therefore R = (1/2)~k In[l + AJcZk/ B]. (7) We want to maximize R subject to ~sC(S)2 = 1, which is equivalent to ~Zk = N . Using the Lagrange multiplier method, we maximize A == R + 11-(~Zk - N) over all nonnegative {Zk}' Solving for (JA/ (JZk = 0 and requiring Zk ~ 0 for all k gives the solution: Zk = max[( -1/211-) - (B/Ak)' 0], (8) where (given B) 11- is chosen such that ~Zk = N. Note that while the optimal {Zk} are uniquely determined, the phases of the {ck} are completely arbitrary [except that since the {C(s)} are real, we must have Ck * = c_ k for all k]. The {C(s)} values are therefore not uniquely determined. Fig. 2a shows two of the solutions for .an example in which QL(S) = exp[ - (s/ So)2] with So = 6, N=N'=64, and B.:..:.l. Both solutions have ZO.±1 ..... ±6=5.417, 5.409, 5.378, 5.306, 5.134,4.689,3.376, and all other Zk == O. Setting all Ck phases to zero yields the solid curve; a particular random choice of phases yields the dotted dHve. We shall later see that imposing locality conditions on the {C(s)} (e.g., penalizing nonzero C(s) for large I s I) can remove the phase ambiguity. Our solution (Eqn. 8) can be described in terms of a so-called ""water-filling"" analogy: If one plots B /Ak versus k, then Zk is the depth of ""water"" at k when one ""pours"" into the ""vessel"" defined by the B / Ak curve a total quantity of ""water"" that corresponds to ~Zk = N and brings the ""water level"" to ( -1/211-). Let us contrast this problem with two other problems to which the ""water-filling"" analogy has been applied in the information-theory literature. In our notation, they are: 1. Given a transfer function {C(s)} and the noise variance B, how should a given total input signal power ~Ak be apportioned among the various wavenumbers k so as to maximize the information rate R [Gallager, 1968]? Our problem is complementary to this: we fix the input signal properties and seek an optimal transfer function subject to constraints. 190 Linsker 2. Rate-distortion (R-D) calculation [Berger, 1971]: Given a distortion measure (that defines a ""distance"" between the actual input signal and an estimate of it that can be reconstructed from the channel's output), and the input power spectrum p.k}, what choice of {Zk} minimizes the average distortion for given information rate (or minimizes the required rate for given distortion)? In the R-D problem there is a process of reconstruction, and a given measure for assessing the ""goodness"" of reconstruction. In contrast, in our network there is no reconstruction of the input signal, and no criterion of the ""goodness"" of such a hypothetical reconstruction is provided. Note also that infomax optimization is not the same as computing which channel (that is, which mapping !:L .... M) selected from an allowed set has the maximum information-theoretic capacity. In that problem, one is free to encode the inputs before transmission so as to make optimal use of (Le., ""achieve the capacity of"") the channel. In our case, there is no such pre-encoding; the input ensemble is prescribed (by the environment or by the output of an earlier processing stage) and we need to maximize the channel rate for that ensemble. The simplifying condition that N = N' (above) is unnecessarily restrictive. Eqn. 7 can be easily generalized to the case in which N is a mUltiple of N' and the N' M cells are uniformly spaced on the unit interval. Moreover, in the limit that 1/ N' is much smaller than the correlation length scale of QL, it can be shown that R is unchanged when we simultaneously increase N' and B by the same factor. (For example, two adjacent M cells each having noise variance 2B jointly convey the same information Figure 2. c c \ ,: \,/ (0) l ..-.' s (b) -10 Example infomax solutions C(s) for locally-correlated inputs: (a) Model A; region of nonnegligible C(s) extends over all s; phase ambiguity in Ck yields non unique C(s) solutions, two of which are shown. See text for details. (b) Models C (solid curve) and D (dotted curve) with Gaussian g(S)-l favoring short connections; shows center-surround receptive fields, more pronounced in Model D. (c) ""Temporal receptive field"" using Model D for temporally correlated scalar input to a single M cell; C(s) is the weight applied to the input signal that occurred s time steps ago. Spacing between ordinate marks is 0.1; ~ C(S)2 = 1 in each case. c Maximum Information Preservation to Linear Systems 191 about L as one M cell having noise variance B.) For biological applications we are mainly interested in cases in which there are many L cells [so that C(s) can be treated as a function of a continuous variable] and many M cells (so that the effect of the noise process is described by the single parameter B/ N). The analysis so far shows two limitations of Model A. First, the constraint ~iqi = 1 is quite arbitrary. (It certainly does not appear to be a biologically natural constraint to impose!) Second, for biological applications we are interested in predicting the favored values of {C(s)}, but the phase ambiguity prevents this. In the next section we show that a modified noise model leads naturally, without arbitrary constraints on ~iqi' to the same results derived above. We then turn to a model that favors local connections over long-range ones, and that resolves the phase ambiguity issue. Model B -- Independent noise on each input line In Model B of Fig. 1 each input Li to the nth M cell is corrupted by Li.d. Gaussian noise Vl1i of mean zero and variance B. The output is (9) Since each Vni is independent of all other noise terms (and of the inputs {Li}), we find (10) We may rewrite the last term as B~l1m (~iqy!2 (~jc;)l/2. The information rate is then R = (1/2) In DetWwhere (11) Define c' ni == Cl1i(~kqk)-1/2 ; then J¥,.m = ~lIm + (~,.jc'lIiQbC' mj)/ B. Note that this is identical (except for the replacement C ~ C') to the expression following Eqn. (5), in which QM was given by Eqn. (4). By definition, the {C' nil satisfy ~iC';i = 1 for all n. Therefore, the problem of maximizing R for this model (with no constraints on ~jq;) is identical to the problem we solved in the previous section. Model C -- Favoring of local connections Since the arborizations of biological cells tend to be spatially localized in many cases, we are led to consider constraints or cost terms that favor localization. There are various ways to implement this. Here we present a way of modifying the noise process so that the infomax principle itself favors localized solutions, without requiring additional terms unrelated to information transmission. Model C of Fig. 1 is the same as Model B, except that now the longer connections are ""noisier"" than the shorter ones. That is, the variance of VIIi is <V;i> = B~(sn;) where g(s) increases with 1 s I. [Equivalently, one could attenuate the signal on the (i ~ n) line by g(sll;) 1/2 and have the same noise variance Bo on all lines.] 192 Linsker This change causes the last term of Eqn. 10 to be replaced by Bo8I1m~g(SIl)qi . Under the conditions discussed earlier (Toeplitz QL and QM, and N = N), we derive (12) Recall that the {ck } are related to {C(s)} by a Fourier transform (see just before Eqn. 7). To cotppute which choice of IC(s)} maximizes R for a given problem, we used a gradient ascent algorithm several times, each time using a different random set of initial I C(s)} values. For the problems whose solutions are exhibited in Figs. 2b and 2c, multiple starting points usually yielded the same solution to within the error tolerance specified for the algorithm [apart from an arbitrary factor by which all of the C(s)'s can be multiplied without affecting R], and that solution had the largest R of any obtained for the given problem. That is, a limitation sometimes associated with gradient ascent algorithms -- namely, that they may yield multiple ""solutions"" that are local, but far from global, maxima -- did not appear to be a difficulty in these cases. Fig. 2b (solid curve) shows the infomax solution for an example having QL(S) = exp[ - (S/sO)2] and g(s) = exp[(s/s.)2] with So = 4, s. = 6, N = N = 32, and Bo = 0.1. There is a central excitatory peak flanked by shallow inhibitory sidelobes (and weaker additional oscillations). (As noted, the negative of this solution, having a central inhibitory region and excitatory sidelobes, gives the same R.) As Bo is increased (a range from 0.001 to 20 was studied), the peak broadens, the sidelobes become shallower (relative to the peak), and the receptive fields of nearby M cells increasingly overlap. This behavior is an example of the ""redundancy-diversity"" tradeoff discussed in [Linsker, 1988]. Model D -- Bounded output variance Our previous models all produce output values Mn whose variance is not explicitly constrained. More biologically realistic cells have limited output variance. For example, a cell's firing rate must lie between zero and some maximum value. Thus, the output of a model nonlinear cell is often taken to be a sigmoid function of (~iCII;L)· Within the context of linear cell models, we can capture the effect of a bounded output variance by using Model D of Fig. 1. We pass the intermediate output ~iClIi(Li + VIIi) through a gain control QC that normalizes the output variance to unity, then we add a final (Li.d. Gaussian) noise term V'II of variance R.. That is, (13) Without the last term, this model wo~ld be identical to Model C, since mUltiplying both the signal and the VIIi noise by the same factor GC would not affect R. The last term in effect fixes the number of output values that can be discriminated (Le., not confounded with each other by the noise process V'II) to be of order Rl1!2. The information rate for this model is derived to be (cf. Eqn. 12): Maximum Information Preservation to Linear Systems 193 (14) where V( C) is the variance of the intermediate output before it is passed through GC: (15) Fig. 2b (dotted curve) shows the infomax solution (numerically obtained as above) for the same QL(S) and g(s) functions and parameter values as were used to generate the solid curve (for Model C), but with the new parameter Bl = 0.4. The effect of the new Bl noise process in this case is to deepen the inhibitory sidelobes (relative to the central peak). The more pronounced center-surround character of the resulting M cell dampens the response of the cell to differences (between different input patterns) in the spatially uniform component of the input pattern. This response property allows the L .... M mapping to be info max-optimal when the dynamic range of the cells' output response is constrained.· (A competing effect can complicate the analysis: If Bl is increased much further, for example to 50 in the case discussed, the sidelobes move to larger s and become shallower. This behavior resembles that discussed at the end of the previous section for the case of increasing Bo; in the present case it is the overall noise level that is being increased when Bl increases and Bo is kept constant.) TemporaUy-correlated input patterns Let us see how infomax can be used to extract regularities in input time series, as contrasted with the spatially-correlated input patterns discussed above. We consider a single M cell that, at each discrete time denoted by n, can process inputs {LJ from earlier times i ~ n (via delay !ines, for example). We use the same Model D as before. There are two differences: First, we want g(s) = 00 for all s > 0 (input lines from future times are ""infinitely noisy""). [A technical point: Our use of periodic boundary conditions, while computationally convenient, means that the input value that will occur s time steps from now is the same value that occurred (N - s) steps ago. We deal with this by choosing g(s) to equal 1 at s = 0, to increase as s .... -N/2 (going into the past), and to increase further as s decreases from +N/2 to 1, corresponding to increasingly remote past times. The periodicity causes no unphysical effects, provided that we make g(s) increase rapidly enough (or make N large enough) so that C(s) is negligible for time intervals comparable to N.] Second, the fact that C,,; is a function only of s'"" is now a consequence of the constancy of connection weights C(s) of a single M cell with time, rather than merely a convenient Ansatz to facilitate the infomax computation for a set of many M cells (as it was in previous sections). The infomax solution is shown in Fig. 2c for an example having QL(S) = exp[ - (S/So)2]; g(s) = exp[ -t(s}/s.J with t(s} = s for s ~ 0 and t(s} = s - N for s ~ 1; So = 4, Sl = 6, N = 32, Bo = 0.1, and Bl = 0.4. The result is that the ""temporal receptive field"" of the M cell is excitatory for recent times, and 194 Linsker inhibitory for somewhat more remote times (with additional weaker oscillations). The cell's output can be viewed approximately as a linear combination of a smoothed input and a smoothed first time derivative of the input, just as the output of the center-surround cell of Fig. 2b can be viewed as a linear combination of a smoothed input and a smoothed second spatial derivative of the input. As in Fig. 2b, setting BI = 0 (not shown) lessens the relative inhibitory contribution. SUMMARY To gain insight into the operation of the principle of maximum information preservation, we have applied the principle to the problem of the optimal design of an array of linear filters under various conditions. The filter models that have been used are motivated by certain features that appear to be characteristic of biological networks. These features include the favoring of short connections and the constrained range of output signal values. When nearby input signals (in space or time) are correlated, the infomax-optimal solutions for the cases studied include (1) center-surround cells and (2) cells sensitive to temporal variations in input. The results of the mathematical analysis presented here apply also to arbitrary input covariance functions of the form QL( I i - j I). We have also presented more general expressions for the information rate, which can be used even when QL is not of this form. The cases discussed illustrate the operation of the infomax principle in some relatively simple but instructive situations. The analysis and results suggest how the principle may be applied to more biologically realistic networks and input ensembles. References T. Berger, Rate Distortion Theory (Prentice-Hall, Englewood Cliffs, N.J., 1971), chap. 4. R. G. Gallager, Information Theory and Reliable Communication (John Wiley and Sons, N.Y., 1968), p. 388. R. Linsker, in: Neural Information Processing Systems (Denver, Nov. 1987), ed. D. Z. Anderson (Amer. Inst. of Physics, N.Y.), pp. 485-494. R. Linsker, Computer 21 (3) 105-117 (March 1988). C. E. Shannon and W. Weaver, The Mathematical Theory of Communication (Univ. of Illinois Press, Urbana, 1949).","[-0.03355740010738373, -0.05124766752123833, 0.034716393798589706, -0.03171086311340332, 0.02163890190422535, 0.04512663185596466, 0.03231312707066536, -0.06704042106866837, 0.09120155870914459, -0.02716299518942833, -0.09170413017272949, 0.0945662185549736, 0.10543069988489151, -0.023238353431224823, -0.11915892362594604, 0.0562683641910553, 0.12169025838375092, 0.0628357008099556, -0.1579439342021942, -0.033724889159202576, 0.05520201101899147, -0.031688183546066284, -0.07544863969087601, 0.013909471221268177, 0.047369059175252914, -0.004970800597220659, -0.06851078569889069, -0.0331706702709198, 0.008247684687376022, -0.04637829214334488, 0.015469773672521114, -0.0578894279897213, 0.0894094780087471, -0.010298199020326138, -0.08908311277627945, -0.007829579524695873, -0.04632972180843353, 0.014956275001168251, 0.05118439346551895, -0.004231430124491453, -0.057272445410490036, 0.046118903905153275, -0.06212220340967178, 0.03295239061117172, 0.013627538457512856, 0.1009613648056984, 0.03269394859671593, -0.08315479010343552, -0.0400702990591526, -0.0002551809011492878, -0.09413959085941315, 0.02328404039144516, -0.0010079079074785113, 0.029589645564556122, 0.030035119503736496, 0.0027416148222982883, -0.03814173489809036, -0.020227469503879547, -0.0390162393450737, 0.013760998845100403, -0.0037888502702116966, 0.007777479011565447, -0.019411079585552216, 0.02708893083035946, 0.05956675857305527, 0.0354270413517952, -0.009707758203148842, 0.043540168553590775, 0.006608135066926479, 0.04039614275097847, -0.06954158842563629, 0.08606766909360886, -0.08045921474695206, -0.013711107894778252, 0.0823344960808754, 0.003004597034305334, 0.00037758206599391997, 0.02315732091665268, 0.056574515998363495, 0.051682181656360626, 0.049542494118213654, 0.01701672188937664, -0.004901268519461155, -0.04455485939979553, 0.039764534682035446, -0.04052411764860153, -0.03944467753171921, 0.04854974150657654, 0.09100186824798584, -0.05732753127813339, -0.11212896555662155, -0.034973058849573135, -0.05214836448431015, 0.06732495874166489, 0.011455926112830639, -0.034545112401247025, -0.00436109583824873, 0.01150531880557537, 0.1396160125732422, 0.04675887152552605, 0.007045110687613487, -0.04318031296133995, 0.06265284866094589, 0.029469266533851624, 0.0892312154173851, -0.08857634663581848, 0.03984229639172554, 0.02713020332157612, 0.020638663321733475, -0.0397498793900013, -0.04822586104273796, 0.011788240633904934, -0.0453784316778183, 0.04154511168599129, 0.07068246603012085, -0.011097369715571404, 0.07169554382562637, 0.06332313269376755, 0.025604642927646637, 0.00964485015720129, 0.010883664712309837, -0.08143100142478943, -0.03241993486881256, -0.006819605827331543, 0.05393831804394722, 0.05658429116010666, -0.025013601407408714, 2.3150415333834525e-33, -0.028118237853050232, -0.04142811521887779, -0.01447868999093771, -0.008822057396173477, 0.013613034039735794, -0.023786034435033798, -0.0299144946038723, -0.08834971487522125, 0.08852997422218323, 0.05039514601230621, -0.10997757315635681, 0.022694095969200134, 0.0010607377626001835, 0.0727519541978836, 0.07016915082931519, 0.017362454906105995, -0.03467358648777008, 0.06954649090766907, 0.022689659148454666, -0.14293023943901062, 0.03958921879529953, -0.00826902687549591, -0.0006336618098430336, -0.0793476551771164, 0.01532130129635334, -0.04675767198204994, 0.034178148955106735, -0.05726410821080208, -0.09059800952672958, -0.004586436320096254, -0.08093827217817307, 0.03907712548971176, 0.004823981784284115, -0.05216478928923607, 0.015714092180132866, 0.005137785337865353, 0.03280196711421013, 0.021180273965001106, 0.02540408819913864, -0.007513667922466993, 0.06570563465356827, 0.05102702975273132, -0.003506499109789729, -0.011852914467453957, -0.011971421539783478, -0.0244916919618845, -0.046758513897657394, 0.07359995692968369, -0.0682097002863884, -0.07947002351284027, -0.015924353152513504, -0.03982251137495041, 0.00916988030076027, -0.03587249666452408, -0.042251795530319214, 0.06101393327116966, -0.013461834751069546, 5.729322765546385e-06, -0.011133953928947449, 0.08175534754991531, -0.06142965331673622, 0.03727664425969124, 0.07675281167030334, 0.025565601885318756, 0.12507760524749756, 0.04629570245742798, -0.044451598078012466, -0.017970819026231766, 0.10507576167583466, -0.05793945491313934, 0.05198613181710243, 3.0953826808399754e-06, -0.0019540037028491497, -0.06912901252508163, 0.07692918926477432, -0.06665592640638351, 0.012076884508132935, -0.008470428176224232, -0.06578072160482407, 0.005892491899430752, -0.017564766108989716, -0.06447804719209671, -0.06454504281282425, -0.05532209202647209, -0.05796821787953377, 0.0463716983795166, 0.044584523886442184, -0.0074213179759681225, -0.0950678214430809, 0.01795634627342224, -0.008959805592894554, 0.0038621793501079082, 0.007568299304693937, -0.01789993792772293, -0.02400810644030571, -2.988620000213778e-33, 0.024829167872667313, -0.01961401104927063, -0.002008696785196662, 0.004678697790950537, 0.007499766536056995, 0.01053478941321373, -0.07627051323652267, -0.00410703057423234, -0.0647323876619339, 0.01042709406465292, -0.031070547178387642, -0.014785902574658394, 0.05068019777536392, 0.030588245019316673, -0.020512986928224564, 0.005537576507776976, -0.004148225300014019, 0.011879438534379005, 0.03826279938220978, -0.040573809295892715, -0.044503409415483475, -0.007690630853176117, -0.04009249806404114, 0.013314735144376755, 0.04140320047736168, 0.02817823737859726, -0.06187032535672188, 0.12082535773515701, 0.016223009675741196, -0.03201146051287651, -0.10573957860469818, -0.0659797191619873, -0.0011746621457859874, -0.015030103735625744, 0.00020915376080665737, 0.013147734105587006, 0.022824814543128014, -0.01233961433172226, 0.01947813481092453, 0.048948902636766434, 0.011322719976305962, 0.05601489543914795, -0.05022728070616722, -0.046019524335861206, 0.09256026148796082, -0.08112367242574692, -0.06775574386119843, 0.004979044198989868, -0.016011936590075493, -0.007826041430234909, 0.033653803169727325, 0.02910839021205902, -0.024531960487365723, -0.009989502839744091, -0.024961799383163452, 0.06527433544397354, -0.008015075698494911, 0.023863693699240685, 0.1104154884815216, 0.02006302960216999, -0.04701687768101692, -0.060930680483579636, -0.12676100432872772, 0.0463726781308651, 0.02205839939415455, 0.07564212381839752, 0.04040496423840523, 0.0024159885942935944, 0.054663337767124176, 0.02418234758079052, 0.03661584481596947, 0.002044645370915532, 0.011085211299359798, 0.018933886662125587, 0.037795890122652054, 0.04217842221260071, 0.013906752690672874, -0.02254563383758068, -0.048640307039022446, 0.039223190397024155, -0.11221081018447876, 0.026099223643541336, 0.06309638917446136, 0.02738894335925579, 0.02847122959792614, 0.02242695353925228, 0.03146045282483101, -0.05308152362704277, 0.06279969215393066, -0.1363103687763214, 0.015409206040203571, -0.009055059403181076, -0.08546745777130127, -0.04420478641986847, -0.08248085528612137, -5.83847494795009e-08, -0.015309148468077183, 0.029171524569392204, -0.0631590187549591, 0.009480168111622334, 0.05234108865261078, -0.03262081369757652, 0.05731252580881119, 0.026247570291161537, 0.0008501387201249599, -0.009512282907962799, 0.12051624804735184, 0.04433049261569977, -0.02737848460674286, 0.00027468870393931866, 0.014096606522798538, -0.020383521914482117, -0.01360638439655304, -0.046856366097927094, 0.008343140594661236, -0.04778964817523956, 0.03984260931611061, -0.03151089698076248, -0.04935441166162491, 0.08889187127351761, 0.08844861388206482, 0.02253505401313305, 0.04314694553613663, 0.03232244774699211, 0.07355187833309174, 0.07688070088624954, -0.03067968226969242, 0.06376784294843674, 0.021674182265996933, 0.016690555959939957, 0.04109025001525879, 0.021160444244742393, 0.00016088578558992594, -0.04373522102832794, -0.060964543372392654, -0.005143987014889717, -0.043797507882118225, -0.003001944860443473, -0.05731567367911339, -0.020436318591237068, 0.13194023072719574, 0.022408941760659218, 0.038310740143060684, -0.09671809524297714, 0.03965428099036217, -0.0016839071176946163, 0.055910829454660416, 0.01915081776678562, 0.007615566719323397, 0.029450509697198868, 0.030615953728556633, -0.05298018455505371, -0.007604902144521475, -0.03941475227475166, 0.005943290423601866, -0.019366439431905746, 0.0037586914841085672, 0.08999071270227432, -0.0963410884141922, -0.06939121335744858]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\CricketWindDetection.pdf,Deep Learning,"TRAINING A LIMITED-INTERCONNECT, SYNTHETIC NEURAL IC M.R. Walker. S. Haghighi. A. Afghan. and L.A. Akers Center for Solid State Electronics Research Arizona State University Tempe. AZ 85287-6206 mwalker@enuxha.eas.asu.edu ABSTRACT Hardware implementation of neuromorphic algorithms is hampered by high degrees of connectivity. Functionally equivalent feedforward networks may be formed by using limited fan-in nodes and additional layers. but this complicates procedures for determining weight magnitudes. No direct mapping of weights exists between fully and limited-interconnect nets. Low-level nonlinearities prevent the formation of internal representations of widely separated spatial features and the use of gradient descent methods to minimize output error is hampered by error magnitude dissipation. The judicious use of linear summations or collection units is proposed as a solution. HARDWARE IMPLEMENTATIONS OF FEEDFORWARD, SYNTHETIC NEURAL SYSTEMS The pursuit of hardware implementations of artificial neural network models is motivated by the need to develop systems which are capable of executing neuromorphic algorithms in real time. The most significant barrier is the high degree of connectivity required between the processing elements. Current interconnect technology does not support the direct implementation of large-scale arrays of this type. In particular. the high fan-in/fan-outs of biology impose connectivity requirements such that the electronic implementation of a highly interconnected biological neural networks of just a few thousand neurons would require a level of connectivity which exceeds the current or even projected interconnection density ofULSI systems (Akers et al. 1988). Highly layered. limited-interconnected architectures are however. especially well suited for VLSI implementations. In previous works. we analyzed the generalization and fault-tolerance characteristics of a limited-interconnect perceptron architecture applied in three simple mappings between binary input space and binary output space and proposed a CMOS architecture (Akers and Walker. 1988). This paper concentrates on developing an understanding of the limitations on layered neural network architectures imposed by hardware implementation and a proposed solution. 777 778 Walker, Haghighi, Afghan and Akers TRAINING CONSIDERATIONS FOR LIMITED .. INTERCONNECT FEEDFORWARD NETWORKS The symbolic layout of the limited fan-in network is shown in Fig. 1. Re-arranging of the individual input components is done to eliminate edge effects. Greater detail on the actual hardware architecture may be found in (Akers and Walker, 1988) As in linear filters, the total number of connections which fan-in to a given processing element determines the degrees of freedom available for forming a hypersurface which implements the desired node output function (Widrow and Stearns, 1985). When processing elements with fixed, low fan-in are employed, the affects of reduced degrees of freedom must be considered in order to develop workable training methods which permit generalization of novel inputs. First. no direct or indirect relation exists between weight magnitudes obtained for a limited-interconnect, multilayered perceptron, and those obtained for the fully connected case. Networks of these types adapted with identical exemplar sets must therefore fonn completely different functions on the input space. Second, low-level nonlinearities prevent direct internal coding of widely separated spatial features in the input set. A related problem arises when hyperplane nonlinearities are used. Multiple hyperplanes required on a subset of input space are impossible when no two second level nodes address identical positions in the input space. Finally, adaptation methods like backpropagation which minimize output error with gradient descent are hindered since the magnitude of the error is dissipated as it back-propagates through large numbers of hidden layers. The appropriate placement of linear summation elements or collection units is a proposed solution. 1 2 3 4 5 12 11 10 9 6 7 8 Figure 1. Symbolic Layout of Limited-Interconnect Feedforward Architecture Training a Limited-Interconnect, Synthetic Neural Ie 779 COMPARISON OF WEIGHT VALVES IN FULLY CONNECTED AND LIMITED-INTERCONNECT NETWORKS Fully connected and limited-interconnect feedforward structures may be functionally equivalent by virtue of identical training sets, but nonlinear node discriminant functions in a fully-connected perceptron network are generally not equivalent to those in a limited-interconnect, multilayered network. This may be shown by comparing the Taylor series expansion of the discriminant functions in the vicinity of the threshold for both types and then equating terms of equivalent order. A simple limited-interconnect network is shown in Fig. 2. x1 x2 y3 x3 x4 Figure 2. Limited-Interconnect Feedforward Network A discriminant function with a fan-in of two may be represented with the following functional form, where e is the threshold and the function is assumed to be continuously differentiable. The Taylor series expansion of the discriminant is, Expanding output node three in Fig. 2 to second order, where fee), fee) and f'(e) are constant terms. Substituting similar expansions for Yl and Y2 into Y3 yields the expression, 780 Walker, Haghighi, Afghan and Akers The output node in the fully-connected case may also be expanded, x1 x2 __ ~y3 x3 x4 Figure 3. Fully Connected Network where Expanding to second order yields, We seek the necessary and sufficient conditions for the two nonlinear discriminant functions to be analytically equivalent. This is accomplished by comparing terms of equal order in the expansions of each output node in the two nets. Equating the constant terms yields, w =-w 5 6 Equating the fIrst order terms, W =W = 1 5 6 f(9) Equating the second order terms, Training a Limited-Interconnect, Synthetic Neural Ie 781 The ftrst two conditions are obviously contradictory. In addition, solving for w5 or w6 using the ftrst and second constraints or the frrst and third constraints yields the trivial result, w5=w6=O. Thus, no relation exists between discriminant functions occurring in the limited and fully connected feedforward networks. This eliminates the possibility that weights obtained for a fully connected network could be transformed and used in a limited-interconnect structure. More signiftcant is the fact that full and limited interconnect nets which are adapted with identical sets of exemplars must form completely different functions on the input space, even though they exhibit identical output behavior. For this reason, it is anticipated that the two network types could produce different responses to a novel input. NON-OVERLAPPING INPUT SUBSETS Signal routing becomes important for networks in which hidden units do not address identical subsets in the proceeding layer. Figure 4 shows an odd-parity algorithm implemented with a limited-interconnect architecture. Large weight magnitudes are indicated by darker lines. Many nodes act as ""pass-through"" elements in that they have few dominant input and output connections. These node types are necessary to pass lower level signals to common aggregation points. In general, the use of limited fan-in processing elements implementing a nonlinear discriminant function decreases the probability that a given correlation within the input data will be encoded, especially if the ""width"" of the feature set is greater than the fan-in, requiring encoding at a high level within the net. In addition, since lower-level connections determine the magnitudes of upper level connections in any layered net when baclcpropagation is used, the set of points in weight space available to a limited-interconnect net for realizing a given function is further reduced by the greater number of weight dependencies occurring in limited-interconnect networks, all of which must be satisfted during training. Finally, since gradient descent is basically a shortcut through an NP-complete search in weight space, reduced redundancy and overlapping of internal representations reduces the probability of convergence to a near-optimal solution on the training set. DISSIPATION OF ERROR MAGNITUDE WITH INCREASING NUMBERS OF LAYERS Following the derivation of backpropagation in (plaut, 1986), the magnitude change for a weight connecting a processing element in the m-Iayer with a processing element in the I-layer is given by, where 782 Walker, Haghighi, Afghan and Akers Figure 4. Six-input odd parity function implemented with limited-interconnect then [ f [f [f dYa ] dy j ] dy 1 dy = ~ L··· L(ya-da) dx wb-a • ""-d wk_, _k W ~ i..J '1 a=l a x, ) dx l-k dx m k=l J= ) k 1 Where y is the output of the discriminant function, x is the activation level, w is a connection magnitude, and f is the fan-in for each processing element. If N layers of elements intervene between the m-layer and the output layer, then each of the f (N-l) tenns in the above summation consists of the product, Training a Limited-Interconnect, Synthetic Neural Ie 783 dy. ) Wb •• '-d -a x j If we replace the weight magnitudes and the derivatives in each tenn with their mean values, The value of the first derivative of the sigmoid discriminant function is distributed between 0.0 and 0.5. The weight values are typically initially distrlbuted evenly between small positive and negative values. Thus with more layers. the product of the derivatives occurring in each tenn approaches zero. The use of large numbers of perceptron layers therefore has the affect of dissipating the magnitude of the error. This is exacerbated by the low fan-in, which reduces the total number of tenns in the summation. The use of linear collection units (McClelland. 1986), discussed in the following section, is a proposed solution to this problem. LINEAR COLLECTION UNITS As shown in Fig. 5, the output of the limited-interconnect net employing collection units is given by the function, x1 [::>linear summation o non-linear discriminant x2 y3 x3 x4 Figure S. Limited-interconnect network employing linear summations where c 1 and c2 are constants. The position of the summations may be determined by using euclidian k-means clustering on the exemplar set to a priori locate cluster centers 784 Walker, Haghighi, Afghan and Akers and determine their widths (Duda and Hart, 1973). The cluster members would be combined using linear elements until they reached a nonlinear discrminant, located higher in the net and at the cluster center. With this arrangement, weights obtained for a fully-connected net could be mapped using a linear transformation into the limited-interconnect network. Alternatively, backpropagation could be used since error dissipation would be reduced by setting the linear constant c of the summation elements to arbitrarily large values. CONCLUSIONS No direct transformation of weights exists between fully and limited interconnect nets which employ nonlinear discrmiminant functions. The use of gradient descent methods to minimize output error is hampered by error magnitude dissipation. In addition, low-level nonlinearities prevent the formation of internal representations of widely separated spatial features. The use of strategically placed linear summations or collection units is proposed as a means of overcoming difficulties in determining weight values in limited-interconnect perceptron architectures. K-means clustering is proposed as the method for determining placement. References L.A. Akers, M.R. Walker, O.K. Ferry & R.O. Grondin, ""Limited Interconnectivity in Synthetic Neural Systems,"" in R. Eckmiller and C. v.d. Malsburg eds., Neural Computers. Springer-Verlag, 1988. L.A. Akers & M.R. Walker, ""A Limited-Interconnect Synthetic Neural IC,"" Proceedings of the IEEE International Conference on Neural Networks, p. 11-151,1988. B. Widrow & S.D. Stearns, Adaptive Signal Processing. Prentice-Hall, 1985. D.C. Plaut, SJ. Nowlan & G.E. Hinton, ""Experiments on Learning by Back Propagation,"" Carnegie-Mellon University, Dept. of Computer Science Technical Report, June, 1986. J.L. McClelland, ""Resource Requirements of Standard and Programmable Nets,"" in D.E. Rummelhart and J.L. McClelland eds., Parallel Distributed Processing - Volume 1: Foundations. MIT Press, 1986. R.O. Duda & P.E. Hart. Pattern Classification and Scene Analysis. Wiley, 1973.","[-0.07673599570989609, -0.041610464453697205, 0.019184226170182228, 0.0456412248313427, -0.025284070521593094, -0.023395270109176636, -0.037959709763526917, -0.011087615974247456, -0.011598451063036919, -0.052363160997629166, 0.011547455564141273, -0.08114855736494064, -0.008282328024506569, 0.0004997272044420242, -0.11581333726644516, 0.03808996453881264, 0.052270326763391495, 0.0922423005104065, -0.0509689524769783, -0.0013383091427385807, 0.04110664501786232, -0.027835816144943237, -0.0893363505601883, -0.011269408278167248, 0.07320930808782578, 0.0047226580791175365, -0.00354097131639719, -0.02409033104777336, 0.018938275054097176, 0.025083418935537338, 0.06320735067129135, -0.1022762730717659, -0.07132647931575775, 0.04548453539609909, -0.075655497610569, -0.0016392326215282083, -0.10721749812364578, -0.03678534924983978, 0.011386742815375328, -0.020359667018055916, 0.014569963328540325, 0.012240716256201267, 0.06900287419557571, 0.014574708417057991, 0.08246321231126785, 0.059005483984947205, -0.037771180272102356, -0.047143179923295975, -0.011889157816767693, -0.006700023077428341, -0.037396326661109924, 0.019741181284189224, -0.0001786908833310008, 0.08847913146018982, 0.03055129200220108, 0.04526196047663689, 0.0689917653799057, 0.026567576453089714, -0.0746706873178482, 0.05534710735082626, 0.052414920181035995, -0.06352301687002182, 0.05344521999359131, -0.08063430339097977, -0.027437724173069, 0.021638983860611916, 0.03218022361397743, 0.04047982022166252, 0.0733863115310669, -0.02057965099811554, 0.05601879954338074, 0.0903552770614624, -0.021818920969963074, -0.04776402935385704, 0.037750259041786194, 0.029411472380161285, 0.10308888554573059, 0.06237790733575821, 0.04776664450764656, -0.05156980827450752, 0.01340176910161972, 0.036184921860694885, 0.006944461725652218, -0.060075078159570694, 0.09926381707191467, -0.0013463144423440099, -0.045305803418159485, 0.060150619596242905, -0.011710330843925476, 0.0024099599104374647, 0.008180092088878155, 0.027400953695178032, -0.04247736558318138, -0.055439382791519165, 0.07328332215547562, -0.02718425914645195, -0.018825244158506393, 0.003783813677728176, -0.05804033577442169, 0.08097761124372482, -0.0231279619038105, 0.04483110457658768, 0.012632510624825954, -0.008846680633723736, 0.011826874688267708, 0.07869969308376312, 0.035754695534706116, 0.017567038536071777, -0.004869063850492239, -0.0914522111415863, -0.06844531744718552, 0.021128227934241295, -0.017137108370661736, -0.011522513814270496, 0.004926734603941441, -0.03532656654715538, 0.05246662721037865, -0.0035240750294178724, 0.07374827563762665, 0.0413496159017086, -0.12297897040843964, -0.005574885290116072, -0.10714031755924225, 0.04500074312090874, 0.030421795323491096, -0.044752463698387146, -0.16261200606822968, 5.775450966745556e-33, -0.08538417518138885, 0.04307200014591217, 0.009842210449278355, -0.041841279715299606, -0.03719963878393173, -0.053080614656209946, 0.0196748785674572, 0.015311750583350658, 0.0011714218417182565, 0.022938387468457222, -0.06981830298900604, 0.015286975540220737, -0.02496444620192051, 0.0933418944478035, 0.08963846415281296, -0.06530027091503143, 0.052771806716918945, -0.08546333760023117, 0.09415826946496964, -0.06485658884048462, 0.0019195894710719585, -0.06986051797866821, 0.012542741373181343, 0.0025343371089547873, -0.0018457515398040414, -0.03157024085521698, -0.001486064400523901, 0.021161185577511787, -0.08607650548219681, 0.00424903305247426, -0.005932603031396866, 0.03771224617958069, -0.017525726929306984, -0.04946911334991455, 0.050895873457193375, -0.05121008679270744, 0.06101366505026817, -0.022599713876843452, 0.06362037360668182, 0.0003130834084004164, 0.06512873619794846, 0.05022164434194565, -0.00029844100936315954, 0.014103831723332405, -0.11249007284641266, 0.03966242074966431, 0.016661711037158966, 0.07497818022966385, 0.018734848126769066, -0.024447035044431686, -0.017799336463212967, 0.014319440349936485, -0.027912816032767296, -0.04649889096617699, 0.07232445478439331, -0.04932297021150589, -0.0278183501213789, 0.08390545099973679, 0.14745332300662994, 0.1122022196650505, -0.08298010379076004, -0.023257587105035782, 0.01340408343821764, 0.06821435689926147, -0.02211478166282177, 0.044779907912015915, -0.025877945125102997, 0.02465016394853592, 0.03507855907082558, -0.05759543552994728, -0.005106078460812569, 0.06203344836831093, 0.00857181940227747, -0.029008451849222183, 0.049940213561058044, -0.017514225095510483, -0.012452214024960995, -0.15940725803375244, -0.04720764979720116, 0.029433324933052063, 0.00041872411384247243, 0.07603153586387634, -0.022739050909876823, -0.00957153458148241, -0.017536887899041176, -0.020188746973872185, 0.04455200582742691, 0.03048083931207657, 0.05944553390145302, -0.03460795432329178, -0.07825462520122528, -0.046862829476594925, 0.05549486726522446, -0.03087764047086239, -0.03932105004787445, -4.946679694957841e-33, -0.04045538604259491, 0.05959590896964073, -0.07845613360404968, -0.007134916726499796, -0.038180772215127945, 0.00935857929289341, 0.013134298846125603, -0.037754423916339874, 0.006436169613152742, 0.042645420879125595, 0.026040861383080482, -0.002473522676154971, 0.021660158410668373, -0.020535917952656746, 0.04049292951822281, 0.0038954587653279305, -0.1251060664653778, -0.054948557168245316, 0.12001246213912964, -0.029336310923099518, 0.0072517250664532185, 0.05196934565901756, -0.06519320607185364, -0.004447954706847668, -0.050027091056108475, 0.017214886844158173, -0.08679793775081635, 0.07596045732498169, -0.08384533226490021, 0.03770169988274574, -0.011903418228030205, 0.004042956046760082, 0.007094457279890776, 0.07154764235019684, 0.0018197044264525175, 0.10454349964857101, 0.06494631618261337, -0.07499157637357712, 0.042624082416296005, -0.04811430349946022, 0.03915497288107872, 0.023098742589354515, -0.03086443617939949, 0.009803754277527332, -0.02178216353058815, -0.04396948963403702, -0.030548376962542534, -0.024823691695928574, 0.011411777697503567, 0.0181123074144125, 9.792942000785843e-05, -0.05285698175430298, -0.06541179120540619, -0.02641463279724121, 0.023028340190649033, 0.023252159357070923, -0.05100773274898529, 0.04863348230719566, 0.05610833689570427, -0.08096946030855179, 0.028256086632609367, -0.12327250093221664, -0.013948000967502594, -0.059622496366500854, -0.022144127637147903, -0.017855411395430565, 0.036517996340990067, 0.04425659403204918, 0.06874334812164307, 0.01917210780084133, -0.003987087868154049, 0.0880933329463005, 0.14493542909622192, 0.015800602734088898, -0.06930123269557953, -0.059515055269002914, 0.04267135635018349, 0.01690046116709709, -0.003842985723167658, -0.01593061350286007, -0.032692618668079376, -0.001079604378901422, 0.03418654948472977, -0.012059583328664303, 0.08036667853593826, 0.06446583569049835, -0.007969943806529045, 0.0363011434674263, 0.008671088144183159, -0.034017447382211685, -0.005676978267729282, 0.08484939485788345, 0.029669981449842453, 0.03304949402809143, -0.05799797549843788, -5.216516285599937e-08, 0.03228190541267395, -0.07978973537683487, -0.011558928526937962, 0.00033165703644044697, 0.05367926508188248, -0.05332394316792488, 0.07053954154253006, -0.07233132421970367, -0.038291554898023605, -0.023370210081338882, 0.0650520846247673, -0.0709783285856247, -0.0132142324000597, -0.0478367805480957, 0.03254837542772293, 0.04077458754181862, 0.020058337599039078, -0.01085952389985323, 0.03064998798072338, -0.027276882901787758, -0.023675648495554924, 0.04786660522222519, -0.004489172715693712, 0.05177783966064453, 0.07263029366731644, -0.08013366162776947, -0.0793970450758934, 0.008041299879550934, 0.0007449855329468846, 0.06714660674333572, -0.00861753523349762, -0.017916632816195488, 0.011824040673673153, 0.02383657917380333, 0.11456985026597977, 0.05323401838541031, 0.033982712775468826, 0.059568069875240326, -0.02259407564997673, -0.049216561019420624, -0.0342242456972599, 0.017884301021695137, -0.014929080381989479, -0.014498640783131123, 0.0711522176861763, -0.05433119460940361, -0.005012540612369776, -0.062404878437519073, -0.008159744553267956, 0.036775052547454834, 0.042292140424251556, 0.07393724471330643, -0.03376607224345207, 0.0727473720908165, 0.010414971970021725, -0.034683406352996826, -0.015009002760052681, -0.045329201966524124, -0.017367752268910408, 0.021627290174365044, -0.021915718913078308, -0.03566056117415428, -0.06963739544153214, -0.05566912144422531]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\DigitalRealisationofSelfOrganisingMaps.pdf,Deep Learning,"2 CONSTRAINTS ON ADAPTIVE NETWORKS FOR MODELING HUMAN GENERALIZATION M. Pavel Mark A. Gluck Departm£1Il of Psychology Stanford University Stanford. CA 94305 ABSTRACT Van Henkle The potential of adaptive networks to learn categorization rules and to model human performance is studied by comparing how natural and artificial systems respond to new inputs, i.e., how they generalize. Like humans, networks can learn a detenninistic categorization task by a variety of alternative individual solutions. An analysis of the con- straints imposed by using networks with the minimal number of hidden units shows that this ""minimal configuration"" constraint is not sufficient to explain and predict human performance; only a few solu- tions were found to be shared by both humans and minimal adaptive networks. A further analysis of human and network generalizations indicates that initial conditions may provide important constraints on generalization. A new technique, which we call ""reversed learning"", is described for finding appropriate initial conditions. INTRODUCTION We are investigating the potential of adaptive networks to learn categorization tasks and to model human performance. In particular we have studied how both natural and artificial systems respond to new inputs, that is, how they generalize. In this paper we first describe a computational technique to analyze generalizations by adaptive networks. For a given network structure and a given classification problem, the technique enumerates all possible network solutions to the problem. We then report the results of an empirical study of human categorization learning. The generalizations of human sub- jects are compared to those of adaptive networks. A cluster analysis of both human and network generalizations indicates, significant differences between human perfonnance and possible network behaviors. Finally, we examine the role of the initial state of a net- work for biasing the solutions found by the network. Using data on the relations between human subjects' initial and final performance during training, we develop a new tech- nique, called ""reversed learning"", which shows some potential for modeling human learning processes using adaptive networks. The scope of our analyses is limited to gen- eralizations in deterministic pattern classification (categorization) tasks. Modeling Human Generalization 3 The basic difficulty in generalization is that there exist many different classification rules (""solutions"") that that correctly classify the training set but which categorize novel objects differently. The number and diversity of possible solutions depend on the language defining the pattern recognizer. However, additional constraints can be used in conjunction with many types of pattern categorizers to eliminate some, hopefully undesirable, solutions. One typical way of introducing additional constraints is to minimize the representation. For example minimizing the number of equations and parameters in a mathematical expression, or the number of rules in a rule-based system would assure that some identification maps would not be computable. In the case of adaptive networks, minimiz- ing the size of adaptive networks, which reduces the number of possible encoded func- tions, may result in improved generalization perfonnance (Rumelhart, 1988). The critical theoretical and applied questions in pattern recognition involve characteriza- tion and implementation of desirable constraints. In the first part of this paper we describe an analysis of adaptive networks that characterizes the solution space for any particular problem. ANALYSES OF ADAPTIVE NETWORKS Feed-forward adaptive networks considered in this paper will be defined as directed graphs with linear threshold units (LTV) as nodes and with edges labeled by real-valued weights. The output or activations of a unit is detennined by a monotonic nonlinear func- tion of a weighted sum of the activation of all units whose edges tenninate on that unit There are three types of units within a feed-forward layered architecture: (1) Input units whose activity is determined by external input; (2) output units whose activity is taken as the response; and (3) the remaining units, called hidden units. For the sake of simplicity our discussion will be limited to objects represented by binary valued vectors. A fully connected feed-forward network with an unlimited number of hidden units can compute any boolean function. Such a general network, therefore, provides no con- straints on the solutions. Therefore, additional constraints must be imposed for the net- work to prefer one generalization over another. One such constraint is minimizing the size of the network. In order to explore the effect of minimizing the number of hidden units we first identify the minimal network architecture and then examine its generaliza- tions. Most of the results in this area have been limited to finding bounds on the expected number of possible patterns that could be classified by a given network (e.g. Cover, 1965; Volper and Hampson, 1987; Valiant, 1984; Baum & Haussler, 1989). The bounds found by these researchers hold for all possible categorizations and are, therefore, too broad to be useful for the analysis of particular categorization problems. To determine the generalization behavior for a particular network architecture, a specific 4 Gluck, Pavel and Henkle categorization problem and a training set it is necessary to find find all possible solutions and the corresponding generalizations. To do this we used a computational (not a simu- lation) procedure developed by Pavel and Moore (1988) for finding minimal networks solving specific categorization problems. Pavel and Moore (1988) defined two network solutions to be different if at least one hidden unit categorized at least one object in the training set differently. Using this definition their algorithm finds all possible different solutions. Because finding network solutions is NP-complete (Judd, 1987), for larger problems Pavel and Moore used a probabilistic version of the algorithm to estimate the distribution of generalization responses. One way to characterize the constraints on generalization is in terms of the number of possible solutions. A larger number of possible solutions indicates that generalizations will be less predictable. The critical result of the analysis is that, even for minimal net- works. the number of different network solutions is often quite large. Moreover. the number of solutions increases rapidly with increases in the number of hidden units. The apparent lack of constraints can also be demonstrated by finding the probability that a network with a randomly selected hidden layer can solve a given categorization problem. That is, suppose that we se~t n different hidden units, each unit representing a linear discriminant fwction. The activations of these random hidden wits can be viewed as a ttansformation of the input patterns. We can ask what is the probability that an output unit can be found to perfonn the desired dichotomization. A typical example of a result of this analysis is shown in Figure 1 for the three-dimensional (3~) parity problem. In the minimal configuration involving three hidden units there were 62 different solutions to the 3D parity problem. The rapid increase in probability (high slope of the curve in Figure 1) indicates that adding a few more hidden units rapidly increases the probability that a random hidden layer will solve the 3D parity problem. 100 ...... -. ~. , , 10 "" , , • , z II g 80 , , !; , , -' , i 40 , , ~ , , ---- EXPERIMENT , 20 , -- 3D PARITY ~ , , , 0 0 2 4 6 • 10 12 HIOOENUNITS Figure 1 1be proportion of solutions to 3D parity problem (solid line) and the experimental task (dashed line) as a function of the number of hidden units. The results of a more detailed analysis of the generalization performance of the minimal networks will be discussed following a description of a categorization experiment with Modeling Human Generalization 5 human subjects. HUMAN CA TEGORIZA TION EXPERIMENT In this experiment human subjects learned to categorize objects which were defined by four dimensional binary vectors. Of the 24 possible objects, subjects were trained to clas- sify a subset of 8 objects into two categories of 4 objects each. The specific assignments of objects into categories was patterned after Medin et aI. (1982) and is shown in Figure 2. Eight of the patterns are designated as a training set and the remaining eight comprise the test seL The assignment of the patterns in the training set into two categories was such that there were many combinations of rules that could be used to correctly perfonn the categorization. For example, the first two dimensions could be used with one other dimension. The training patterns could also be categorized on the basis of an exclusive or (XOR) of the last two dimensions. The type of solution obtained by a human subject could only be determined by examining responses to the test set as well as the training seL TRAINING SET TEST SET X1 1 1 0 1 001 0 000 1 1 1 0 1 DIMENSIONS ~ 1 1 1 0 000 1 001 0 1 1 1 0 ~ 101 0 101 0 o 1 0 1 o 1 0 1 X. 101 0 o 1 0 1 o 1 0 1 o 1 0 1 CATEGORY AAAA BBBB ??? ? ???? FigllTe 2. PattemI to be clulmed. (Adapted from Medin et aI .• 1982). In the actual experiments, subjects were asked to perform a medical diagnosis for each pattern of four symptoms (dimensions). The experimental procedure will be described here only briefly because the details of this experiment have been described elsewhere in detail (pavel, Gluck, Henkle, 1988). Each of the patterns was presented serially in a ran- domized order. Subjects responded with one of the categories and then received feed- back. The training of each individual continued until he reached a criterion (responding correctly to 32 consecutive stimuli) or until each pattern had been presented 32 times. The data reported here is based on 78 subjects, half (39) who learned the task to criterion and half who did DOL Following the training phase, subjects were tested using all 16 possible patterns. The results of the test phase enabled us to determine the generalizations perfonned by the subjects. Subjects' generalizations were used to estimate the ""functions"" that they may have been using. For example, of the 39 criterion subjects, 15 used a solution that was consistent with the exclusive-or (XOR) of the dimensions x 3 and X4. We use ""response profiles"" to graph responses for an ensemble of functions, in this case for a group of subjects. A response profile represents the probability of assigning each 6 Gluck, Pavel and Henkle /I) z a:: loll ~ C ~ pattern to category ""A"". For example, the response profile for the XOR solution is shown in Figure 3A. For convenience we define the responses to the test set as the ""gen- eralization profile"". The response profile of all subjects who reached the criterion is shown in Figure 3D. The responses of our criterion subjects to the training set were basi- cally identical and correct The distribution of subjects' genezalization profiles reflected in the overall generalization profile are indicative of considerable individual differences 1001 0110 1101 1110 1011 0100 0011 0000 0101 1010 0001 0010 1000 0111 1100 1111 00 02 04 06 08 10 12 PROPORTION "" .. - /I) z a:: loll ~ C ~ 1001 0110 1101 1110 1011~ 0100 -=:===::-- 0011~ 0000 r--- 0101 1010 0001 0010 1000 0111 1100 1111 . 00 02 04 06 01 10 12 PROPORTION "" It.- Figwe 3. (A) Response profile of the XOR solution. and (B) a proportion of the response ""A"" to all patterns for human subjects (dark bars) and minimal networks (light bars). The lower 8 patterns are from the training set and the upper 8 patterns from the test set. MODEliNG THE RESPONSE PROFILE One of our goals is to model subjects' distribution of categorizations as represented by the response profile in Figure 3D. We considered three natural approaches to such modeling: (1) Statistical/proximity models, (2) Minimal disjunctive normal forms (DNF), and (3) Minimal two-layer networks. The statistical approach is based on the assumption that the response profile over subjects represents the probability of categorizations performed by each subject Our data are not consistent with that assumption because each subject appeared to behave deterministi- cally. The second approach, using the minimal DNF is also not a good candidate because there are only four such solutions and the response profile over those solutions differs considerably from that of the SUbjects. Turning to the adaptive network solutions, we found all the solutions using the linear programming technique described above (pavel & Moore, 1988). The minimal two-layer adaptive network that was capable of solving the training set problem consisted of two hidden units. The proportion of solutions as a Modeling Human Generalization 7 function of the number of hidden units is shown in Figure 1 by the dashed line. For the minimal network there were 18 different solutions. These 18 solutions had 8 dif- ferent individual generalization profiles. Assuming that each of the 18 network solution is equally likely. we computed the generalization profile for minimal network shown in Figure 3B. The response profile for the minimal network represents the probability that a randomly selected minimal network will assign a given pattern to category ""A"". Even without statistical testing we can conclude that the generalization profiles for humans and networks are quite different. It is possible. however. that humans and minimal networks obtain similar solutions and that the differences in the average responses are due to the particular statistical sampling assumption used for the minimal networks (i.e. each solu- tion is equally likely). In order to determine the overlap of solutions we examined the generalization profiles in more detail. CLUSTERING ANALYSIS OF GENERALIZATION PROFILES To analyze the similarity in solutions we defined a metric on generalization profiles. The Hamming distance between two profiles is equal to the number of patterns that are categorized differently. For example. the distance between generalization profile •• A A B A B B B B"" and ""A A B B B B A B"" is equal to two. because the two profiles differ on only the fourth and seventh pattern. Figure 4 shows the results of a cluster analysis using a hierarchical clustering procedure that maximizes the average distance between clusters. o c • • c c • • c c ~ • • • • • • • • • ; ~ • c ! c • • ~ c ~ • ~ • • = ~ • ~ ~ ~ I c • • • • • 3 c c c • • • • c • ~ • • • • • I • • • • • • • • • Figlll'll 4. Results of hierarchical clustering for human (left) and network (right) generalization profiles. • • c • • • c • • 3 c • c In this graph the average distance between any two clusters is shown by the value of the lowest common node in the tree. The clustering analysis indicates that humans and 8 Gluck, Pavel and Henkle networks obtained widely different generalization profiles. Only three generalization profiles were found to be common to human and networks. This number of common generalizations is to be expected by chance if the human and network solutions are independent Thus, even if there exists a learning algorithm that approximates the human probability distribution of responses, the minimal network would not be a good model of human perfonnance in this task. It is clear from the previously described network analysis that somewhat larger networks with different constraints could account for human solutions. In order to characterize the additional constraints, we examined subjects' individual strategies to find out why indivi- dual subjects obtained different solutions. ANALYSIS OF HUMAN LEARNING STRATEGIES Human learning strategies that lead to preferences for particular solutions may best be modeled in networks by imposing constraints and providing hints (Abu-Mostafa 1989). These include choosing the network architecture and a learning rule, constraining con- nectivity, and specifying initial conditions. We will focus on the specification of initial conditions. 30 20 10 o CI .. CONSISTENT • CONSISTENT lOR NON lOR SUBJECT TYPES NO CRrTERION FiglU'e 5. The number of consistent or non-stable responses (black) and the nwnber of stable incorrect responses (light) for XOR, Non-XOR criterion su~ jeers, and for those who never reached criterion. Our effort to examine initial conditions was motivated by large differences in learning curves (Pavel et al., 1988) between subjects who obtained the XOR solutions and those who did not The subjects who did not obtain the XOR solutions would perfonn much better on some patterns (e.g. 0001) then the XOR subjects, but worse on other patterns (e.g. 10(0). We concluded that these subjects during the first few trials discovered rules Modeling Human Generalization 9 that categorized most of the training patterns correctly but failed on one or two training patterns. We examined the sequences of subjects' responses to see how well they adhered to ""incorrect"" rules. We designated a response to a pattern as stable if the individual responded the same way to that pattern at least four times in a row. We designated a response as consistent if the response was stable and correct The results of the analysis are shown in Figure 5. These results indicate that the subjects who eventually achieved the XOR solution were less likely to generate stable incorrect solutions. Another impor- tant result is that those subjects who never learned the correct responses to the training set were not responding randomly. Rather, they were systematically using incorrect rules. On the basis of these results, we conclude that subjects' initial strategies may be important detenninants of their final solutions. REVERSED LEARNING For simplicity we identify subjects' initial conditions by their responses on the first few trials. An important theoretical question is whether or not it is possible to find a network structure, initial conditions and a learning rule such that the network can represent both the initial and final behavior of the subject In order to study this problem we developed a technique we call """"reversed leaming"". It is based on a perturbation analysis of feed- forward networks. We use the fact that the error surface in a small neighborhood of a minimum is well approximated by a quadratic surface. Hence, a well behaved gradient descent procedure with a starting point in the neighborhood of the minimum will find that 'minimum. The reversed learning procedure consists of three phases. (1) A network is trained to a final desired state of a particular individual, using both the training and the test patterns. (2) Using only the training patterns, the network is then trained to achieve the initial state of that individual subject closest to the desired final state (3) The network is trained with only the training patterns and the solution is compared to the subject's response profiles. Our preliminary results indicate that this procedure leads in many cases to initial condi- tions that favor the desired solutions. We are currently investigating conditions for finding the optimal initial states. CONCLUSION The main goal of this study was to examine constraints imposed by humans (experimen- tally) and networks (linear programming) on learning of simple binary categorization tasks. We characterize the constraints by analyzing responses to novel stimuli. We showed that. like the humans, networks learn the detenninistic categorization task and find many, very different. individual solutions. Thus adaptive networks are better models than statistical models and DNF rules. The constraints imposed by minimal networks, however, appear to differ from those imposed by human learners in that there are only a few solutions shared between human and adaptive networks. After a detailed analysis of 10 Gluck, Pavel and Henkle the human learning process we concluded that initial conditions may provide imPOl'Wlt constraints. In fact we consider the set of initial conditions as .powerful ""hints"" (Abu- Mostafa, 1989) which reduces the number of potential solutions. without reducing the complexity of the problem. We demonstrated the potential effectiveness of these con- straints using a perturbation technique. which we call reversed learning, for finding appropriate initial conditions. Acknowledgements This work was supported by research grants from the National Science Foundation (BNS-86-18049) to Gordon Bower and Mark Gluck. and (IST-8511589) to M. Pavel. and by a grant from NASA Ames (NCC 2-269) to Stanford University. We thank Steve Slo- man and Bob Rehder for useful discussions and their comments on this draft References Abu-Mostafa, Y. S. Learning by example with hints. NIPS. 1989. Baum, E. B .• & Haussler. D. What size net gives vaUd generalization? NIPS, 1989. Cover. T. (June 1965). Geometrical and statistical properties of systems of linear inequal- ities with applications in pattern recognition. IEEE Transactions on Electronic Computers. EC-14. 3. 326-334. Judd. J. S. Complexity of connectionist learning with various node functions. Presented at the First IEEE International Conference on Neural Networks. San Diego, June 1987. Medin. D. L .• Altom. M. W .• Edelson. S. M .• & Freko. D. (1982). Correlated symptoms and simulated medical classification. Journal of Experimental Psychology: Learn- ing. Memory. & Cognition, 8(1).37-50. Pavel. M .• Gluck, M. A .• & Henkle. V. Generalization by humans and multi-layer adap- tive networks. Submitted to Tenth Annual Conference of the Cognitive Science Society. August 17-19, 1988. Pavel. M .• & Moore, R. T. (1988). Computational analysis of solutions of two-layer adaptive networks. APL Technical Repon, Dept. of Psychology. Stanford Univer- sity. Valiant, L. G. (1984). A theory of the learnable. Comm. ACM. 27.11.1134-1142. Volper. D. J •• & Hampson. S. E. (1987). Learning and using specific instances. Biological Cybernetics, 56 •.","[-0.03414612635970116, -0.06819866597652435, 0.0331869050860405, 0.08309256285429001, 0.012745536863803864, -0.043142445385456085, 0.00886482559144497, -0.012677437625825405, 0.0016854081768542528, -0.04256734251976013, -0.06904799491167068, -0.017264561727643013, 0.03507070988416672, 0.013580516912043095, -0.04734817519783974, -0.08156578242778778, 0.026744849979877472, 0.08961863070726395, -0.06131603941321373, -0.004930609371513128, 0.006032045930624008, -0.015751156955957413, -0.03296767920255661, -0.019277144223451614, -0.034454043954610825, -0.01994416117668152, -0.017236875370144844, 0.010850581340491772, 0.009712914004921913, -0.02187696285545826, 0.07617036253213882, -0.011227590031921864, 0.07326619327068329, 0.019493108615279198, -0.11144083738327026, 0.026326164603233337, -0.08303048461675644, 0.0896642729640007, 0.008626166731119156, 0.010708882473409176, -0.036408133804798126, -0.012764057144522667, -0.05308971926569939, 0.010582965798676014, 0.05239015817642212, 0.07492569088935852, -0.060093916952610016, -0.06557633727788925, -0.037243183702230453, -0.01083449274301529, -0.08159022778272629, 0.040004536509513855, -0.07209806144237518, 0.04775754362344742, 0.05585024878382683, 0.016374578699469566, 0.013693307526409626, 0.021663250401616096, -0.051516272127628326, -0.04575560614466667, 0.015131911262869835, -0.1590055674314499, -0.04325640946626663, -0.00324295018799603, 0.08362486958503723, 0.035501573234796524, -0.030592964962124825, 0.07674947381019592, 0.03698284924030304, 0.024034105241298676, -0.00046773924259468913, 0.058787018060684204, -0.06443610787391663, 0.03754793107509613, 0.11505024135112762, 0.03212318569421768, 0.05551360547542572, 0.0751335397362709, -0.03552620857954025, -0.0241432785987854, 0.01440875418484211, 0.015367254614830017, 0.022877540439367294, 0.02937903441488743, 0.08465191721916199, -0.0416448675096035, -0.056183766573667526, 0.017368784174323082, 0.050105832517147064, 0.0029868544079363346, -0.05741384997963905, -0.02486470900475979, 0.01597999781370163, -0.04378429800271988, 0.05745638906955719, 0.03204351291060448, 0.004107095301151276, -0.037273108959198, -0.03641968220472336, 0.08557100594043732, -0.010973060503602028, 0.005831969901919365, 0.018956152722239494, 0.009509656578302383, 0.035909008234739304, 0.059196487069129944, 0.006703604944050312, 0.005731428507715464, 0.06722041219472885, -0.07026524096727371, 0.0025184969417750835, 0.05468830466270447, -0.03712374344468117, -0.05454814061522484, -0.02365838550031185, -0.08250214159488678, 0.04015810042619705, 0.014211973175406456, 0.018665127456188202, 0.053485602140426636, 0.005808677989989519, 0.011170119978487492, 0.014006653800606728, -0.025721175596117973, 0.07301948964595795, 0.010380014777183533, -0.10680355876684189, 2.6212919745772292e-33, -0.0020581306889653206, -0.025381870567798615, 0.06459949910640717, -0.0019873897545039654, 0.013801522552967072, -0.03452974185347557, -0.07743044197559357, -0.012309225276112556, 0.07016221433877945, 0.017949683591723442, -0.08125559985637665, 0.09190365672111511, -0.04022911190986633, 0.10825196653604507, 0.07205086946487427, 0.02752348594367504, -0.0215689055621624, 0.043338630348443985, 0.0008031729375943542, -0.11454607546329498, 0.0004437261959537864, -0.060387320816516876, 0.005781229585409164, -0.1003536581993103, -0.025099080055952072, -0.02591603435575962, 0.04313945025205612, -0.037406161427497864, -0.0403188057243824, -0.037796590477228165, -0.03362993150949478, 0.03814062103629112, 0.039431679993867874, 0.04290716350078583, 0.020559759810566902, 0.02092224359512329, 0.028245821595191956, -0.000679589225910604, 0.029510948807001114, 0.013446764089167118, -0.00736777950078249, 0.03231815993785858, 0.0635094866156578, -0.013655445538461208, -0.04529740288853645, -0.05321129411458969, -0.009763429872691631, 0.0006986955995671451, -0.12703830003738403, -0.017967885360121727, -0.019258365035057068, 0.003676757449284196, -0.017339279875159264, -0.08476429432630539, 0.007153554819524288, 0.04709600657224655, -0.004136531613767147, 0.03089211694896221, -0.057957135140895844, 0.025723421946167946, -0.011123687960207462, 0.03408180922269821, 0.021026140078902245, 0.0651053935289383, 0.030352596193552017, 0.021670600399374962, -0.022548124194145203, 0.02319510281085968, 0.048976119607686996, 0.0007116733468137681, -0.05170014500617981, 0.028235988691449165, -0.04399098455905914, 0.024052482098340988, 0.05706031620502472, 0.011868682689964771, 0.01721423864364624, -0.09347120672464371, -0.06230156868696213, 0.016257168725132942, -0.05524852126836777, -0.0025402973406016827, -0.057789627462625504, -0.011887541972100735, -0.09450545907020569, -0.017944037914276123, 0.11317156255245209, -0.06381276249885559, 0.006340128369629383, -0.012279343791306019, -0.061849262565374374, -0.013216293416917324, -0.03166466951370239, 0.09596623480319977, 0.047514256089925766, -4.530750855386545e-33, -0.1127033531665802, 0.009851151145994663, -0.08055119961500168, -0.0019386941567063332, 0.04211526736617088, 0.03337730094790459, -0.02887576073408127, -0.031227128580212593, -0.04944336414337158, -0.033280231058597565, -0.004748614504933357, -0.0516180656850338, 0.021964149549603462, 0.04135720804333687, -0.023406172171235085, 0.024959247559309006, -0.07548511773347855, 0.0237741656601429, 0.057462118566036224, 0.01875939406454563, 0.009192802011966705, 0.1346602886915207, -0.12638401985168457, 0.004123856779187918, -0.0015403738943859935, -0.026870472356677055, 0.006142093800008297, 0.10355813801288605, -0.025446433573961258, 0.013591441325843334, -0.059685710817575455, -0.024150323122739792, -0.0654846578836441, 0.038963936269283295, 0.03605462238192558, 0.10280192643404007, -0.04148641228675842, -0.02353915199637413, -0.0014807537663727999, 0.07919935882091522, 0.00887247733771801, -0.01520415861159563, -0.0850517675280571, -0.02792530506849289, -0.0008762375800870359, -0.06563936173915863, -0.035894617438316345, -0.031861066818237305, -0.05197187140583992, 0.02272205613553524, 0.03193960711359978, 0.020978014916181564, -0.08340602368116379, -0.05399872735142708, -0.06054774671792984, -0.008140342310070992, 0.07208321243524551, -0.031192289665341377, 0.12155748903751373, 0.0578114315867424, -0.07863541692495346, -0.061158277094364166, 0.035205092281103134, 0.016748810186982155, -0.08958190679550171, 0.010144148953258991, 0.0020083857234567404, 0.02433651313185692, 0.004947517067193985, 0.002966598141938448, 0.031048472970724106, 0.04795370250940323, -7.746303890598938e-05, 0.0017304087523370981, 0.015802031382918358, -0.061829909682273865, -0.061198119074106216, 0.013816401362419128, -0.035719841718673706, -0.08012160658836365, -0.015240021981298923, -0.011356853879988194, 0.03648657724261284, 0.024315910413861275, 0.008455488830804825, 0.07576178759336472, 0.08391716331243515, 0.17014285922050476, 0.04443010687828064, -0.0230142530053854, -0.005949426908046007, 0.0051450603641569614, -0.06068084016442299, 0.07842119038105011, -0.07181042432785034, -4.891395022355027e-08, -0.13426995277404785, 0.046398285776376724, -0.01041560247540474, 0.06168944761157036, 0.07160282135009766, 0.03266922011971474, -0.022956985980272293, 0.02417617104947567, -0.10192698985338211, -0.01430025976151228, 0.04061216115951538, 0.04472101479768753, 0.08090992271900177, 0.026751428842544556, 0.045107241719961166, 0.11096718162298203, 0.01954929158091545, 0.013264430686831474, -0.02609885297715664, -0.008776484988629818, 0.08553227037191391, 0.007911824621260166, -0.024447135627269745, 0.03632029891014099, 0.08529389649629593, -0.1049661934375763, -0.06273596733808517, 0.03780407831072807, -0.04581914097070694, 0.05953503027558327, 0.018643992021679878, 0.08374291658401489, -0.0316997654736042, -0.007559777703136206, 0.07561580091714859, 0.10270588099956512, 0.004425897262990475, -0.08317220956087112, -0.03123878687620163, -0.03022409789264202, -0.03984244540333748, 0.06798091530799866, -0.03906268626451492, 0.024263881146907806, 0.014705653302371502, -0.05103018507361412, 0.01377750001847744, -0.059806276112794876, -0.02886868268251419, -0.025633400306105614, 0.09208974987268448, -0.0016561226220801473, 0.017522050067782402, -0.03937918320298195, 0.11195223778486252, -0.018848076462745667, -0.007641032803803682, -0.006627662573009729, 0.00673535093665123, 0.08054416626691818, -0.021953511983156204, 0.03658997640013695, -0.054380666464567184, -0.05254426226019859]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\DoestheNeuronLearnliketheSynapse.pdf,Deep Learning,"Mapping Classifier Systems Into Neural Networks Lawrence Davis BBN Laboratories BBN Systems and Technologies Corporation 10 Moulton Street Cambridge, MA 02238 January 16, 1989 Abstract Classifier systems are machine learning systems incotporating a genetic al- gorithm as the learning mechanism. Although they respond to inputs that neural networks can respond to, their internal structure, representation fonnalisms, and learning mechanisms differ marlcedly from those employed by neural network re- searchers in the same sorts of domains. As a result, one might conclude that these two types of machine learning fonnalisms are intrinsically different. This is one of two papers that, taken together, prove instead that classifier systems and neural networks are equivalent. In this paper, half of the equivalence is demonstrated through the description of a transfonnation procedure that will map classifier systems into neural networks that are isomotphic in behavior. Several alterations on the commonly-used paradigms employed by neural networlc researchers are required in order to make the transfonnation worlc. These alterations are noted and their appropriateness is discussed. The paper concludes with a discussion of the practical import of these results, and with comments on their extensibility. 1 Introd uction Classifier systems are machine learning systems that have been developed since the 1970s by 10hn Holland and, more recently, by other members of the genetic algorithm research community as welll . Classifier systems are varieties of genetic algorithms - algorithms for optimization and learning. Genetic algorithms employ techniques inspired by the process of biological evolution in order to ""evolve"" better and better IThis paper has benefited from discussions with Wayne Mesard, Rich Sutton, Ron Williams, Stewart Wilson, Craig Shaefer, David Montana, Gil Syswerda and other members of BARGAIN, the Boston Area Research Group in Genetic Algorithms and Inductive Networks. 49 50 Davis individuals that are taken to be solutions to problems such as optimizing a function, traversing a maze, etc. (For an explanation of genetic algorithms, the reader is referred to [Goldberg 1989].) Classifier systems receive messages from an external source as inputs and organize themselves using a genetic algorithm so that they will ""learn"" to produce responses for internal use and for interaction with the external source. This paper is one of two papers exploring the question of the fonnal relationship between classifier systems and neural networks. As normally employed, the two sorts of algorithms are probably distinct, although a procedure for translating the operation of neural networks into isomorphic classifier systems is given in [Belew and Gherrity 1988]. The technique Belew and Gherrity use does not include the conversion of the neural network learning procedure into the classifier system framework, and it appears that the technique will not support such a conversion. Thus, one might conjecture that the two sorts of machine learning systems employ learning techniques that cannot be reconciled, although if there were a subsumption relationship, Belew and Gherrity's result suggests that the set of classifier systems might be a superset of the set of neural networks. The reverse conclusion is suggested by consideration of the inputs that each sort of learning algorithm processes. When viewed as ""black boxes"", both mechanisms for learning receive inputs, carry out self-modifying procedures, and produce outputs. The class of inputs that are traditionally processed by classifier systems - the class of bit strings of a fixed length - is a subset of the class of inputs that have been traditionally processed by neural networks. Thus, it appears that classifier systems operate on a subset of the inputs that neural networks can process, when viewed as mechanisms that can modify their behavior. In fact, both these impressions are correct. One can translate classifier systems into neural networks, preserving their learning behavior, and one can translate neural networks into classifier systems, again preserving learning behavior. In order to do so, however, some specializations of each sort of algorithm must be made. This paper deals with the translation from classifier systems to neural networks and with those specializations of neural networks that are required in order for the translation to take place. The reverse translation uses quite different techniques, and is treated in [Davis 1989]. The following sections contain a description of classifier systems, a description of the transformation operator, discussions of the extensibility of the proof, comments on some issues raised in the course of the proof, and conclusions. 2 Classifier Systems A classifier system operates in the context of an environment that sends messages to the system and provides it with reinforcement based on the behavior it displays. A classifier system has two components - a message list and a population of rule-like entities called classifiers. Each message on the message list is composed of bits, and Mapping Classifier Systems Into Neural Networks 51 each has a pointer to its source (messages may be generated by the environment or by a classifier.) Each classifier in the population of classifiers has three components: a match string made up of the characters 0,1, and # (for ""don't care""); a message made up of the characters 0 and 1; and a strength. The top-level description of a classifier system is that it contains a population of production rules that attempt to match some condition on the message list (thus ""classifying"" some input) and post their message to the message list, thus potentially affecting the envirorunent or other classifiers. Reinforcement from the environment is used by the classifier system to modify the strengths of its classifiers. Periodically, a genetic algorithm is invoked to create new classifiers, which replace certain members of the classifier set. (For an explanation of classifier systems, their potential as machine learning systems, and their formal properties, the reader is referred to [Holland et al 1986].) Let us specify these processing stages more precisely. A classifier system operates by cycling through a fixed list of procedures. In order, these procedures are: Message List Processing. 1. Clear the message list. 2. Post the envirorunental messages to the message list. 3. Post messages to the message list from classifiers in the post set of the previous cycle. 4. Implement envirorunental reinforcement by analyzing the messages on the message list and altering the strength of classifiers in the post set of the previous cycle. Form the Bid Set. 1. Determine which classifiers match a message in the message list. A classifier matches a message if each bit in its match field matches its corresponding message bit. A 0 matches a 0, a 1 matches a I, and a # matches either bit. The set of all matching classifiers forms the current bid set. 2. Implement bid taxes by subtracting a portion of the strength of each classifier c in the bid set. Add the strength taken from c to the strength of the classifier or classifiers that posted messages matched by c in the prior step. Form the Post Set. 1. If the bid set is larger than the maximum post set size, choose classifiers stochastically to post from the bid set, weighting them in proportion to the magnitude of their bid taxes. The set of classifiers chosen is the post set. Reproduction Reproduction generally does not occur on every cycle. When it does occur, these steps are carried out: 1. Create n children from parents. Use crossover and/or mutation, chOOSing parents stochastically but favoring the strongest ones. (Crossover and mutation are two of the operators used in genetic algorithms.) 2. Set the strength of each child to equal the average of the strength of that child's parents. (Note: this is one of many ways to set the strength of a new classifier. The transformation will work in analogous ways for each of them.) 3. Remove n members of the classifier population and add the n new children to the classifier population. 3 Mapping Classifiers Into Classifier Networks The mapping operator that I shall describe maps each classifier into a classifier network. Each classifier network has links to environmental input units, links to 52 Davis other classifier networks, and match, post, and message units. The weights on the links leading to a match node and leaving a post node are related to the fields in the match and message lists in the classifier. An additional link is added to provide a bias term for the match node. (Note: it is assumed here that the environment posts at most one message per cycle. Modifications to the transfonnation operator to accommodate multiple environmental messages are described in the final comments of this paper.) Given a classifier system CS with n classifiers, each matching and sending mes- sages of length m, we can construct an isomorphic neural network composed of n classifier networks in the following way. For each classifier c in CS, we construct its corresponding classifier network, composed of n match nodes, I post node, and m message nodes. One match node (the environmental match node) has links to inputs from the environment. Each of the other match nodes is linked to the message and post node of another classifier network. The reader is referred to Figure 2 for an example of such a transformation. Each match node in a classifier network has m + 1 incoming links. The weights on the first m links are derived by applying the following transformation to the m elements of c's match field: 0 is associated with weight -1, 1 is associated with weight 1, and # is associated with weight O. The weight. of the final link is set to m + 1 - l, where l is the number of links with weight = 1. Thus, a classifier with match field (1 0 # 0 1) would have an associated network with weights on the links leading to its match nodes of 1, -1, 0, -I, 1, and 4. A classifier with match field (1 0#) would have weights of 1, -I, 0, and 3. The weights on the links to each message node in the classifier network are set to equal the corresponding element of the classifier's message field. Thus, if the message field of the classifier were (0 1 0), the weights on the links leading to the three message nodes in the corresponding classifier network would be 0, I, and O. The weights on all other links in the classifier network are set to 1. Each node in a classifier network uses a threshold function to determine its acti- vation level. Match nodes have thresholds = m + .9. All other nodes have thresholds = .9. If a node's threshold is exceeded, the node's activation level is set to 1. If not, it is set to O. Each classifier network has an associated quantity called strength that may be altered when the network is run, during the processing cycle described below. A cycle of processing of a classifier system CS maps onto the following cycle of processing in a set of classifier networks: Message List Processing. 1. Compute the activation level of each message node in CS. 2. If the environment supplies reinforcement on this cycle, divide that reinforcement by the number of post nodes that are currently active, plus 1 if the environment posted a message on the preceding cycle, and add the quotient to the strength of each active post node's classifier network. 3. If there is a message on this cycle from the environment, map it onto the first m environment nodes so that each node associated with a 0 is off and each node associated with a 1 is on. Tum the final environmental node on. If there is no environmental message, turn all environmental Mapping Classifier Systems Into Neural Networks 53 nodes off. Form the Bid Set. 1. Compute the activation level of each match node in each classifier network. 2. Compute the activation level of each bid node in each classifier network (the set of classifier networks with an active bid node is the bid set). 3. Subtract a fixed proportion of the strength of each classifier network cn in the bid set. Add this amount to the strength of those networks connected to an active match node in cn. (Strength given to the environment passes out of the system.) Form the Post Set. 1. If the bid set is larger than the maximum post set size, choose networks stochastically to post from the bid set, weighting them in proportion to the magnitude of their bid taxes. The set of networks chosen is the post set. (This might be viewed as a stochastic n-winners-take-all procedure). Reproduction. If this is a cycle on which reproduction would occur in the classifier system, carry out its analog in the neural network in the following way. 1. Create n children from parents. Use crossover and/or mutation, choosing parents stochastically but favoring the strongest ones. The ternary alphabet composed of -I, I, and 0 is used instead of the classifier alphabet of 0, 1, and #. After each operator is applied, the final member of the match list is set to m + 1 - l. 2. Write over the weights on the match links and the message links of n classifier networks to match the weights in the children. Choose networks to be re-weighted stochastically, so that the weakest ones are most likely to be chosen. Set the strength of each re-weighted classifier network to be the average of the strengths of its parents. It is simple to show that a classifier network match node will match a message in just those cases in which its associated classifier matched a message. There are three cases to consider. If the original match character was a #, then it matched any message bit. The corresponding link weight is set to 0, so the state of the node it comes from will not affect the activation of the match node it goes to. If the original match character was a 1, then its message bit had to be a 1 for the message to be matched. The corresponding link weight is set to 1, and we see by inspection of the weight on the final link, the match node threshold, and the fact that no other type of link has a positive weight, that every link with weight I must be connected to an active node for the match node to be activated. Finally, the link weight corresponding to a 0 is set to -1. If any of these links is connected to a node that is active, then the effect is that of turning off a node connected to a link with weight 1, and we have just seen that this will cause the match node to be inactive. Given this correspondence in matching behavior, one can verify that a set of classifier networks associated with a classifier system has the following properties: During each cycle of processing of the classifier system, a classifier is in the bid set in just those cases in which its associated networlc has an active bid node. Assuming that both systems use the same randomizing technique, initialized in the same way, the classifier is in the post set in just those cases when the network is in the post set. Finally, the parents that are chosen for reproduction are the transform as of those chosen in the classifier system, and the children produced are the transformations of the classifier system parents. The two systems are isomorphic in operation, assuming that they use the same random number generator. 54 Davis CLASSIFIER NETWORK 1 CLASSIFIER NETWORK 2 strength = 49.3 strength = 21.95 2 Figure 1: Result of mapping a classifier system witH two classifiers into a neural network. MESSAGE NODES TH = .9 POST NODES TH =.9 MATCH NODES TH = 3.9 ENVIRONMENT INPUT NODES Classifier 1 has match field (0 1 #), message field (1 1 0), and strength 49.3. Classifier 2 has match field (1 1 #), message field (0 1 1), and strength 21.95. Mapping Classifier Systems Into Neural Networks 55 4 Concluding Comments The transfonnation procedure described above will map a classifier system into a neural network that operates in the same way. There are several points raised by the techniques used to accomplish the mapping. In closing, let us consider four of them. First, there is some excess complexity in the classifier networks as they are shown here. In fact, one could eliminate all non-environmental match nodes and their links, since one can determine whenever a classifier network is reweigh ted whether it matches the message of each other classifier network in the system. If so, one could introduce a link directly from the post node of the other classifier networlc to the post node of the new networlc. The match nodes to the environment are necessary, as long as one cannot predict what messages the environment will post. Message nodes are necessary as long as messages must be sent out to the environment. If not, they and their incoming links could be eliminated as well. These simplifications have not been introduced here because the extensions discussed next require the complexity of the current architecture. Second, on the genetic algorithm side, the classifier system considered here is an extremely simple one. There are many extensions and refinements that have been used by classifier system researchers. I believe that such refinements can be handled by expanded mapping procedures and by modifications of the architecture of the classifier networks. To give an indication of the way such modifications would go, let us consider two sample cases. The first is the case of an environment that may produce multiple messages on each cycle. To handle multiple messages, an additional link must be added to each environmental match node with weight set to the match node's threshold. This link will latch the match node. An additional match node with links to the environment nodes must be added, and a latched counting node must be attached to it. Given these two architectural modifications, the cycle is modified as follows: During the message matching cycle, a series of subcycles is carried out, one for each message posted by the environment. In each subcycle, an environmental message is input and each environmental match node computes its activation. The environmental match nodes are latched., so that each will be active if it matched any environmental message. The count nodes will record how many were matched by each classifier network. When bid strength'is paid from a classifier network to the posters of messages that it matched, the divisor is the number of environmental messages matched as recorded by the count node, plus the number of other messages matched. Finally, when new weights are written onto a classifier network's links, they are written onto the match node connected to the count node as well. A second sort of complication is that of pass-through bits - bits that are passed from a message that is matched to the message that is posted. This sort of mechanism can be implemented in an obvious fashion by complicating the structure of the classifier networlc. Similar complications are produced by considering multiple-message matching, negation, messages to effectors, and so forth. It is an open question whether all such cases can be handled by modifying the architecture and the mapping operator, but I have not yet found one that cannot be so handled. 56 Davis Third, the classifier networks do not use the sigmoid activation functions that sup- port hill-c~bing techniques such as back-propagation. Further, they are recurrent networks rather than strict feed-forwanl networks. Thus, one might wonder whether the fact that one can carry out such transformations should affect the behavior of researchers in the field. This point is one that is taken up at greater length in the companion paper. My conclusion there is that several of the techniques imported into the neural network domain by the mapping appear to improve the performance of neu- ral networks. These include tracking strength in order to guide the learning process, using genetic operators to modify the network makeup. and using population-level measurements in order to determine what aspects of a network to use in reproduction. The reader is referred to [Montana and Davis 1989] for an example of the benefits to be gained by employing these techniques. Finally, one might wonder what the import of this proof is intended to be. In my view, this proof and the companion proof suggest some exciting ways in which one can hybridize the learning techniques of each field. One such approach and its successful application to a real-world problem is characterized in [Montana and Davis 1989]. References [1] Belew, Richard K. and Michael Gherrity, ""Back Propagation for the Classifier System"", in preparation. [2] Davis, Lawrence, ""Mapping Neural Networks into Classifier Systems"", submit- ted to the 1989 International Conference on Genetic Algorithms. [3] Goldberg, David E. Genetic Algorithms in Search, Optimization, and Machine Learning, Addison Wesley 1989. [4] Holland, John H, Keith J. Holyoak, Richard E. Nisbett, and Paul R. Thagard, Induction, MIT Press, 1986. [5] Montana, David J. and Lawrence Davis, ""Training Feedforward Neural Net- works Using Genetic Algorithms"", submitted to the 1989 International Joint Conference on Artificial Intelligence.","[-0.09341678768396378, -0.1371537744998932, -0.027282871305942535, -0.0003176205209456384, -0.0331018902361393, 0.02583346702158451, -0.011639897711575031, -0.028213776648044586, -0.033978041261434555, -0.032511066645383835, 0.0060005392879247665, -0.04179592430591583, 0.06650751084089279, -0.014099875465035439, -0.08411990851163864, -0.015357296913862228, -0.014804903417825699, 0.08713577687740326, -0.0554533451795578, 0.045282699167728424, 0.05218513682484627, 0.045934516936540604, -0.04114615544676781, 0.03944257274270058, -0.04674818739295006, 0.04425989091396332, -0.028752965852618217, 0.03775293752551079, -0.011291911825537682, -0.054537370800971985, 0.04161600396037102, 0.005812356248497963, -0.03835589066147804, 0.012861478142440319, -0.0819658637046814, 0.02465549297630787, -0.027296222746372223, 0.02280023694038391, 0.03734616935253143, -0.06291625648736954, -0.030836327001452446, -0.07063587009906769, -0.015761004760861397, 0.03398861363530159, 0.0854480117559433, 0.1142415702342987, 0.005187867674976587, -0.021167660132050514, -0.07069101184606552, -0.013280651532113552, -0.08992619812488556, -0.011102310381829739, -0.051477011293172836, 0.07921141386032104, 0.004291873425245285, 0.04022650793194771, 0.02876199223101139, 0.06000738590955734, -0.06992019712924957, 0.03754016011953354, -0.021486274898052216, -0.028102654963731766, 0.018948057666420937, -1.2734064512187615e-05, 0.009456791914999485, 0.07806061208248138, -0.02698570117354393, 0.008123260922729969, 0.06307472288608551, -0.08166451752185822, 0.006967207416892052, 0.045778192579746246, -0.009403555653989315, 0.0327565036714077, 0.04923428222537041, 0.06134021654725075, 0.04881109669804573, 0.10652615875005722, 0.025308096781373024, -0.025911683216691017, -0.027087921276688576, -0.01494301576167345, 0.01930944062769413, 0.03645925223827362, 0.07655299454927444, 0.013962591998279095, -0.10047866404056549, -0.008096729405224323, -0.009420245885848999, -0.029874589294195175, -0.018734049052000046, -0.0398002453148365, 0.011372667737305164, -0.0809771865606308, 0.0006001145229674876, -0.029040854424238205, -0.02838611602783203, -0.005815237294882536, 0.02921840362250805, 0.06079176813364029, -0.09900299459695816, -0.015300367958843708, 0.029270565137267113, -0.050288792699575424, 0.04666070640087128, 0.042897503823041916, 0.018147697672247887, 0.012114239856600761, 0.10523443669080734, -0.12142947316169739, -0.0667114183306694, -0.013796799816191196, -0.02286614663898945, -0.05895472317934036, -0.032001230865716934, -0.007328997366130352, -0.013335000723600388, -0.052943386137485504, 0.0009854575619101524, 0.048965148627758026, -0.045454639941453934, -0.005002487450838089, -0.039131421595811844, 0.030916793271899223, 0.011051267385482788, -0.0567682720720768, -0.11088480800390244, 6.077381833542045e-33, -0.048861537128686905, 0.03777538985013962, -0.029639266431331635, 0.009250825271010399, 0.0698404461145401, -0.09033186733722687, 0.02417285181581974, -0.0074503482319414616, 0.07699737697839737, -0.014123612083494663, -0.07266483455896378, 0.031094660982489586, 0.030676590278744698, 0.08119513839483261, 0.06429421156644821, 0.031859032809734344, -0.0005512009374797344, -0.060311201959848404, 0.018059279769659042, -0.1298392117023468, 0.01100066863000393, 0.08720334619283676, 0.0815662294626236, -0.03716723993420601, 0.01241912692785263, 0.028786007314920425, -0.007291995920240879, -0.0301812756806612, -0.06045181676745415, 0.014502034522593021, 0.04861218482255936, 0.01629277504980564, -0.010025114752352238, -0.07558370381593704, 0.04157447814941406, -0.019939713180065155, 0.044366516172885895, -0.0010247197933495045, 0.0014023405965417624, -0.02535846456885338, 0.03590778261423111, -0.013309687376022339, -0.02335534058511257, 0.032667335122823715, 0.02567996457219124, 0.011704543605446815, -0.025231460109353065, 0.04580949991941452, -0.012141360901296139, -0.00020075010252185166, 0.049747876822948456, -0.004617948550730944, -0.028765032067894936, -0.04728635400533676, 0.05675152689218521, 0.060256920754909515, -0.03376171737909317, 0.06118851155042648, -0.01454364974051714, 0.07668367773294449, -0.052173465490341187, 0.04424396902322769, 0.08806559443473816, 0.015704885125160217, 0.016676580533385277, -0.012752428650856018, 0.038273800164461136, 0.03147983178496361, 0.04988810792565346, -0.008643219247460365, 0.09213194251060486, -0.0122995525598526, -0.036474842578172684, 0.011775077320635319, 0.0647764801979065, -0.028132878243923187, 0.003957310691475868, -0.08924614638090134, -0.035316336899995804, 0.0037599310744553804, -0.03169974312186241, 0.05276558920741081, -0.04405134916305542, -0.06655240058898926, -0.08647047728300095, -0.02545139193534851, 0.0455397367477417, -0.022813186049461365, 0.035701412707567215, 0.05517001822590828, -0.039255961775779724, 0.008649064227938652, -0.033831946551799774, 0.02417425997555256, 0.014160383492708206, -7.534507365748075e-33, -0.08683449029922485, 0.025294672697782516, -0.13132329285144806, 0.02681383304297924, -0.08717548102140427, -0.008520686998963356, -0.06256583333015442, 0.030220234766602516, -0.08217731863260269, 0.06092645600438118, 0.01634434051811695, 0.019920561462640762, 0.030062668025493622, -0.02359042875468731, -0.09365768730640411, 0.011756807565689087, -0.04054201394319534, 0.04476577043533325, 0.05775731801986694, -0.022179674357175827, 0.04926592856645584, 0.1425911784172058, -0.1211850717663765, 0.051517389714717865, -0.02400856651365757, 0.03411169722676277, -0.07516731321811676, 0.1658104956150055, 0.03306739777326584, 0.0589243546128273, -0.02470875158905983, -0.0311606265604496, -0.0537729449570179, 0.027237851172685623, 0.03888680785894394, 0.050265733152627945, 0.03246118128299713, 0.010090912692248821, -0.05687214061617851, -0.002537015127018094, -0.020922744646668434, -0.0036332125309854746, -0.08595065027475357, -0.05897399038076401, 0.03897745534777641, -0.03681735694408417, -0.0183127261698246, 0.01067985501140356, 0.04516899213194847, -0.031113041564822197, 0.07076193392276764, 0.021655814722180367, -0.07084409147500992, -0.035523660480976105, -0.030256347730755806, 0.04309982806444168, 0.049639277160167694, -0.03039383515715599, -0.029414484277367592, 0.07625298202037811, -0.0279457475990057, -0.09980455785989761, 0.0502544566988945, -0.0009682109230197966, -0.00022931471175979823, 0.003048720769584179, 0.023636313155293465, 0.08379144221544266, 0.08163566142320633, -0.036522552371025085, 0.03418303653597832, 0.04153664782643318, 0.054101910442113876, -0.0023850782308727503, -0.037032026797533035, -0.043735817074775696, 0.00559235131368041, -0.002223936142399907, -0.08542955666780472, -0.050798844546079636, -0.03085111267864704, -0.07597681134939194, 0.021846719086170197, 0.06683681160211563, 0.048765260726213455, 0.028210124000906944, 0.056324832141399384, 0.011023654602468014, 0.062111176550388336, -0.0781707763671875, 0.0652039498090744, 0.05935470759868622, -0.05508234724402428, 0.01915646903216839, -0.012746361084282398, -5.691009619113174e-08, -0.058643195778131485, 0.030167123302817345, 0.0470544807612896, 0.024066943675279617, 0.02796139381825924, -0.017140023410320282, -0.001374104875139892, 0.023807300254702568, -0.08687042444944382, -0.021923022344708443, 0.003984389360994101, 0.03182476758956909, 0.011609025299549103, -0.003580093616619706, 0.025632154196500778, -0.0005990518257021904, -0.0037837717682123184, -0.011408383958041668, 0.01121127512305975, 0.0021305675618350506, 0.07498057186603546, -0.05667354539036751, 0.013439570553600788, 0.03315900266170502, 0.03782374784350395, -0.1280485987663269, -0.11113429069519043, 0.015278356149792671, -0.025787414982914925, 0.10798734426498413, -0.012990863062441349, 0.07518532127141953, -0.008989421650767326, -0.0034468614030629396, 0.10777321457862854, 0.03462051972746849, 0.02151268906891346, -0.055867962539196014, -0.13964888453483582, 0.05162880942225456, -0.033413227647542953, 0.017810214310884476, -0.0683053731918335, 0.01804535649716854, 0.006113202776759863, -0.08517300337553024, 0.08974918723106384, -0.10200252383947372, 0.0073121171444654465, 0.016778020188212395, 0.09034667909145355, 0.018046488985419273, -0.015836432576179504, 0.011777102015912533, -0.0014157509431242943, -0.045825421810150146, -0.012883063405752182, -0.1064564660191536, -0.05623815953731537, 0.0840328112244606, 0.012286413460969925, 0.05161222070455551, 0.02643594890832901, -0.09503620117902756]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\DynamicsofAnalogNeuralNetworkswithTimeDelay.pdf,Deep Learning,"ALVINN: AN AUTONOMOUS LAND VEHICLE IN A NEURAL NETWORK Dean A. Pomerleau Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 ABSTRACT ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Cur- rently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perfOIm the task differs dra- matically when the networlc is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand. INTRODUCTION Autonomous navigation has been a difficult problem for traditional vision and robotic techniques, primarily because of the noise and variability associated with real world scenes. Autonomous navigation systems based on traditional image processing and pat- tern recognition techniques often perform well under certain conditions but have problems with others. Part of the difficulty stems from the fact that the processing performed by these systems remains fixed across various driving situations. Artificial neural networks have displayed promising performance and flexibility in other domains characterized by high degrees of noise and variability, such as handwritten character recognition [Jackel et al., 1988] [Pawlicki et al., 1988] and speech recognition [Waibel et al., 1988]. ALVINN (Autonomous Land Vehicle In a Neural Network) is a connectionist approach to the navigational task of road following. Specifically, ALVINN is an artificial neural network designed to control the NAVLAB, the Carnegie Mellon autonomous navigation test vehicle. NETWORK ARCmTECTURE ALVINN's current architecture consists of a single hidden layer back-propagation network 305 306 Pomerleau Road Intensity Feedback Unit 30x32 Video Input Retina 45 Direction Output Units 8x32 Range Finder Input Retina Figure 1: AL VINN Architecture (See Figure 1). The input layer is divided into three sets of units: two ""retinas"" and a single intensity feedback unit. The two retinas correspond to the two forms of sensory input available on the NAVLAB vehicle; video and range information. The first retina, consisting of 3002 units, receives video camera input from a road scene. The activation level of each unit in this retina is proportional to the intensity in the blue color band of the corresponding patch of the image. The blue band of the color image is used because it provides the highest contrast between the road and the non-road. The second retina, consisting of 8x32 units, receives input from a laser range finder. The activation level of each unit in this retina is proportional to the proximity of the corresponding area in the image. The road intensity feedback unit indicates whether the road is lighter or darker than the non-road in the previous image. Each of these 1217 input units is fully connected to the hidden layer of 29 units, which is in tum fully connected to the output layer. The output layer consists of 46 units, divided into two groups. The first set of 45 units is a linear representation of the tum curvature along which the vehicle should travel in order to head towards the road center. The middle unit represents the ""travel straight ahead"" condition while units to the left and right of the center represent successively sharper left and right turns. The network is trained with a desired output vector of all zeros except for a ""hill"" of activation centered on the unit representing the correct tum curvature, which is the curvature which would bring the vehicle to the road center 7 meters ahead of its current position. More specifically, the desired activation levels for ALVlNN: An Autonomous Land Vehicle in a Neural Network 307 Real Road Image Simulated Road Image Figure 2: Real and simulated road images the nine units centered around the correct tum curvature unit are 0.10, 0.32, 0.61, 0.89, 1.00,0.89,0.61,0.32 and 0.10. During testing, the tum curvature dictated by the network is taken to be the curvature represented by the output unit with the highest activation level. The final output unit is a road intensity feedback unit which indicates whether the road is lighter or darker than the non-road in the current image. During testing, the activation of the output road intensity feedback unit is recirculated to the input layer in the style of Jordan [Jordan, 1988] to aid the network's processing by providing rudimentary in- fonnation concerning the relative intensities of the road and the non-road in the previous image. TRAINING AND PERFORMANCE Training on actual road images is logistically difficult, because in order to develop a general representation, the network must be presented with a large number of training exemplaIS depicting roads under a wide variety of conditions. Collection of such a data set would be difficult, and changes in parameters such as camera orientation would require collecting an entirely new set of road images. To avoid these difficulties we have developed a simulated road generator which creates road images to be used as training exemplars for the network. Figure 2 depicts the video images of one real and one artificial road. Although not shown in Figure 2, the road generator also creates corresponding simulated range finder images. At the relatively low resolution being used it is difficult to distinguish between real and simulated roads. NetwoIk: training is performed using these artificial road ""snapshots"" and the Warp back- · 308 Pomerleau Figure 3: NAVLAB, the eMU autonomous navigation test vehicle. propagation simulator described in [Pomerleau et al., 1988]. Training involves first cre- ating a set of 1200 road snapshots depicting roads with a wide variety of retinal orienta- tions and positions, under a variety of lighting conditions and with realistic noise levels. Back-propagation is then conducted using this set of exemplars until only asymptotic performance improvements appear likely. During the early stages of training, the input road intensity unit is given a random activation level. This is done to prevent the net- work from merely learning to copy the activation level of the input road intensity unit to the output road intensity unit, since their activation levels should almost always be identical because the relative intensity of the road and the non-road does not often change between two successive images. Once the network has developed a representation that uses image characteristics to determine the activation level for the output road intensity unit, the network is given as input whether the road would have been darker or lighter than the non-road in the previous image. Using this extra information concerning the relative brightness of the road and the non-road, the network is better able to determine the correct direction for the vehicle to trave1. After 40 epochs of training on the 1200 simulated road snapshots, the network correctly dictates a tum curvature within two units of the correct answer approximately 90% of the time on novel simulated road images. The primary testing of the ALVINN's performance has been conducted on the NAVLAB (See Figure 3). The NAVLAB is a modified Chevy van equipped with 3 Sun computers, a Warp, a video camera, and a laser range finder, which serves as a testbed for the CMU autonomous land vehicle project [Thorpe et al., 1987]. Performance of the network to date is comparable to that achieved by the best traditional vision-based autonomous navigation algorithm at CMU under the limited conditions tested. Specifically, the network can accurately drive the NAVLAB at a speed of 1/2 meter per second along a 400 meter path through a wooded ALVINN: An Autonomous Land Vehicle in a Neural Network 309 Weights to Direction Output Units t~li'C Weight to Output Feedback Unit D Weight from Input Feedback Unit n Weight from Bias Unit • jjllil Weights from Video Camera Retina Weights from Range Finder Retina Road 1 Edges Excitatory Periphery Connections Inhibitory Central Connections Figure 4: Diagram of weights projecting to and from a typical hidden unit in a network trained on roads with a fixed width. The schematics on the right are aids for interpretation. area of the CMU campus under sunny fall conditions. Under similar conditions on the same course, the ALV group at CMU has recently achieved similar driving accuracy at a speed of one meter per second by implementing their image processing autonomous navigation algorithm on the Watp computer. In contrast, the ALVINN network is currently simulated using only an on-boani Sun computer, and dramatic speedups are expected when tests are perfonned using the Watp. NETWORK REPRESENTATION The representation developed by the network to perfonn the road following task depends dramatically on the characteristics of the training set. When trained on examples of roads with a fixed width, the network develops a representations consisting of overlapping road filters. Figure 4 is a diagram of the weights projecting to and from a single hidden unit in such a network. . As indicated by the weights to and from the feedback units, this hidden unit expects the road to be lighter than the non-road in the previous image and supports the road being lighter than the non-road in the current image. More specifically, the weights from the 310 Pomerleau Weights to Direction Output Units Weight to Output Feedback Unit - Weight from Input Feedback Unit I[ Weight from Bias Unit • 1::tllII:UIllt k¥ Weights from Video Camera Retina Weights from Range Finder Retina . ' .' : ' : . . . : ' : .' .: . mDIRoad ~ N road ~ on- 11111111 1111111 111111111111 Figure 5: Diagram of weights projecting to and from a typical hidden unit in a network trained on roads with different widths. video camera retina support the interpretation that this hidden unit is a filter for two light roads, one slightly left and the other slightly right or center (See schematic to the right of the weights from the video retina in Figure 4). This interpretation is also supported by the weights from the range finder retina, which has a much wider field of view than the video camera. This hidden unit is excited if there is high range activity (Le. obstacles) in the periphery and inhibited if there is high range activity in the central region of the scene where this hidden unit expects the road to be (See schematic to the right of the weights from the range finder retina in Figure 4). Finally, the two road filter interpretation is reflected in the weights from this hidden unit to the direction output units. Specifically, this hidden unit has two groups of excitatory connections to the output units, one group dictating a slight left turn and the other group dictating a slight right turn. Hidden units which act as filters for 1 to 3 roads are the representation structures most commonly developed when the network is trained on roads with a fixed width. The network develops a very different representation when trained on snapshots with widely varying road widths. A typical hidden unit from this type of representation is depicted in figure 5. One important feature to notice from the feedback weights is that this unit is filtering for a road which is darlcer than the non-road. More importantly, it is evident from the video camera retina weights that this hidden unit is a filter solely for the left edge of the road (See schematic to the right of the weights from the range finder ALVINN: An Autonomous Land Vehicle in a Neural Network 311 retina in Figure 5). This hidden unit supports a rather wide range of travel directions. This is to be expected, since the correct travel direction for a road with an edge at a particular location varies substantially depending on the road's width. This hidden unit would cooperate with hidden units that detect the right road edge to determine the correct travel direction in any particular situation. DISCUSSION AND EXTENSIONS The distinct representations developed for different circumstances illustrate a key advan- tage provided by neural networks for autonomous navigation. Namely, in this paradigm the data, not the programmer, determines the salient image features crucial to accurate road navigation. From a practical standpoint, this data responsiveness has dramatically sped ALVINN's development. Once a realistic artificial road generator was developed, back-propagation producted in half an hour a relatively successful road following system. It took many months of algorithm development and parameter tuning by the vision and autonomous navigation groups at CMU to reach a similar level of performance using traditional image processing and pattern recognition techniques. More speculatively, the flexibility of neural network representations provides the pos- sibility of a very different type of autonomous navigation system in which the salient sensory features are determined for specific driving conditions. By interactively training the network on real road images taken as a human drives the NAVLAB, we hope to develop a system that adapts its processing to accommodate current circumstances. This is in contrast with other autonomous navigation systems at CMU [Thorpe et al., 1987] and elsewhere [Dunlay & Seida, 1988] [Dickmanns & Zapp, 1986] [Kuan et al., 1988]. Each of these implementations has relied on a fixed, highly structured and therefore rela- tively inflexible algorithm for finding and following the road, regardless of the conditions at hand. There are difficulties involved with training ""on-the-fly"" with real images. If the network is not presented with sufficient variability in its training exemplars to cover the conditions it is likely to encounter when it takes over driving from the human operator, it will not develop a sufficiently robust representation and will perform poorly. In addition, the network must not solely be shown examples of accurate driving, but also how to recover (i.e. return to the road center) once a mistake has been made. Partial initial training on a variety of simulated road images should help eliminate these difficulties and facilitate better performance. Another important advantage gained through the use of neural networks for autonomous navigation is the ease with which they assimilate data from independent sensors. The current ALVINN implementation processes data from two sources, the video camera and the laser range finder. During training, the network discovers how information from each source relates to the task, and weights each accordingly. As an example, range data is in some sense less important for the task of road following than is the video data. The range data contains information concerning the position of obstacles in the scene, but nothing explicit about the location of the road. As a result, the range data is given less significance in the representation, as is illustrated by the relatively small 312 Pomerleau magnitude weights from the range finder retina in the weight diagrams. Figures 4 and 5 illustrate that the range finder connections do correlate with the connections from the video camera, and do contribute to choosing the correct travel direction. Specifically, in both figures, obstacles located outside the area in which the hidden unit expects the road to be located increase the hidden unit's activation level while obstacles located within the expected road boundaries inhibit the hidden unit. However the contributions from the range finger connections aren't necessary for reasonable performance. When ALVINN was tested with normal video input but an obstacle-free range finder image as constant input, there was no noticeable degradation in driving performance. Obviously under off-road driving conditions obstacle avoidance would become much more important and hence one would expect the range finder retina to playa much more significant role in the network's representation. We are currently working on an off-road version of ALVINN to test this hypothesis. Other current directions for this project include conducting more extensive tests of the network's performance under a variety of weather and lighting conditions. These will be crucial for making legitimate performance comparisons between ALVINN and other autonomous navigation techniques. We are also working to increase driving speed by implementing the network simulation on the on-board Warp computer. Additional extensions involve exploring different network architectures for the road fol- lowing task. These include 1) giving the network additional feedback information by us- ing Elman's [Elman, 1988] technique of recirculating hidden activation levels, 2) adding a second hidden layer to facilitate better internal representations, and 3) adding local connectivity to give the network a priori knowledge of the two dimensional nature of the input In the area of planning, interesting extensions include stopping for, or planning a path around, obstacles. One area of planning that clearly needs work is dealing sensibly with road forks and intersections. Currently upon reaching a fork, the network may output two widely discrepant travel directions, one for each choice. The result is often an oscillation in the dictated travel direction and hence inaccurate road following. Beyond dealing with individual intersections, we would eventually like to integrate a map into the system to enable global point-to-point path planning. CONCLUSION More extensive testing must be performed before definitive conclusions can be drawn con- cerning the peiformance of ALVINN versus other road followers. We are optimistic con- cerning the eventual contributions neural networks will make to the area of autonomous navigation. But perhaps just as interesting are the possibilities of contributions in the other direction. We hope that exploring autonomous navigation, and in particular some of the extensions outlined in this paper, will have a significant impact on the field of neural networks. We certainly believe it is important to begin researching and evaluating neural networks in real world situations, and we think autonomous navigation is an interesting application for such an approach. ALVINN: An Autonomous Land Vehicle in a Neural Network 313 Acknowledgements This work would not have been possible without the input and support provided by Dave Touretzky, Joseph Tebelskis, George Gusciora and the CMU Warp group, and particularly Charles Thorpe, Till Crisman, Martial Hebert, David Simon, and rest of the CMU ALV group. This research was supported by the Office of Naval Research under Contracts NOOOI4-87-K-0385, NOOOI4-87-K-0533 and NOOOI4-86-K-0678, by National Science Foundation Grant EET-8716324, by the Defense Advanced Research Projects Agency (DOD) monitored by the Space and Naval Warfare Systems Command under Contract NOOO39-87-C-0251, and by the Strategic Computing Initiative of DARPA, through ARPA Order 5351, and monitored by the U.S. Army Engineer Topographic Laboratories under contract DACA 76-85-C-0003 titled ""Road Following"". References [Dickmanns & Zapp, 1986] Dickmanns, E.D., Zapp, A. (1986) A curvature-based scheme for improving road vehicle guidance by computer vision. ""Mobile Robots"", SPIE-Proc. Vol. 727, Cambridge, MA. [Elman, 1988] Elman, J.L, (1988) Finding structure in time. Technical report 8801. Cen- ter for Research in Language, University of California, San Diego. [Dunlay & Seida, 1988] Dunlay, R.T., Seida, S. (1988) Parallel off-road perception pro- cessing on the ALV. Proc. SPIE Mobile Robot Conference, Cambridge MA. [Jackel et al., 1988] Jackel, L.D., Graf, H.P., Hubbard, W., Denker, J.S., Henderson, D., Guyon, 1. (1988) An application of neural net chips: Handwritten digit recognition. Proceedings of IEEE International Conference on Neural Networks, San Diego, CA. [Jordan, 1988] Jordan, M.l. (1988) Supervised learning and systems with excess degrees of freedom. COINS Tech. Report 88-27, Computer and Infolll1ation Science, Uni- versity of Massachusetts, Amherst MA. [Kuan et al., 1988] Kuan, D., Phipps, G. and Hsueh, A.-C. Autonomous Robotic Vehicle Road Following. IEEE Trans. on Pattern Analysis and Machine Intelligence, Vol. 10, Sept. 1988. [pawlicki et al., 1988] Pawlicki, T.E, Lee, D.S., Hull, J.J., Srihari, S.N. (1988) Neural network models and their application to handwritten digit recognition. Proceedings of IEEE International Conference on Neural Networks, San Diego, CA. [pomerleau et al., 1988] Pomerleau, D.A., Gusciora, G.L., Touretzky, D.S., and Kung, H.T. (1988) Neural network simulation at Waq> speed: How we got 17 million connections per second. Proceedings of IEEE International Conference on Neural Networks, San Diego, CA. [Th0IJle et al., 1987] Thorpe, c., Herbert, M., Kanade, T., Shafer S. and the members of the Strategic Computing Vision Lab (1987) Vision and navigation for the Carnegie Mellon NAVLAB. Annual Revi~ of Computer Science Vol. 11, Ed. Joseph Traub, Annual Reviews Inc., Palo Alto, CA. [Waibel et al., 1988] Waibel, A, Hanazawa, T., Hinton, G., Shikano, K., Lang, K. (1988) Phoneme recognition: Neural Networks vs. Hidden Markov Models. Proceedings from Int. Conf. on Acoustics, Speech and Signal Processing, New York, New York.","[-0.08508371561765671, -0.11371893435716629, -0.03564462438225746, -0.0363212525844574, -0.04943475127220154, -0.0278771985322237, -0.013965674676001072, -0.03367985785007477, -0.02526720054447651, -0.06906211376190186, 0.03639693558216095, 0.027152391150593758, 0.035676129162311554, 0.002012865152209997, -0.08422061800956726, 0.024896003305912018, -0.012503329664468765, 0.06967141479253769, -0.024104053154587746, -0.029037902131676674, -0.021038422361016273, 0.04424741864204407, -0.001590971602126956, 0.0051594078540802, -0.09063079208135605, 0.03466720134019852, 0.0658470019698143, -0.01737535372376442, -0.029483122751116753, -0.06582921743392944, 0.07987292855978012, -0.07284565269947052, 0.011857246048748493, -0.0012397239916026592, -0.041749682277441025, 0.021921545267105103, -0.0858742818236351, -0.003990025259554386, -0.018967771902680397, -0.07528284192085266, 0.017434600740671158, -0.02164134383201599, -0.005851187277585268, -0.03286881744861603, 0.14534969627857208, 0.04254217445850372, 0.056949444115161896, -0.03279578685760498, 0.06664309650659561, -0.01051273476332426, -0.07046462595462799, -0.02549637295305729, -0.008495029993355274, -0.04485339671373367, -0.058696016669273376, 0.08190450817346573, -0.03021789900958538, 0.014234928414225578, -0.026088420301675797, 0.010616361163556576, 0.04684736579656601, 0.003375521395355463, -0.004613804630935192, -0.01938445121049881, 0.02426578477025032, 0.05556024610996246, -0.045961301773786545, -0.013359855860471725, 0.04728686809539795, -0.03575596585869789, 0.05343747138977051, 0.05714552477002144, 0.07743903994560242, -0.010723419487476349, -0.0020541243720799685, -0.04805873706936836, 0.059418290853500366, 0.04411568120121956, 0.006102557294070721, -0.030122580006718636, 0.07200061529874802, 0.0023854949977248907, 0.04178992658853531, 0.08273694664239883, 0.05436436086893082, 0.011868756264448166, -0.09060890972614288, 0.047748323529958725, -0.017887551337480545, -0.02037332020699978, -0.011496064253151417, -0.10925617814064026, -0.007107977289706469, -0.06251011788845062, 0.025424733757972717, -0.028820054605603218, 0.012591871432960033, -0.08338312059640884, 0.0019173923647031188, 0.03872719034552574, -0.015600048005580902, 0.03115730732679367, 0.0072662848979234695, -0.032117683440446854, 0.03762966766953468, 0.11827339231967926, 0.04625783488154411, -0.011443342082202435, 0.10233139991760254, -0.03990921750664711, -0.01239995751529932, -0.06394068151712418, -0.05705993250012398, -0.054040294140577316, -0.045749105513095856, -0.05821528658270836, -0.034828078001737595, -0.003886960679665208, -0.003433628473430872, 0.011390349827706814, -0.09409819543361664, -0.09074927866458893, 0.04197341576218605, 0.05263587832450867, 0.11398863792419434, -0.051879946142435074, 0.0426236055791378, 4.109262662219828e-33, -0.09128467738628387, 0.000611618859693408, -0.003121111076325178, -0.08993629366159439, 0.09058333933353424, -0.02706792578101158, 0.010048138909041882, -0.03782792389392853, 0.023675713688135147, 0.013595160096883774, -0.030073681846261024, -0.026379570364952087, -0.07740948349237442, 0.05589863285422325, 0.05716584250330925, -0.0715596005320549, -0.01685578189790249, -0.059201501309871674, -0.06572510302066803, -0.042398419231176376, 0.0589400939643383, -0.019598156213760376, 0.05195270851254463, -0.018086276948451996, 0.011001478880643845, -0.003359466325491667, 0.016503654420375824, 0.07769254595041275, 0.07141765207052231, -0.022148817777633667, 0.011941732838749886, 0.08228370547294617, -0.06513570994138718, 0.012305019423365593, 0.06737009435892105, -0.022068234160542488, -0.056756552308797836, 0.017330655828118324, 0.030577069148421288, 0.10002374649047852, 0.04129602015018463, -0.00588299660012126, 0.02291109599173069, -0.025121692568063736, -0.019301319494843483, -0.01226888783276081, 0.013037735596299171, 0.06915654242038727, -0.06006717309355736, 0.04881799593567848, -0.004854436032474041, -0.013817014172673225, -0.08214487880468369, -0.10652559250593185, 0.01456473395228386, -0.027552634477615356, 0.02248021960258484, 0.04914466291666031, 0.0034139270428568125, -0.012622617185115814, 0.035908084362745285, -0.06222320720553398, 0.029752301052212715, 0.08180214464664459, -0.05299347639083862, 0.014832613058388233, 0.01105140894651413, 0.03426530212163925, 0.06337437778711319, 0.057897817343473434, 0.022694502025842667, 0.00013443951320368797, -0.01791570894420147, 0.005873026791960001, 0.11062117666006088, -0.009241060353815556, -0.07802433520555496, 0.0049727666191756725, -0.027381915599107742, 0.04216590151190758, -0.10022424161434174, 0.09897357225418091, -0.07443592697381973, 0.040930237621068954, 0.03474920988082886, 0.04265759885311127, -0.02449546381831169, -0.01232185773551464, 0.054931603372097015, -0.021190539002418518, -0.01911330781877041, -0.0063053700141608715, -0.07433324307203293, 0.017792362719774246, 0.02303607575595379, -4.798337818013838e-33, 0.03212705999612808, 0.03901151940226555, 0.022924799472093582, 0.04108267277479172, -0.08058813959360123, -0.018319498747587204, 0.028673695400357246, 0.07398361712694168, 0.05495895445346832, 0.04393057897686958, -0.05336582660675049, 0.06114702299237251, 0.07142873108386993, 0.02918432094156742, -0.004707414656877518, 0.030560439452528954, -0.005550962407141924, -0.022131014615297318, -0.039591703563928604, 0.0067311543971300125, -0.05922512337565422, 0.09777829796075821, -0.06423701345920563, -0.002445768564939499, -0.04037214815616608, 0.03165273740887642, 0.030396999791264534, 0.12332624942064285, 0.03545009344816208, 0.009461422450840473, -0.027854058891534805, -0.017857521772384644, 0.034566354006528854, -0.009036668576300144, 0.00911946501582861, 0.05264301598072052, 0.05563943460583687, -0.051389165222644806, -0.14073917269706726, 0.00045676654553972185, 0.0675349235534668, -0.03662233054637909, 0.024675028398633003, -0.012572458945214748, 0.018710872158408165, -0.04832923784852028, -0.01874966360628605, 0.007008308544754982, -0.041107334196567535, 0.04456830769777298, 0.04207821562886238, 0.018804719671607018, -0.014230847358703613, 0.047450918704271317, 0.033188384026288986, 0.05336972326040268, 0.001435574027709663, 0.03747393190860748, 0.05579383298754692, 0.0056465500965714455, -0.028661690652370453, -0.10252171009778976, 0.007145400159060955, 0.027248241007328033, -0.020901720970869064, -0.02554072067141533, -0.0636897087097168, 0.06304194778203964, 0.008046048693358898, 0.009714433923363686, 0.025727398693561554, -0.007462556939572096, 0.07376381009817123, 0.02834460325539112, -0.027400614693760872, -0.06378732621669769, -0.037754468619823456, 0.053486183285713196, 0.04921254888176918, -0.02719009295105934, -0.03981549292802811, -0.08362185955047607, -0.02982102334499359, 0.1128007248044014, 0.03346877172589302, 0.09529469907283783, 0.013505794107913971, -0.05739080160856247, 0.08693715184926987, 0.0012287143617868423, 0.10408651828765869, 0.02295742742717266, -0.014860374853014946, 0.03372396528720856, -0.09715913236141205, -4.898374328377031e-08, -0.0926659107208252, 0.031882449984550476, -0.006601189728826284, -0.04552711173892021, 0.025617847219109535, -0.060924649238586426, 0.09972992539405823, -0.017787432298064232, -0.10826034843921661, -0.07843446731567383, 0.0309440940618515, 0.01632414385676384, -0.08332609385251999, 0.018087685108184814, 0.018491923809051514, 0.02639838121831417, 0.052655208855867386, -0.055886175483465195, 0.032696984708309174, 0.0029357410967350006, -0.0024717384949326515, -0.032355308532714844, -0.002692306414246559, 0.07662300765514374, 0.005525651853531599, 0.011922615580260754, -0.11486490070819855, 0.008176588453352451, -0.04160964861512184, -0.0061532920226454735, 0.05172251537442207, 0.023580709472298622, 0.034197475761175156, 0.022363711148500443, 0.05198994651436806, 0.024629617109894753, 0.020898805931210518, -0.024897029623389244, 0.013111908920109272, -0.02582542411983013, -0.020275143906474113, 0.03387263044714928, 0.004367721267044544, 0.0025782270822674036, -0.033760640770196915, 0.005718117114156485, 0.01992049254477024, -0.13071133196353912, -0.011723172850906849, -0.04371333122253418, 0.021884312853217125, -0.022163938730955124, -0.05521087348461151, 0.03152026608586311, 0.06502769142389297, -0.027443701401352882, 0.09054267406463623, -0.18400920927524567, -0.06226928532123566, 0.16063594818115234, -0.02907881885766983, 0.01873844675719738, 0.005088239908218384, -0.059397872537374496]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\EfficientParallelLearningAlgorithmsforNeuralNetworks.pdf,Deep Learning,"LINEAR LEARNING: LANDSCAPES AND ALGORITHMS Pierre Baldi Jet Propulsion Laboratory California Institute of Technology Pasadena, CA 91109 What follows extends some of our results of [1] on learning from ex- amples in layered feed-forward networks of linear units. In particu- lar we examine what happens when the ntunber of layers is large or when the connectivity between layers is local and investigate some of the properties of an autoassociative algorithm. Notation will be as in [1] where additional motivations and references can be found. It is usual to criticize linear networks because ""linear functions do not compute"" and because several layers can always be reduced to one by the proper multiplication of matrices. However this is not the point of view adopted here. It is assumed that the architecture of the network is given (and could perhaps depend on external constraints) and the purpose is to understand what happens during the learning phase, what strategies are adopted by a synaptic weights modifying algorithm, ... [see also Cottrell et al. (1988) for an example of an ap- plication and the work of Linsker (1988) on the emergence of feature detecting units in linear networks}. Consider first a two layer network with n input units, n output units and p hidden units (p < n). Let (Xl, YI), ... , (XT, YT) be the set of centered input-output training patterns. The problem is then to find two matrices of weights A and B minimizing the error function E: E(A, B) = L IIYt - ABXtIl2. (1) l<t<T 65 66 Baldi Let ~x x, ~XY, ~yy, ~y x denote the usual covariance matrices. The main result of [1] is a description of the landscape of E, characerised by a multiplicity of saddle points and an absence of local minima. More precisely, the following four facts are true. Fact 1: For any fixed n x p matrix A the function E(A, B) is convex in the coefficients of B and attains its minimum for any B satisfying the equation A'AB~xx = A/~yX. (2) If in addition ~x X is invertible and A is of full rank p, then E is strictly convex and has a unique minimum reached when (3) Fact 2: For any fixed p x n matrix B the function E(A, B) is convex in the coefficients of A and attains its minimum for any A satisfying the equation AB~xxB' = ~YXB'. (4) If in addition ~xx is invertible and B is of full rank p, then E is strictly convex and has a unique minimum reached when (5) Fact 3: Assume that ~x X is invertible. If two matrices A and B define a critical point of E (i.e. a point where 8E / 8aij = 8E /8bij = 0) then the global map W = AB is of the form (6) where P A denotes the matrix of the orthogonal projection onto the subspace spanned by the columns of A and A satisfies (7) Linear Learning: Landscapes and Algorithms 67 with ~ = ~y X ~x~~XY. If A is of full rank p, then A and B define a critical point of E if and only if A satisties (7) and B = R(A), or equivalently if and only if A and W satisfy (6) and (7). Notice that in (6), the matrix ~y X ~x~ is the slope matrix for the ordinary least square regression of Y on X. Fact 4: Assume that ~ is full rank with n distinct eigenvalues At > ... > An. If I = {i t , ... ,ip}(l < it < ... < ip < n) is any or- dered p-index set, let Uz = [Uit , ••• , Uip ] denote the matrix formed by the orthononnal eigenvectors of ~ associated with the eigenvalues Ail' ... , Aip • Then two full rank matrices A and B define a critical point of E if and only if there exist an ordered p-index set I and an invertible p x p matrix C such that A=UzC For such a critical point we have E(A,B) = tr(~yy) - L Ai. iEZ (8) (9) (10) (11 ) Therefore a critical point of W of rank p is always the product of the ordinary least squares regression matrix followed by an orthogonal projection onto the subspace spanned by p eigenvectors of~. The map W associated with the index set {I, 2, ... ,p} is the unique local and global minimum of E. The remaining (;) -1 p-index sets correspond to saddle points. All additional critical points defined by matrices A and B which are not of full rank are also saddle points and can be characerized in terms of orthogonal projections onto subspaces spanned by q eigenvectors with q < p. 68 Baldi Deep Networks Consider now the case of a deep network with a first layer of n input units, an (m + 1 )-th layer of n output units and m - 1 hidden layers with an error function given by E(AI, ... ,An)= L IIYt-AIAl ... AmXtll2. (12) l<t<T It is worth noticing that, as in fact 1 and 2 above, if we fix any m-1 of the m matrices AI, ... , Am then E is convex in the remaining matrix of connection weights. Let p (p < n) denote the ntunber of units in the smallest layer of the network (several hidden layers may have p units). In other words the network has a bottleneck of size p. Let i be the index of the corresponding layer and set A = AI A2 ... Am-i+1 B = Am-i+2 ... Am (13) When we let AI, ... , Am vary, the only restriction they impose on A and B is that they be of rank at most p. Conversely, any two ma- trices A and B of rank at most p can always be decomposed (and in many ways) in products of the form of (13). It results that any local minima of the error function of the deep network should yield a local minima for the corresponding ""collapsed"" .three layers network induced by (13) and vice versa. Therefore E(AI , ... , Am) does not have any local minima and the global minimal map W· = AIA2 ... Am is unique and given by (10) with index set {I, 2, ... , p}. Notice that of course there is a large number of ways of decomposing W· into a product of the form A I A2 ... Am . Also a saddle point of the error function E(A, B) does not necessarily generate a saddle point of the corresponding E (A I , ... , Am) for the expressions corresponding to the two gradients are very different. Linear Learning: Landscapes and Algorithms 69 Forced Connections. Local Connectivity Assume now an error function of the form E(A) = L IIYt - AXt[[2 l~t~T (14) for a two layers network but where the value of some of the entries of A may be externally prescribed. In particular this includes the case of local connectivity described by relations of the form aij = 0 for any output unit i and any input unit j which are not connected. Clearly the error function E(A) is convex in A. Every constraint of the form aij =cst defines an hyperplane in the space of all possible A. The intersection of all these constraints is therefore a convex set. Thus minimizing E under the given constraints is still a convex optimization problem and so there are no local minima. It should be noticed that, in the case of a network with even only three constrained layers with two matrices A and B and a set of constraints of the form aij =cst on A and bk1 =cst for B, the set of admissible matrices of the form W = AB is, in general, not convex anymore. It is not unreasonable to conjecture that local minima may then arise, though this question needs to be investigated in greater detail. Algorithmic Aspects One of the nice features of the error landscapes described so far is the absence of local minima and the existence, up to equivalence, of a unique global minimum which can be understood in terms of principal component analysis and least square regression. However the landscapes are also characterized by a large number of saddle points which could constitute a problem for a simple gradient descent algorithm during the learning phase. The proof in [1] shows that the lower is the E value corresponding to a saddle point, the more difficult it is to escape from it because of a reduction in the possible number of directions of escape (see also [Chauvin, 1989] in a context of Hebbian learning). To assert how relevant these issues are for practical implementations requires further simulation experiments. On a more 70 Baldi speculative side, it remains also to be seen whether, in a problem of large size, the number and spacing of saddle points encountered during the first stages of a descent process could not be used to ""get a feeling"" for the type of terrain being descented and as a result to adjust the pace (i. e. the learning rate). We now turn to a simple algorithm for the auto-associative case in a three layers network, i. e. the case where the presence of a teacher can be avoided by setting Yt = Xt and thereby trying to achieve a compression of the input data in the hidden layer. This technique is related to principal component analysis, as described in [1]. If Yt = Xt, it is easy to see from equations (8) and (9) that, if we take the matrix C to be the identity, then at the optimum the matrices A and B are transpose of each other. This heuristically suggests a possible fast algorithm for auto-association, where at each iteration a gradient descent step is applied only to one of the connection matrices while the other is updated in a symmetric fashion using transposition and avoiding to back-propagate the error in one of the layers (see [Williams, 1985] for a similar idea). More formally, the algorithm could be concisely described by A(O) = random B(O) = A'(O) 8E A(k+l)=A(k)-11 8A B(k+l)=A'(k+l) (15) Obviously a similar algorithm can be obtained by setting B(k + 1) = B(k) -118E/8B and A(k + 1) = B'(k + 1). It may actually even be bet ter to alternate the gradient step, one iteration with respect to A and one iteration with respect to B. A simple calculation shows that (15) can be rewritten as A(k + 1) = A(k) + 11(1 - W(k))~xxA(k) B(k + 1) = B(k) + 11B(k)~xx(I - W(k)) (16) Linear Learning: Landscapes and Algorithms 71 where W(k) = A(k)B(k). It is natural from what we have already seen to examine the behavior of this algorithm on the eigenvectors of ~xx. Assume that u is an eigenvector of both ~xx and W(k) with eigenvalues ,\ and /-l( k). Then it is easy to see that u is an eigenvector of W(k + 1) with eigenvalue (17) For the algorithm to converge to the optimal W, /-l( k + 1) must con- verge to 0 or 1. Thus one has to look at the iterates of the func- tion f( x) = x[l + 7],\(1 - x)]2. This can be done in detail and we shall only describe the main points. First of all, f' (x) = 0 iff x = 0 or x = Xa = 1 + (1/7],\) or x = Xb = 1/3 + (1/37],\) and f""(x) = 0 iff x = Xc = 2/3 + (2/37],\) = 2Xb. For the fixed points, f(x) = x iff x = 0, x = 1 or x = Xd = 1 + (2/7],\). Notice also that f(xa) = a and f(1 + (1/7],\)) = (1 + (1/7],\)(1 - 1? Points cor- responding to the values 0,1, X a , Xd of the x variable can readily be positioned on the curve of f but the relative position of Xb (and xc) depends on the value assumed by 7],\ with respect to 1/2. Obviously if J1(0) = 0,1 or Xd then J1( k) = 0,1 or Xd, if J1(0) < 0 /-l( k) ~ -00 and if /-l( k) > Xd J1( k) ~ +00. Therefore the algorithm can converge only for a < /-leO) < Xd. When the learning rate is too large, i. e. when 7],\ > 1/2 then even if /-leO) is in the interval (0, Xd) one can see that the algorithm does not converge and may even exhibit complex oscillatory behavior. However when 7],\ < 1/2, if 0 < J1(0) < Xa then J1( k) ~ 1, if /-leO) = Xa then /-l( k) = a and if Xa < J1(0) < Xd then /-l(k) ~ 1. In conclusion, we see that if the algorithm is to be tested, the learning rate should be chosen so that it does not exceed 1/2,\, where ,\ is the largest eigenvalue of ~xx. Even more so than back propaga- tion, it can encounter problems in the proximity of saddle points. Once a non-principal eigenvector of ~xx is learnt, the algorithm rapidly incorporates a projection along that direction which cannot be escaped at later stages. Simulations are required to examine the effects of ""noisy gradients"" (computed after the presentation of only a few training examples), multiple starting points, variable learning rates, momentum terms, and so forth. 72 Baldi Aknowledgement Work supported by NSF grant DMS-8800323 and in part by ONR contract 411P006-01. References (1) Baldi, P. and Hornik, K. (1988) Neural Networks and Principal Component Analysis: Learning from Examples without Local Min- ima. Neural Networks, Vol. 2, No. 1. (2) Chauvin, Y. (1989) Another Neural Model as a Principal Compo- nent Analyzer. Submitted for publication. (3) Cottrell, G. W., Munro, P. W. and Zipser, D. (1988) Image Com- pression by Back Propagation: a Demonstration of Extensional Pro- gramming. In: Advances in Cognitive Science, Vol. 2, Sharkey, N. E. ed., Norwood, NJ Abbex. (4) Linsker, R. (1988) Self-Organization in a Perceptual Network. Computer 21 (3), 105-117. ( 5) Willi ams, R. J. (1985) Feature Discovery Through Error-Correction Learning. ICS Report 8501, University of California., San Diego.","[-0.07357207685709, -0.03432875871658325, -0.005432286765426397, 0.028418393805623055, 0.053524307906627655, 0.045193083584308624, -0.018338588997721672, -0.08622477948665619, 0.007939971052110195, -0.015707338228821754, 0.020045878365635872, 0.015481695532798767, 0.021105019375681877, 0.024674611166119576, -0.11474309861660004, 0.036119621247053146, 0.020857002586126328, 0.07395804673433304, -0.04502704739570618, 0.022825956344604492, -0.01050427183508873, -0.01353739108890295, -0.011935197748243809, 0.04332699999213219, 0.03573226556181908, 0.05308613181114197, -0.044540885835886, -0.05092495679855347, 0.011791691184043884, -0.01582353748381138, 0.049967214465141296, 0.007327370345592499, 0.04000842943787575, 0.054181717336177826, -0.04291492700576782, 0.03439502418041229, -0.048297811299562454, 0.06482182443141937, -0.008579516783356667, 0.02182454615831375, 0.0129291582852602, -0.025794895365834236, -0.027782928198575974, 0.03178836777806282, 0.06340481340885162, 0.045316796749830246, -0.008031404577195644, -0.019903559237718582, -0.007489955052733421, -0.002879195613786578, -0.043998703360557556, 0.009176813997328281, -0.04338805750012398, 0.022505689412355423, 0.009463228285312653, -0.02066284418106079, 0.027288362383842468, 0.06503751873970032, -0.080501988530159, 0.04603266716003418, 0.02357999049127102, -0.06750225275754929, -0.06232096627354622, 0.008961199782788754, 0.034531332552433014, -0.018132714554667473, -0.05746155604720116, 0.023588884621858597, -0.016921943053603172, 0.027254236862063408, 0.05007898434996605, 0.041174307465553284, -0.022852353751659393, -0.02008836902678013, 0.1201702281832695, 0.06491215527057648, 0.042775463312864304, -0.020688623189926147, -0.014037172310054302, -0.046365659683942795, 0.06644612550735474, 0.07079236954450607, -0.005396562162786722, -0.05254342034459114, 0.04766988009214401, -0.019146528095006943, -0.022583425045013428, -0.04339441657066345, 0.04617132619023323, -0.005850373301655054, -0.023826370015740395, -0.04668222740292549, -0.09579842537641525, -0.03130900487303734, 0.10739119350910187, -0.03643115982413292, 0.004693135619163513, -0.07210781425237656, -0.0385415181517601, 0.07596006989479065, -0.024360530078411102, -0.025953074917197227, 0.017292605713009834, 0.015532988123595715, 0.04686225578188896, 0.060574792325496674, 0.03351343423128128, 0.02673720009624958, 0.09310555458068848, -0.1556205004453659, -0.010273899883031845, -0.031194718554615974, -0.03925293684005737, -0.041248682886362076, -0.0284590944647789, -0.08720329403877258, 0.0931307002902031, -0.02860986813902855, 0.0298821609467268, -0.020369671285152435, -0.040715280920267105, -0.026371831074357033, -0.008177511394023895, -0.00536987092345953, 0.0730176642537117, -0.03325772285461426, -0.07907276600599289, 5.622393520744832e-33, -0.019109202548861504, 0.006437317468225956, -0.030779555439949036, 0.006717597600072622, -0.0162776168435812, -0.0926358550786972, 0.02207273803651333, -0.07675725966691971, 0.09724121540784836, -0.017908263951539993, -0.08732564747333527, 0.07610032707452774, 0.031223153695464134, 0.1301891803741455, 0.08630621433258057, -0.03128940612077713, 0.059383194893598557, -0.022943103685975075, 0.08174660056829453, -0.13526704907417297, 0.016555583104491234, -0.037055615335702896, 0.051038555800914764, -0.08022153377532959, -0.08064967393875122, -0.04014277085661888, 0.020240385085344315, 0.013083923608064651, -0.08310901373624802, 0.05354395508766174, -0.01907895691692829, 0.10401704162359238, -0.06193595379590988, 0.0416560173034668, 0.021699020639061928, 0.011566593311727047, -0.0016853456618264318, -0.03158609941601753, 0.0745888501405716, -0.029586708173155785, 0.03868431597948074, 0.009071356616914272, 0.01579553633928299, -0.030998582020401955, -0.05477164685726166, -0.047628164291381836, 0.0076141697354614735, 0.10678670555353165, -0.06493739783763885, -0.04930916056036949, 0.02303391508758068, -0.06256555765867233, -0.04507359862327576, -0.051979124546051025, 0.06702149659395218, 0.07977153360843658, -0.03276289626955986, 0.06817499548196793, 0.08914514631032944, 0.06951884925365448, 0.014679896645247936, -0.012420528568327427, 0.009156100451946259, 0.02728375606238842, -0.01083667017519474, 0.05071348696947098, 0.027680959552526474, -0.001000085030682385, 0.06620850414037704, -0.005262312013655901, -0.030597558245062828, 0.0243410412222147, -0.07129040360450745, -0.04019130393862724, 0.13251088559627533, -0.02874571830034256, 0.014566777274012566, -0.07165402919054031, -0.02695508487522602, 0.0906958281993866, -0.006081779021769762, 0.054369255900382996, -0.005425622686743736, 0.008027474395930767, -0.09448728710412979, -0.01921176165342331, 0.045040324330329895, -0.04499991610646248, -0.00713813491165638, 0.019104650244116783, -0.0420527346432209, -0.03647426888346672, 0.04768560826778412, 0.04167997092008591, 0.04424469172954559, -7.013354884046891e-33, 0.010530682280659676, 0.033949583768844604, -0.04035315662622452, -0.011721422895789146, -0.04748077690601349, -0.0484868586063385, -0.03547666594386101, -0.04735657200217247, -0.09674358367919922, 0.02445138618350029, -0.049724031239748, 0.022590210661292076, -0.03328565135598183, -0.03145599365234375, 0.021713826805353165, -0.001015084097161889, -0.028232194483280182, -0.03912664204835892, 0.04633334279060364, -0.0456610731780529, 0.01674984209239483, 0.05742191895842552, -0.04785500094294548, -0.014734089374542236, 0.022562969475984573, -0.018411308526992798, -0.07980312407016754, 0.10126425325870514, -0.013259008526802063, 0.05461136996746063, -0.06043408438563347, 0.0005719769396819174, -0.05977784842252731, 0.03743189573287964, -0.011616287752985954, 0.1721496731042862, -0.010412407107651234, 0.01908520795404911, -0.03983133286237717, 0.043242909014225006, -0.004130632616579533, -0.07382100820541382, -0.061352238059043884, -0.05333065241575241, 0.02991613559424877, -0.07537754625082016, 0.026081183925271034, 0.05698845908045769, -0.04380522295832634, 0.012121662497520447, 0.043205343186855316, 0.042619768530130386, 0.020163752138614655, -0.02223281003534794, -0.03163864091038704, 0.08614738285541534, 0.053353529423475266, 0.04705320671200752, 0.10504873842000961, -0.02307097241282463, -0.054600317031145096, -0.07461211830377579, -0.06944689899682999, 0.0149125587195158, -0.05575147271156311, -0.03491786867380142, 0.04830990359187126, 0.013885349966585636, 0.009132559411227703, 0.087431401014328, 0.003748925868421793, 0.033030346035957336, 0.008177860639989376, -0.024355238303542137, -0.05442444980144501, -0.03840165212750435, -0.017684616148471832, -0.006198513321578503, -0.032713815569877625, -0.029312191531062126, -0.02481505088508129, -0.020852278918027878, 0.0031267234589904547, 0.04107090085744858, 0.05360334739089012, 0.02036004513502121, 0.0453045479953289, -0.03374340385198593, 0.006320628337562084, -0.04857785999774933, 0.09412103146314621, -0.00549976946786046, -0.032921720296144485, 0.020086001604795456, -0.02031891979277134, -6.262320795258347e-08, -0.11010339856147766, 0.029426436871290207, 0.020371561869978905, -0.061394695192575455, 0.10632649064064026, -0.09176686406135559, 0.08160725980997086, 0.12455500662326813, -0.0379483662545681, 0.02083432301878929, 0.039729051291942596, 0.03256097435951233, 0.03193513676524162, -0.037210121750831604, -0.027742961421608925, 0.022197162732481956, -0.023966239765286446, -0.05584636330604553, -0.02930331602692604, -0.03483228012919426, -0.01836419850587845, -0.05781691521406174, -0.028416134417057037, 0.02612706646323204, 0.03977185860276222, -0.10804808139801025, -0.06522168219089508, 0.0023376732133328915, 0.03077590838074684, 0.05071539431810379, 0.03407894819974899, 0.0631222203373909, -0.00892147421836853, -0.00246395799331367, 0.07289215922355652, 0.13906718790531158, 0.015145640820264816, -0.05552322044968605, -0.058368980884552, -0.053851496428251266, -0.044943228363990784, 0.08335675299167633, 0.038127779960632324, -0.02434404194355011, 0.049573905766010284, 0.047601163387298584, -0.0327826663851738, -0.02331400103867054, -0.05950111895799637, -0.055286526679992676, 0.09546854346990585, 0.020110206678509712, -0.017595263198018074, 0.04812720790505409, 0.05759366974234581, -0.06120976060628891, -0.0012922213645651937, -0.11348555982112885, 0.022406961768865585, 0.05677501857280731, -0.048889439553022385, 0.05733209103345871, -0.02768857218325138, -0.076047383248806]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\FastLearninginMultiResolutionHierarchies.pdf,Deep Learning,"LEARNING SEQUENTIAL STRUCTURE IN SIMPLE RECURRENT NETWORKS David Servan-Schreiber. Axel Cleeremans. and James L. McClelland Departtnents of Computer Science and Psycholgy Carnegie Mellon University Pittsburgh, PA 15213 ABSTRACT We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t-l, together with element t, to predict element t+ 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. Cluster analyses of the hidden-layer patterns of activation showed that they encode prediction-relevant information about the entire path traversed through the network. We illustrate the phases of learning with cluster analyses performed at different points during training. Several connectionist architectures that are explicitly constrained to capture sequential infonnation have been developed. Examples are Time Delay Networks (e.g. Sejnowski & Rosenberg. 1986) -- also called 'moving window' paradigms -- or algorithms such as back-propagation in time (Rumelhart. Hinton & Williams. 1986), Such architectures use explicit representations of several consecutive events. if not of the entire history of past inputs. Recently. Elman (1988) has introduced a simple recurrent network (SRN) that has the potential to master an infinite corpus of sequences with the limited means of a learning procedure that is completely local in time (see Figure I.). CONIlSrr UNI'l'S Figure 1. The simple recurrent network (Elman, 1988) 643 644 Servan-Schreiber, Cleeremans and McClelland In the SRN. the pattern of activation on the hidden units at time t-I. together with the new input pattern. is allowed to influence the pattern of activation at time t . This is achieved by copying the pattern of activation on the hidden layer at time t·I to a set of input units -- called the 'context units' -- at time t. The forward connections in the network are subject to training via back-propagation. but there is no backpropagation through time. In this paper. we show that the SRN can learn to mimic closely a flnite state automaton. both in its behavior and in its state representations. In particular. we show that it can learn to process an infinite corpus of strings based on experience with afinite set of training exemplars. We then describe the phases through which the appropriate internal representations are discovered during training. MASTERING A FINITE STATE GRAMMAR In our first experiment. we asked whether the network could learn the contingencies implied by a small flnite state grammar (see Figure 2). The network was presented with strings derived from this grammar. and was required to try to predict the next letter at every step. These predictions are context dependent since each letter appears twice in the grammar and is followed in each case by different successors. A single unit on the input layer represented a given letter (six input units in total; flve for the letters and one for a begin symbol 'B'). Similar local representations were used on the output layer (with the 'begin' symbol being replaced by an end symbol 'E'). There were three hidden units. s .tart -~ T Figure 2. The small fmite-state grammar (Reber. 1967) Training. On each of 60.000 training trials. a string was generated from the grammar. starting with 'B'. Successive arcs were selected randomly from the 2 possible continuations with a probability of 0.5. Each letter was Learning Sequential Structure in Simple Recurrent Networks 645 then presented sequentially to the network. The activations of the context units were reset to 0.5 at the beginning of each string. After each letter, the error between the network's prediction and the actual successor specified by the string was computed and back-propagated. The 60,000 randomly generated strings ranged from 3 to 30 letters (mean: 7; sd: 3.3). Performance. Three tests were conducted. First, we examined the network's predictions on a set of 70,000 random strings. During this test, the network is first presented with the start signal, and one of the five letters or E is then selected at random as a successor. If that letter is predicted by the network as a legal successor (Le, activation is above 0.3 for the corresponding unit), it is then presented to the input layer on the next time step, and another letter is drawn at random as its successor. This procedure is repeated as long as each letter is predicted as a legal successor until the end signal is selected as the next letter. The procedure is interrupted as soon as the actual successor generated by the random procedure is not predicted by the network, and the string of letters is then considered 'rejected'. A string is considered 'accepted' if all its letters have been predicted as possible continuations up to, and including, the end signal. Of the 70,000 random strings, 0.3 % were grammatical, and 99.7 % were ungrammatical. The network performed flawlessly, accepting all the grammatical strings and rejecting all the others. In a second test, we presented the network with 20,000 generated at random from the grammar, i.e, all these strings were grammatical. Using the same criterion as above, all of these strings were correctly 'accepted'. Finally, we constructed a set of very long grammatical strings -- more than 100 letters long -- and verified that at each step the network correcdy predicted all the possible successors (activations above 0.3) and none of the other letters in the grammar. Analysis of internal representations. What kind of internal representations have developed over the set of hidden units that allow the network to associate the proper predictions to intrinsically ambiguous letters? One way to answer this question is to record the hidden units' activation patterns generated in response to the presentation of individual letters in different contexts. These activation vectors can then be used as input to a cluster analysis program. Figure 3.A. shows the results of such an analysis conducted on a small random set of grammatical strings. The patterns of activation are grouped according to the nodes of the grammar: all the patterns that are used to predict the successors of a given node are grouped together independently of the current letter. This observation sheds some light on the behavior of the network: at each point in a sequence, the pattern of activation stored over the context units provides information about the current node in the grammar. Together with information about the current letter (represented on the input layer), this contextual information is used to produce a new pattern of activation over the hidden layer, that uniquely specifies the next node. In that sense, the network closely approximates the finite-state automaton that would encode the grammar from which the training exemplars were derived. However, a closer look at the cluster analysis reveals that within a cluster corresponding to a particular node, patterns are further divided according to the path traversed before the node is reached. For example, looking at the bottom cluster -- node #5 -- patterns produced by a 'VV', 'PS', 'XS' or 'SXS' ending are grouped separately: 646 Servan-Schreiber, Cleeremans and McClelland I I I '~'--------~----~------------ I • • Ie .. ! • • Ie I • • ... Figure 3. A. Hieruchical cluster analysis of the hidden unit activation patternS after 60.000 presentations of Slrings generated at random from the finite-Slate grammar. B. Cluster analysis of the H.U. activaDon pauans following 2000 epochs of 1raining 011 a set of 22 strings with a maximum length of eightlettets. Learning Sequential Structure in Simple Recurrent Networks 64 7 they are more similar to each other than to the abstract prototype of node #5. This tendency to preserve information about the path is not a characteristic of traditional finite-state automata. ENCODING PATH INFORMATION In a different set of experiments, we asked whether the SRN could learn to use the infonnation about the path that is encoded in the hidden units' patterns of activation. In one of these experiments, we tested whether the network could master length constraints. When strings generated from the small finite-state grammar may only have a maximum of 8 letters, the prediction following the presentation of the same letter in position number six or seven may be different. For example, following the sequence 'TSSSXXV', 'V' is the seventh letter and only another 'V' would be a legal successor. In contrast, following the sequence 'TSSXXV', both 'V' and 'P' are legal successors. A network with 15 hidden units was trained on a small set of length-limited (max. 8 letters) grammatical strings. It was able to use the small activation differences present over the context units - and due to the slightly different sequences presented - to master contingencies such as those illustrated above (see table 1). Table 1. Activation of each output unit following the presentation of 'Y' as the 6th or 7th letter in the string tssxxV tsssxxV T 0.0 0.0 S P X 0.0 0.54 0.0 0.0 0.02 0.0 V E 0.48 0.0 0.97 0.0 A cluster analysis of all the patterns of activation on the hidden layer generated by each letter in each sequence demonstrates how the influence of the path is reflected in these patterns (see figure 3.B.)*. We labeled the arcs according to the letter being presented (the 'current letter') and its position in the grammar defined by Reber. Thus 'VI' refers to the f11'st 'V' in the grammar and 'V2' to the second 'V' which immediately precedes the end of the string. 'Early' and 'Late' refer to whether the letter occurred early or late in the sequence (for example in 'PT . .' 'T2' occurs early; in 'PVPXT . .' it occurs late). Finally, in the left margin we indicated what predictions the corresponding patterns yield on the output layer (e.g, the hidden unit pattern generated by 'BEGIN' predicts 'T' or 'P'). From the figure, it can be seen that the patterns are grouped according to three distinct principles: (1) according to similar predictions, (2) according to similar letters presented on the input units, and (3) according to similar paths. These factors do not necessarily overlap since several occurrences of the same letter in a sequence usually implies different predictions and since similar paths also lead to different predictions depending on the current letter. For example, the top cluster in the figure corresponds to all occurrences of the letter 'V' and is further subdivided among 'V I' and 'V2"" * Information about the leaves of the cluster analyses in this and the remaining figures is available in Servan-Schreiber. Cleeremans and McCleUand (1988). 648 Servan-Schreiber, Cleeremans and McClelland The 'V l' cluster is itself further divided between groups where 'V l' occurs early in the sequence (e.g, 'pV .. .') and groups where it occurs later (e.g, 'tssxxV .. .' and 'pvpxV .. .'). Note that the division according to the path does not necessarily correspond to different predictions. For example, 'V 2' always predicts 'END' and always with maximum certainty. Nevertheless, sequences up to 'V 2' are divided according to the path traversed. PHASES OF LEARNING How can information about the path be progressively encoded in the hidden layer patterns of activation? To clarify how the network learns to use the context of preceding letters in a sequence, we will illustrate the different phases of learning with cluster analyses of the hidden layer patterns generated at each phase. To make the analyses simpler, we used a smaller training set than the training set mentioned previously. The corresponding finite-state grammar is shown in Figure 4. In this simpler grammar, the main difference -- besides the reduced number of patterns -- is that the letters 'P' and 'T' appear only once. s x .11It --4IF-----~ ----~Eu T Figure 4. The reduced rmite-state grammar from which 12 strings were generated for training Discovering letters. At epoch 0, before the network has received any training, the hidden unit patterns clearly show an organization by letter: to each letter corresponds an individual cluster. These clusters are already subdivided according to preceding sequences -- the 'path'. This fact illustrates how a pattern of activation on the context units naturally tends to encode the path traversed so far independently of any error correcting procedure. The average distance between the different patterns -- the 'contrast' as it were -- is nonetheless rather small; the scale only goes up to 0.6 (see Figure 5.A.)**. But this is due to the very small initial random ** In all the following figures. the scaJe was automatically detennined by the cluster analysis program. It is important to keep this in mind when comparing the figures to . . : I Learning Sequential Structure in Simple Recurrent Netw:,l.'"" 649 Figure 5. Cluster Analyses of the H.U. aclivation pattmls obcained with the reduced set of strings: A. before lraining. B. After 100 epochs of lraining. C. After 700 epochs of training. 650 Servan-Schreiber, Cleeremans and McClelland values of the weights from the input and context layers to the hidden layer. Larger initial values would enhance the network's tendency to capture path infonnation in the hidden unit patterns before training is even started After 100 epochs of training, an organization by letters is still prevalent, however letters have been regrouped according to similar predictions. 'START', 'P' and'S' all make the common prediction of 'X or S' (although'S' also predicts 'END'); 'T' and 'V' make the common prediction of 'V' (although 'V' also predicts 'END' and 'P'). The path information has been almost eliminated: there is very little difference between the patterns generated by two different occurrences of the same letter (see Figure 5.B.). For example, the hidden layer pattern generated by 'S I' and the corresponding output pattern are almost identical to the patterns generated by 'S2' (see table 2). Table 2. Activation of each output unit fOllowing the presentation of the flrst S in the grammar (SI) or the second S (S2> after 100 epochs of training SI S2 T 0.0 0.0 S P 0.36 0.0 0.37 0.0 X 0.33 0.33 V E 0.16 0.17 0.16 0.17 In this phase, the network is learning to ignore the pattern of activation on the context units and to produce an output pattern appropriate to the letter 'S' in any context. This is a direct consequence of the fact that the patterns of activation on the hidden layer -- and hence the context layer -- are continuously changing from one epoch to the next as the weights from the input units (the letters) to the hidden layer are modified. Consequently, adjustments made to the weights from the context layer to the hidden layer are inconsistent from epoch to epoch and cancel each other. In contrast, the network is able to pick up the stable association between each letter and all of its possible successors. Discovering arcs. At the end of this phase, individual letters consistently generate a unique pattern of activation on the hidden layer. This is a crucial step in developing a sensitivity to context: patterns copied onto the context layer have become a unique code designating which letter immediately preceded the current letter. The learning procedure can now exploit the regular association between the pattern on the context layer and the desired output. Around epoch 700, the cluster analysis shows that the network has used this infonnation to differentiate clearly between the fust and second occurrence of the same letter (Figure 5.C.). The pattern generated by 'S2' -- which predicts 'END' -- clusters with the pattern generated by 'V2"" which also predicts 'END'. The overall difference between all the hidden layer patterns has also more than roughly doubled, as indicated by the change in scale. Encoding the path. During the last phase of learning, the network learns to make different predictions to the same occurrence of a letter (e.g, 'V I') each other. Learning Sequential Structure in Simple Recurrent Networks 651 on the basis of the previous sequence. For example, it learns to differentiate between 'ssxxV' which predicts either 'P' or 'V', and 'sssxxV' which predicts only 'V' by exploiting the small difference between the activation patterns generated by X 2 in the two different contexts. The process through which path information is encoded can be conceptualized in the following way: As the initial papers about back- propagation pointed out, the hidden unit patterns of activation represent an 'encoding' of the features of the input patterns that are relevant to the task. In the recurrent network, the hidden layer is presented with information about the current letter, but also -- on the context layer -- with an encoding of the relevant features of the previous letter. Thus, a given hidden layer pattern can come to encode information about the relevant features of two consecutive letters. When this pattern is fed back on the context layer, the new pattern of activation over the hidden units can come to encode information about three consecutive letters, and so on. In this manner, the context layer patterns can allow the network to maintain prediction-relevant features of an entire sequence. However, it is important to note that information about the path that is not relevant locally (Le, that does not contribute to predicting successors of the current letter) tends not to be encoded in the next hidden layer pattern. It may then be lost for subsequent processing. This tendency is lessened when the network has extra degrees of freedom -- i.e, extra hidden units -- so as to allow small and locally useless differences to survive for several processing steps. CONCLUSION We have shown that the network architecture first proposed by Elman (1988) is capable of mastering an infinite coIpus of strings generated from a finite-state grammar after training on a finite set of exemplars with a learning algorithm that is local in time. The network develops internal representations that correspond to the nodes of the grammar and closely approximates the corresponding minimal finite-state recognizer. We have also shown that the simple recurrent network is able to encode information about contingencies that are not local to a given letter and its immediate predecessor, such as those implied by a length constraint on the strings. Encoding of sequential structure in the patterns of activation over the hidden layers proceeds in stages. The network first develops stable hidden-layer representations for individual letters, and then for individual arcs in the grammar. Finally, the network is able to exploit slight differences in the patterns of activation which denote a specific path through the grammar. Our current work is exploring the relevance of this architecture to the processing of embedded sequences typical of natural language. The results of some preliminary experiments are available in Servan-Schreiber, Cleeremans and McClelland (1988). 652 Servan-Schreiber, Cleeremans and McClelland References Elman. J.L. (1988). Finding structure in time. CRL Technical report 9901. Center for Research in Language. University of California. San Diego. Reber. A.S. (1967). Implicit learning of artificial grammars. Journal of Verbal Learning and Verbal Behavior. S. 855-863. Rumelhart. D.E .• Hinton. G.E .• and Williams. R.I. (1986). Learning internal representations by backpropagating errors. Nature 323:533-536. Sejnowski. T J. and Rosenberg C. (1986). NETta1k: A parallel network that learns to read aloud. Technical Report.lohns Hopkins University lHU-EECS-86-01. Servan-Schreiber D. Cleeremans A. and McClelland JL (1988) Encoding sequential structure in simple recurrent networks. Technical Report CMU-CS-88-183. Computer Science Department. Carnegie Mellon University. Pittsburgh. PA 15213. Williams. R.J. and Zipser. D. (1988). A learning algorithm for continually running fully recurrent neural networks. ICS Technical report 8805. Institute for Cognitive Science. UCSD. La lolla. CA 92093.","[-0.06968226283788681, -0.08107119053602219, 0.022718487307429314, 0.04822809621691704, 0.020677903667092323, 0.07393913716077805, -0.0012995683355256915, -0.04598291218280792, 0.04335183650255203, -0.12898696959018707, -0.02248094044625759, -0.01807670295238495, 0.024820255115628242, 0.01136190164834261, -0.06700992584228516, -0.031296875327825546, -0.04067949950695038, 0.05368192493915558, 0.017546527087688446, -0.10924773663282394, 0.0007549480651505291, 0.058741893619298935, -0.03716503828763962, 0.02700728364288807, 0.03233003988862038, 0.09958168864250183, -0.04026870056986809, -0.032358892261981964, 0.03579622879624367, 0.07064931094646454, 0.04283322021365166, -0.04208822920918465, 0.04187050461769104, 0.05083070695400238, -0.07380659878253937, 0.04915664717555046, -0.048781272023916245, 0.057283513247966766, -0.057665418833494186, -0.019348977133631706, 0.015442172065377235, 0.013150475919246674, 0.03920355066657066, 0.04720597341656685, 0.023958515375852585, -0.023157769814133644, 0.0010265117743983865, -0.03374084457755089, -0.09712487459182739, -0.020526614040136337, -0.0988648533821106, 0.027530988678336143, 0.001161088002845645, 0.10064427554607391, 0.004309209529310465, 0.05879756435751915, 0.04887425899505615, 0.09754081070423126, -0.0871116891503334, -0.014998922124505043, -0.011569430120289326, -0.15212321281433105, -0.07233881950378418, -0.053463928401470184, 0.023594627156853676, 0.09824773669242859, -0.01663440652191639, 0.030502421781420708, 0.02934407815337181, -0.025987712666392326, 0.008517496287822723, 0.017993036657571793, -0.0519561767578125, 0.07009332627058029, 0.02635309472680092, 0.03390604630112648, 0.11590670794248581, 0.0025977259501814842, 0.0173408854752779, -0.027322473004460335, 0.058220379054546356, -0.011711849831044674, -0.0014016296481713653, -0.0015248190611600876, 0.05229110270738602, 0.0335027277469635, -0.008956365287303925, 0.042544253170490265, -0.016731666401028633, -0.010454810224473476, -0.003004583762958646, -0.12295030057430267, -0.019167719408869743, -0.0629820004105568, 0.09364819526672363, 0.0764382854104042, -0.05754905194044113, -0.015743080526590347, 0.015478493645787239, 0.08846000581979752, -0.0391673818230629, 0.030243683606386185, 0.02648662030696869, -0.0365537665784359, -0.07920718938112259, 0.025377999991178513, 0.027067141607403755, 0.012934008613228798, 0.05766894295811653, -0.05781470984220505, -0.018858974799513817, 0.03213205933570862, 0.009843701496720314, 0.02221647836267948, 0.06160764396190643, -0.06306210160255432, 0.03012232296168804, -0.017761072143912315, 0.04417744278907776, 0.12880241870880127, 0.007557596545666456, 0.005718414206057787, -0.02634190395474434, 0.025683613494038582, 0.07356896251440048, -0.05527929961681366, -0.06118620187044144, 1.9477169974957577e-33, 0.016363026574254036, -0.0225937832146883, -0.0022357963025569916, -0.012000571936368942, 0.021970970556139946, -0.007074397522956133, -0.0010608264710754156, -0.015896452590823174, 0.021499546244740486, -0.018160739913582802, -0.01795126125216484, -0.019778132438659668, -0.03433068469166756, 0.07529475539922714, 0.03701276332139969, -0.02643537148833275, 0.053101327270269394, 0.019220121204853058, 0.04137946292757988, -0.08257465064525604, -0.0013821395114064217, -0.010388074442744255, 0.014813484624028206, -0.0706494078040123, -0.0215191338211298, -0.03631214052438736, 0.04066264629364014, -0.06119807809591293, 0.009205571375787258, 0.0037127418909221888, -0.08353792130947113, 0.031025070697069168, -0.07547903060913086, -0.00031033690902404487, 0.07335470616817474, 0.031325869262218475, 0.0530952550470829, -0.0698661208152771, 0.06665988266468048, -0.05664241313934326, 0.0035382206551730633, -0.007978158071637154, 0.029654670506715775, 0.07441821694374084, -0.07280737906694412, -0.05491136014461517, -0.049593761563301086, 0.03763839602470398, -0.0386618934571743, -0.04998841509222984, 0.008275968953967094, -0.002986449981108308, -0.043217387050390244, -0.11245004087686539, 0.0001588981249369681, 0.04329630360007286, 0.010226987302303314, 0.08448480069637299, 0.013180308975279331, 0.09633450210094452, 0.051901835948228836, 0.07514824718236923, 0.013005790300667286, 0.050407733768224716, -0.0027157010044902563, 0.046176034957170486, -0.1026918962597847, 0.05453217774629593, 0.10877176374197006, -0.017347147688269615, -0.03027929924428463, 0.036036621779203415, -0.040196340531110764, 0.01615837588906288, 0.08636445552110672, 0.012425445020198822, -0.009137778542935848, -0.105475053191185, -0.10105327516794205, 0.08471190184354782, -0.03638136386871338, -0.07438942044973373, -0.050819337368011475, 0.038266610354185104, -0.01464223861694336, -0.01778508350253105, 0.07099854201078415, -0.012134966440498829, -0.015749309211969376, -0.052467118948698044, -0.019163060933351517, -0.02872377634048462, 0.03934309259057045, 0.06244846433401108, -0.015482448041439056, -3.073671975612563e-33, -0.008080928586423397, 0.012740177102386951, -0.011515138670802116, 0.005862533114850521, 0.021943556144833565, -0.022505205124616623, -0.09745656698942184, 0.038357213139534, -0.06102437525987625, -0.017694024369120598, 0.06367475539445877, -0.013778425753116608, 0.04005947709083557, -0.01829792931675911, 0.06055394560098648, 0.028307951986789703, 0.038534976541996, -0.005113264545798302, 0.08188138902187347, -0.02194485440850258, 0.06117257848381996, 0.02799910493195057, -0.1475086361169815, 0.030709557235240936, -0.039781756699085236, -0.04567749425768852, 0.043429162353277206, 0.0978083685040474, -0.0056054056622087955, 0.05342439189553261, -0.05229324474930763, -0.03388660401105881, 0.008009582757949829, 0.02284770831465721, -0.04309628903865814, 0.0925588607788086, 0.004285699222236872, -0.07494185864925385, 9.629719716031104e-05, -0.024541083723306656, 0.10563908517360687, -0.01623767800629139, -0.012514953501522541, 0.009589571505784988, -0.006265755742788315, -0.06245061382651329, -0.07280603051185608, 0.06253853440284729, -0.051096487790346146, 0.045271143317222595, -0.0014163535088300705, 0.081854909658432, -0.041626088321208954, -0.09179609268903732, -0.0310959555208683, 0.0734860971570015, -0.0354669988155365, -0.028441190719604492, 0.07596345990896225, -0.014596698805689812, -0.023226168006658554, -0.04829229786992073, 0.017713818699121475, -0.04204699024558067, 0.05399267002940178, -0.07054515928030014, -0.005876926705241203, -0.021746603772044182, 0.003601910313591361, -0.036335818469524384, 0.016482826322317123, 0.029504621401429176, 0.002449728548526764, 0.06939949840307236, -0.016595833003520966, -0.11764488369226456, -0.034720852971076965, 0.018790923058986664, -0.06485463678836823, -0.0066691855899989605, -0.040923167020082474, 0.04347663372755051, -0.013637690804898739, 0.022276269271969795, 0.10438992828130722, 0.0853687971830368, 0.05619514361023903, 0.07189980894327164, 0.08633214980363846, 0.009148488752543926, -0.02441112883388996, 0.03409678861498833, -0.00367635115981102, 0.022189747542142868, -0.06816370040178299, -5.330949193194101e-08, -0.081575408577919, -0.03853590413928032, 0.006771445740014315, 0.0391714908182621, 0.06035979092121124, -0.04525447636842728, 0.033794503659009933, 0.0024606629740446806, -0.048547111451625824, -0.056068308651447296, 0.07935579121112823, 0.01603444665670395, -0.04591221362352371, -0.021721307188272476, -0.014893189072608948, 0.044374730437994, 0.05695280805230141, -0.08472195267677307, -0.00839557871222496, -0.013622524216771126, 0.08768382668495178, -0.027268389239907265, -0.0004438363539520651, 0.07580263912677765, 0.011618967168033123, -0.060839030891656876, -0.05220688879489899, 0.10318613052368164, -0.0003009612555615604, 0.002027180278673768, -0.010270060040056705, 0.04566212743520737, -0.040207888931035995, -0.018434161320328712, -0.05249052122235298, 0.09383200109004974, 0.05979122593998909, -0.017534518614411354, 0.005280569661408663, -0.014385989867150784, 0.006266366224735975, -0.05655835568904877, -0.008905052207410336, 0.01481184083968401, 0.012968817725777626, -0.04872957989573479, -0.06840718537569046, -0.09229400753974915, 0.007473695557564497, -0.09532494097948074, 0.027637599036097527, 0.011436848901212215, 0.019967181608080864, 0.06489729136228561, 0.07360681146383286, -0.025157535448670387, 0.0007362640462815762, -0.07320868968963623, -0.018090911209583282, 0.03374683856964111, -0.05935429409146309, 0.08419967442750931, -0.05125821381807327, -0.03425329178571701]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\FurtherExplorationsinVisuallyGuidedReachingMakingMURPHYSmarter.pdf,Deep Learning,"Self Organizing Neural Networks for the Identification Problem Manoel Fernando Tenorio School of Electrical Engineering Purdue University VV. Lafayette, UN. 47907 tenoriQ@ee.ecn.purdue.edu ABSTRACT VVei-Tsih Lee School of Electrical Engineering Purdue University VV. Lafayette, UN. 47907 lwt@ed.ecn.purdue.edu This work introduces a new method called Self Organizing Neural Network (SONN) algorithm and demonstrates its use in a system identification task. The algorithm constructs the network, chooses the neuron functions, and adjusts the weights. It is compared to the Back-Propagation algorithm in the identification of the chaotic time series. The results shows that SONN constructs a simpler, more accurate model. requiring less training data and epochs. The algorithm can be applied and generalized to appilications as a classifier. I. INTRODUCTION 1.1 THE SYSTEM IDENTIFICATION PROBLEM In various engineering applications, it is important to be able to estimate, interpolate, and extrapolate the behavior of an unknown system when only its input-output pairs are available. Algorithms which produce an estimation of the system behavior based on these pairs fall under the category of system identification techniques. 1.2 SYSTEM IDENTIFICATION USING NEURAL NETWORKS A general form to represent systems, both linear and nonlinear, is the Kolmogorov- Garbor polynomial tGarbor. 19611 shown below: y = ao + L aixi + L L aijxiXj + ... 1 i J (1) 57 58 Tenorio and Lee where the y is the output. and x the input to the system. [Garbor .1961] proposed a learning method that adjusted the coefficient of (1) by minimizing the mean square error between each desired output sample and the actual output This paper describes a supervised learning algorithm for structure construction and adjustment Here. systems which can be described by (1) are presented. The computation of the function for each neuron performs a choice from a set of possible functions previously assigned to the algorithm. and it is general enough to accept a wide range of both continuous and discrete functions. In this work. the set is taken from variants of the 2-input quadratic polynomial for simplicity. although there is no requirement making it so. This approach abandons the simplistic mean-square error for perfonnance measure in favor of a modified Minimum Description Length (MOL) criterion [Rissanen,1975]. with provisions to measure the complexity of the model generated. The algorithm searches for the simplest model which generates the best estimate. The modified MDL. from hereon named the Structure Estimation Criterion (SEC). is applied hierarchically in the selection of the optimal neuron transfer function from the function set. and then used as an optimal criterion to guide the construction of the structure. The connectivity of the resulting structure is arbitrary. and under the correct conditions [Geman&Geman. 84] the estimation of the struCture is optimal in tenns of the output error and low function complexity. This approach shares the same spirit of GMDH-type algorithms. However, the concept of parameter estimation from Information Theory. combined with a stochastic search algorithm - Simulated Annealing. was used to create a new tool for system identification. This work is organized as follows: section II presents the problem formulation and the Self Organizing Neural Network (SONN) algorithm description; section III describes the results of the application of SONN to a well known problem tested before using other neural network algorithms [Lapede8&Farber. 1987; Moody. 1988]; and fmally, section IV presents a discussion of the results and future directions for this work. II. THE SELF ORGANIZING NEURAL NETWORK ALGORITHM 11.1 SELF ORGANIZING STRUCTURES The Self Organizing Neural Network (SONN) algorithm performs a search on the model space by the construction of hypersurfaces. A network of nodes. each node representing a hypersurface. is organized to be an approximate model of the real system. SONN can be fully characterized by three major components. which can be modified to incorporate knowledge about the process: (1) a generating rule of the primitive neuron transfer functions. (2) an evaluation method which accesses the quality of the model. and. (3) a structure search strategy. Below. the components of SONN are discussed. ll.2 THE ALGORITHM STRUCTURE Self Organizing Neural Networks 59 11.2.1 The Generating Rule Given a set of observations S: S = {(Xl, Yl),(Xl, Yl)"",,(XI, YI)} Yi = f(XV + 11 generated by (2) where f(.) is represented by a Kolmogorov-Garbor polynomial. and the random variable 11 is nonnally distributed. N(O.l). The dimensions of Y is m. and the dimensions of X is n. Every component Yk of Y fonns a hypersurface Yk = fk(X) in the space of dim (X) + 1. The problem is to fmd f(.). given the observations S. which is a corrupted version of the desired function. In this work. the model which estimates f(.) is desired to be as accurate and simple (small number of parameters. and low degree of non linearity) as possible. The approach taken here is to estimate the simplest model which best describes f(.) by generating optimal functions for each neuron. which can be viewed as the construction of a hypersurface based on the observed data. It can be described as follows: given a set of observations S; use p components of the n dimensional space of X to create a hypersurface which best describes Yk = f(X). through a three step process. First, given X = [xl' x2' x3' .... xn) and Yk' and the mapping '¥ n: [Xl' x2' x3' .... Xn) -> [x'¥(1)' x'¥(2)' x,¥(3)' .... x'¥(n»)' construct the hypersurface hi (x'¥(1)' x'¥(2)' x,¥(3)' .... x'¥(n» (hi after the fIrst iteration) of p+ 1 dimensions. where '¥ n is a projection from n dimensions to p dimensions. The elements of the domain of '¥ n are called tenninals. Second. If the global optimality criterion is reached by the construction of hi(x'¥(l)' x'¥(2)' x,¥(3)' .... x'¥(n»' then stop. otherwise continue to the third step. Thud. generate from [Xl' x2' x3' .... xn.hl(x'¥(l)' x'¥(2)' x,¥(3)' .... x'¥(n») a new p+l dimensional hypersurface hi+ I through the extended mapping '¥ n+ 1 (.). and reapply the second step.The resulting model is a multilayered neural network whose topology is arbitrarily complex and created by a stochastic search guided by a structure estimation criterion. For simplicity in this work. the set of prototype functions (F) is restricted to be 2-input quadratic surfaces or smaller. with only four possible types: y = 8o+alxl +a2x2 (3) y = 8o+alxl +a2x2+a3xlx2 (4) Y = 3o+alxl+a2x1 (5) Y = 8o+alxl+a2x2+a3xlx2+~x1+a5x~ (6) 11.2.2 Evaluation or the Model Based on the MDL Criterion The selection rule (T) of the neuron transfer function was based on a modifIcation of the Minimal Description Length (MOL) information criterion. In [Rissanen. 1975] the principle of minimal description for statistical estimation was developed. The MDL provides a trade-off between the accuracy and the complexity of the model by including the structure estimation tenn of the fInal model. The final model (with the minimal 60 Tenorio and Lee MOL) is optimum in the sense of being a consistent estimate of the number of parameters while achieving the minimum error [Rissanen.1980]. Given a sequence of observation xl,x2,x3 •...• xN from the random variable X. the dominant tenn of the MDL in [Rissanen. 1975] is: MDL = -log f(xI8) + 0.5 k log N where f(xI8) is the estimated probability density function of the model. k is the number of parameters. and N is the number of observations. The first tenn is actually the negative of the maximum likelihood (ML) with respect to the estimated parameter. The second term describes the structure of the models and it is used as a penalty for the complexity of the model. In the case of linear polynomial regression. the MOL is: MDL = -0.5 N log S~ + 0.5 k log N (8) where k is the number of coefficients in the model selected. In the SONN algorithm. the MDL criterion is modified to operate both recursively and hierarchically. First. the concept of the MDL is applied to each candidate prototype surface for a given neuron. Second. the acceptance of the node. based on Simulated Annealing. uses the MDL measure as the system energy. However. since the new neuron is generated from terminals which can be the output of other neurons. the original defmition of the MDL is unable to compute the true number of system parameters of the final function. Recall that due to the arbitrary connectivity. feedback loops and other configurations it is non trivial to compute the number of parameters in the entire structure. In order to reflect the hierarchical nature of the model. a modified MDL called Structure Estimation Criterion (SEC) is used in conjunction with an heuristic estimator of the number of parameters in the system at each stage of the algorithm. A computationally efficient heuristic for the estimation of the number of parameters in the model is based on the fact that SONN creates a tree-like structure with multiple roots at the input terminals. Then k. in expression (8). can be estimated recursively by: k = kL + kR + (no. of parameters of the current node) (9) where kL and kR are the estimated number of parameters of the left and right parents of the current node. respectively. This heuristic estimator is neither a lower bound nor an upper bound of the true number of parameter in the model. 11.2.3 The SONN Algorithm To explain the algorithm. the following definitions are necessary: Node - neuron and the associated function. connections. and SEC; BASIC NODE - A node for the system input variable; FRONT NODE - A node without children; IN1ERMIDIA TE NODE - The nodes that are neither front or basic nodes; STATE - The collection of nodes. and the configuration of their interconnection; INITIAL STATE (SO - The state with only basic nodes; PARENT AND CHILD STATE - The child state is equal to the parent state except for f a new node and its interconnection generated on the parent state structure; NEIGHBOR STATE - A state that is either a child or a parent state of another; ENERGY Self Organizing Neural Networks 61 OF THE STATE (SEC-Si) - The energy of the state is defined as the minimum SEC of all the front nodes in that state. In the SONN algorithm. the search for the correct model structure is done via Simulated Annealing. Therefore the algorithm at times can accept partial structures that look less than ideal. In the same way. it is able to discard partially constructed substructures in search for better results. The use of this algorithm implies that the node accepting rule (R) varies at run-time according to a cooling temperature m schedule. The SONN algorithm is as follows: Initialize T, and S[ Repeat Repeat Sj = generate (Si), - application of P. If accept ( SEC_Sj. SEC_Si, T) then Si = Sj. - application ofR. WUiI the number of new neurons is greater than N. Decrease the temperature T. until The temperature T is smaller than tend (Terminal temperature for Simulated Annealing). Each neuron output and the system input variables are called terminals. Tenninals are viewed as potential dimensions from which a new hypersurface can be constructed. Every tenninal represents the best tentative to approximate the system function with the available infmnatioo. and are therefore treated equally. lll. EXAMPLE - THE CHAOTIC TIME SERIES In the following results. the chaotic time series generated by the Mackay-Glass differential equations was used. The SONN with the SEC. and its heuristic variant were used to obtain the approximate model of the system. The result is compared with those obtained by using the nonlinear signal processing method [LapedeS&Farber. 1987] . The advantages and disadvantages of both approaches are analyzed in the next section. 111.1 Structure of the Problem The MacKay-Glass differential equation used here can be described as: dX(t) = a x(t - t) _ b x(t) at 1 + x10(t - t) (10) By setting a = 0.2. b = 0.1. and t = 17. a chaotic time series with a' strange attractor of fractal dimension about 3.5 will be produced [Lapedes&Farber. 1987] . To compare the accuracy of prediction the nonnalized root mean square error is used as a perfonnance index: 62 Tenorio and Lee nnalized RMSE - RMSE no - Standard Deviation (ll) 111.2. SONN WITH THE HEURISTIC SEC (SONN.H) In the following examples, a modified hewistic version of the SEC is used. The estimator of the number of parameters is given by (9), and the fmal configuraion is shown in figure 1. 111.2.1 Node 19 In this subsection, SONN is allowed to generate up to the 19th accepted node. In this first version of the algorithm, all neurons have the same number of interconnections. and therefore draw their transfer function from the same pool of functions .. Generalizations of the algorithm can be easily made to accommodate multiple input functions, and neuron transfer function assignment being drawn from separate pools. In this example, the first one hundred points of the time series was used for training, and samples 101 through 400 used for prediction testing. The total number of weights in the network is 27. The performance index average 0.07. The output of the network is overlapped in the figure 2 with the original time series. For comparison purposes, a GDR network with the structure used in [LapedeS&Farber, 1987] is trained for 6500 epochs. The training data consisted of the first 500 points of the time series, and the testing data ran from the 501st sample to the 832nd. The total number of weights is 165. and the fmal performance index equal to 0.12. This was done to give both algorithms similar computational resources. Figure 3 shows the original time series overlapped with the GDR network output. Ill.2.2 NODE 37 In this subsection, the model chosen was formed by the 37th accepted node. The network was trained in a similar manner to the flfSt example, sioce it is part of the same run. The final number of weights is 40, and the performance index 0.018. Figure 4 shows the output of the network overlapped with the original time series. Figure 5 shows the GDR with 11,500 epochs. Notice that in both cases, the GDR network demands 150 connections and 150 weights. as compared to 12 connections and 27 weights for the first example and 10 connections and 40 weights for the second example. The comparison of the performance of different models is listed in figure 6. IV. Conclusion and Future Work In this study, we proposed a new approach for the identification problem based on a flexible, self-organizing neural network (SONN) structure. The variable structure provides the opportunity to search and construct the optimal model based on input-output observations. The hierarchical version of the MDL, called the structure estimation criteria, Self Organizing Neural Networks 63 was used to guide the trade-off between the model complexity and the accuracy of the estimation. The SONN approach demonstrates potential usefulness as a tool for system identification through the example of modeling a chaotic time series. REFERENCE Garber, D .• eL al. ,""A universal nonlinear filter, predicator and simulator which optimizes itsekf by a learning process,"" IEE Proc.,18B, pp. 422-438, 1961 RissanenJ. ""Modeling by shortest data description,"" Automatica. vo1.14, pp. 465- 471.1975 Gemen, S, and Gernen D., ""Stochastic relaxation, gibbs deisribution, and the bayesian restoration of images."" IEEE PAMI .• PAMI-6,pp.721-741. 1984 Lapedes.A. and Farber, R. ,""Nonlinear signal processing using neural networks: Predication and system modeling,"" TR.. LA-UR-87-2662. 1987 Moody. J. This volume Rissanen:,,J. ""Consistent order estimation of autoregressive processing by shortest description of data."" Analysis and optimization of stochastic system. Jacobs et. al. Eds. N.Y. Academic. 1980 Figure 1. The 37th State Generated --------_ .. _ .. - .. _---_. S""s._IO.·SOHNI_ 191 0> o. o. , ••. f..---.....---....---__.-----.------I '. ,.,. '.0 0'"" ,IIA Figure 2. SONN 19th Model. P.I. = 0.06 64 Tenorio and Lee ..--------------------. +----~-- - _- .- .-~ -. --l 'lft ~nn nn. Figure 3. GDR after 6.500 Epochs. P.I. = 0.12 no "".0 '.00 Figure 4. SONN 37th Model. P.I. = 0.038 ~------------------- . ------ <; •• 1_ 10 .. ""ICk P'''Dlq1I!O"" -III II 500 EDOCM , \ ~ r , \ Ii \ I rl , \ \ .. , i ' , • ...-()t,... -. ·,-...C)o .... .. .0 .no ~ ~o . no OOD Figure 5. GDR after 11.500 Epochs. P.I. = 0.018 Comparison ollhe Perform alice Index 014~-------------------------. 0.12 0. 10 002 000 .,.-._._.;,_."", .......... .o. _._._ .. _~ .. _._._._._.- .. -.~.,..~ .............. ~ ........ .. ............... ...................... . ... -_.... ..... . . ... . -. .. .. ~ ..... . 002 _______ --------'--- 12u W' I>:'UU t: I'''~''~ ~Vt~tllfl""""Uaj l~, SOliN (o .. ~j ~7) BP \ I :;W Ep~I"" Figure 6. Perfonnance Index Versus the Number of Predicted Points","[-0.12469486147165298, -0.026098452508449554, 0.0369102917611599, -0.0064876084215939045, -0.04674284905195236, 0.018803393468260765, 0.012101014144718647, 0.008422143757343292, 0.009221063926815987, -0.10636621713638306, -0.06746578216552734, 0.008230583742260933, 0.04719230532646179, -0.08624283224344254, -0.15584519505500793, -0.015735818073153496, 0.004996608477085829, -0.009801831096410751, 0.0015193559229373932, -0.055559221655130386, 0.05807828530669212, 0.012300439178943634, -0.07500263303518295, 0.06093556433916092, 0.003225203137844801, 0.09712915867567062, 0.01715897209942341, 0.04895540326833725, -0.036750588566064835, -0.02936028689146042, 0.10197571665048599, -0.05248421058058739, 0.007351066451519728, -0.024167606607079506, -0.04073861241340637, -0.012008784338831902, -0.025753242895007133, -0.008455743081867695, -0.012055565603077412, -0.021052587777376175, -0.014324093237519264, -0.017731742933392525, 0.014014197513461113, 0.02666645310819149, 0.06084173917770386, 0.06352652609348297, 0.02427758276462555, -0.05552975833415985, -0.036283865571022034, -0.005228993017226458, 0.00986757967621088, 0.06931337714195251, -0.02181578055024147, 0.06413740664720535, 0.0423416830599308, 0.08173616230487823, 0.04743955284357071, 0.04873421788215637, -0.05092277750372887, 0.039233844727277756, 0.04576271399855614, 0.028266666457057, 0.06846634298563004, 0.01621479168534279, 0.006865531671792269, 0.07609579712152481, 0.010403245687484741, 0.09467844665050507, 0.02668115310370922, -0.061322521418333054, 0.043387845158576965, 0.08564064651727676, 0.045063089579343796, 0.0013227349845692515, 0.0325293131172657, 0.027455830946564674, 0.004206293728202581, 0.049133576452732086, -0.01016109436750412, -0.05600008741021156, 0.012544529512524605, 0.03570687025785446, -0.051975756883621216, 0.028061313554644585, 0.0647854208946228, 0.054263852536678314, -0.07242089509963989, 0.026377394795417786, 0.09046340733766556, -0.01841616816818714, 0.00359360221773386, -0.03807225823402405, -0.018088817596435547, -0.04160324111580849, 0.11531530320644379, 0.035205427557229996, 0.044999416917562485, 0.006758718751370907, 0.028397226706147194, 0.0319022536277771, -0.05965923145413399, -0.03760949522256851, 0.031432174146175385, -0.004013221710920334, 0.11341004818677902, 0.058805692940950394, 0.04738835245370865, -0.010268233716487885, 0.11364580690860748, -0.09860379248857498, -0.08289515972137451, -0.01214874628931284, -0.08421885967254639, 0.02047421596944332, 0.08327102661132812, -0.05680888146162033, 0.043902311474084854, 0.02979561872780323, -0.010785208083689213, 0.08147679269313812, 0.04342886433005333, -0.060713838785886765, 0.04530101642012596, -0.006775807123631239, 0.06086062639951706, 0.0498247891664505, -0.1087014377117157, 2.55148303850174e-33, -0.09858442097902298, 0.012227313593029976, -0.031116116791963577, -0.09107007831335068, 0.015583612956106663, -0.007998250424861908, -0.007284785155206919, -0.01809779182076454, 0.0709080845117569, -0.008765130303800106, -0.1014287993311882, 0.05110620707273483, -0.008995085954666138, -0.003981300629675388, 0.03913411498069763, 0.014618223533034325, 0.022311246022582054, -0.013218037784099579, 0.027712790295481682, -0.16735397279262543, 0.013087671250104904, 0.002797907218337059, 0.058612722903490067, -0.08884149044752121, 0.010793686844408512, 0.031734321266412735, 0.04893505945801735, 0.002077885903418064, -0.024334734305739403, 0.012304220348596573, 0.0648542195558548, -0.007943934760987759, -0.05676715821027756, -0.0809314027428627, 0.051727354526519775, -0.0005436391802504659, 0.06423783302307129, 0.05367530882358551, 0.005474792327731848, 0.001758606405928731, 0.023588838055729866, -0.04080045223236084, -0.03751056268811226, 0.031889624893665314, -0.03702361881732941, -0.03105982206761837, 0.017688479274511337, 0.005070791579782963, 0.02571343071758747, -0.07494840770959854, -0.06641092896461487, -0.07714314758777618, 0.04851720482110977, -0.11973333358764648, 0.0038506805431097746, -0.04550175741314888, 0.029364926740527153, 0.04963682219386101, 0.026439063251018524, 0.0943506583571434, -0.04479917138814926, -0.023494046181440353, 0.01636696606874466, 0.013997819274663925, 0.10421176999807358, -0.021026961505413055, 0.04143913835287094, -0.02552528865635395, 0.030755486339330673, -0.03639177978038788, 0.04421979561448097, -0.03297139331698418, -0.05016222223639488, 0.029994886368513107, 0.06295962631702423, -0.029511358588933945, 0.018809445202350616, 0.017113832756876945, -0.11089715361595154, -0.04928990826010704, -0.024660972878336906, -0.04697462171316147, 0.039730384945869446, -0.0127283725887537, -0.007203000131994486, 0.02229497954249382, 0.03210601210594177, 0.015270804055035114, -0.06294823437929153, 0.004415260627865791, -0.03513919562101364, 0.007010091561824083, 0.0931280329823494, 0.05688251554965973, -0.01858167164027691, -2.4488732182635008e-33, -0.03192354366183281, 0.08716505765914917, -0.05466082692146301, 0.010048966854810715, 0.07386375218629837, 0.016219118610024452, -0.07076946645975113, 0.06584832817316055, -0.01825404353439808, 0.041114576160907745, -0.03132716938853264, -0.028461875393986702, 0.04598892852663994, 0.020500266924500465, 0.04055982455611229, 0.024496931582689285, -0.014476251788437366, -0.03564394637942314, 0.08744410425424576, 0.009871055372059345, -0.06028897315263748, 0.05347398668527603, -0.10357292741537094, -0.0332704596221447, -0.030355213209986687, -0.022315308451652527, -0.0350387841463089, 0.07537557929754257, -0.0021283861715346575, 0.0011509027099236846, -0.07187065482139587, -0.033237408846616745, -0.010535283014178276, 0.05326490476727486, -0.010290210135281086, 0.06594155728816986, -0.009335610084235668, -0.0342213399708271, 0.029875226318836212, -0.00962569285184145, 0.0023299846798181534, 0.08251267671585083, -0.05747929960489273, -0.04639383405447006, -0.028445595875382423, -0.04847589135169983, -0.03990856185555458, 0.1416061967611313, -0.0012978551676496863, 0.09185285121202469, -0.00022303228615783155, -0.026502110064029694, -0.055458176881074905, -0.037763580679893494, 0.032832663506269455, 0.10220889747142792, 0.07841567695140839, 0.027263449504971504, -0.004335889592766762, 0.02919274941086769, -0.02704177238047123, -0.0670543760061264, 0.06185116246342659, -0.0033667234238237143, -0.012265685945749283, -0.010613948106765747, -0.031468506902456284, 0.01429128460586071, 0.006107416935265064, 0.016655832529067993, 0.06656074523925781, 0.0148985730484128, 0.02438070811331272, 0.0056651863269507885, -0.0804920494556427, -0.07886572927236557, -0.040257059037685394, -0.009028994478285313, -0.01928912289440632, -0.056718964129686356, -0.027414843440055847, -0.005380837246775627, 0.03372255712747574, 0.013936174102127552, -0.043919339776039124, 0.033415041863918304, 0.13004375994205475, 0.04354007542133331, 0.04907602071762085, -0.03499338775873184, 0.06297918409109116, 0.024927979335188866, -0.10050404816865921, 0.07896346598863602, -0.05314265936613083, -4.800616082434317e-08, -0.05855735391378403, -0.06788072735071182, 0.046391215175390244, 0.00392634142190218, 0.10464224964380264, -0.0328386127948761, 0.046211954206228256, -0.011324509978294373, 0.003588673658668995, -0.09505939483642578, 0.06294821202754974, -0.06011920049786568, -0.07752916216850281, -0.008515800349414349, -0.018613960593938828, -0.021019790321588516, 0.007415485568344593, 0.0372198149561882, -0.06874287128448486, -0.024976413697004318, 0.06224410980939865, -0.004598408006131649, -0.04963165894150734, -0.02296050451695919, 0.12354254722595215, -0.060247793793678284, -0.06850571185350418, -0.025659359991550446, -0.009572602808475494, 0.028827110305428505, -0.03699038177728653, 0.03204275667667389, 0.000864398549310863, -0.006990000139921904, 0.021009411662817, 0.10248768329620361, 0.035433150827884674, 0.024732667952775955, -0.05237947404384613, -0.04332788661122322, -0.02345561422407627, 0.043797023594379425, 0.0279543437063694, -0.052256278693675995, 0.04988755285739899, -0.04253285005688667, 0.043207090348005295, -0.13216882944107056, 0.052556708455085754, 0.004670141730457544, 0.0828092098236084, -0.06350600719451904, -0.0076451352797448635, -0.03924821689724922, 0.0025515675079077482, -0.020916026085615158, -0.018881337717175484, -0.05271504446864128, -0.0068419030867516994, 0.010505089536309242, -0.04276302084326744, 0.06631793081760406, -0.012373202480375767, -0.036360546946525574]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\GEMINIGradientEstimationThroughMatrixInversionAfterNoiseInjection.pdf,Deep Learning,"NEURAL NETWORK RECOGNIZER FOR HAND-WRITTEN ZIP CODE DIGITS J. S. Denker, W. R. Gardner, H. P. Graf, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, H. S. Baird, and I. Guyon AT &T Bell Laboratories Holmdel, New Jersey 07733 ABSTRACT This paper describes the construction of a system that recognizes hand-printed digits, using a combination of classical techniques and neural-net methods. The system has been trained and tested on real-world data, derived from zip codes seen on actual U.S. Mail. The system rejects a small percentage of the examples as unclassifiable, and achieves a very low error rate on the remaining examples. The system compares favorably with other state-of-the art recognizers. While some of the methods are specific to this task, it is hoped that many of the techniques will be applicable to a wide range of recognition tasks. MOTIVATION The problem of recognizing hand-written digits is of enormous practical and the~ retical interest [Kahan, Pavlidis, and Baird 1987; Watanabe 1985; Pavlidis 1982]. This project has forced us to formulate and deal with a number of questions rang- ing from the basic psychophysics of human perception to analog integrated circuit design. This is a topic where ""neural net"" techniques are expected to be relevant, since the task requires closely mimicking human performance, requires massively parallel processing, involves confident conclusions based on low precision data, and requires learning from examples. It is also a task that can benefit from the high throughput potential of neural network hardware. Many different techniques were needed. This motivated us to compare various clas- sical techniques as well as modern neural-net techniques. This provided valuable information about the strengths, weaknesses, and range of applicability of the nu- merous methods. The overall task is extremely complex, so we have broken it down into a great number of simpler steps. Broadly speaking, the recognizer is divided into the pre- processor and the classifier. The two main ideas behind the preprocessor are (1) to remove meaningless variations (i.e. noise) and (2) to capture meaningful variations (i.e. salient features). Most of the results reported in this paper are based on a collection of digits taken from hand-written Zip Codes that appeared on real U.S. Mail passing through the 323 324 Denker, et al (j{OL3l-/.jGIJ JI<=t OI~.?4--:; ~789 d/~~'f.!Jd,7 87 012dL/-S-67 get Figure 1: Typical Data Buffalo, N.Y. post office. Details will be discussed elsewhere [Denker et al., 1989]. Examples of such images are shown in figure 1. The digits were written by many different people, using a great variety of writing styles and instruments, with widely varying levels of care. Important parts of the task can be handled nicely by our lab's custom analog neural network VLSI chip [Gra! et aI., 1987; Gra! & deVegvar, 1987], allowing us to perform the necessary computations in a reasonable time. Also, since the chip was not designed with image processing in mind, this provided a good test of the chips' versatility. THE PREPROCESSOR Acquisition The first step is to create a digital version of the image. One must find where on the envelope the zip code is, which is a hard task in itself (Wang and Srihari 1988]. One must also separate each digit from its neighbors. This would be a relatively simple task if we could assume that a character is contiguous and is disconnected from its neighbors, but neither of these assumptions holds in practice. It is also common to find that there are meaningless stray marks in the image. Acquisition, binarization, location, and preliminary segmentation were performed by Poetal Service contractors. In some images there were extraneous marks, so we developed some simple heuristics to remove them while preserving, in most cases, all segments of a split character. Scaling and Deskewing At this point, the size of the image is typically 40 x 60 pixels, although the scaling routine can accept images that are arbitrarily large, or as small as 5 x 13 pixels. A translation and scale factor are then applied to make the image fit in a rectangle Neural Network Recognizer for Hand-Written Zip Code Digits 325 20 x 32 pixels. The character is centered in the rectangle, and just touches either the horizontal or vertical edges, whichever way fits. It is clear that any extraneous marks must be removed before this step, lest the good part of the image be radically compressed in order to make room for some wild mark. The scaling routine changes the horizontal and vertical size of the image by the same factor, so the aspect ratio of the character is preserved. As shown in figure 1, images can differ greatly in the amount of skew, yet be considered the same digit. This is an extremely significant noise source. To remove this noise, we use the methods of [Casey 1970]; see also [Naylor 1971]. That is, we calculate the XY and YY moments of the image, and apply a linear transformation that drives the XY moment to zero. The transformation is a pure shear, not a rotation, because we find that rotation is much less common than skew. The operations of scaling and deskewing are performed in a single step. This yields a speed advantage, and, more importantly, eliminates the quantization noise that would be introduced by storing the intermediate images as pixel maps, were the calculation carried out in separate steps. Skeletonization For the task of digit recognition, the width of the pen used to make the characters is completely meaningless, and is highly variable. It is important to remove this noise source. By deleting pixels at the boundaries of thick strokes. After a few iterations of this process, each stroke will be as thin as possible. The idea is to remove as many pixels as possible without breaking the connectivity. Connectivity is based on the 8 nearest neighbors. This can be formulated as a pattern matching problem - we search the image looking for situations in which a pixel should be deleted. The qecisions can be expressed as a convolution, using a rather small kernel, since the identical decision process is repeated for each location in the image, and the decision depends on the configuration of the pixel's nearest and next-nearest neighbors. Figure 2 shows an example of a character before (e) and after (I) skeletonization. It also shows some of the templates we use for skeletonization, together with an indication of where (in the given image) that template was active. To visualize the convolution process, imagine taking a template, laying it over the image in each possible place, and asking if the template is ""active"" in that place. (The template is the convolution kernel; we use the two terms practically interchangeably.) The portrayal of the template uses the following code: Black indicates that if the cor- responding pixel in the image is ON, it will contribute +1 to the activity level of this template. Similarly, gray indicates that the corresponding pixel, if ON, will contribute -5, reducing the activity of this template. The rest of the pixels don't matter. If the net activity level exceeds a predetermined threshold, the template is considered active at this location. The outputs of all the skeletonizer templates 326 Denker, et al a) b) c) d) Figure 2: Skeletonization are eombined in a giant logieal OR, that is, whenever any template is aetive, we eonelude that the pixel presently under the eenter of the template should be deleted. The skeletonization eomputation involves six nested loops: for each iteration I for all X in the image (horizontal eoordinate) for all Y in the image (vertical eoordinate) for all T in the set of template shapes for all P in the template (horizontal) for all Q in the template (vertical) compare image element(X +P, Y +Q) with template(T) element(P, Q) The inner three loops (the loops over T, P, and Q) are performed in parallel, in a single cyde of our special-purpose ehip. The outer three loops (1, X, and Y) are performed serially, calling the ehip repeatedly. The X and Y loops eould be performed in parallel with no change in the algorithms. The additional parallelism would require a proportionate increase in hardware. Neural Network Recognizer for Hand-Written Zip Code Digits 327 The purpose of template a is to detect pixels at the top edge of a thick horizontal line. The three ""should be OFF"" (light grey shade in figure 2) template elements enforce the requirement that this should be a boundary, while the three ""should be ON"" (solid black shade in figure 2) template elements enforce the requirement that the line be at least two pixels wide. Template b is analogous to template a, but rotated 90 degrees. Its purpose is to detect pixels at the left edge of a thick vertical line. Template c is similar to, but not exactly the same as, template a rotated 180 degrees. The distinction is necessary because all templates are applied in parallel. A stroke that is only two pixels thick ·must not be attacked from both sides at once, lest it be removed entirely, changing the connectivity of the image. Previous convolutional line-thinning schemes [Naccache 1984] used templates of size 3 x 3, and therefore had to use several serial sub-stages. For parallel operation at least 3 x 4 kernels are needed, and 5 x 5 templates are convenient, powerful, and flexible. Feature Maps Having removed the main sources of meaningless variation, we turn to the task of extracting the meaningful information. It is known from biological studies [Hubel and Wiesel 1962] that the human vision system is sensitive to certain features that occur in images, particularly lines and the ends of lines. We therefore designed detectors for such features. Previous artificial recognizers [\Vatanabe 1985] have used similar feature extractors. Once again we use a convolutional method for locating the features of interest - we check each location in the image to see if each particular feature is present there. Figure 3 shows some of the templates we use, and indicates where they become active in an example image. The feature extractor templates are 7 x 7 pixels - slightly larger than the skeletonizer templates. Feature b is designed to detect the right-hand end of (approximately) horizontal strokes. This can be seen as follows: in order for the template to become active at a particular point, the image must be able to touch the ""should be ON"" pixels at the center of the template without touching the surrounding horseshoe-shaped collection of ""'must be OFF"" pixels. Essentially the only way this can happen is at the right-hand end of a stroke. (An isolated dot in the image will also activate this template, but the images, at this stage, are not supposed to contain dots). Feature d detects (approximately) horizontal strokes. There are 49 different feature extractor templates. The output of each is stored separately. These outputs are called feature maps, since they show what feature(s) occurred where in the image. It is possible, indeed likely, that several different features will occur in the same place. Whereas the outputs of all the skeletonizer templates were combined in a very simple way (a giant OR), the outputs of the feature extractor templates are combined in 328 Denker, et al a) b) c) • I ~------~.~ ~.~------~ Figure 3: Feature Extraction various artful ways. For example, feature"" and a similar one are O~d to form a single combined feature that responds to right-hand ends in general. Certain other features are ANDed to form detectors for arcs (long curved strokes). There are 18 combined features, and these are what is passed to the next stage. We need to create a compact representation, but starting from the skeletonized image, we have, instead, created 18 feature maps of the same size. Fortunately, we can now return to the theme of removing meaningless variation. If a certain image contains a particular feature (say a left-hand stroke end) in the upper left corner, it is not really necessary to specify the location of that feature with great precision. To recognize the Ihope of the feature required considerable precision at the input to the convolution, but the pOlitiora of the feature does not require so much precision at the output of the convolution. We call this Coarse Blocking or Coarse Coding of the feature maps. We find that 3 x 5 is sufficent resolution. CLASSIFIERS If the automatic recognizer is unable to classify a particular zip code digit, it may be possible for the Post Office to determine the correct destination by other means. This is costly, but not nearly so costly as a misclassification (substitution error) that causes the envelope to be sent to the wrong destination. Therefore it is critically Neural Network Recognizer for Hand-Written Zip Code Digits 329 important for the system to provide estimates of its confidence, and to reject digits rather than misclassify them. The objective is not simply to maximize the number of classified digits, nor to minimize the number of errors. The objective is to minimize the cost of the whole operation, and this involves a tradeoff between the rejection rate and the error rate. Preliminary Inves tigations Several different classifiers were tried, including Parzen Windows, K nearest neigh- bors, highly customized layered networks, expert systems, matrix associators, fea- ture spins, and adaptive resonance. We performed preliminary studies to identify the most promising methods. We determined that the top three methods in this list were significantly better suited to our task than the others, and we performed systematic comparisons only among those three. Classical Clustering Methods We used two classical clustering techniques, Parzen Windows (PW) and K Near- est Neighbors (KNN), which are nicely described in Duda and Hart [1973]. In this application, we found (as expected) that they behaved similarly, although PW consistently outperformed KNN by a small margin. These methods have many advantages, not the least of which is that they are well motivated and easily un- derstood in terms of standard Bayesian inference theory. They are well suited to implementation on parallel computers and/or custom hardware. They provide ex- cellent confidence information. Unlike modern adaptive network methods, PW and KNN require no ""learning time"", Furthermore the performance was reproducible and responded smoothly to improvements in the preprocessor and increases in the size of the training set. This is in contrast to the ""noisy"" performance of typical layered networks. This is con- venient, indeed crucial, during exploratory work. Adaptive Network Methods In the early phases of the project, we found that neural network methods gave rather mediocre results. Later, with a high-performance preprocessor, plus a large training database, we found that a layered network gave the best results, surpassing even Parzen Windows. We used a network with two stages of processing (i.e., two layers of weights), with 40 hidden units and using a one-sided objective function (as opposed to LMS) as described in [Denker and Wittner 1987]. The main theoretical advantage of the layered network over the classical methods is that it can form ""higher order"" features - conjunctions and disjunctions of the features provided by our feature extractor. Once the network is trained, it has the advantage that the classification of each input is very rapid compared to PW or KNN. Furthermore, the weights represent a compact distillation of the training data and thus have a smaller memory requirement. The network provides confidence information that is 330 Denker, et al just as good as the classical methods. This is obtained by comparing the activation level of the most active output against the runner-up unit(s). To check on the effectiveness of the preprocessing stages, we applied these three classification schemes (PW, KNN, and the two-layer network) on 256-bit vectors consisting of raw bit maps of the images - with no skeletonization and no feature extraction. For each classification scheme, we found the error rate on the raw bit maps was at least a factor of 5 greater than the error rate on the feature vectors, thus clearly demonstrating the utility of feature extraction. TESTING It is impossible to compare the performance of recognition systems except on iden- tical databases. Using highly motivated ""friendly"" writers, it is possible to get a dataset that is so clean that practically any algorithm would give outstanding re- sults. On the other hand, if the writers are not motivated to write clearly, the result will be not classifiable by machines of any sort (nor by humans for that matter). It would have been much easier to classify digits that were input using a mouse or bitpad, since the lines in the such an image have zero thickness, and stroke-order information is available. It would also have been much easier to recognize digits from a single writer. The most realistic test data we could obtain was provided by the US Postal Service. It consists of approximately 10,000 digits (1000 in each category) obtained from the zip codes on actual envelopes. The data we received had already been binarized and divided into images of individual digits, rather than multi-digit zip codes, but no further processing had been done. On this data set, our best performance is as follows: if 14% of the images are rejected as unclassifiable, only 1% of the remainder are misclassified. If no images are re- jected, approximately 6% are misclassified. Other groups are working with the same dataset, but their results have not yet been published. Informal communications indicate that our results are among the best. CONCLUSIONS We have obtained very good results on this very difficult task. Our methods include low-precision and analog processing, massively parallel computation, extraction of biologically-motivated features, and learning from examples. We feel that this is, therefore, a fine example of a Neural Information Processing System. We empha- size that old-fashioned engineering, classical pattern recognition, and the latest learning-from-examples methods were all absolutely necessary. Without the careful engineering, a direct adaptive network attack would not succeed, but by the same token, without learning from a very large database, it would have been excruciating to engineer a sufficiently accurate representation of the probability space. Neural Network Recognizer for Hand-Written Zip Code Digits 331 Acknowledgements It is a pleasure to acknowledge useful discussions with Patrick Gallinari and tech- nical assistance from Roger Epworth. We thank Tim Barnum of the U.S. Postal Service for making the Zip Code data available to us. References 1. R. G. Casey, ""Moment Normalization of Handprinted Characters"", IBM J. Res. Develop., 548 (1970) 2. J. S. Denker et al., ""Details of the Hand-Written Character Recognizer"", to be published (1989) 3. R. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis, John Wiley and Sons (1973) 4. E. Gullichsen and E. Chang, ""Pattern Classification by Neural Network: An Experimental System for Icon Recognition"", Proc. IEEE First Int. Conf. on Neural Networks, San Diego, IV, 725 (1987) 5. H. P. Graf, W. Hubbard, L. D. Jackel, P.G.N. deVegvar, ""A CMOS Associative Memory Chip"", Proc. IEEE First Int. Conf. on Neural Networks, San Diego, 111,461 (1987) 6. H.P Graf and P. deVegvar, ""A CMOS Implementation of a Neural Network Model"", Proc. 1987 Stanford Conf. Advanced Res. VLSI, P. Losleben (ed.) MIT Press, 351 (1987) 7. D. H. Hubel and T. N. Wiesel, ""Receptive fields, binocular interaction and functional architecture in the cat's visual cortex"", J. Physiology 160, 106 (1962) 8. S. Kahan, T. Pavlidis, and H. S. Baird, ""On the Recognition of Printed Char- acters of Any Font and Size"", IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-9, 274 (1987) 9. N. J. Naccache and R. Shinghal, ''SPTA: A Proposed Algorithm for Thinning Binary Patterns"", IEEE Trans. Systems, Man, and Cybernetics, SMC-14, 409 (1984) 10. W. C. Naylor, ""Some Studies in the Interactive Design of Character Recogni- tion Systems"", IEEE Transactions on Computers, 1075 (1971) 11. T. Pavlidis, Algorithms for Graphics and Image Processing, Computer Science Press (1982) 12. C. Y. Suen, M. Berthod, and S. Mori, ""Automatic Recognition of Handprinted Characters - The State of the Art"", Proceedings of the IEEE 68 4, 469 (1980). 13. C-H. Wang and S. N. Srihari, ""A Framework for Object Recognition in a Vi- sually Complex Environment and its Application to Locating Address Blocks on Mail Pieces"", IntI. J. Computer Vision 2, 125 (1988) 14. S. Watanabe, Pattern Recognition, John Wiley and Sons, New York (1985)","[-0.13734658062458038, -0.019197942689061165, -0.0195452980697155, -0.04927574843168259, -0.06602086871862411, 0.08843628317117691, 0.020314594730734825, -0.0074791987426579, -0.03547811880707741, -0.07757639139890671, -0.040625449270009995, 0.11211231350898743, 0.07530099898576736, 0.04056273773312569, -0.1763543039560318, -0.07218722254037857, -0.012061572633683681, 0.0527307391166687, 0.004609254654496908, 0.019735317677259445, 0.007441478781402111, 0.03117995895445347, 0.014857790432870388, -0.062483303248882294, 0.052826739847660065, 0.04354780912399292, -0.04520726576447487, 0.0105288065969944, 0.0016814288683235645, 0.0010876290034502745, 0.06892078369855881, 0.07820502668619156, 0.04316462203860283, 0.04700561985373497, -0.007614079862833023, -0.0044546956196427345, -0.037741970270872116, 0.023538898676633835, 0.02445903792977333, -0.009367753751575947, -0.0106129739433527, -0.03688646852970123, 0.014335673302412033, 0.08150645345449448, 0.08230305463075638, 0.03363875299692154, 0.000381677586119622, 0.054866667836904526, -0.0456656776368618, -0.03388389199972153, -0.02678280510008335, 0.052663177251815796, -0.025175582617521286, 0.0722164586186409, -0.00581635907292366, -0.03853703662753105, 0.035706937313079834, 0.021587710827589035, -0.13300307095050812, 0.029662692919373512, -0.011160244233906269, 0.03039737418293953, -0.06010197103023529, -0.04871479794383049, 0.021614080294966698, 0.03176601603627205, -0.012015385553240776, 0.011894220486283302, -0.0309612900018692, -0.10682828724384308, 0.06378956139087677, 0.11224843561649323, -0.09032262116670609, 0.04894213750958443, 0.02703377977013588, 0.00620447751134634, 0.020595457404851913, 0.046373941004276276, 0.020841630175709724, -0.029382890090346336, -0.05349281057715416, -0.01769118942320347, 0.0651499480009079, 0.0673174038529396, 0.020662054419517517, 0.027296653017401695, -0.06903695315122604, 0.0920284315943718, -0.031619325280189514, -0.05239002779126167, -0.03488932549953461, -0.07323480397462845, 0.00986809004098177, -0.10094892233610153, -0.019684258848428726, 0.06834077835083008, -0.0006186348618939519, 0.011843668296933174, -0.06944064795970917, 0.08575824648141861, -0.042554501444101334, 0.023692119866609573, 0.03303191438317299, -0.11079339683055878, 0.06490276753902435, 0.017583543434739113, 0.0671694353222847, -0.04546418786048889, 0.05817359685897827, -0.1375543475151062, 0.012061970308423042, -0.016009874641895294, -0.08061809837818146, -0.0371745228767395, 0.0619116872549057, -0.09283946454524994, 0.00613011047244072, 0.02929922379553318, 0.0909251794219017, 0.010908539406955242, -0.10123719274997711, -0.012569976970553398, -0.08139552175998688, 0.03401500731706619, 0.003688865341246128, 0.052350856363773346, -0.02758331224322319, 8.649620796201445e-34, -0.05759228393435478, 0.051622647792100906, 0.01078744139522314, -0.013475123792886734, 0.045061055570840836, -0.021548975259065628, -0.05144843831658363, 0.018936239182949066, 0.022403661161661148, 0.03491251543164253, -0.043041858822107315, 0.04341119900345802, 0.03404848650097847, 0.10284373164176941, 0.0746857076883316, 0.04026755690574646, -0.015267559327185154, -0.003820613259449601, -0.01431282889097929, -0.03312879428267479, 0.0053091044537723064, -0.053251754492521286, 0.10433338582515717, 0.008480273187160492, 0.02380230464041233, -0.026401633396744728, -0.009780624881386757, -0.039515770971775055, 0.06108648329973221, -0.01238712202757597, 0.02724999189376831, 0.006115112453699112, 0.035522136837244034, -0.03854828327894211, 0.020915474742650986, -0.021620212122797966, 0.055132023990154266, 0.010579943656921387, -0.0003220442740712315, 0.00014377744810190052, -0.002018499653786421, 0.007132015191018581, -0.010984999127686024, 0.018894227221608162, -0.015698352828621864, 0.0015494439285248518, 0.04518363997340202, 0.04198039323091507, 0.03380080685019493, -0.03864184021949768, -0.009605104103684425, 0.039119113236665726, -0.0740484669804573, -0.03578897938132286, 0.0077846166677773, -0.0433514378964901, 0.05013960227370262, 0.036609992384910583, 0.033532220870256424, 0.01992366835474968, -0.011357249692082405, 0.04717140272259712, 0.03694775700569153, 0.060797106474637985, 0.019032880663871765, -0.04292536526918411, -0.026953240856528282, 0.014982335269451141, 0.04495678469538689, -0.00610994640737772, -0.007641545031219721, -0.009085374884307384, -0.014803857542574406, -0.06564900279045105, 0.02449130266904831, 0.026967046782374382, 0.07053511589765549, -0.07638977468013763, -0.03039700724184513, -0.008172607980668545, 0.042092449963092804, 0.0160821583122015, -0.014149169437587261, 0.03439607843756676, -0.006947923917323351, 0.00029651724616996944, 0.06435033679008484, -0.04960944876074791, 0.007691868115216494, 0.013099207542836666, -0.026250271126627922, -0.04099937528371811, -0.012645025737583637, -0.040808919817209244, 0.004723045509308577, -1.9817809870726644e-33, -0.03836645931005478, 0.04729839041829109, -0.09141983836889267, -0.029000572860240936, -0.110122911632061, -0.01030307449400425, -0.019805746152997017, 0.006205921061336994, -0.055398449301719666, 0.003061501309275627, -0.01070970669388771, 0.0057974946685135365, 0.020076723769307137, -0.003823742736130953, -0.006736666429787874, -0.031971558928489685, -0.07304921746253967, 0.09094331413507462, 0.07583165168762207, 0.010589206591248512, 0.014308953657746315, 0.07948672026395798, -0.10979561507701874, 0.06127675995230675, -0.007638345472514629, -0.0438191182911396, -0.006356590893119574, 0.024575229734182358, -0.021928630769252777, -0.006340111140161753, -0.05321183428168297, -0.05989104509353638, -0.024048835039138794, 0.04036303982138634, -0.049218472093343735, -0.009985651820898056, 0.06335501372814178, 0.06263861060142517, 0.0008302197675220668, 0.04699863865971565, 0.06843405216932297, 0.026654714718461037, -0.07305778563022614, -0.022687437012791634, -0.006312723737210035, -0.11824257671833038, -0.15103915333747864, 0.047871656715869904, -0.016649775207042694, 0.05419811233878136, 0.005405329167842865, 0.004790209233760834, -0.06742043048143387, -0.03258313611149788, 0.004257810302078724, 0.07751821726560593, -0.03264062479138374, 0.010998751036822796, 0.053205862641334534, 0.05369583144783974, -0.0709429383277893, -0.08528691530227661, 0.059349045157432556, -0.021366000175476074, 0.06306926161050797, -0.07911960780620575, -0.01240477990359068, 0.045353878289461136, -0.026990970596671104, -0.021420519798994064, 0.015017126686871052, -0.007653483655303717, 0.06848165392875671, 0.01312959473580122, -0.015835026279091835, 0.010833927430212498, -0.043308790773153305, 0.04324011877179146, -0.05655497685074806, -0.03338497132062912, 0.013281112536787987, -0.0436389222741127, -0.023691322654485703, 0.10350930690765381, 0.07492788136005402, 0.061493128538131714, 0.04394852742552757, -0.03310193493962288, 0.03758648410439491, -0.003662957577034831, 0.009640461765229702, 0.135030597448349, 0.0049296156503260136, 0.05684814229607582, -0.0434972308576107, -4.9490793685436074e-08, -0.05679628998041153, 0.024734295904636383, 0.008270341902971268, -0.028054561465978622, 0.03804844617843628, 0.016679363325238228, 0.047683797776699066, 0.025831224396824837, -0.041589077562093735, -0.04671601206064224, 0.1102081909775734, -0.09734378010034561, -0.10535632818937302, -0.055323973298072815, 0.0164767038077116, 0.04013001173734665, 0.057127829641103745, -0.047781266272068024, -0.031956084072589874, 0.05994417890906334, 0.11112131923437119, 0.02602395974099636, 0.07756104320287704, 0.04547016695141792, -0.048253417015075684, -0.10162148624658585, -0.05136683210730553, 0.06593151390552521, 0.007457276340574026, 0.011217253282666206, 0.016581542789936066, 0.015159553848206997, 0.0051961541175842285, -0.05718114599585533, 0.05600891634821892, 0.027566002681851387, 0.02878718636929989, -0.021342940628528595, 0.021478919312357903, 0.0060192374512553215, 0.06131496652960777, -0.02193303033709526, -0.10791172832250595, -0.01910751312971115, 0.0632288008928299, -0.10277126729488373, 0.09167139232158661, -0.12463323026895523, -0.009772606194019318, 0.037468601018190384, 0.015587246045470238, 0.02810961753129959, -0.012362735345959663, 0.05497199669480324, 0.01526299025863409, -0.025892436504364014, -0.01082362700253725, -0.05147911235690117, -0.002187918173149228, 0.1087622418999672, -0.023207494989037514, 0.0737316831946373, -0.07191246002912521, -0.0008141875732690096]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\HeterogeneousNeuralNetworksforAdaptiveBehaviorinDynamicEnvironments.pdf,Deep Learning,"PROGRAMMABLE ANALOG PULSE-FIRING NEURAL NETWORKS Alan F. Murray Alister Hamilton Dept. of Elec. Eng., Dept. of Elec. Eng., University of Edinburgh, University of Edinburgh, Mayfield Road, Mayfield Road, Edinburgh, EH9 3JL Edinburgh, EH9 3JL United Kingdom. United Kingdom. ABSTRACT Lionel Tarassenko Dept. of Eng. Science, University of Oxford, Parks Road, Oxford, OX1 3PJ United Kingdom. We describe pulse - stream firing integrated circuits that imple- ment asynchronous analog neural networks. Synaptic weights are stored dynamically, and weighting uses time-division of the neural pulses from a signalling neuron to a receiving neuron. MOS transistors in their ""ON"" state act as variable resistors to control a capacitive discharge, and time-division is thus achieved by a small synapse circuit cell. The VLSI chip set design uses 2.5J.1.m CMOS technology. INTRODUCTION Neural network implementations fall into two broad classes - digital [1,2] and analog (e.g. [3,4]). The strengths of a digital approach include the ability to use well-proven design techniques, high noise immunity, and the ability to implement programmable networks. However digital circuits are synchronous, while biological neural networks are asynchronous. Further- more, digital multipliers occupy large areas of silicon. Analog networks offer asynchronous behaviour, smooth neural activation and (potentially) small circuit elements. On the debit side, however, noise immunity is low, arbitrary high precision is not possible; and no reliable ""mainstream"" analog nonvolatile memory technology exists. Many analog VLSI implementations are nonprogrammable, and therefore have fixed functionality. For instance, subthreshold MOS devices have been used to mimic the nonlinearities of neural behaviour, in implementing Hop- field style nets [3] , associative memory [5] , visual processing functions [6] , and auditory processing [7]. Electron-beam programmable resistive inter- connects have been used to represent synaptic weights between more con- ventional operational-amplifier neurons [8,4]. We describe programmable analog pulse-firillg neural networks that use 00- chip dynamic analog storage capacitors to store synaptic weights, currently 671 672 Hamilton, Murray and Tarassenko refreshed from an external RAM via a Digital -Analog converter. PULSE-FIRING NEURAL NETWORKS A pulse-firing neuron, i is a circuit which signals its state, V. by generating a stream of 0-5V pulses on its output. The pulse rate R.' varies from 0 when neuron i is OFF to R.(max) when neuron i is fully ION. Switching between the OFF and ON stAtes is a smooth transition in output pulse rate between these lower and upper limits. In a previous system, outlined below, the synapse allows a proportion of complete presynaptic neural pulses V. to be added (electrically OR-ed) to its output. A synaptic ""gating"" function, determined by T .. , allowed bursts of complete pulses through the synapse. Moving down a'l column of synapses, therefore, we see an ever more crowded asynchronous mass of pulses, representing the aggregated activity of the receiving neuron. In the system that forms the substance of this paper, a proportion (determined by T .. ) of each presynaptic pulse is passed to the postsynaptic summation. l] INTEGRATOR RING OSCLLATOR ~------------------------~I~I --------------------------------~ 11111111111111111111111111 Excitatory ""-.... A~ ~ 11111111111111111111111111 III! Wibitory Activity XI I I I I Pll.SE GENERATOR Figure 1. Neuron Circuit NEURON CIRCUIT Figure 1 shows a CMOS implementation of the pulse-firing neuron function in a system where excitatory and inhibitory pulses are accumulated on separate channels. The output stage of the neuron consists of a ""ring oscilla- tor"" - a feedback circuit containing an odd number of logic inversions, with the loop broken by a NAND gate, controlled by a smoothly varying voltage representing the neuron's total activity, j="" -1 Xj = L TjjV, j=O Programmable Analog Pulse-Firing Neural Networks 673 This activity is increased or decreased by the dumping or removal of charge packets from the ""integrator"" circuit. The arrival of an excitatory pulse dumps charge, while an inhibitory pulse removes it. Figure 2 shows a device level (SPICE) simulation of the neuron circuit. A strong excitatory input causes the neural potential to rise in steps and the neuron turns ON. Subse- quent inhibitory pulses remove charge packets from the integrating capacitor at a higher rate, driving the neuron potential down and switching the neu- ron OFF. 5 Ol------J N euro n Output 5 Neural Potential (V4) '0 > O---------J Inhibitory input 5 o Excitatory input o 9 Figure 2. SPICE Simulation of Neuron SYNAPSE CIRCUIT - USING CHOPPING CLOCKS In an earlier implementation, ""chopping clocks"" were introduced - synchro- nous to one another, but asynchronous to the neural firing. One bit of the (digitally stored) weight T .. indicates its sign, and each other bit of precision is represented by a chopping clock. The clocks are non-overlapping, the MSB clock is high for lh of the time, the next for % of the time, etc. These clocks are used to gate bursts of pulses such that a fraction T .. of the pulses are passed from the input of the synapse to either the excita¥ory or inhibi- tory output channel. 674 Hamilton, Murray and Tarassenko CHOPPING CLOCK SYSTEM - PROBLEMS A custom VLSI synaptic array has been constructed [9] with the neural function realised in discrete SSI to allow flexibility in the choice of time con- stants. The technique has proven successful, but suffers from a number of problems:- - Digital gating (""using chopping clocks"") is clumsy - Excitation and Inhibition on separate lines - bulky - Synapse complicated and of large area - < 100 synapses per chip - < 10 neurons per chip In order to overcome these problems we have devised an alternative arith- metic technique that modulates individual pulse widths and uses analog dynamic weight storage. This results in a much smaller synapse. < w » L WxTij xTij -----,I L Figure 3. Pulse Multiplication Increment Activity SYNAPSE CIRCUIT - PULSE MULTIPLICATION The principle of operation of the new synapse is illustrated in Figure 3. Each presynaptic pulse of width W is modulated by the synaptic weight T .. such that the resulting postsynaptic pulse width is lJ W.Tij This is achieved by using an analog voltage to modulate a capacitive discharge as illustrated in Figure 4. The presynaptic pulse enters a CMOS inverter whose positive supply voltage (V dd) is controlled by T ... The capa- citor is nominally charged to Vdd, but begins to discharge at a gonstant rate when the input pulse arrives. When the voltage on the capacitor falls below the threshold of the following inverter, the synapse output goes high. At the end of the presynaptic pulse the capacitor recharges rapidly and the synapse output goes low, having output a pulse of length W.T "". The circuit is now lJ Programmable Analog Pulse-Firing Neural Networks 675 ready for the next presynaptic pulse. This mechanism gives a linear rela- tionship between multiplier Wand inverter supply voltage, Vdd. Tik Determines Vdd for inverter Vk Ire1 Figure 4. Improved Synapse Circuit FULL SYNAPSE Synaptic weight storage is achieved using dynamic analog storage capacitors refreshed from off-chip RAM via a Digital-Analog converter. A CMOS active-resistor inverter is used as a buffer to isolate the storage capacitor from the multiplier circuit as shown in the circuit diagram of a full synapse in Figure 5. SYNAPTIC WEIGHT TIt I T PRESYNAPTIC STATE Vk BIAS VOl. TAGE Vdd TIt -11- Figure s. Full Synapse Circuit A capacitor distributed over a column of synaptic outputs stores neural activity, x., as an analog voltage. The range over which the synapse voltage - pulse tithe multiplier relationship is linear is shown in Figure 6. This wide 676 Hamilton, Murray and Tarassenko (:=c2V) range may be used to implement inhibition and excitation in a single synapse, by ""splitting"" the range such that the lower volt (l-2V) represents inhibition, and the upper volt (2-3V) excitation. Each presynaptic pulse removes a packet of charge from the activity capacitor while each postsynap- tic pulse adds charge at twice the rate. In this way, a synaptic weight voltage of 2V, giving a pulse length multiplier of lh, gives no net change in neuron activity x .. The synaptic weight voltage range 1-2V therefore gives a net reduction'in neuron activity and is used to represent inhibition, the range 2-3V gives a net increase in neuron activity and is used to represent excita- tion. 1.0 0.6 - o . 4 - ---- -- ------- ------------- 0.2 o . -,' .' . . . . --------- -- -- o 123 4 Synapse Voltage Tij (V) Figure 6. Multiplier Linearity 5 The resulting synapse circuit implements excitation and inhibition in 11 transistors per synapse. It is estimated that this technique will yield more than 100 fu.ly programmable neurons per chip. FURTHER WORK There is still much work to be done to refine the circuit of Figure 5 to optimise (for instance) the mark-space ratio of the pulse firing and the effect of pulse overlap, and to minimise the power consumption. This will involve the creation of a custom pulse-stream simulator, implemented directly as code, to allow these parameters to be studied in detail in a way that probing an actual chip does not allow. Finally, as Hebbian- (and modified Hebbian - for instance [10]) learning schemes only require a synapse to ""know"" the presynaptic and postsynaptic states, we are able to implement it on-chip at little cost, as the chip topology makes both of these signals available avail- able to the synapse locally. This work introduces as many exciting possibili- ties for truly autonomous systems as it does potential problems! Programmable Analog Pulse-Firing Neural Networks 677 Acknowledgements The authors acknowledge the support of the Science and Engineering Research Council (UK) in the execution of this work. References 1. A. F. Murray, A. V. W. Smith, and Z. F. Butler, ""Bit - Serial Neural Networks,"" Neural Information Processing Systems (Proc. 1987 NIPS Conference), p. 573, 1987. 2. S. C. J. Garth, ""A Chipset for High Speed Simulation of Neural Net- work Systems,"" IEEE Conference on Neural Networks, San Diego, vol. 3, pp. 443 - 452, 1987. 3. M~ A. SiviloUi, M. R. Emerling, and C. A. Mead, ""VLSI Architec- tures for Implementation of Neural Networks,"" Proc. AlP Conference on Neural Networks for Computing, Snowbird, pp. 408 - 413, 1986. 4. H. P. Graf, L. D. Jackel, R. E. Howard, B. Straughn, J. S. Denker, W. Hubbard, D. M. Tennant, and D. Schwartz, ""VLSI Implementa- tion of a Neural Network Memory with Several Hundreds of Neu- rons,"" Proc. AlP Conference on Neural Networks for Computing, Snowbird, pp. 182 - 187, 1986. 5. M. Sivilotti, M. R. Emerling, and C. A. Mead, ""A Novel Associative Memory Implemented Using Collective Computation,"" Chapel Hill Conf. on VLSI, pp. 329 - 342, 1985. 6. M. A. Sivilotti, M. A. Mahowald, and C. A. Mead, ""Real - Time Visual Computations Using Analog CMOS Processing Arrays,"" Stan- ford VLSI Confeence, pp. 295-312, 1987. 7. C. A. Mead, in Analog VLSI and Neural Systems, Addison-Wesley, 1988. 8. W. Hubbard, D. Schwartz, J. S. Denker, H. P. Graf, R. E. Howard, L. D. Jackel, B. Straughn, and D. M. Tennant, ""Electronic Neural Networks,"" Proc. AlP Conference on Neural Networks for Computing, Snowbird, pp. 227 - 234, 1986. 9. A. F. Murray, A. V. W. Smith, and L. Tarassenko, ""Fully- Programmable Analogue VLSI Devices for the Implementation of Neural Networks,"" Int. Workshop on VLSI for Artificial Intelligence, 1988. 10. S. Grossberg, ""Some Physiological and Biochemical Consequences of Psychological Postulates,"" Proc. Natl. Acad. Sci. USA, vol. 60, pp. 758 - 765, 1968.","[-0.088609479367733, -0.0034321725834161043, 0.0083015076816082, 0.0241934172809124, -0.046440109610557556, -0.07467646896839142, 0.010947958566248417, -0.0490407831966877, 0.026215972378849983, -0.06965421140193939, -0.048785723745822906, -0.03685383498668671, -0.059236329048871994, 0.01755407266318798, -0.03164537250995636, 0.011923108249902725, -0.008247626014053822, 0.046958260238170624, 0.007785864174365997, -0.012459841556847095, 0.045538514852523804, -0.012445313856005669, -0.019393321126699448, 0.03982757031917572, -0.036524467170238495, -0.012306015007197857, -0.0445324182510376, -0.056329283863306046, -0.019345739856362343, -0.037804484367370605, 0.05945470929145813, -0.1029861643910408, -0.03500095382332802, 0.03516809269785881, -0.07754797488451004, -0.003416604595258832, 0.058092910796403885, -0.020060794427990913, 0.0654686763882637, -0.005845271982252598, 0.028729140758514404, -0.06007595732808113, 0.03299133479595184, 7.060806092340499e-05, 0.041693516075611115, 0.014003640040755272, 0.030994169414043427, -0.0502333790063858, -0.06073516234755516, 0.02676960453391075, 0.01629526913166046, 0.01792771928012371, 0.017502959817647934, 0.041384220123291016, -0.02472117729485035, 0.08094688504934311, 0.002007263246923685, 0.08171810209751129, -0.07877769321203232, -0.020065398886799812, -0.07614858448505402, -0.016731537878513336, 0.053101327270269394, -0.005172200966626406, -0.05880562588572502, 0.008525924757122993, 0.02297261543571949, 0.06682702898979187, 0.1448230892419815, -0.034914229065179825, 0.052229974418878555, 0.045033711940050125, 0.033719535917043686, 0.011510955169796944, -0.05565707013010979, -0.055048223584890366, 0.050918497145175934, 0.06657374650239944, 0.00022486119996756315, -0.010578546673059464, 0.043055903166532516, -0.05887777730822563, -0.051249098032712936, 0.0005839758086949587, 0.0041749379597604275, 0.07343701273202896, -0.028426295146346092, 0.02215597964823246, -0.007267296314239502, -0.04732871428132057, -0.04448118805885315, -0.00702103553339839, 0.05247047543525696, -0.03447480872273445, 0.06573240458965302, -0.0954422578215599, 0.1076706051826477, 0.05351278558373451, -0.036922164261341095, 0.048307884484529495, -0.029758188873529434, 0.04141027107834816, 0.0024073461536318064, -0.015234211459755898, 0.03913966193795204, -0.05771246552467346, 0.030150311067700386, 0.088633693754673, -0.006332695484161377, -0.08468939363956451, 0.0068921358324587345, 0.011150958947837353, -0.019236912950873375, -0.019022291526198387, 0.11115434020757675, 0.006336059421300888, -0.060349758714437485, -0.02573322504758835, 0.08516890555620193, -0.02848660759627819, -0.10128940641880035, -0.00152390799485147, -0.19561906158924103, -0.013474457897245884, 0.022795522585511208, 0.03346816450357437, -0.09729944914579391, 3.441824316662626e-33, -0.0463034026324749, 0.04117242991924286, -0.011735932901501656, -0.05702345818281174, 0.020004358142614365, -0.06130543723702431, 0.030453519895672798, 0.0022978142369538546, 0.037660013884305954, 0.01790938712656498, -0.0198585893958807, -0.06459873169660568, 0.009217746555805206, 0.11855757981538773, 0.0702715516090393, -0.08888863027095795, -0.05806993320584297, -0.048597656190395355, -0.010743357241153717, -0.13154424726963043, 0.020466376096010208, -0.014869114384055138, -0.014096762984991074, 0.07567659765481949, -0.03336291015148163, -0.031138498336076736, -0.07287607342004776, 0.061358410865068436, -0.006309195887297392, 0.03097723051905632, -0.009550673887133598, 0.04436933249235153, -0.0576288215816021, -0.11212455481290817, 0.06835956871509552, -0.030257748439908028, -0.007264841813594103, -0.02429027110338211, 0.07687138020992279, 0.01074977032840252, -0.024751553311944008, -0.00943692121654749, 0.0005707989912480116, -0.027540888637304306, -0.036824584007263184, -0.0612715482711792, -0.02771109528839588, 0.1019756868481636, -0.027019275352358818, -0.07767993956804276, -0.010212203487753868, -0.04671569541096687, -0.0007550654117949307, -0.0027287465054541826, 0.08352937549352646, -0.03630272299051285, 0.029773762449622154, 0.06662027537822723, 0.05387840047478676, 0.20239631831645966, 0.006327077746391296, -0.0014522288693115115, -0.06016474962234497, 0.03775150701403618, -0.014263016171753407, 0.13944317400455475, -0.014752265997231007, -0.011004277504980564, 0.08000200986862183, -0.029356636106967926, 0.056843239814043045, 0.095366932451725, -0.010512007400393486, -0.04469706118106842, 0.101573146879673, 0.01632470265030861, 0.08380135893821716, 0.029887661337852478, -0.020228607580065727, 0.04501872882246971, 0.02426725998520851, -0.06332630664110184, -0.0538407638669014, 0.08150862157344818, 0.030922820791602135, 0.020711258053779602, 0.054685696959495544, 0.011865890584886074, -0.032633226364851, -0.0008936311933211982, -0.0452163927257061, -0.040092311799526215, 0.09481208771467209, -0.06683201342821121, -0.06355462968349457, -3.1038704254851876e-33, 0.055623073130846024, 0.037951014935970306, -0.03928743675351143, 0.013430838473141193, -0.036043327301740646, 0.062218401581048965, -0.030806249007582664, -0.021278388798236847, 0.012119817547500134, 0.016592010855674744, -0.033222272992134094, 0.028078965842723846, 0.002226276323199272, 0.08520684391260147, -0.03811856731772423, -0.005820152349770069, -0.04348129406571388, -0.07309785485267639, 0.08764220774173737, -0.0562458299100399, 0.11225475370883942, 0.058917682617902756, -0.047236450016498566, 0.011177940294146538, 0.02203909121453762, -0.039581943303346634, -0.11318281292915344, 0.08310388028621674, -0.04722835496068001, -0.04579773545265198, -0.011391966603696346, -0.027866991236805916, 0.007091791369020939, -0.043335698544979095, 0.04370109736919403, -0.019490834325551987, 0.07163891196250916, 0.024880683049559593, 0.03629712760448456, -0.029894564300775528, 0.02815740928053856, -0.04361774027347565, 0.06632021814584732, 0.03902026265859604, 0.03987887501716614, 0.06432360410690308, -0.09166309982538223, 0.06702132523059845, -0.03360160067677498, 0.07718979567289352, -0.03813260421156883, -0.026335103437304497, -0.015038269571959972, -0.06242234632372856, 0.03450532630085945, -0.013087492436170578, 0.002323322696611285, 0.03085261397063732, 0.031721871346235275, -0.018048007041215897, 0.007442248985171318, -0.09537521749734879, 0.017394481226801872, -0.04397617653012276, -0.035844627767801285, -0.025237882509827614, 0.026175154373049736, 0.03511562570929527, 0.04485752061009407, -0.013987762853503227, -0.009336760267615318, 0.01243616547435522, 0.10910999029874802, -0.04525008425116539, -0.12300065904855728, -0.0850963220000267, -0.07184392213821411, -0.01706170290708542, -0.0051991986110806465, 0.013077767565846443, -0.03351929783821106, -0.02934078872203827, -0.008081302978098392, -0.00932419765740633, 0.024723993614315987, 0.08948485553264618, 0.012071995064616203, -0.01936155930161476, 0.04883432388305664, -0.010560567490756512, -0.004550640936940908, 0.07725238800048828, -0.02331271767616272, 0.04093017801642418, -0.03981967642903328, -5.700263372432346e-08, 0.052481554448604584, 0.05553275719285011, 0.009395986795425415, 0.014921383000910282, 0.05053170770406723, 0.016700509935617447, 0.06950458139181137, -0.060232095420360565, 0.04079373925924301, -0.047425758093595505, 0.05090789869427681, -0.0014418221544474363, 0.01573292165994644, -0.0448683463037014, 0.017198463901877403, 0.008145640604197979, 0.02610056661069393, -0.044829439371824265, 0.03041243553161621, -0.03957591950893402, 0.08763739466667175, 0.002108454005792737, 0.02560098096728325, 0.08483602106571198, 0.0673811212182045, -0.0511794313788414, 0.008568824268877506, 0.020328732207417488, 0.014274884946644306, -0.042529668658971786, 0.007784239947795868, 0.07033167779445648, 0.04034505784511566, 0.018408942967653275, 0.06709285825490952, -0.02487226389348507, 0.014271803200244904, 0.038827214390039444, -0.00898296944797039, 0.01914139650762081, -0.04311266541481018, -0.049313221126794815, -0.012119106948375702, -0.013441314920783043, -0.03108515590429306, -0.1305408626794815, 0.004635646007955074, -0.02627178281545639, -0.01579626090824604, 0.004502442199736834, 0.014071672223508358, 0.04300060123205185, -0.01040265616029501, 0.01241218950599432, 0.048107706010341644, -0.016759652644395828, -0.03442228212952614, -0.06587792187929153, -0.07119221240282059, 0.09030893445014954, 0.05960314720869064, 0.017841262742877007, -0.013745643198490143, -0.00242556631565094]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\LearningbyChoiceofInternalRepresentations.pdf,NLP,"224 USE OF MULTI-LAYERED NETWORKS FOR CODING SPEECH WITH PHONETIC FEATURES Yoshua Bengio, Regis Cardin and Renato De Mori Computer Science Dept. McGill University Montreal, Canada H3A2A7 ABSTRACT Piero Cosi Centro di Studio per Ie Ricerche di Fonetica, C.N.R., Via Oberdan,10, 35122 Padova, Italy Preliminary results on speaker-independant speech recognition are reported. A method that combines expertise on neural networks with expertise on speech recognition is used to build the recognition systems. For transient sounds, event- driven property extractors with variable resolution in the time and frequency domains are used. For sonorant speech, a model of the human auditory system is preferred to FFT as a front-end module. INTRODUCTION Combining a structural or knowledge-based approach for describing speech units with neural networks capable of automatically learning relations between acoustic properties and speech units is the research effort we are attempting. The objective is that of using good generalization models for learning speech units that could be reliably used for many recognition tasks without having to train the system when a new speaker comes in or a new t~sk is considered. Domain (speech re.pognition) specific knowledge is applied for - segmentation and labeling of speech, - definition of event-driven property extractors, - use of an ear model as preproqf3ssing applied to some modules, - coding of network outputs with phonetic features, - modularization of the speech recognition task by dividing the workload into smaller networks performing Simpler tasks. Optimization of learning time and of generalization for the neural networks is sought through the use of neural networks techniques : - use of error back-propagation for learning, Multi-Layered Networks for Coding Phonetic Features 225 - switching between on-line learning and batch learning when appropriate, - convergence acceleration with local (weight specific) learning rates, - convergence acceleration with adaptive learning rates based on information on the changes in the direction of the gradient, - control of the presentation of examplars in order to ba I an ce examplars among the different classes, - training of small modules in the first place: - simpler architecture (e.g. first find out the solution to the linearly separable part of the problem), - use of simple recognition task, combined using either Waibel's glue units [Waibel 88] or with simple heuristics. - training on time-shifted inputs to learn ti me invariance and insensitivity to errors in the segmentation preprocessing. - controlling and improving generalization by using several test sets and using one of them to decide when to stop training. EAR MODEL In recent years basilar membrane, inner cell and nerve fiber behavior have been extensively studied by auditory physiologists and neurophysiologists and knowledge about the human auditory pathway has become more accurate [Sachs79,80,83][Delgutte 80,84][Sinex 83]. The computational scheme proposed in this paper for modelling the human auditory system is derived from the one proposed by S. Seneff [Seneff 84,85,86]. The overall system structure which is illustrated in Fig. 1 includes three blocks: the first two of them deal with peripheral transformations occurring in the early stages of the hearing process while the third one attempts to extract information relevant to perception. The first two blocks represent the periphery of the earing system. They are designed using knowledge of the rather well known responses of the corresponding human auditory stages [Delgutte 84]. The third unit attempts to apply a useful processing strategy for the extraction of important speech properties like spectral lines related to formants. The speech signal, band-limited and sampled at 16 kHz, is first pre- filtered through a set of four complex zero pairs to eliminate the very high and very low frequency components. The Signal is then analyzed by the first block, a 40-channel critical-band linear filter bank. Filters were designed to optimally fit physiological data [Delgutte 84) such as those observed by [N.V.S. Kiang et al.] and are implemented as a 226 Bengio, Cardin, De Mori and Cosi cascade of complex high frequency zero pairs with taps after each zero pair to individual tuned resonators. The second block of the model is called the hair cell synapse model, it is nonlinear and is intended to capture prominent features of the transformation from basilar membrane vibration, represented by the outputs of the filter bank, to probabilistic response properties of auditory nerve fibers. The outputs of this stage, in accordance with [Seneff 88], represent the probability of firing as a function of time for a set of similar fibers acting as a group. Four different neural mechanisms are modeled in this nonlinear stage. The rectifier is applied to the signal to simulate the high level distinct directional sensitivity present in the inner hair cell current response. The short-term adaptation which seems due to the neurotransmitter release in the synaptic region between the inner hair cell and its connected nerve fibers is Simulated by the ""membrane model"". The third unit represents the observed gradual loss of synchrony in nerve fiber behaviour as stimulus frequency is increased. The last unit is called ""Rapid Adaptation"", it performs ""Automatic Gain Control"" and implements a model of the refractory phenomenon of nerve fibers. The third and last block of the ear model is the synchrony detector which implements the known ""phase locking"" property of the nerve fibers. It enhances spectral peaks due to vocal tract resonances. IN PUT.J,.SIGNAL 40 • channels Critical Band Filter Bank BASILARi,.MEMBRANE RESPONSE Hair Cell Synapse Model "" RRING PROBABILITY Synchrony Detector SYNCHRO~SPECTRUM Figure 1 : Structure of the ear model OUTPUT LAYER HIDD~NO 0 ~o _ 0 LAYER ... __ ~ ... tt HIDD~NO o~O 0 LAYER 0 o 0 tt ~ time Figure 2 : Multi·layered network vlith variable resolution Property Extractor Multi-Layered Networks for Coding Phonetic Features 227 PROPERTY EXTRACTORS For many of the experiments described in this paper, learning is performed by a set of multi-layered neural networks (MLNs) whose execution is decided by a data-driven strategy. This strategy analyzes morphologies of the input data and selects the execution of one or more MLNs as well as the time and frequency resolution of the spectral samples that are applied at the network input. An advantage of using such specialized property extractors is that the number of necessary input connections (and thus of connections) is then minimized, thus improving the generalizing power of the MLNs. Fine time resolution and gross frequency resolution are used, for example, at the onset of a peak of signal energy, while the opposite is used in the middle of a segment containing broad-band noise. The latter situation will allow the duration of the segment analyzed by one instantiation of the selected MLN to be larger than the duration of the signal analyzed in the former case. Property extractors (PEs) are mostly rectangular windows subdivided into cells, as illustrated in Figure 2. Property extractors used in the experiments reported here are described in [Bengio, De Mori & Cardin 88]. A set of PEs form the input of a network called MLN1, executed when a situation characterized by the following rule is detected: SITUATION S1 ( ( deep_dip) (t*)(peak)) or ((ns)(t*)(peak)) or (deep_dip)(sonorant-head)(t*)(peak)) --> execute (MLN1 at t*) (deep_dip), (peak), (ns) are symbols of the PAC alphabet representing respectively a deep dip, a peak in the time evolution of the signal energy and a segment with broad-band nOise; t* is the time at which the first description ends, sonorant-head is a property defined in [De Mori, Merlo et al. 87]. Similar preconditions and networks are established for nonsonorant segments at the end of an utterance. Another MLN called MLN2 is executed only when frication noise is detected. This situation is characterized by the following rule: SITUATION S2 (pr1 = (ns)) --> execute (MLN2 every T =20 msecs.) 228 Bengio, Cardin, De Morl and Cosi EXPERIMENTAL RESULTS EXPERIMENT 1 - task : perform the classification among the following 10 letters of the alphabet, from the E-set : { b,c,d,e,g,k,p,t,v,3} - Input coding defined in [Bengio, De Mori & Cardin 88]. - architecture: two modules have been defined, MLN1 and MLN2. The input units of each PE window are connected to a group of 20 hidden units, which are connected to another group of 10 hidden units. All the units in the last hidden layer are then connected to the 10 output units. - database : in the learning phase, 1400 samples corresponding to 2 pronounciations of each word of the E-set by 70 speakers were used for training MLN1 and MLN2. Ten new speakers were used for testing. The data base contains 40 male and 40 female speakers. - results: an overall error rate of 9.5% was obtained with a maximum error of 20% for the letter Id/. These results are much better than the ones we obtained before and we published recently [De Mori, Lam & Gilloux 87]. An observation of the confusion matrix shows that most of the errors represent cases that appear to be difficult even in human perception. EXPERIMENT 2 - task : similar to the one in experiment 1 i.e. to recognize the h ea d consonant in the context of a certain vowel : lae/'/o/'/ul and Ia!. -subtask 1 : classify pronounciations of the first phoneme of letters A,K,J,Z and digit 7 into the classes {/vowel/,lkI,/j/'/zl,/s/}. -subtask 2 : classify pronounciations of the first phoneme of letter a and digit 4 into the classes {/vowel/,/f/}. -subtask 3 : classify pronounciations of the first phoneme of the letter Y and the digits 1 and 2 into the classes {/vowel/,lt!}. -subtask 4 : classify pronounciations of the first phoneme of letters I,R,W and digits 5 and 9 into the classes {/vowel/,/d/,/f/'/n/} - input coding : as for experiment 1 except that only PEs pertaining to situation 81 were used, as the input to a single MLN. - architecture : two layers of respectively 40 and 20 hidden units followed by an output unit for each of the classes defined for the subtask. - database : 80 speakers (40 males, 40 females) each pronouncing two utterances of each letter and each digit. The first 70 speakers are used for training, the last 10 for testing. - results : subtask 1 : {/vowel/,/k/,/j/'/z/,/s/} preceding vowel lael. 4 % error on test set. Multi-Layered Networks for Coding Phonetic Features 229 subtask 2 : {/vowel/.!f!} preceding vowel 10/. o % error on test set. subtask 3 : {/vowel/.!tI} preceding vowel luI. o % error on test set. subtask 4 : {/vowel/,/d/,/f/,/n/} preceding vowel lal. 3 % error on test set. EXPERIMENT 3 - task: speaker-Independant vowel recognition to discrimine among ten vowels extracted from 10 english words {BEEP,PIT,BED,BAT,BUT,FUR,FAR,SAW,PUT,BOOT}. - input coding : the signal processing method used for this experiment is the one described in the section ""ear model"". The output of the Generalised Synchrony Detector (GSD) was collected every 5 msecs. and represented by a 40-coefficients vector. Vowels were automatically singled out by an algorithm proposed in [De Mori 85] and a linear interpolation procedure was used to reduce to 10 the variable number of frames per vowel (the first and the last 20 ms were not considered in the interpolation procedure). - architecture: 400 input units (10 frames x 40 filters), a single hidden layer with 20 nodes, 10 output nodes for the ten vowels. - database: speech material consisted in 5 pronounciations of the ten monosyllabic words by 13 speakers (7 male, 6 female) for training and 7 new speakers (3 male, 4 female) for test. - results: In 95.4% of the cases, correct hypotheses were generated with the highest evidence, in 98.5% of the cases correct hypotheses were found in the top two candidates and in 99.4 % of the cases in the top three candidates. The same experiment with FFT spectra instead of data from the ear model gave 870/0 recognition rate in similar experimental conditions. The use of the ear model allowed to produce spectra with a limited number of well defined spectral lines. This represents a good use of speech knowledge according to which formants are vowel parameters with low variance. The use of male and female voices allowed the network to perform an excellent generalization with samples from a limited number of speakers. CONCLUSION The preliminary experiments reported here on speaker normalization combining multi-layered neural networks and speech recognition expertise show promising results. For transient sounds, event-driven property extractors with variable resolutions in the time and frequency domains were used. For sonorant speech with formants, a new model of 230 Bengio, Cardin, De Mori and Cosi the human auditory system was preferred to the classical FFT or LPC representation as a front-end module. More experiments have to be carried out to build an integrated speaker-independant phoneme recognizer based on multiple modules and multiple front-end coding strategies. In order to tune this system, variable depth analysis will be used. New small modules will be designed to specifically correct the deficiencies of trained modules. In addition, we consider strategies to perform recognition at the word level, using as input the sequence of outputs of the MLNs as time flows and new events are encountered. These strategies are also useful to handle slowly varying transitions such as those in diphtongs. REFERENCES Bengio Y., De Mori R. & Cardin R., (1988)""Data-Driven Execution of Multi-Layered Networks for Automatic Speech Recognition"", Proceedings of AAAI 88, August 88, Saint Paul, Minnesota,pp.734-738. Bengio Y. & De Mori R. (1988), ""Speaker normalization and automatic speech recognition uSing spectral lines and neural networks"", Proceedings of the Canadian Conference on Artificial Intelligence (CSCSI-88), Edmonton, AI., May 1988. Delgutte B. (1980), ""Representation of speech-like sounds in the discharge patterns of auditory-nerve fibers"" , Journal of the Acoustical Society of America, N. 68, pp. 843-857. Delgutte B. & Kiang N.Y.S. (1984) , ""Speech coding in the auditory nerve"", Journal of Acoustical Society of America, N. 75, pp. 866-907. De Mori R., Laface P. & Mong Y. (1985), ""Parallel algorithms for syllable recognition in continuous speech"", IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-7, N. 1, pp. 56- 69, 1985. De Morl R., Merlo E., Palakal M. & Rouat J.(1987),""Use of procedural knowledge for automatic speech recognition"", Proceedings of the tenth International Joint Conference on Artificial Intelligence, Milan, August 1987, pp. 840-844. De Mori R., Lam L. & Gilloux M. (1987), ""Learning and plan refinement in a knowledge-based system for automatic speech recognition"", IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI-9, No.2, pp.289-305. Multi-Layered Networks for Coding Phonetic Features 231 Kiang N.Y.S., Watanabe T., Thomas E.C. & Clark L.F., ""Discharge patterns of single fibers in the eat's aUditory-nerve fibers"", Cambridge, MA: MIT press. Rumelhart D.E., Hinton G.E. & Williams R.J. (1986),""Learning internal representation by error propagation"", Parallel Distributed Processing : Exploration in the Microstructure of Cognition, vol. 1, pp.318-362, MIT Press, 1986. Seneff S. (1984), ""Pitch and spectral estimation of speech based on an auditory synchrony model"", Proceedings of ICASSP-84, San Diego, CA. Seneff S. (1985), ""Pitch and spectral analysis of speech based on an auditory synchrony model"", RLE Technical Report 504 , MIT. Seneff S. (1986), ""A computational model for the peripheral auditory system: application to speech recognition research"", Proceedings of ICASSP-86, Tokyo, pp. 37.8.1-37.8.4. Seneff S. (1988), ""A joint synchrony/mean-rate model of auditory speech processing"", Journal of Phonetics, January 1988. Sachs M.B. & Young E.D. (1979),""Representation of steady-state vowels in the temporal aspects of the discharge pattern of populations of auditory nerve fibers"", Journal of Acoustical Society of America, N. 66, pp. 1381-1403. Sachs M.B. & Young E.D. {1980},""Effects of nonlinearities on speech encoding in the auditory nerve"", Journal of Acoustical SOCiety of America, N. 68, pp. 858-875. Sachs M.B. & Miller M.1. (1983), ""Representation of stop consonants in the discharge patterns of auditory-nerve fibers"", Journal of Acoustical Society of America, N. 74, pp. 502-517. Sinex D.G. & Geisler C.D. (1983), ""Responses of aUditory-nerve fibers to consonant-vowel syllables"", Journal of Acoustical Society of America, N. 73, pp. 602-615. Waibel A. (1988),""Modularity in Neural Networks for Speech Recognition"", Proc. of the 1988 IEEE Conference on Neural Information Processing Systems, Denver, CO.","[-0.09592948108911514, -0.13781195878982544, 0.010500248521566391, -0.04766859486699104, -0.07570433616638184, 0.07266812771558762, 0.06607368588447571, -0.05261605605483055, 0.01233395840972662, -0.08913713693618774, -0.031141024082899094, -0.05532426759600639, -0.03666254132986069, 0.0787496417760849, 0.00841742567718029, -0.0476636067032814, 0.03557838872075081, 0.12612555921077728, -0.06521628051996231, -0.04738137125968933, 0.10098735988140106, 0.07078176736831665, -0.0020031630992889404, 0.04858537018299103, 0.017838558182120323, -0.0019739449489861727, 0.002852588426321745, 0.03355211764574051, 0.05043398216366768, 0.04295387864112854, 0.04903743416070938, 0.06671763211488724, 0.14676488935947418, 0.03670088201761246, -0.05172266438603401, -0.011372394859790802, -0.019275899976491928, 0.055023159831762314, -0.01064069103449583, -0.011801456101238728, 0.004739539697766304, -0.005002336576581001, 0.023888831958174706, -0.012699350714683533, 0.06360062956809998, -0.042243339121341705, -0.0338999442756176, -0.008022618480026722, -0.03074178658425808, 0.06570237874984741, -0.008021616376936436, -0.007684146054089069, -0.035659704357385635, 0.10820478200912476, -0.016393985599279404, 0.04095704108476639, 0.06264641880989075, 0.038498710840940475, -0.028257688507437706, 0.005854799412190914, -0.07736767828464508, -0.1001143604516983, -0.038212552666664124, -0.06693322956562042, 0.011281881481409073, 0.014068778604269028, -0.07267991453409195, 0.05917952582240105, 0.0042986199259757996, -0.008530210703611374, -0.054560691118240356, 0.08152575045824051, -0.01552375964820385, 0.07361777871847153, 0.0359824039041996, 0.05585139989852905, 0.024890588596463203, 0.00989396870136261, 0.03080195188522339, -0.028531083837151527, 0.07816313952207565, 0.03284430131316185, 0.054954033344984055, -0.0619494654238224, 0.09206262975931168, 0.007599379401654005, -0.08657459914684296, 0.025536756962537766, -0.023359235376119614, -0.04091234877705574, -0.08074720948934555, -0.042381513863801956, 0.05177338048815727, -0.011313860304653645, 0.08125168085098267, 0.0697910413146019, -0.0005817228811793029, -0.07094420492649078, 0.005351188592612743, 0.053060662001371384, -0.014224004931747913, 0.013630406931042671, -0.005745794158428907, -0.04564056918025017, -0.012021083384752274, -0.0057302191853523254, -0.01639017090201378, 0.043522000312805176, 0.0568481981754303, -0.061163779348134995, -0.026611432433128357, -0.0031321437563747168, -0.004560594912618399, -0.07328987866640091, 0.062231630086898804, 0.025257252156734467, -0.006239081732928753, -0.0075120339170098305, 0.05186313018202782, -0.014073573052883148, -0.047651007771492004, 0.010350845754146576, -0.0361027866601944, 0.018466012552380562, 0.08451379090547562, -0.02005142718553543, -0.05352785065770149, 3.8178631227807526e-33, 0.012714921496808529, 0.12270235270261765, -0.03777189180254936, 0.033278852701187134, -0.005729129537940025, -0.07793115824460983, -0.07025252282619476, -0.014218450523912907, 0.0592491514980793, 0.0022073225118219852, -0.024839799851179123, 0.06992953270673752, -0.07584716379642487, -0.05903472751379013, 0.035652078688144684, 0.07386662811040878, 0.010814325883984566, 0.024683235213160515, 0.03671630099415779, -0.09420481324195862, 0.047084540128707886, 0.08024520426988602, 0.09161049872636795, 0.021376660093665123, 0.07819594442844391, -0.03025403618812561, 0.07947947829961777, -0.08885854482650757, -0.06124543398618698, 0.02749142237007618, 0.00636831670999527, -0.0493367500603199, 0.02715122699737549, -0.017914805561304092, 0.005061829928308725, -0.06408479809761047, 0.02836618386209011, 0.016792016103863716, -0.023419827222824097, -0.0600464902818203, -0.06860126554965973, 3.3263950172113255e-05, -0.02077615074813366, 0.023076239973306656, -0.06636770069599152, -0.10391343384981155, -0.014753764495253563, 0.05593423172831535, 0.006627560593187809, -0.02456623502075672, 0.018613368272781372, 0.0030261166393756866, -0.08408604562282562, -0.030044106766581535, 0.005106505937874317, 0.03259928524494171, 0.02153712697327137, 0.04709639772772789, 0.049813222140073776, -0.008501527830958366, -0.007806941866874695, 0.04116443544626236, 0.05264117941260338, 0.0284616369754076, 0.05954674631357193, -0.02237027697265148, -0.08645644783973694, 0.027924973517656326, 0.05269937589764595, -0.02959406189620495, 0.042286232113838196, 0.004737786017358303, 0.03232676163315773, 0.0006386161549016833, 0.01517841313034296, 0.04464070126414299, 0.05443720892071724, -0.05943744629621506, -0.06644980609416962, 0.062422819435596466, -0.04851069301366806, 0.02193896658718586, -0.061201754957437515, 0.0007606876315549016, 0.017382560297846794, 0.01476233545690775, 0.04472319781780243, -0.04518295079469681, -0.02565033547580242, -0.0170450396835804, -0.06781920045614243, -0.004561535082757473, -0.00827033817768097, 0.04352767765522003, -0.013501189649105072, -4.415157883714466e-33, -0.021229568868875504, 0.11116788536310196, -0.0484524704515934, 0.0034557783510535955, -0.011132540181279182, -0.04086487740278244, -0.02487684041261673, 0.03443825617432594, -0.05808134377002716, -0.058587249368429184, -0.0015776194632053375, -0.046735648065805435, 0.05657215788960457, -0.0387236662209034, 0.013518004678189754, 0.030635839328169823, -0.04870389774441719, -0.009601905010640621, 0.06739679723978043, 0.09234414249658585, -0.028872555121779442, 0.005295100621879101, -0.13830320537090302, 0.06848686933517456, -0.07933361083269119, -0.03947441279888153, -0.07665811479091644, 0.028826912865042686, -0.014897223562002182, 0.010468396358191967, -0.06846151500940323, -0.06826429814100266, -0.04832381755113602, 0.015596664510667324, -0.034790970385074615, 0.035885222256183624, 0.04233153164386749, -0.04198182746767998, 0.008260377682745457, 0.004468629602342844, 0.016371307894587517, 0.010729645378887653, -0.013272738084197044, -0.11209160089492798, 0.018200641497969627, -0.09179732948541641, -0.051462627947330475, 0.06123000383377075, 0.04455859959125519, -0.016024965792894363, 0.052971791476011276, -0.025541773065924644, -0.016550574451684952, -0.03798748552799225, -0.02126474864780903, 0.02601012773811817, 0.054995179176330566, -0.0460253469645977, 0.01796388067305088, 0.016621537506580353, 0.011012183502316475, -0.08577036112546921, 0.018827104941010475, -0.005228091962635517, -0.009409544989466667, 0.012501807883381844, -0.003880235832184553, 0.0679803416132927, 0.022511323913931847, -0.037750329822301865, -0.024575062096118927, 0.01156831718981266, -0.013932082802057266, 0.0053230104967951775, -0.08587174862623215, -0.027462508529424667, -0.07179027050733566, -0.06271287798881531, -0.021761640906333923, -0.05450495705008507, -0.07798886299133301, -0.020124711096286774, -0.032310355454683304, 0.024182729423046112, 0.08957371860742569, 0.09635201841592789, 0.05715273693203926, 0.010051161050796509, 0.10187292844057083, -0.0791030004620552, -0.013619654811918736, 0.12259440869092941, 0.002631854498758912, 0.010487943887710571, -0.043748192489147186, -5.1901366759921075e-08, -0.05819607898592949, -0.015007897280156612, 0.026755761355161667, -0.022922703996300697, 0.050103750079870224, -0.12484746426343918, -0.02978994883596897, -0.057315435260534286, 0.0005335439927875996, -0.05395291745662689, 0.08213409036397934, -0.03228582441806793, -0.04959564656019211, 0.030786912888288498, -0.05136071890592575, 0.014573690481483936, 0.03831642493605614, 0.10241837054491043, -0.042900968343019485, -0.12493176013231277, 0.08921917527914047, 0.041267216205596924, 0.06245765462517738, 0.07308771461248398, 0.047053154557943344, -0.07864014059305191, 0.01567714847624302, 0.07402895390987396, 0.06042328104376793, 0.06347900629043579, -0.009014720097184181, 0.03994588181376457, -0.02395659126341343, -0.015777211636304855, 0.07382000237703323, 0.012557531706988811, 0.0329129695892334, -0.034640341997146606, 0.00902498234063387, 0.028858695179224014, 0.04626431316137314, 0.019018735736608505, -0.09286846965551376, 0.04475676268339157, 0.05698856711387634, -0.044837698340415955, 0.031584158539772034, -0.08661458641290665, 0.03045455552637577, 0.056824203580617905, 0.012188030406832695, 0.06141425296664238, -0.03050832264125347, 0.02815214730799198, 0.08627194166183472, 0.050554439425468445, 0.04168419539928436, -0.019679993391036987, 0.040559086948633194, -0.01928297057747841, -0.03487540781497955, 0.0485115684568882, -0.06731060892343521, -0.04043678939342499]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\LearningwithTemporalDerivativesinPulseCodedNeuronalSystems.pdf,Reinforcement Learning,"642 LEARNING BY ST ATE RECURRENCE DETECfION Bruce E. Rosen, James M. Goodwint, and Jacques J. Vidal University of California, Los Angeles, Ca. 90024 ABSTRACT This research investigates a new technique for unsupervised learning of nonlinear control problems. The approach is applied both to Michie and Chambers BOXES algorithm and to Barto, Sutton and Anderson's extension, the ASE/ACE system, and has significantly improved the convergence rate of stochastically based learning automata. Recurrence learning is a new nonlinear reward-penalty algorithm. It exploits information found during learning trials to reinforce decisions resulting in the recurrence of nonfailing states. Recurrence learning applies positive reinforcement during the exploration of the search space, whereas in the BOXES or ASE algorithms, only negative weight reinforcement is applied, and then only on failure. Simulation results show that the added information from recurrence learning increases the learning rate. Our empirical results show that recurrence learning is faster than both basic failure driven learning and failure prediction methods. Although recurrence learning has only been tested in failure driven experiments, there are goal directed learning applications where detection of recurring oscillations may provide useful information that reduces the learning time by applying negative, instead of positive reinforcement. Detection of cycles provides a heuristic to improve the balance between evidence gathering and goal directed search. INTRODUCflON This research investigates a new technique for unsupervised learning of nonlinear con trol problems with delayed feedback. Our approach is compared to both Michie and Chambers BOXES algorithml, to the extension by Barto, et aI., the ASE (Adaptive Search Element) and to their ASE/ACE (Adaptive Critic Element) system2, and shows an improved learning time for stochastically based learning automata in failure driven tasks. We consider adaptively controlling the behavior of a system which passes through a sequence of states due to its internal dynamics (which are not assumed to be known a priori) and due to choices of actions made in visited states. Such an adaptive controller is often referred to as a learning automaton. The decisions can be deterministic or can be made according to a stochastic rule. A learning automaton has to discover which action is best in each circumstance by producing actions and observing the resulting information. This paper was motivated by the previous work of Barto, et al. to investigate neuronlike adaptive elements that affect and learn from their environment. We were inspired by their current work and the recent attention to neural networks and connectionist systems, and have chosen to use the cart-pole control problem2, to enable a comparison of our results with theirs . ... ! Permanent address: California State University, Stanislaus; Turlock, California. @ American Institute of Physics 1988 643 THE CART ·POLE PROBLEM In their work on the cart-pole problem, Barto, Sutton and Anderson considered a learning system composed of an automaton interacting with an environment. The problem requires the automaton to balance a pole acting as an inverted pendulum hinged on a moveable cart. The cart travels left or right along a bounded one dimensional track; the pole may swing to the left or right about a pivot attached to the cart. The automaton must learn to keep the pole balanced on the cart, and to keep the cart within the bounds of the track. The parameters of the cart/pole system are the cart po~ition and velocity, and the pole angle and angular velocity. The only actions available to the automaton are the applications of a fixed impulsive force to the cart in either right or left direction; one of these actions must be taken. This balancing is an extremely difficult problem if there is no a priori knowledge of the system dynamics, if these dynamics change with time, or if there is no preexisting controller that can be imitated (e.g. Widrow and Smith's3 ADALINE controller). We assumed no a priori knowledge of the dynamics nor any preexisting controller and anticipate that the system will be able to deal with any changing dynamics. Numerical simulations of the cart-pole solution via recurrence learning show substantial improvement over the results of Barto et aI., and of Michie and Chambers, as is shown in figure 1. The algorithms used, and the results shown in figure 1, will be discussed in detail below. 500000 T'unc Until Fai1\R 100000 ~------------------------------ 25 so 75 110 Trial No. Figure 1: Perfonnance of the ASE, ASE/ACE, Constant Recurrence (HI) and Shon Recurrence (H2) Algorithms. THE GENERAL PROBLEM: ASSIGNMENT OF CREDIT The cart-pole problem is one of a class of problems known as ""credit assignment""4, and in particular temporal credit assignment. The recurrence learning algorithm is an approach to the general temporal credit assignment problem. It is characterized by seeking to improve learning by making decisions about early actions. The goal is to find actions responsible for improved or degraded perfonnance at a much later time. An example is the bucket brigade algorithmS. This is designed to assign credit to rules in the system according to their overall usefulness in attaining their goals. This is done by adjusting the strength value (weight) of each rule. The problem is of modifying these strengths is to permit rules activated early in the sequence to result in successful actions later. 644 Samuels considered the credit assignment problem for his checkers playing program6. He noted that it is easy enough to credit the rules that combine to produce a triple jump at some point in the game; it is much harder to decide which rules active earlier were responsible for changes that made the later jump possible. State recurrence learning assigns a strength to an individual rule or action and modifies that action's strength (while the system accumulates experience) on the basis of the action's overall usefulness in the situations in which it has been invoked. In this it follows the bucket brigade paradigm of Holland. PREVIOUS WORK The problems of learning to control dynamical systems have been studied in the past by Widrow and Smith3, Michie and Chambers!, Barto, Sutton, and Anderson2, and Conne1l7. Although different approaches have been taken and have achieved varying degrees of success, each investigator used the cart/pole problem as the basis for empirically measuring how well their algorithms work. Michie and Chambersl built BOXES, a program that learned to balance a pole on a cart. The BOXES algorithm choose an action that had the highest average time until failure. After 600 trials (a trial is a run ending in eventual failure or by some time limit expiration), the program was able to balance the pole for 72,000 time steps. Figure 2a describes the BOXES learning algorithm. States are penalized (after a system failure) according to recency. Active states immediately preceding a system failure are punished most. Barto, Sutton and Anderson2 used two neuronlike adaptive elements to solve the control problem. Their ASE/ACE algorithm chose the action with the highest probability of keeping the pole balanced in the region, and was able to balance the pole for over 60,000 time steps before completion of the lOOth trial. Figure 2a and 2b: The BOXES and ASE/ACE (Associative Search Elelement - Adpative Critic Element) algorithms Figure 2a shows the BOXES (and ASE) learning algorithm paradigm When the automaton enters a failure state (C), all states that it has traversed (shaded rectangles) are punished, although state B is punished more than state A. (Failure states are those at the edges of the diagram.) Figure 2b describes the ASE/ACE learning algorithm. If a system failure occurs before a state's expected failure time, the state is penalized. If a system failure occurs after its expected failure time, the state is rewarded. State A is penalized because a failure occurred at B sooner than expected. State A's expected 645 failure time is the time for the automaton to traverse from state A to failure point C. When leaving state A, the weights are updated if the new state's expected failure time differs from that of state A. Anderson8 used a connectionist system to learn to balance the pole. Unlike the previous experiments, the system did provide well-chosen states a priori. On the average, 10,000 trials were necessary to learn to balance the pole for 7000 time steps. Connell and Utgoff'7 developed an approach that did not depend on partitioning the state space into discrete regions. They used Shepard's function9,l0 to interpolate the degree of desirability of a cart-pole state. The system learned the control task after 16 trials. However, their system used a knowledge representation that had a priori information about the system. O'n-lER RELATED WORK Klopfll proposed a more neurological class of differential learning mechanisms that correlates earlier changes of inputs with later changes of outputs. The adaptation formula used multiplies the change in outputs by the weighted sum of the absolute value of the t previous inputs weights (~Wj)' the t previous differences in inputs (~Xj)' and the t previous time coefficients (c/ Sutton's temporal differences (TD)12 approach is one of a class of adaptive prediction methods. Elements of this class use the sum of previously predicted output values multiplied by the gradient and an exponentially decaying coefficient to modify the weights. Barto and Sutton 13 used temporal differences as the underlying learning procedure for classical conditioning. THERECURRENCELE~G~HOD DEFINITIONS A state is the set of values (or ranges) of parameters sufficient to specify the instantaneous condition of the system. The input decoder groups the environmental states into equivalence classes: elements of one class have identical system responses. Every environmental input is mapped into one of n input states. (All further references to ""states"" assumes that the input values fall into the discrete ranges determined by the decoder, unless otherwise specified. ) States returned to after visiting one or more alternate states recur. An action causes the modification of system parameters, which may change the system state. However, no change of state need occur, since the altered parameter values may be decoded within the same ranges. A weight, wet), is associated with each action for each state, with the probability of an allowed action dependent on the current value of its weight. A rule determines which of the allowable actions is taken. The rule is not deterministic. It chooses an action stochastically, based on the weights. Weight changes, ~w(t), are made to reduce the likelihood of choosing an action which will cause an eventual failure. These changes are made based on the idea that the previous action of an element, when presented with input x(t), had some influence in causing a similar pattern to occur again. Thus, weight changes are made to increase the likelihood that an element produces the same action f(t) when patterns similar to x(t) occur in the future. 646 For example, consider the classic problem of balancing a pole on a moving cart. The state is specified by the positions and velocities of both the cart and the pole. The allowable actions are fixed velocity increments to the right or to the left, and the rule determines which action to take, based on the current weights. THE ALGORITHM The recurrence learning algorithm presented here is a nonlinear reward-penalty method 14. Empirical results show that it is successful for stationary environments. In contrast to other methods, it also may be applicable to nonstationary environments'. Our efforts have been to develop algorithms that reward decision choices that lead the controller/environment to quasi-stable cycles that avoid failure (such as limit cycles, converging oscillations and absorbing points). Our technique exploits recurrence information obtained during learning trials. The system is rewarded upon return to a previous state, however weight changes are only permitted when a state transition occurs. If the system returns to a state, it has avoided failure. A recurring state is rewarded. A sequence of recurring states can be viewed as evidence for a (possibly unstable) cycle. The algorithm forms temporal ""cause and effect"" associations. To optimize performance, dynamic search techniques must balance between choosing a search path with known solution costs, and exploring new areas of the search space to find better or cheaper solutions. This is known as the two armed bandit problem l5, i.e. given a two handed slot machine with one arm's observed reward probabilities higher than the other, one should not exclude playing with the arm with the lesser payoff. Like the ASE/ACE system, recurrence learning learns while searching, in contrast to the BOXES and ASE algorithms which learn only upon failure. RANGE DECODING In our work, as in Barto and others, the real valued input parameters are analyzed as members of ranges. This reduces computing resource demands. Only a limited number of ranges are allowed for each parameter. It is possible for these ranges to overlap, although this aspect of range decoding is not discussed in this paper, and the ranges were considered nonoverlapping. When the parameter value falls into one of the ranges that range is active. The specification of a state consists of one of the active ranges for each of the parameters. If the ranges do not overlap, then the set of parameter values specify one unique state; otherwise the set of parameter values may specify several states. Thus, the parameter values at any time determine one or several active states Si from the set of n possible states. The value of each environmental parameter falls into one of a number of ranges, which may be different for different parameters. A state is specified by the active range for each parameter. The set of input parameter values are decoded into one (or more) of n ranges Si' 0<= i <= n. For this problem, boolean values are used to describe the activity level of a state Si. The activity value of a state is 1 if the state is active, or 0 if it is inactive. ACfION DECISIONS Our model is the same as that of the BOXES and ASE/ACE systems, where only one input (and state) is active at any given time. All states were nonoverlapping and mutually exclusive, although there was no reason to preclude them from overlapping 647 other than for consistency with the two previous models. In the ASE/ACE system and in ours as well, the output decision rule for the controller is based on the weighted sum of its inputs plus some stochastic noise. The action (output) decision of the controller is either + 1 or -1, as given by: ( 1) where f( ) = [+ 1 .i f z ~ 0 ] z -llfz<O (2) and noise is a real randomly (Gaussian) distributed value with some mean 11 and variance 0'2. An output, fez), for the car/pole controller is interpreted as a push to the left if fez) = -lor to the right if fez) = + 1. RECURRENCELE~G The goal of the recurrence learning algorithm is to avoid failure by moving toward states that are part of cycles if such states exist, or quasi-stable oscillations if they don't. This concept can be compared to juggling. As long as all the balls are in the air, the juggler is judged a success and rewarded. No consideration is given to whether the balls are thrown high or low, left or right; the controller, like the juggler, tries for the most stable cycles. Optimum performance is not demanded from recurrence learning. Two heuristics have been devised. The fundamental basis of each of them is to reward a state for being repeatedly visited (or repeatedly activated). The first heuristic is to reward a state when it is revisited, as part of a cycle in which no failure had occurred. The second heuristic augments the first by giving more reward to states which panicipate in shorter cycles. These heuristics are discussed below in detail. HEURISTIC HI: If a state has been visited more than once during one trial, reward it by reinforcing its weight. RATIONALE This heuristic assumes that states that are visited more than once have been part of a cycle in which no failure had occurred. The action taken in the previous visit is assumed to have had some influence on the recurrence. By reinforcing a weight upon state revisitation, it is assumed to increase the likelihood that the cycle will occur again. No assumptions are made as to whether other states were responsible for the cycle. RESTRICfION An action may not immediately cause the environment to change to a different state. There may be some delay before a transition, since small changes of parameters may be decoded into the same input ranges, and hence the same state. This inertia is incorporated into the heuristics. When the same state appears twice in succession, its weight is not reinforced, since that would assume that the action, rather than inertia, directly caused the state's immediate recurrence. 648 THE RECURRENCE EQUATIONS The recurrence learning equations stem in part from the weight alteration formula used in the ASE system. The weight of a state is a sum of its previous weight, and the product of the learning rate (a), the reward (r), and the state's eligibility (e). ret) E {-I,O} (3) The eligibility index e/t) is an exponentially decaying trace function. (4) where O<=P<=I, Xi E {0,1}, and Yi E {-I,I}. The output value Yi is the last output decision, and P determines the decay rate. The reward function is: { -1 ret) = ° when the system fails at time t } otherwise REINFORCEMENT OF CYCLES (5) Equations (1) through (5) describe the basic ASE system. Our algorithm extends the weight updating procedure as follows: (6) The term ar(t)ei(t) is the same as in (3), providing failure reinforcement. The term a2r2(t)e2,i(t) provides reinforcement for success. When state i is eligible (by virtue of Xi > 0), there is a weight change by the amount: CXz multiplied by the reward value, r2(t), and the current eligibility e2,i(t). For simplicity, the reward value, r2(t), may be taken to be some positive constant, although it need not be; any environmental feedback, yielding a reinforcement value as a function of time could be used instead. The second eligibility function e2,i(t) yields one of three constant values for HI: -P2' 0, or P2 according to formula (7) below: if t-ti,last = 1 or ti,last = ° } otherwise (7) where ti,last is the last time that state was active. If a state has not previously been active (i.e. xi(t) = ° for all t) then ti,last=O. As the formula shows, e2,i(t) = ° if the state has not been previously visited or if no state transition occurred in the last time step; otherwise, e2,i(t) = P2Xj(t)y(ti,last)· The direction (increase or decrease) of the weight change due to the final term in (6) is that of the last action taken, y(ti,last). 649 Heuristic HI is called constant recurrence learning because the eligibility function is designed to reinforce any cycle. HEURISTIC H2: Reward a short cycle more than a longer one. Heuristic 82 is called short recurrence learning because the eligibility function is designed to reinforce shorter cycle more than longer cycles. REINFORCEMENT OF SHORTER CYCLES The basis of the second heuristic is the conjecture that short cycles converge more easily to absorbing points than long ones, and that long cycles diverge more easily than shorter ones, although any cycle can ""grow"" or diverge to a larger cycle. The following extension to the our basic heuristic is proposed. The formula for the recurrence eligibility function is: { o if t-ti,last = e2,i(t) = P2 xi(t) y(ti,last) otherwise (P2+t-ti,last) 1 or li,last = 0 } (8) The current eligibility function e2/t) is similar to the previous failure eligibility function in (7); however, e2 i(t) reinforces shorter cycles more, because the eligibility decays with time. The value'returned from e2it) is inversely proportional to the period of the cycle from ti,last to t. H2 reinforces converging oscillations; the term (X.2r2(t)e2/t) in (6) ensures weight reinforcement for returning to an already visited state. Figure 3a and 3b: The Constant Recurrence algorithm and Short Recurrence algorithms Figure 3A shows the Constant Recurrence algorithm (HI). A state is rewarded when it is reactivated by a transition from another state. In the example below. state A is reward by a constant regardless of weather the cycle traversed states B or C. Figure 3b describes the Short Recurrence algorithm (m). A state is rewarded according to the difference between the current time and its last activation time. Small differences are rewarded more than large differences In the example below, state A is rewarded more 650 when the cycle (through state C) traverses the states shown by the dark heavy line rather than when the cycle (through state B) traverses the lighter line, since state A recurs sooner when traversing the darker line. SIMULATION RESULTS We simulated four algorithms: ASE, ASE/ACE and the two recurrence algorithms. Each experiment consisted of ten runs of the cart-pole balancing task, each consisting of 100 trials. Each trial lasted for 500,000 time steps or until the cart-pole system failed (i.e. the pole fell or the cart went beyond the track boundaries). In an effort to conserve cpu time, simulations were also terminated when the system achieved two consecutive trials each lasting for over 500,000 time steps; all remaining trials were assumed to also last 500,000 time steps. This assumption was reasonable: the resulting weight space causes the controller to become deterministic regardless of the influence of stochastic noise. Because of the long time require to run simulations, no attempts were made to optimize parameters of the algorithm. As in Bart02, each trial began with the cart centered, and the pole upright. No assumptions were made as to the state space configuration, the desirability of the initial states, or the continuity of the state space. The first experiment consisted of failure and recurrence reward learning. The ASE failure learning runs averaged 1578 time steps until failure after 100 trials*. Next, the predictive ASE/ACE system was run as a comparative metric, and it was found that this method caused the controller to average 131,297 time steps until failure; the results are comparable to that described by Barto, Sutton and Anderson. In the next experiment, short recurrence learning system was added to the ASE system. Again, ten 100 trial learning session were executed. On the average, the short recurrence learning algorithm ran for over 400,736 time steps after 100th trial, bettering the ASE/ACE system by 205%. In the final experiment, constant recurrence learning with the ASE system was simulated. The constant recurrence learning eliminated failure after only 207,562 time steps. Figure 1 shows the ASE, ASE/ACE, Constant recurrence learning (HI) and Short recurrence learning (H2) failure rates averaged over 10 simulation runs. DISCUSSION Detection of cycles provides a heuristic for the ""two armed bandit"" problem to decide between evidence gathering, and goal directed search. The algorithm allows the automaton to search outward from the cycle states (states with high probability of revisitation) to the more unexplored search space. The rate of exploration is proportional to the recurrence learning parameter~; as ~ is decreased, the influence of the cycles governing the decision process also decreases and the algorithm explores more of the search space that is not part of any cycle or oscillation path. * However, there was a relatively large degree of variance in the final trials. The last 10 trails (averaged over each of the 10 simulations) ranged from 607 to 15,459 time steps until failure 651 THEFUfURE Our future experiments will study the effects of rewarding predictions of cycle lengths in a manner similar to the prediction of failure used by the ASE/ACE system. The effort will be to minimize the differences of predicted time of cycles in order to predict their period. Results of this experiment will be shown in future reports. We hope to show that this recurrence prediction system is generally superior to either the ASE/ACE predictive system or the short recurrence system operating alone. CONCLUSION This paper presented an extension to the failure driven learning algorithm based on reinforcing decisions that cause an automaton to enter environmental states more than once. The controller learns to synthesize the best values by reinforcing areas of the search space that produce recurring state visitation. Cycle states, which under normal failure driven learning algorithms do not learn, achieve weight alteration from success. Simulations show that recurrence reward algorithms show improved overall learning of the cart-pole task with a substantial decrease in learning time. REFERENCES 1. D. Michie and R. Chambers, Machine Intelligence, E. Dale and D. Michie, Ed.: (Oliver and Boyd, Edinburgh, 1968), p. 137. 2. A. Barto, R. Sutton, and C. Anderson, Coins Tech. Rept., No. 82-20, 1982. 3. B. Widrow and F. Smith, in Computer and Information Sciences, 1. Tou and R. Wilcox Eds., (Clever Hume Press, 1964). 4. M. Minsky, in Proc. IRE, 49, 8, (1961). 5. J. Holland, in Proc. Int. Conj., Genetic Algs. and their Appl., 1985, p. 1. 6. A. Samuel, IBM Journ. Res.and Dev. 3, 211, (1959) 7. M. Connell and P. Utgoff, in Proc. AAAl-87 (Seattle, 1987), p. 456. 8. C. Anderson, Coins Tech. Rept., No. 86-50: Amherst, MA. 1986. 9. R. Barnhill, in Mathematical Software I II, (Academic Press, 1977). 10. L. Schumaker, in Approximation Theory II. (Academic Press, 1976). 11. A. H. Klopf, in IEEE Int. Conf. Neural Networks"" June 1987. 12. R. Sutton, GTE Tech. Rept.TR87-509.1, GTE Labs. Inc., Jan. 1987 13. R. Sutton and A. G. Barto, Tech. Rept. TR87-5902.2 March 1987 14. A. Barto and P. Anandan, IEEE Trans. SMC 15, 360 (1985). 15. M. Sato, K. Abe, and H. Takeda, IEEE Trans.SMC 14,528 (1984).","[-0.11135736107826233, -0.09587544202804565, 0.007902633398771286, 0.08229529112577438, -0.040459081530570984, 0.022569773718714714, -0.04418467357754707, -0.02510949969291687, 0.034634433686733246, 0.011828124523162842, -0.04943572357296944, -0.043977271765470505, 0.0373794250190258, 0.0011745287338271737, -0.08432120829820633, 0.006807140540331602, 0.01764621213078499, 0.007224279455840588, 0.031883638352155685, -0.028025344014167786, 0.02435004897415638, 0.04169999063014984, 0.03523164987564087, 0.017486440017819405, -0.03454935550689697, 0.04397829249501228, -0.06844598054885864, -0.02274913340806961, -0.013831699267029762, -0.07510323077440262, 0.051280248910188675, -0.10388940572738647, 0.028772912919521332, -0.0536920502781868, -0.04154172167181969, 0.031466588377952576, -0.12087547779083252, -0.02535208687186241, -0.07292743027210236, -0.02212713286280632, -0.0627606213092804, 0.05402236059308052, -0.018375420942902565, 0.022160330787301064, 0.12271442264318466, -0.04799336940050125, -0.006033905316144228, -0.05330657586455345, 0.005433356389403343, -0.020628098398447037, -0.025900034233927727, 0.02386251650750637, 0.05217241868376732, 0.05204017460346222, 0.04609128087759018, 0.03481807932257652, 0.09996991604566574, 0.03501370921730995, -0.030611788854002953, -0.0491076223552227, 0.03327326104044914, -0.08517896384000778, -0.03954743221402168, -0.11429796367883682, -0.06553762406110764, 0.03540957719087601, -0.011399674229323864, 0.031231332570314407, 0.0031441582832485437, 0.031285807490348816, -0.01878505013883114, -0.0028325302992016077, 0.014943397603929043, 0.0700560212135315, 0.07776571810245514, 0.005325440317392349, -0.04636814817786217, -0.05236287787556648, -0.02162501960992813, 0.06601163744926453, -0.0028639757074415684, -0.00027805232093669474, -0.012596492655575275, 0.031710077077150345, 0.08604152500629425, -0.029348142445087433, 0.024552255868911743, 0.03044833056628704, 0.14246319234371185, -0.035350654274225235, 0.02346586063504219, 0.01344129629433155, 0.0069527821615338326, -0.04322315379977226, -0.005608196835964918, 0.00933719053864479, 0.0005058258539065719, 0.002956463722512126, -0.002503620693460107, 0.05774734541773796, 0.0026878765784204006, 0.058178484439849854, -0.02872134931385517, 0.01538851484656334, 0.05365458130836487, 0.00871003046631813, 0.002364746294915676, 0.024553565308451653, 0.06938692182302475, -0.12497127801179886, -0.02577071450650692, -0.02154659852385521, 0.0049191792495548725, 0.08174413442611694, -0.018091198056936264, 0.05472589284181595, 0.012565829791128635, -0.08426670730113983, -0.019546328112483025, 0.09210685640573502, 0.00958762876689434, -0.08983857184648514, -0.028396017849445343, 0.02452230080962181, 0.006893428508192301, -0.048093926161527634, -0.036699943244457245, 3.681839324607413e-33, -0.04862108454108238, -0.01770804263651371, 0.017414430156350136, -0.054141029715538025, 0.012153072282671928, -0.04576718807220459, 0.036121729761362076, -0.013883058913052082, 0.024227101355791092, -0.04836194962263107, -0.010538306087255478, 0.011699638329446316, -0.022768203169107437, -0.011676345020532608, 0.08627643436193466, -0.08529649674892426, -0.005624198354780674, -0.0023697051219642162, 0.016656000167131424, -0.1274229735136032, -0.005381794646382332, -0.012334014289081097, 0.015769464895129204, -0.11417344212532043, 0.02422551065683365, 0.025846973061561584, 0.02386990748345852, 0.07259383797645569, -0.00626002810895443, 0.013264672830700874, 0.024828601628541946, 0.0737387090921402, -0.05889720469713211, 0.023092424497008324, 0.028335383161902428, -0.027005350217223167, -0.032016556710004807, 0.025269391015172005, -0.03475591540336609, -0.035054806619882584, -0.03668464347720146, 0.0060509247705340385, -0.009994961321353912, 0.033585142344236374, -0.021643051877617836, -0.14581748843193054, 0.044402044266462326, -0.09061188995838165, -0.06529811024665833, -0.05826738476753235, 0.04700395092368126, 0.010400116443634033, -0.00709835160523653, -0.04800171032547951, 0.03271110728383064, 0.05365084856748581, 0.019970299676060677, 0.07662612944841385, -0.005074149928987026, 0.026697373017668724, 0.017052287235856056, -0.0678153783082962, 0.033552344888448715, 0.03049563430249691, -0.03855793923139572, 0.10516173392534256, -0.010473919101059437, 0.047200072556734085, -0.03217260539531708, 0.009040339849889278, 0.06618565320968628, -0.01047524157911539, -0.03155510872602463, -0.05803721770644188, 0.08095458149909973, -0.05162450298666954, 0.11552795022726059, -0.00600722711533308, -0.03400438651442528, -0.05780261009931564, 0.03188448026776314, -0.0023420006036758423, -0.04912728816270828, 0.014595689252018929, -0.05319809541106224, -0.07080934941768646, 0.03143363818526268, -0.024369852617383003, -0.04473567381501198, 0.0021909927017986774, -0.02470454014837742, -0.04398118332028389, 0.013792594894766808, 0.0775241106748581, 0.04535095766186714, -4.328689988635917e-33, 0.027573861181735992, -0.025503918528556824, 0.0037209042347967625, 1.4204803846951108e-05, -0.0316828228533268, 0.014793558046221733, -0.036763038486242294, -0.02655421569943428, 0.06074212118983269, -0.0798673927783966, -0.08072052150964737, 0.031266115605831146, 0.02330513298511505, 0.10806755721569061, 0.0020937055815011263, -0.0020056969951838255, 0.035553574562072754, 0.006120137870311737, 0.022225117310881615, -0.006371464114636183, 0.028656061738729477, 0.10420745611190796, -0.10535556077957153, 0.029336195439100266, -0.04668432101607323, 0.018372371792793274, -0.019321395084261894, 0.1001930981874466, -0.010856127366423607, 0.03790995478630066, 0.04153868556022644, -0.04452738165855408, -0.020200075581669807, 0.08200684934854507, -0.03393518924713135, 0.07189308106899261, 0.09668290615081787, 0.02268218994140625, -0.05734466016292572, 0.07986363023519516, 0.09090880304574966, 0.027020735666155815, 0.05505051463842392, 0.03191421553492546, 0.02597653679549694, -0.05187267065048218, -0.06724768877029419, 0.046107854694128036, 0.03019757941365242, 0.006380816455930471, -0.0032474270556122065, -0.00677851028740406, -0.08870045095682144, -0.020201995968818665, -0.07069475948810577, 0.0407278873026371, 0.011918934062123299, 0.044359419494867325, 0.022107552736997604, 0.046719685196876526, -0.11731666326522827, 0.030004071071743965, 0.017887601628899574, 0.0690774917602539, -0.029365334659814835, -0.06600470840930939, -0.05096540227532387, 0.04410330206155777, 0.07903513312339783, 0.0015251262811943889, -0.01807987131178379, 0.1271991729736328, -0.0037254069466143847, -0.005383880343288183, 0.0414392426609993, -0.0494086854159832, -0.08772080391645432, 0.022183621302247047, -0.05302085354924202, -0.05966988950967789, -0.05278502404689789, 0.011962534859776497, 0.03741493448615074, 0.05119280889630318, -0.04049382731318474, 0.06763982772827148, 0.06671390682458878, -0.0136026069521904, 0.06819489598274231, -0.09745190292596817, 0.013619056902825832, -0.026275761425495148, 0.026481471955776215, 0.06556114554405212, 0.010188594460487366, -4.863472469196495e-08, -0.03225211426615715, 0.012113210745155811, 0.10024899244308472, 0.043750446289777756, 0.10754752159118652, -0.001478390651755035, -0.02935604751110077, -0.03638951852917671, -0.014854956418275833, -0.10530956089496613, 0.06661731004714966, 0.05152970924973488, 0.008368872106075287, -0.05117151886224747, -0.10512334853410721, 0.050848618149757385, 0.06772011518478394, -0.0090839434415102, 0.03131771832704544, 0.046905770897865295, 0.07544223219156265, -0.019434774294495583, 0.05346907302737236, -0.020837251096963882, 0.00437318067997694, -0.05491997301578522, -0.013308783993124962, 0.04939401522278786, -0.014781731180846691, -0.030910801142454147, 0.07833512872457504, 0.0021091222297400236, 0.08323320746421814, -0.005216014571487904, 0.04093613848090172, 0.042352233082056046, 0.06677694618701935, -0.11171316355466843, -0.046578262001276016, -0.07230312377214432, 0.033613428473472595, 0.09669402241706848, -0.008053378202021122, -0.02303825318813324, -0.08042854070663452, 0.05776166170835495, -0.0358150489628315, -0.0801130011677742, 0.059462882578372955, -0.06449375301599503, 0.07682813704013824, -0.041812997311353683, -0.02660924196243286, 0.010540658608078957, 0.06779887527227402, -0.05006922408938408, 0.032691750675439835, -0.05881807208061218, 0.004014318808913231, 0.08331179618835449, -0.026105232536792755, -0.0007776184938848019, -0.012916872277855873, 0.022108592092990875]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\LinearLearningLandscapesandAlgorithms.pdf,Deep Learning,"HETEROGENEOUS NEURAL NETWORKS FOR ADAPTIVE BEHAVIOR IN DYNAMIC ENVIRONMENTS Hillel J. Chiel Biology Dept. & CAISR CWRU Randall D. Beer Dept. of Computer Engineering and Science and Center for Automation and Intelligent Systems Research Case Western Reserve University Cleveland, OH 44106 ABSTRACT Leon S. Sterling CS Dept. & CAISR CWRU Research in artificial neural networks has genera1ly emphasized homogeneous architectures. In contrast, the nervous systems of natural animals exhibit great heterogeneity in both their elements and patterns of interconnection. This heterogeneity is crucial to the flexible generation of behavior which is essential for survival in a complex, dynamic environment. It may also provide powerful insights into the design of artificial neural networks. In this paper, we describe a heterogeneous neural network for controlling the wa1king of a simulated insect. This controller is inspired by the neuroethological and neurobiological literature on insect locomotion. It exhibits a variety of statically stable gaits at different speeds simply by varying the tonic activity of a single cell. It can also adapt to perturbations as a natural consequence of its design. INTRODUCTION Even very simple animals exhibit a dazzling variety of complex behaviors which they continuously adapt to the changing circumstances of their environment. Nervous systems evolved in order to generate appropriate behavior in dynamic, uncertain situations and thus insure the survival of the organisms containing them. The function of a nervous system is closely tied to its structure. Indeed, the heterogeneity of nervous systems has been found to be crucial to those few behaviors for which the underlying neura1 mecha- nisms have been worked out in any detail [Selverston, 1988]. There is every reason to believe that this conclusion will remain valid as more complex nervous systems are stud- ied: The brain as an ""organ"" is much more diversified than, for example, the kidney or the liver. If the performance of relatively few liver cells is known in detail, there is a good chance of defining the role of the whole organ. In the brain, different ce))s perform different, specific tasks ... Only rarely can aggregates of neurons be treated as though they were homogeneous. Above all, the cells in the brain are connected with one another according to a complicated but specific design that is of far greater complexity than the connections between cells in other organs. ([Kuffler, Nicholls, & Martin, 1984], p. 4) 577 578 Beer, Chiel and Sterling In contrast to research on biological nervous systems, work in artificial neural networks has primarily emphasized uniform networks of simple processing units with a regular in- terconnection scheme. These homogeneous networks typically depend upon some gener- al learning procedure to train them to perform specific tasks. This approach has certain advantages. Such networks are analytically tractable and one can often prove theorems about their behavior. Furthermore, such networks have interesting computational proper- ties with immediate practical applications. In addition, the necessity of training these net- works has resulted in a resurgence of interest in learning, and new training procedures are being developed. When these procedures succeed, they allow the rapid construction of networks which perform difficult tasks. However, we believe that the role of learning may have been overemphasized in artificial neural networks, and that the architectures and heterogeneity of biological nervous sys- tems have been unduly neglected. We may learn a great deal from more careful study of the design of biological nervous systems and the relationship of this design to behavior. Toward this end, we are exploring the ways in which the architecture of the nervous systems of simpler organisms can be utilized in the design of artificial neural networks. We are particularly interested in developing neural networks capable of continuously synthesizing appropriate behavior in dynamic, underspecified, and uncertain environments of the sort encountered by natural animals. THE ARTIFICIAL INSECT PROJECT In order to address these issues, we have begun to construct a simulated insect which we call Periplaneta compUlatrix. Our ultimate goal is to design a nervous system capable of endowing this insect with a1l of the behaviors required for long-term survival in a com- plex and dynamic simulated environment similar to that of natural insects. The skills re- quired to survive in this environment include the basic abilities to move around, to find and consume food when necessary, and to escape from predators. In this paper, we focus on the design of that portion of the insect's nervous system which controls its locomo- tion. In designing this insect and the nervous system which controls it, we are inspired by the biological literature. It is important to emphasize, however, that this is not a modeling project. We are not altempting to reproduce the experimental data on a particular animal; rather, we are using insights gleaned from Biology to design neural networks capable of generating similar behaviors. In this manner, we hope to gain a better understanding of the role heterogeneity plays in the generation of behavior by nervous systems, and to ab- stract design principles for use in artificial neural networks. Figure 1. Periplaneta computatrix Heterogeneous Neural Networks for Adaptive Behavior 579 BODY The body of our artificial insect is shown in Figure 1. It is loosely based on the American Cockroach. Periplaneta americana [Bell & Adiyodi. 1981]. However. it is a reasonable abstraction of the bodies of most insects. It consists of an abdomen. head. six legs with feet. two antennae. and two cerci in the rear. The mouth can open and close and contains tactile and chemical sensors. The antennae also contain tactile and chemical sensors. The cerci contain tactile and wind sensors. The feet may be 'either up or down. When a foot is down. it appears as a black square. Finally. a leg can apply forces which translate and rotate the body whenever its foot is down. In addition. though the insect is only two-dimensional, it is capable of ""falling down."" Whenever its center of mass falls outside of the polygon formed by its supporting feet, the insect becomes statically unstable. If this condition persists for any length of time. then we say that the insect has ""fallen down"" and the legs are no longer able to move the body. NEURAL MODEL The essential challenge of the Artificial Insect Project is to design neural controllers ca- pable of generating the behaviors necessary to the insect's survival. The neural model that we are currently using to construct our controllers is shown in Figure 2. It represents the firing frequency of a cell as a function of its input potential. We have used saturating linear threshold functions for this relationship (see inset). The RC characteristics of the cell membrane are also represented. These cells are interconnected by weighted synapses which can cause currents to flow through this membrane. Finally, our model includes the possibility of additional intrinsic currents which may be time and voltage dependent. These currents aJlow us to capture some of the intrinsic propenies which make real neu- rons unique and have proven to be important components of the neural mechanisms un- derlying many behaviors. Synaptic Currents Intrinsic Currents C Cell Membrane I(V)le v v Firing Properties Figure 2. Neural Model Firing Frequency 580 Beer, Chiel and Sterling For example, a pacemaker cell is a neuron which is capable of endogenously producing rhythmic bursting. Pacemakers have been implicated in a number of temporally pat- terned behaviors and playa crucial role in our locomotion controller. As described by Kandel (1976, pp. 260-268), a pacemaker cell exhibits the following characteristics: (1) when it is sufficiently inhibited, it is silent, (2) when it is sufficiently excited, it bursts continuously, (3) between these extremes, the interburst interval is a continuous function of the membrane potential, (4) a transient excitation which causes the cell to fire between bursts can reset the bursting rhythm, and (5) a transient inhibition which prematurely ter- minates a burst can also reset the bursting rhythm. These characteristics can be reproduced with our neural model through the addition of two intrinsic currents. IH is a depolarizing current which tends to pull the membrane po- tential above threshold. IL is a hyperpolarizing current which tends to pull the membrane potential below threshold. These currents change according to the following rules: (1) IH is triggered whenever the cell goes above threshold or IL terminates, and it then re- mains active for a fixed period of time, and (2) IL is triggered whenever IH terminates, and it then remains acti ve for a variable period of time whose duration is a function of the membrane potential. In our work to date, the voltage dependence of IL has been linear. LOCOMOTION An animal's ability to move around its environment is fundamental to many of its other behaviors. In most insects, this requirement is fulfilled by six-legged walking. Thus, this was the first capability we sought to provide to P. computatrix. Walking involves the generation of temporally patterned forces and stepping movements such that the insect maintains a steady forward motion at a variety of speeds without falling down. Though we do not address all of these issues here, it is worth pointing out that locomotion is an interesting adaptive behavior in its own right. An insect robustly solves this complex co- ordination problem in real Lime in the presence of variations in load and terrain, develop- mental changes, and damage to the walking apparatus itself [Graham, 1985]. LEG CONTROLLER The most basic components of walking are the rhythmic movements of each individual leg. These consist of a swing phase, in which the foot is up and the leg is swinging for- ward, and a stance phase, in which the foot is down and the leg is swinging back, propel- ling the body forward. In our controller, these rhythmic movements are produced by the leg controller circuit shown in Figure 3. There is one command neuron, C, for the entire controller and six copies of the remainder of this circuit, one for each leg. The rhythmic leg movements are primarily generated centrally by the portion of the leg controller shown in solid lines in Figure 3. Each leg is controlled by three motor neurons. The stance and swing motor neurons determine the force with which the leg is swung backward or forward, respectively, and the foot motor neuron controls whether the foot is up or down. Normally, the foot is down and the stance motor neuron is active, pushing Heterogeneous Neural Networks for Adaptive Behavior 581 the leg back and producing a stance phase. Periodically, however, this state is interrupted by a burst from the pacemaker neuron P. This burst inhibits the foot and stance motor neurons and excites the swing motor neuron, lifting the foot and swinging the leg for- ward. When this burst terminates, another stance phase begins. Rhythmic bursting in P thus produces the basic swing/stance cycle required for walking. The force applied dur- ing each stance phase as well as the time between bursts in P depend upOn the level of ex- citation supplied by the command neuron C. This basic design is based on the flexor burst-generator model of cockroach walking [pearson, 1976]. In order to properly time the transitions between the swing and stance phases, the control- ler must have some information about where the legs actually are. The simplest way to provide this information is to add sensors which signal when a leg has reached an ex- treme forward or backward angle, as shown with dashed lines in Figure 3. When the leg is all the way back, the backward angle sensor encourages P to initiate a swing by excit- ing it. When the leg is all the way forward, the forward angle sensor encourages P to ter- minate the swing by inhibiting it. These sensors serve to reinforce and fine-tune the cen· trally generated stepping rhythm. They were inspired by the hair plate receptors in P. americana, which seem LO playa similar role in its locomotion [Pearson, 1976]. The RC characteristics of our neural model cause delays at the end of each swing before the next stance phase begins. This pause produces a ""jerky"" walk which we sought to avoid. In order to smooth out this effect, we added a stance reflex comprised of the dot- ted connections shown in Figure 3. This reflex gives the motor neurons a slight ""kick"" in the right direction to begin a stance whenever the leg is swung all the way forward and is also inspired by the cockroach [Pearson, 1976]. Stance Foot Swing ""- Backward Angle Sensor O~ · · ... : ........ '. '. ' ... ' .. ~:~ :D •...................... Forward Angle Sensor E>- Excitatory Connection _ Inhibitory Connection Figure 3. Leg Controller Circuit 582 Beer, Chiel and Sterling Figure 4. Central Coupling between Pacemakers LOCOMOTION CONTROLLER In order for these six individual leg controllers to serve as the basis for a locomotion con- troller, we must address the issue of stability. Arbitrary patterns of leg movements will not, in general, lead to successful locomotion. Rather, the movements of each leg must be synchronized in such a way as to continuously maintain stability. A good rule of thumb is that adjacent legs should be discouraged from swinging at the same time. As shown in Figure 4, this constraint was implemented by mutual inhibition between the pacemakers of adjacent legs. So, for example, when leg L2 is swinging, legs LI, L3 and R2 are discouraged from also swinging, but legs RI and R3 are unaffected (see Figure Sa for leg labelings). This coupling scheme is also derived from Pearson's (1976) work. The gaits adopted by the controller described above depend in general upon the initial an- gles of the legs. To further enhance stability, it is desirable to impose some reliable order to the stepping sequence. Many animals exhibit a stepping sequence known as a metach- ronal wave, in which a wave of stepping progresses from back to front. In insects, for ex- ample, the back leg swings, then the middle one, then the front one on each side of the body. This sequence is achieved in our controller by slightly increasing the leg angle ranges of the rear legs, lowering their stepping frequency. Under these conditions, the rear leg oscillators entrain the middle and front ones, and produce metachronal waves [Graham, 1977]. RESULTS When this controller is embedded in the body of our simulated insect, it reliably produces successful walking. We have found that the insect can be made to walk at different speeds with a variety of gaits simply by varying the flring frequency of the command neuron C. Observed gaits range from the wave gait, in which the metachronal waves on each side of the body are very nearly separated, to the tripod gait, in which the front and back legs on each side of the body step with the middle leg on the opposite side. These gaits fall out of the interaction between the dynamics of the neural controller and the body in which it is embedded. Heterogeneous Neural Networks for Adaptive Behavior 583 L~R I I t Z S 3 .. -.... .. AS ""...,., ,,--'.. \.- ''''_ ~ - tI,: I :~I . p . - -. _ 1t,1_11, _ _ I ;: : LS! ,1 .. _: , •.••• _ "" •.•• _ L, '...-,', .• - - ll ·· .. ~ - .... ' ~ - - A. .$1q'plq ,dIrII It3 _ _ - JI2 _ - - a, _ _ - L3 _ - - L2_ - - • Ll ii i ,nT, iii ii i i., 1m. iii' iii i 1m: i Ii iii, , 25~dlv stq'pfq 'dIrII It3 _ _ _ - 112. ___ - al ___ _ L3. _ - - • U _ _ _ - Ll _ _ - - .""iI .. hi 'liihi """""""" Ii i hi iii Ii liiii i i 25 --=vdlv It3 R2 _ _IIIJ11l'aMm - - - ----• .1 _ - - - L3 __ - - • L2 __ - - Ll ~'iii Ii ,.""'ii.i 'irm •• ; •• .wriii'iilWt 25 -'Ct,v &ea'p.ptq I'dIrII 1t3 __ - - - -- - - - al ___ - - 1.2 ___ _ u ___ - - L 1 ""Ii mT II Ii 1i'l'lT! "" Ii i """"Ii "" i ~'mT i Ii ill 25~d,v B. Figure S. (A) Description of Some Gaits Observed in Natural Insects (from [Wilson, 1966]). (B) Selected Gaits Observed in P. computatrix. If the legs .are labeled as shown at the top of Figure Sa, then gaits may be conveniently described by their stepping patterns. In this representation, a black bar is displayed dur- ing the swing phase of each leg. The space between bars represents the stance phase. Selected gaits observed in P. computatrix at different speeds are shown in Figure 5b as the command neuron firing frequency is varied from lowest (top) to highest (bottom). At the lower speeds, the metachronal waves on each ~ide of the body are very apparent. The metachronal waves can still be discerned in fas~r walks. However, they increasingly overlap as the stance phases shorten, until the tripod gait appears at the highest speeds. This sequence of gaits bears a strong resemblance to some of those that have beep de- scribed for natural insects, as shown in Figure Sa [Wilson, 1966]. In order to study the robustness of this controller and to gain insight into the detailed mechanisms of its operation, we have begun a series of lesion studies. Such studies ex- 584 Beer, Chiel and Sterling amine the behavioral effects of selective damage to a neural controller. This study is still in progress and we only report a few preliminary results here. In general, we have been repeatedly surprised by the intricacy of the dynamics of this controller. For example, re- moval of all of the forward angle sensors resulted in a complete breakdown of the metachronal wave at low speeds. However, at higher speeds, the gait was virtually unaf- fected. Only brief periods of instability caused by the occasional overlap of the slightly longer than normal swing phases were observed in the tripod gait, but the insect did not fall down. Lesioning single forward angle sensors often dynamically produced compen- satory phase shifts in the other legs. Lesions of selected central connections produced similarly interesting effects. In general, our studies seem to suggest subtle interactions between the central and peripheral components of the controller which deserve much more exploration. Finally, we have observed the phenomena of reflex stepping in P. computalrix. When the central locomotion system is completely shut down by strongly inhibiting the command neuron and the insect is continuously pushed from behind, it is still capable of producing an uncoordinated kind of walking. As the insect is pushed forward, a leg whose foot is down bends back until the backward angle sensor initiates a swing by exciting the pace- maker neuron P. When the leg has swung all the way forward, the stance reflex triggered by the forward angle sensor puts the foot down and the cycle repeats. Brooks (1989) has described a semi-distributed locomotion controller for an insect-like autonomous robot. We are very much in agreement with his general approach. However, his controller is not as fully distributed as the one described above. It relies on a central leg lift sequencer which must be modified to produce different gaits. Donner (1985) has also implemented a distributed hexapod locomotion controller inspired by an early model of Wilson's (1966). His design used individual leg controllers driven by leg load and position information. These leg controllers were coupled by forward excitation from posterior legs. Thus, his stepping movements were produced by reflex-driven pe- ripheral oscillators rather than the central oscillators used in our model. He did not report the generation of the series of gaits shown in Figure Sa. Donner also demonstrated the ability of his controller to adapt to a missing leg. We have experimented with leg ampu- tations as well, but with mixed success. We feel that more accurate three-dimensional load information than we currently model is necessary for the proper handling of amputa- tions. Neither of these other locomotion controllers utilize neural networks. CONCLUSIONS AND FUTURE WORK We have described a heterogeneous neural network for controlling the walking of a simu- lated insect. This controller is completely distributed yet capable of reliably producing a range of statically stable gaits at different walking speeds simply by varying the tonic ac- tivity of a single command neuron. Lesion studies have demonstrated that the controller is robust, and suggested that subtle interactions and dynamic compensatory mechanisms are responsible for this robustness. This controller is serving as the basis for a number of other behaviors. We have already implemented wandering, and are currently experimenting with controllers for recoil re- Heterogeneous Neural Networks for Adaptive Behavior 585 sponses and edge following. In the near future, we plan to implement feeding behavior and an escape response, resulting in what we feel is the minimum complement of behav- iors necessary for survival in an insect-like environment Finally, we wish to introduce plasticity into these controllers so that they may better adapt to the exigencies of particu- lar environments. We believe that learning is best viewed as a means by which additional flexibility can be added to an existing controller. The locomotion controller described in this paper was inspired by the literature on insect locomotion. The further development of P. compUlalrix will continue to draw inspiration from the neuroethology and neurobiology of simpler natural organisms. In trying to de- sign autonomous organisms using principles gleaned from Biology, we may both im- prove our understanding of natural nervous systems and discover design principles of use to the construction of artificial ones. A robot with ""only"" the behavioral repertoire and adaptability of an insect would be an impressive achievement indeed. In particular, we have argued in this paper for a more careful consideration of the intrinsic architecture and heterogeneity of biological nervous systems in the design of artificial neural networks. The locomotion controller we have described above only hints at how productive such an approach can be. References Bell, W.J. and K.G. Adiyodi eds (1981). The American Cockroach. New York: Chapman and Hall. Brooks, R.A. (1989). A robot that walks: emergent behaviors from a carefully evolved network. Neural Computation 1(1). Donner, M. (1987). Real-time control of walking (Progress in Computer Science, Volume 7). Cambridge, MA: Birkhauser Boston, Inc. Graham, D. (1977). Simulation of a model for the coordination of leg movements in free walking insects. Biological Cybernetics 26:187-198. Graham, D. (1985). Pattern and control of walking in insects: Advances in Insect PhYSiology 18:31-140. Kandel, E.R. (1976). Cellular Basis of Behavior: An Introduction to Behavioral Neurobiology. W.H. Freeman. Kuffler, S.W., Nicholls, J.G., and Martin, A. R. (1984). From Neuron to Brain: A Cellular Approach to the Function of the Nervous System. Sunderland, MA: Sinauer Associates Inc. Pearson, K. (1976). The control of walking. Scientific American 235:72-86. Selverston, A.I. (1988). A consideration of invertebrate central pattern generators as com- putational data bases. Neural Networks 1:109-117. Wilson, D.M. (1966). Insect walking. Annual Review of Entomology 11:103-122.","[-0.06485418975353241, -0.06022150442004204, 0.0734521746635437, 0.022669540718197823, -0.05219933018088341, 0.008985612541437149, -0.0040490697138011456, -0.042096953839063644, -0.03610406070947647, -0.004825218580663204, 0.007351142354309559, -0.07947488874197006, -0.004921158775687218, 0.008076824247837067, -0.08641977608203888, 0.04660145938396454, -0.024999668821692467, 0.028346125036478043, -0.07959543913602829, 0.006883333902806044, 0.015418765135109425, 0.04698191210627556, 0.015035878866910934, -0.0362544022500515, -0.09488340467214584, 0.08087288588285446, -0.09428803622722626, 0.024084899574518204, 0.014382258988916874, -0.06849924474954605, 0.08225169032812119, -0.044366199523210526, -0.021162832155823708, 0.011348646134138107, -0.09364363551139832, -0.0068795569241046906, -0.07780683040618896, -0.06276988238096237, 0.04892661049962044, 0.05641926825046539, 0.01507000532001257, 0.011370150372385979, -0.03266683593392372, 0.010119331069290638, 0.007179104257375002, 0.05858062952756882, -0.03576800227165222, -0.017395518720149994, 0.018077876418828964, -0.02017783187329769, -0.021438170224428177, -0.08509138226509094, 0.07594548910856247, 0.07691546529531479, 0.06641911715269089, 0.007511650677770376, -0.018975166603922844, 0.04619273915886879, -0.014108340255916119, -0.09397962689399719, 0.024412360042333603, 0.02497720532119274, 0.08825262635946274, -0.04101951792836189, -0.060158200562000275, 0.022752726450562477, -0.07750838249921799, 0.025683263316750526, 0.040970250964164734, -0.05658063665032387, 0.021300751715898514, 0.03884799778461456, -0.029859395697712898, -0.009834714233875275, -0.015192506834864616, 0.010989522561430931, -0.024260245263576508, 0.034918542951345444, 0.05735978111624718, -0.003104408038780093, -0.02128751389682293, -0.04966101050376892, -0.011038033291697502, 0.036878637969493866, 0.06335350126028061, -0.030025696381926537, 0.012761878781020641, 0.09230169653892517, 0.041087277233600616, -0.02331460826098919, -0.0068892501294612885, 0.040074799209833145, 0.02462203986942768, -0.0964602530002594, 0.0218234583735466, 0.0042055225931108, -0.009159459732472897, -0.06653565913438797, -0.016106972470879555, 0.07110390812158585, -0.013578676618635654, -0.053120553493499756, -0.0014501814730465412, 0.09829423576593399, 0.07523936033248901, 0.0017250460805371404, 0.08866804093122482, 0.023314164951443672, 0.07652927935123444, -0.0011438877554610372, -0.054645657539367676, 0.04450397565960884, 0.0248525720089674, 0.10009642690420151, -0.05789123475551605, -0.07238125801086426, -0.06241467222571373, -0.026442592963576317, 0.03645321726799011, 0.06232510507106781, 0.06809630990028381, -0.06126280128955841, -0.07262737303972244, -0.005454228259623051, 0.06847099959850311, -0.037559136748313904, -0.06908277422189713, 2.7057464682380103e-33, -0.03109140507876873, 0.013676239177584648, -0.003777985228225589, -0.02727530151605606, 0.05254897102713585, -0.07571727782487869, -0.011224203743040562, -0.03199033811688423, 0.033963438123464584, -0.016953958198428154, -0.12441946566104889, 0.08102305233478546, -0.058551184833049774, 0.08226950466632843, 0.06024223938584328, 0.029783692210912704, -0.020990870893001556, -0.05174081400036812, 0.1146460473537445, -0.0796637162566185, 0.05161769688129425, -0.042439837008714676, -0.00018967555661220104, -0.03411678597331047, 0.012492701411247253, 0.052988529205322266, 0.02868395857512951, -0.017793457955121994, -0.003706039162352681, -0.0036513009108603, 0.0517018623650074, -0.02337140589952469, -0.054437410086393356, -0.0009690580191090703, 0.030939681455492973, 0.02869325689971447, 0.026339903473854065, -0.027055593207478523, -0.0013181716203689575, 0.017411338165402412, 0.004583562724292278, -0.02590426430106163, 0.06892931461334229, -0.0016035564476624131, -0.014773961156606674, 0.007704115007072687, 0.03740391135215759, 0.085880808532238, -0.033748988062143326, -0.010901236906647682, -0.030765123665332794, 0.044561177492141724, 0.11840681731700897, -0.05685705319046974, 0.06668063253164291, 0.050544049590826035, 0.03882266953587532, -0.009342233650386333, -0.05339089408516884, 0.029267817735671997, -0.019373202696442604, 0.0005167238414287567, 0.00035142264096066356, 0.0010572833707556129, 0.09164709597826004, 0.019990870729088783, -0.06050654500722885, 0.047312237322330475, 0.04934614524245262, -0.00851754192262888, 0.06720966845750809, -0.02147507667541504, 0.007306182757019997, -0.045154523104429245, -0.04855024814605713, 0.01204497180879116, 0.011821682564914227, -0.09631917625665665, -0.15728096663951874, -0.0010857636807486415, 0.012400628067553043, 0.033978499472141266, -0.05371858924627304, -0.015825552865862846, -0.019266918301582336, -0.009201270528137684, 0.07007995247840881, -0.03852587565779686, 0.006191176362335682, 0.010306918062269688, -0.006088757887482643, -0.030779417604207993, 0.035244304686784744, -0.006648258771747351, 0.026185346767306328, -3.379274425545449e-33, -0.04352888837456703, -0.007650959771126509, -0.05119991675019264, 0.025349607691168785, -0.0006045041955076158, 0.10469569265842438, -0.07217435538768768, 0.012822193093597889, -0.04924613982439041, 0.03792351856827736, -0.04335010051727295, 0.11671353876590729, 0.05243009701371193, 0.043130792677402496, 0.021813752129673958, -0.015023916959762573, -0.06864272803068161, 0.0004141188401263207, 0.1384238302707672, -0.013623169623315334, -0.03322306647896767, 0.09448616206645966, -0.11459283530712128, -0.027516862377524376, -0.03123241849243641, 0.048199377954006195, -0.03325636684894562, 0.13114793598651886, -0.017557993531227112, 0.01491470355540514, -0.05015339329838753, -0.045258063822984695, -0.02325655333697796, 0.02503903955221176, 0.04895707592368126, 0.06452831625938416, 0.03579380735754967, -0.0445643775165081, 0.04053672403097153, -0.039617568254470825, -0.009691915474832058, -0.05758097767829895, -0.004005450289696455, -0.008772045373916626, 0.05387280508875847, 0.03984963148832321, -0.07960842549800873, 0.040734633803367615, -0.07132431864738464, 0.022917086258530617, 0.01422831229865551, 0.040459904819726944, -0.1270805299282074, -0.07445604354143143, 0.05144808441400528, 0.04545563459396362, -0.01704324223101139, 0.0028790368232876062, 0.031013190746307373, -0.07640745490789413, -0.04638222977519035, -0.053685471415519714, -0.024458933621644974, 0.05709713697433472, -0.11793629080057144, 0.03331000730395317, -0.041570402681827545, 0.05037630721926689, 0.060341645032167435, -0.04828466847538948, -0.01059909351170063, 0.10992105305194855, 0.10048576444387436, -0.03886662423610687, 0.023593265563249588, 0.00019400627934373915, -0.0325225368142128, -0.048393115401268005, 0.02399427630007267, -0.05628029629588127, -0.004734995774924755, -0.012302580289542675, -0.0009524869965389371, -0.013563187792897224, 0.011181401088833809, 0.07594790309667587, -0.08000121265649796, 0.08832327276468277, -0.0013401360483840108, -0.014307619072496891, 0.07679525762796402, 0.042388737201690674, 0.028665555641055107, 0.02395193837583065, -0.05140116810798645, -4.722311075511243e-08, -0.04609907045960426, 0.03919615596532822, 0.03223290666937828, -0.026499811559915543, 0.04724399372935295, -0.042503949254751205, 0.03405221924185753, -0.1358793079853058, -0.000983026809990406, 0.009279666468501091, 0.07544048875570297, -0.03737606480717659, 0.05713367462158203, 0.018338574096560478, 0.024258336052298546, 0.05568448454141617, 0.0005890447064302862, -0.02033925987780094, 0.010395781137049198, 0.023781074211001396, -0.00545850396156311, 0.058086153119802475, -0.04771468788385391, 0.021651940420269966, 0.053835075348615646, -0.09478511661291122, -0.05082977935671806, -0.061465855687856674, -0.0467391200363636, 0.09285285323858261, 0.01204073429107666, 0.0905255675315857, -0.006309773772954941, 0.0096892723813653, 0.014593662694096565, 0.054431527853012085, 0.01688043214380741, -0.011060802266001701, 0.013838829472661018, -0.0813257247209549, -0.02112753689289093, 0.07211730629205704, -0.06691550463438034, -0.05015541985630989, 0.0018244751263409853, -0.05237073078751564, 0.0417470782995224, -0.13307319581508636, 0.03539358824491501, -0.015466072596609592, -0.019723180681467056, 0.039742812514305115, -0.04070769250392914, 0.020056962966918945, 0.03164521977305412, -0.027605906128883362, 0.010621149092912674, -0.1343250423669815, -0.0038853005971759558, 0.08432763814926147, -0.0050789774395525455, -0.011435916647315025, -0.00834248960018158, -0.07670236378908157]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\MappingClassifierSystemsIntoNeuralNetworks.pdf,Deep Learning,"272 NEURAL NET RECEIVERS IN MULTIPLE-ACCESS COMMUNICATIONS Bernd-Peter Paris, Geoffrey Orsak, Mahesh Varanasi, Behnaam Aazhang Department of Electrical and Computer Engineering Rice University Houston, TX 77251-1892 ABSTRACT The application of neural networks to the demodulation of spread-spectrum signals in a multiple-access environment is considered. This study is motivated in large part by the fact that, in a multiuser system, the conventional (matched fil- ter) receiver suffers severe performance degradation as the relative powers of the interfering signals become large (the ""near-far"" problem). Furthermore, the optimum receiver, which alleviates the near-far problem, is too complex to be of practical use. Receivers based on multi-layer perceptrons are considered as a simple and robust alternative to the opti- mum solution. The optimum receiver is used to benchmark the performance of the neural net receiver; in particular, it is proven to be instrumental in identifying the decision regions of the neural networks. The back-propagation algorithm and a modified version of it are used to train the neural net. An importance sampling technique is introduced to reduce the number of simulations necessary to evaluate the performance of neural nets. In all examples considered the proposed neu- ral ~et receiver significantly outperforms the conventional recelver. INTRODUCTION In this paper we consider the problem of demodulating signals in a code-division multiple-access (CDMA) Gaussian channel. Multiple accessing in code domain is achieved by spreading the spectrum of the transmitted signals using preassigned code waveforms. The conventional method of demodulating a spread-spectrum sig- nal in a multiuser environment employs one filter matched to the desired signal. Since the conventional receiver ignores the presence of interfering signals it is reli- able only when there are few simultaneous transmissions. Furthermore, when the relative received power of the interfering signals become large (the ""near-far"" prob- lem), severe performance degradation of the system is observed even in situations with relatively low bandwidth efficiencies (defined as the ratio of the number of channel subscribers to the spread of the bandwidth) [Aazhang 87]. For this reason there has been an interest in designing optimum receivers for multi-user communica- tion systems [Verdu 86, Lupas 89, Poor 88]. The resulting optimum demodulators, Neural Net Receivers in Multiple-Access Communications 273 however, have a variable decoding delay with computational and storage complexity that depend exponentially on the number of active users. Unfortunately, this com- putational intensity is unacceptable in many applications. There is hence a need for near optimum receivers that are robust to near-far effects with a reasonable computational complexity to ensure their practical implementation. In this study, we introduce a class of neural net receivers that are based on mul- tilayer perceptrons trained via the back-propagation algorithm. Neural net receivers are very attractive alternatives to the optimum and conventional receivers due to their highly parallel structures. As we will observe, the performance of the neural net receivers closely track that of the optimum receiver in all examples considered. SYSTEM DESCRIPTION In the multiple-access network of interest, transmitters are assumed to share a radio band in a combination of the time and code domain. One way of multiple accessing in the code domain is spread spectrum, which is a signaling scheme that uses a much wider bandwidth than necessary for a given data rate. Let us assume that in a given time interval there are K active transmitters in the network. In a simple setting, the kth active user, in a symbol interval, transmits a signal from a binary signal set derived from the set of code waveforms assigned to the corresponding user. The signal is time limited to the interval [a, T], where T is the symbol duration. In this paper we will concentrate on symbol-synchronous CDMA systems. Syn- chronous systems find applications in time slotted channels with the central (base) station transmitting to remote (mobile) terminals and also in relays between cen- tral stations. The synchronous problem will also be construed as providing us with a manageable setting to better understand the issues in the more difficult asyn- chronous situation. In a synchronous CDMA system, the users maintain time syn- chronism so that the relative time delays associated with all users are assumed to be zero. To illustrate the potentials of the proposed multiuser detector, we present the application to binary PSK direct-sequence signals in coherent systems. Therefore, the signal at a given receiver is the superposition of the K transmitted signals in additive channel noise (see [Aazhang 87, Lupas 89] and references within) P K ret) = L L b~i) Akak(t - iT) cos(we[t - iT] + Ok) + nt, t E ~, (1) i=1 k=1 where P is the packet length, Ak is the signal amplitude, We is the carrier frequency, Ok is the phase angle. The symbol b1i) E {-I, + I} denotes the bit that the kth user is transmitting in the ith time interval. In this model, nt is the additive channel noise which is assumed to be a white Gaussian random process. The time-limited code waveform, denoted.by ak(t), is derived from the spreading sequence assigned to the kth user. That is, ak(t) = Ef=-~/ a)k)p(t - jTe) where pet) is the unit rectangular pulse of duration Te and N is the length of the spreading sequence. One code period !!(k) = [a~k),a~k), . . . ,a~~I] is used for spreading the signal per symbol so 274 Paris, Orsak, Varanasi and Aazhang that T = NTc • In this system, spectrum efficiency is measured as the ratio of the number of channel users to the spread factor, K/ N. In the next two sections, we first consider optimum synchronous demodulation of the multiuser spread-spectrum signal. Then, we introduce the application of neural networks to the multiuser detection problem. OPTIMUM RECEIVER Multiuser detection is an active research area with the objective of developing strate- gies for demodulation of information sent by several transmitters sharing a channel [Verdu 86, Poor 88, Varanasi 89, Lupas 89]. In these situations with two or more users of a multiple-access Gaussian channel, one filter matched to the desired signal is no longer optimum since the decision statistics are effected by the other signals (e.g., the statistics are disturbed by cross-correlations with the interfering signals). Employing conventional matched filters, because of its structural simplicity, may still be justified if the system is operating at a low bandwidth efficiency. However, as the number of users in the system with fixed bandwidth grows or as the rel- ative received powers of the interfering signals become large, severe performance degradation of the conventional matched filter is observed [Aazhang 87]. For direct- sequence spread-spectrum systems, optimum receivers obtained by Verdu and Poor require an extremely high degree of software complexity and storage, which may be unacceptable for most multiple-access systems [Verdu 86, Lupas 89]. Despite imple- mentation problems, studies on optimum demodulation illustrate that the effects of interfering signals in a CDMA system, in principle, can be neutralized. A complete study of the suboptimum neural net receiver requires a review of the maximum likelihood sequence detection formulation. Assuming that all possible information sequences are independent and equally likely, and defining !L{ i) = [b~i), b~i), ... , b}2]', it is easy to see that an optimum decision on fL{ i) is a one-shot decision in that it requires the observation of the received signal only in the ith time interval. Without loss of generality, we will therefore focus our attention on i = 0 and drop the time superscript and consider the demodulation of the vector of bits !L with the observation of the received signal in the interval [0,11- In a K -user Gaussian channel, the most likely information vector is chosen as that which maximizes the log of the likelihood function (see [Lupas 89]) where Sk(t) = Akak(t) cos(wct + Ok) is the modulating signal of the kth user. The optimum decision can also be written as ~pt = arg max {2y'IL - !L'HIL} , te{ _l,+l}K - (3) where H is the K x K matrix of signal cross-correlations such that the (k,l)th element is hk,r =< Sk(t), Sr(t) >. The vector of sufficient statistics '[ consists of the Neural Net Receivers in Multiple-Access Communications 275 outputs of a bank of J{ filters each matched to one of the signals Yk = iT r(t)Sit;(t)dt, for k = 1,2, ... ,K. (4) The maximization in (3) has been shown to be NP-complete [Lupas 89], i.e., no algorithm is known that can solve the maximization problem in polynomial time in K. This computational intensity is unacceptable in many applications. In the next section, we consider a suboptimum receiver that employs artificial neural networks for finding a solution to a maximization problem similar to (3). NEURAL NETWORK Until now the application of neural networ,ks to multiple-access communications has not drawn much attention. In this study we employ neural networks for classifying different signals in synchronous additive Gaussian channels. We assume that the information bits of the first of the K signals is of interest, therefore, the phase angle of the desired signal is assumed to be zero (i.e., (}1 = 0). Two configurations with multi-layer perceptrons and sigmoid nonlinearity are considered for multiuser detection of direct-sequence spread-spectrum signals. One structure is depicted in Figure 1.b where a layered network of percep- trons processes the sufficient statistics (4) of the multi-user Gaussian channel. In this structure the first layer of the net (referred to as the hidden layer) processes [Y1, Y2, ... , YK]. The output layer may only have one node since there is only one signal that is being demodulated. This feed-forward structure is then trained using the back-propagation algorithm [Rumelhart 86]. In an alternate configuration, the continuous-time received signal is converted to an N-dimensional vector by sampling the output of the front-end filter at the chip rate Te- 1 as illustrated in Figure 1.a. The input vector to the net can be written so that the demodulation of the first signal is viewed as a classification problem: (5) where £1(1) is the spreading code vector of the first user, 1] is a length-N vector of filtered Gaussian noise samples and L = E[=2 bkA~ COS(8k)!!(k) is the multiple- access interference vector with Ak = AkTel2, Vk = 1,2, ... ,K. The layered neural net is then trained to process the input vector for demodulation of the first user's information bits via the back-propagation algorithm. For this configuration we consider two training methods, first the multi-layer receiver is trained, via the back- propagation algorithm, to classify the parity of the desired signal (referred to as the ""trained"" example) [Lippmann 87]. In another attempt (referred to as the ""preset"" example), the input layer of the net is preset as Gaussian classifiers and the other layers are trained using the back propagation algorithm [Gschwendtner 88]. Since we are interested in understanding the internal representation of knowledge by the weights of the net, a signal space method is developed to illustrate decision regions. In a K -user system where the spreading sequences are not orthogonal, the 276 Paris, Orsak, Varanasi and Aazhang signals can be represented by orthonormal bases using the Gram-Schmidt procedure. The optimum decision regions in the signal space for the demodulation of 61 are known [Poor 88] and can be directly compared to ones for the neural net. Figure 2 illustrates decision regions for the optimum receiver and for ""preset"" and ""trained"" neural net receivers. In this example, two users are sharing a channel with N = 3, signal to noise ratio of user 1 (SN Rd equal to 8dB and relative energies of the two user, E2/ E1 = 6dB. As it is seen in this figure the decision region of the ""preset"" example is almost identical to the optimum boundary, however, the decision boundary for the ""trained"" example is quite conservative. Such comparisons are instrumental not only in identifying the pattern by which decisions are made by the neural networks but also in understanding the characteristics of the training algorithms. PERFORMANCE ANALYSIS In this paper, we motivate the application of neural nets to single-user detection in multiuser channels by comparing the performance of the receivers in Figure 1 to that of the conventional and the optimum [Poor 88]. Since exact analysis of the bit error probabilities for the neural net receivers are analytically intractable, we con- sider Monte Carlo simulations. This method can produce very accurate estimates of bit-error probability if the number of simulations is sufficiently large to ensure occurrence of several erroneous decisions. The fact that these multiuser receivers operate with near optimum error rates puts a tremendous computational burden on the computer system. The new variance reduction scheme, developed by Orsak and Aazhang in [Orsak 89], first shifts the simulated channel noise to bias the simula- tions and then scales the error rate to obtain an unbiased estimate with a reduced variance. This importance sampling technique, which proved to be extremely effec- tive in single-user detection [Orsak 89], is applied to the analysis of the multiuser systems. As discussed in [Orsak 89], the fundamental issue is to generate more errors by biasing the simulations in cases where the error rate is very small. This strategy is better described by the two-user Gaussian example in Figure 2. In this example the simulation is carried out by generating zero-mean Gaussian noise vectors 'I} , random phase (}2 and random values of the interfering bit 62 . Considering 61 = 1. (corresponding to signals +a1 + a2 or +a1 - a2 which are marked by ""+"" in Figure 2) error occurs if the statistics fall on the left side of the decision boundary. It can be shown that the most efficient biasing scheme corresponds to a shift of the mean of the Gaussian noise and the multiple-access interference such that the mean of the statistics are placed on the decision boundary (the shifted signals are marked by ""0"" in Figure 2). Since this strategy generates much more errors than the standard Monte Carlo, errors are weighted to obtain an unbiased estimate of the error rate. The importance sampling technique substantially reduces the number of simulation trials compared to standard Monte Carlo for a given accuracy. In Figure 3 the gain which is defined as the ratio of the number of trials required for a fixed variance using Monte Carlo to that using the importance sampling method, is plotted versus Neural Net Receivers in Multiple-Access Communications 277 the bit-error probability. In this example, the spreading sequence length, N is equal 3 and relative energies of the two user, E2/ El = 6dB. The gain in this example of severe near-far problem is inversely proportional to the error rate. Furthermore, results from extensive analysis indicated that the proposed importance sampling technique is well suited for problems in multi-user communications and less than 100 trials is sufficient for an accurate error probability estimate. NUMERICAL RESULTS The performance of the conventional, optimum [Poor 88] and the neural net re- ceivers are compared via Monte Carlo simulations employing the importance sam- pling method. Except for a difference in length of training periods, the two configu- rations in Figure 1 result in similar average bit-error probabilities. Results presented here correspond to the neural net receiver in Figure l.a. A two-user Gaussian channel is considered with severe near-far problem where E2/ El = 6dB and spreading sequence length N = 3. In Figure 4, the average bit-error probabilities of the four receivers (conventional, optimum, neural nets for the ""trained"" and ""preset"" examples) are plotted versus the signal to noise ratio of the first user (SN RI). It is clear from this figure that the two neural net receivers outperform the matched filter receiver over the range of SN R l . Figure 5 depicts these average error probabilities versus the relative energies of the two users (i.e., E2/ El ) for a fixed SN Rl = 8dB and N = 3. As expected the conventional receiver becomes multiple-access limited as E2 increases, however, the performance of the neural net receivers closely track that of the optimum receiver for all values of E2 • We also considered a three-user Gaussian example with a high bandwidth effi- ciency and severe near-far problem where spreading sequence length N = 3 and first and third users have equal energy and second user has four times more energy (Le., E2/ El = 6dB ). The average error probabilities of the four receivers versus SN Rl are depicted in Figure 6. The neural net receivers maintained their near optimum performance even in this three user example with a spread fae tor of 3 corresponding to a bandwidth efficiency of 1. CONCLUSIONS In this paper, we consider the problem of demodulating a signal in a multiple- access Gaussian channel. The error probability of different neural net receivers were compared with the conventional and optimum receivers in a symbol-synchronous system. As expected the performance of the conventional receiver (matched filter) is very sensitive to the strength of the interfering users. However, the error probability of the neural net receiver is independent of the strength of the other users and is at least one order of magnitude better than the conventional receiver. Except for a difference in the length of training periods, the two configurations in Figure 1 result in similar average bit-error probabilities. However, the training strategies, ""preset"" and ""trained"", resulted in slightly different error rates and decision regions. The multi-layer perceptron was very successful in the classification problem in the presence of interfering signals. In all the examples that were considered, two layers 278 Paris, Orsak, Varanasi and Aazhang of perceptrons proved to be sufficient to closely approximate the decision boundary of the optimum receiver. We anticipate that this application of neural networks will shed more light on the potentials of neural nets in digital communications. The issues facing the project were quite general in nature and are reported in many neural network studies. However, we were able to address these issues in multiple-access communications since the disturbances are structured and the optimum receiver (which is NP-hard) is well understood. References [Aazhang 87] B. Aazhang and H. V. Poor. Performance of DS/SSMA Com- munications in Impulsive Channels-Part I: Linear Correlation Receivers. IEEE Trans. Commun., COM-35(1l):1l79-1188, November 1987. [Gschwendtner 88] A. B. Gschwendtner. DARPA Neural Network Study. AFCEA International Press, 1988. [Lippmann 87] [Lupas 89] [Orsak 89] [Poor 88] [Rumelhart 86] [Varanasi 89] [Verdu 86] R. P. Lippmann and B. Gold. Neural-Net Classifiers Useful for Speech Recognition. In IEEE First Conference on Neural Net- works, pages 417-425, San Diego, CA, June 21-24, 1987. R. Lupas and S. Verdu. Linear Multiuser Detectors for Syn- chronous Code-Division Multiple-Access Channels. IEEE Trans. Info. Theory, IT-34, 1989. G. Orsak and B. Aazhang. On the Theory of Importance Sam- pling Applied to the Analysis of Detection Systems. IEEE Trans. Commun., COM-37, April, 1989. H. V. Poor and S. Verdu. Single-User Detectors for Multiuser Channels. IEEE Trans. Commun., COM-36(1):50-60, January, 1988. D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning Internal Representation by Error Propagation. In D. E. Rumel- hart and J. L. McClelland, editors, Parallel Distributed Pro- cessing: Explorations in the Microstructure of Cognition. Vol. I: Foundations, pages 318-362, MIT Press, 1986. M. K. Varanasi and B. Aazhang. Multistage Detection in Asynchronous Code-Division Multiple-Access Communications. IEEE Trans. Commun., COM-37, 1989. S. Verdu. Optimum Multiuser Asymptotic Efficiency. IEEE Trans. Commun., COM-34(9):890-897, September, 1986. Neural Net Receivers in Multiple-Access Communications 279 Sampler (n+l)T c (a) ret) Figure 1. Two Neural Net Receiver Structures. 4,---------------~.------rr------~ 2 o -2 • o : • Matched Filter .... Neural Net (preset) l , l ,. "" r' ~ l ~' Optimum Receiver A- Neural Net (trained) f I "" I I : I : .... I -4~----~~~~--~----~----~--~ -3 -2 -1 o 2 3 (b) Figure 2. Decision Boundaries of the Various Receivers. 1012~ ________________________________ ~ 10 10 10 8 .~ 10 6 C!' 10 4 10 2 Opt. Receiver Neural Net (preset) Neural Net (trained) Matched Filter 10° ~~--~~~~~~--~~--~~--~~ 10-13 10 -11 10-9 10 -7 10-5 10-3 10 -1 Prob. of Error Figure 3. Importance Sampling Gain versus Error Rate for 2-user Example. ~1 280 Paris, Orsak, Varanasi and Aazhang 10-1 r--::::~~::::=----I 10-2 10-3 15 10-4 .. 10-5 ~ 10-6 'Q 10-7 ~ 10-8 Q 10-9 ""= 10-10 to-11 to-I Matched Filter Neural Net (trained) Neural Net (preset) Opt Receiver to -13+-_...,.._--. __ ,--_...,.._--. __ ,--_ .... 2 4 6 8SNR l~ dB 12 14 16 Figure 4. Prob. of Error as a Function of the SNR (E2/El = 4). 10-1 ~------------------------------~ Matched Filter Neural Net (trained) Neural Net (preset) Opt. Receiver 104+---~--~--~--~--~--,---~~ o 1 2 E2/El 3 4 Figure 5. Influence of MA-Interference (SNR = 8dB). 10-1 '---~~-'--~F.:~~==~====---, 10 -2 10 -3 10-4 10 -5 10 -6 10 -7 10 -8 10 -9 10 -10 10 ~1l 10 -12 Neural Net (trained) Neural Net (preset) Opt. Receiver 10 -13-+-'I'""'""""""II""""""""I""' ........ ~.......,--r' ........ ~......., __ -r-..,--.......,.--.--r--t 2 4 6 8 10 12 14 16 SNR in dB Figure 6. Error Curves for the 3-User Example.","[-0.01931804046034813, -0.012656583450734615, -0.022243479266762733, -0.006042798049747944, -0.003934812266379595, 0.02541000209748745, 0.06695020943880081, -0.02965458855032921, -0.017373280599713326, -0.05072537809610367, -0.04858224838972092, 0.04367094486951828, -0.0021095143165439367, -0.09347005933523178, -0.04290129244327545, -0.0030190187972038984, 0.05333699658513069, 0.003051815088838339, -0.03284216299653053, -0.0419476144015789, 0.011096841655671597, 0.0017394755268469453, 0.0036487088073045015, -0.044584114104509354, 0.008936172351241112, -0.017235644161701202, 0.006747575011104345, 0.017298130318522453, 0.019996054470539093, 0.004740218631923199, 0.02975151501595974, -0.005492661148309708, 0.026193896308541298, 0.007121475413441658, -0.11475541442632675, -0.003570371074602008, -0.051303353160619736, 0.02055760659277439, 0.012405440211296082, 0.04533792659640312, -0.014014351181685925, -0.0044570923782885075, -0.038435280323028564, 0.022828366607427597, 0.056943707168102264, -0.01081772893667221, 0.0007679625996388495, 0.04561099782586098, 0.04784873500466347, -0.08104806393384933, 0.01448191050440073, 0.03285115212202072, -0.007229569833725691, 0.11172490566968918, -0.001309889368712902, -0.024035532027482986, -0.061789244413375854, 0.03634240850806236, -0.07135146111249924, 0.08793678879737854, -0.042930494993925095, 0.044418055564165115, -0.02408689260482788, -0.0868750810623169, 0.01930186152458191, 0.02076050639152527, 0.03789328411221504, 0.04526515677571297, 0.019953692331910133, -0.03029111959040165, -0.017834236845374107, 0.047029945999383926, -0.00045719192712567747, 0.03787379711866379, 0.03689569607377052, 0.0056587099097669125, -0.002998871263116598, -0.026197852566838264, 0.007776571437716484, 0.004922803491353989, 0.004509679041802883, -0.008877262473106384, -0.06031116470694542, -0.07271502166986465, 0.037020131945610046, 0.02744241990149021, -0.06330954283475876, 0.010402808897197247, -0.013698894530534744, -0.05007171258330345, -0.04228341579437256, -0.0007860639598220587, -0.020374735817313194, 0.01960926502943039, 0.046826958656311035, 0.01920795813202858, -0.04446626082062721, -0.022517330944538116, 0.001722179469652474, 0.07861936837434769, 0.009315558709204197, -0.04781344532966614, -0.004834455903619528, -0.05573633685708046, -0.003090196056291461, -0.010924381203949451, 0.05761931464076042, 0.04144006967544556, 0.10342264920473099, -0.0882706269621849, -0.04814776033163071, 0.04423898085951805, -0.054878346621990204, 0.03661452233791351, -0.0038513559848070145, 0.04909659922122955, 0.062016360461711884, 0.06412570178508759, 0.02593994140625, -0.018643001094460487, -0.09869392961263657, -0.02607543393969536, 0.0040493253618478775, 0.06167314574122429, 0.04181919991970062, -0.035471029579639435, -0.07074786722660065, 7.85179073977928e-34, -0.11482309550046921, 0.02897164598107338, -0.0424809530377388, -0.035194236785173416, -0.02257908694446087, -0.015048899687826633, 0.05024155229330063, 0.025964194908738136, 0.045962538570165634, 0.06770957261323929, -0.08742445707321167, 0.0026429505087435246, -0.00635744770988822, -0.03225889801979065, 0.06214073300361633, -0.05895381048321724, -0.006478413473814726, -0.0019036703743040562, -0.005441964138299227, -0.04337942227721214, 0.05979444086551666, -0.05882692337036133, 0.041484441608190536, -0.006824370939284563, 0.07832322269678116, 0.01748473010957241, -0.0026825678069144487, 0.0431220643222332, 0.04697812348604202, 0.019627660512924194, 0.04660560563206673, 0.02762615866959095, -0.011964951641857624, -0.04793127626180649, 0.09491762518882751, -0.059890277683734894, -0.040940698236227036, 0.05834158882498741, 0.05037306249141693, 0.008381123654544353, -0.07206180691719055, -0.04920336976647377, -0.031171530485153198, -0.0007346085621975362, -0.006604465190321207, -0.10285735875368118, 0.0008524778531864285, 0.07992996275424957, -0.016675284132361412, -0.04556263983249664, -0.03157157823443413, -0.022542357444763184, -0.07902402430772781, -0.014495529234409332, 0.04771963134407997, 0.0005552200018428266, 0.06415024399757385, 0.06947162002325058, 0.035600338131189346, 0.14452534914016724, -0.05066899210214615, -0.06014904007315636, 0.01282245572656393, 0.007154218386858702, 0.11281485110521317, -0.014964534901082516, -0.05966459587216377, 0.026378365233540535, 0.031249547377228737, 0.010260753333568573, 0.007538655772805214, 0.036997873336076736, -0.00926939770579338, -0.06422847509384155, 0.008985036984086037, -0.04371968284249306, 0.007509649731218815, 0.12532095611095428, 0.0005513805663213134, 0.004167755600064993, -0.043408650904893875, 0.050107866525650024, -0.10174993425607681, 0.02310679480433464, -0.025387432426214218, 0.137472465634346, 0.020167898386716843, -0.03454968333244324, -0.02279192954301834, 0.023396188393235207, -0.03826531767845154, 0.07166716456413269, -0.005996575113385916, -0.07099495828151703, -0.030156875029206276, -1.0191492858946038e-33, -0.052588049322366714, 0.09190528094768524, -0.06318134069442749, -0.005988575983792543, -0.04394062981009483, -0.06743280589580536, -0.002654709853231907, 0.046361394226551056, -0.015483563765883446, 0.06773550063371658, -0.016054373234510422, -0.02793016843497753, 0.04780125990509987, -0.02409277856349945, 0.010929820127785206, 0.051841817796230316, -0.0342097207903862, 0.011757314205169678, 0.07071904093027115, 0.0031944415532052517, -0.022058019414544106, 0.06521235406398773, -0.0027794819325208664, -0.05553490296006203, -0.030554890632629395, -0.05531078204512596, -0.07556933164596558, 0.1272396296262741, 0.01906058005988598, -0.06540658324956894, -0.006395956035703421, -0.028005890548229218, -0.01868252269923687, -0.005285210441797972, 0.059191133826971054, -0.02645101398229599, 0.07114361971616745, 0.05288344621658325, 0.02802889607846737, 0.07712072134017944, 0.0766618400812149, 0.010924072004854679, -0.03481179475784302, -0.03695736080408096, -0.008225209079682827, -0.023122316226363182, -0.09950579702854156, -0.0046440367586910725, -0.07672623544931412, -0.040525954216718674, 0.11868414282798767, -0.03543626517057419, 0.009308015927672386, -0.05062614753842354, 0.007250735070556402, 0.07857417315244675, 0.03499230742454529, 0.06528786569833755, 0.11717936396598816, 0.02201550267636776, 0.028187915682792664, -0.1404992938041687, 0.10632245242595673, -0.021935509517788887, 0.054782621562480927, 0.044449638575315475, 0.026168067008256912, 0.07689093053340912, 0.07393210381269455, 0.024531735107302666, -0.039820339530706406, -0.028469139710068703, 0.042009007185697556, 0.032673079520463943, -0.07754699885845184, 0.02982088178396225, -0.06027931720018387, -0.002124856226146221, -0.006203086115419865, 0.03379755839705467, -0.06262338906526566, -0.06055663153529167, -0.09806552529335022, -0.02236151322722435, 0.022820182144641876, 0.05052638426423073, 0.10137475281953812, -0.019192412495613098, 0.011405573226511478, -0.06719387322664261, -0.008490069769322872, 0.13494795560836792, -0.009397532790899277, -0.018225261941552162, -0.050993531942367554, -5.574797867780035e-08, -0.09448231011629105, -0.024121118709445, -0.07400964200496674, 0.012688640505075455, 0.09031638503074646, -0.013454793952405453, 0.004669366404414177, -0.09640169143676758, -0.021648939698934555, -0.11074908822774887, 0.032660119235515594, -0.05042142793536186, -0.009450340643525124, 0.01586705446243286, 0.04372456669807434, -0.04196162894368172, -0.06317013502120972, -0.0893203541636467, 0.031662821769714355, 0.03939277306199074, 0.025286953896284103, 0.02620549686253071, 0.03534293174743652, 0.0541590191423893, 0.09017892181873322, 0.0008818572387099266, 0.0003440705477260053, 0.029557004570961, 0.00556530337780714, -0.00820380449295044, 0.031055988743901253, 0.0068716187961399555, 0.03016652911901474, 3.4078497264999896e-05, 0.06008122116327286, 0.11008497327566147, -0.0264480859041214, -0.013254513964056969, -0.04518852382898331, 0.08288803696632385, 0.014327344484627247, 0.01991523616015911, -0.010646219365298748, 0.0467110313475132, 0.030660875141620636, -0.05891608074307442, 0.1261371672153473, -0.061282698065042496, 0.046308476477861404, 0.005542567931115627, 0.10232677310705185, -0.01877610571682453, -0.01962733455002308, -0.06496306508779526, -0.002305918140336871, -0.08218198269605637, -0.0046744090504944324, -0.16136112809181213, -0.018884601071476936, 0.09650922566652298, -0.08455178141593933, 0.08430807292461395, -0.06745963543653488, 0.03535718470811844]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\NeuralAnalogDiffusionEnhancementLayerandSpatioTemporalGroupinginEarlyVision.pdf,NLP,"Consonant Recognition by Modular Construction of Large Phonemic Time-Delay Neural Networks Abstract Alex Waibel Carnegie-Mellon University Pittsburgh, PA 15213, A TR Interpreting Telephony Research Laboratories Osaka, Japan In this paperl we show that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks. The performance of resulting larger phonetic nets was found to be as good as the performance of the subcomponent nets by themselves. This approach avoids the excessive learning times that would be necessary to train larger networks and allows for incremental learning. Large time-delay neural networks constructed incrementally by applying these modular training techniques achieved a recognition performance of 96.0% for all consonants. 1. Introduction Recently we have demonstrated that connectionist architectures capable of capturing some critical aspects of the dynamic nature of speech, can achieve superior recognition performance for difficult but small phonemic discrimination tasks such as discrimination of the voiced consonants B,D and G [Waibel 89, Waibel 88a]. Encouraged by these results we wanted to explore the question, how we might expand on these models to make them useful for the design of speech recognition systems. A problem that emerges as we attempt to apply neural network models to the full speech recognition problem is the problem of scaling. Simply extending neural networks to ever larger structures and retraining them as one monolithic net quickly exceeds the capabilities of the fastest and largest supercomputers. The search complexity of finding a good solutions in a huge space of possible network configurations also soon assumes unmanageable proportions. Moreover, having to decide on all possible classes for recognition ahead of time as well as collecting sufficient data to train such a large monolithic network is impractical to say the least. In an effort to extend our models from small recognition tasks to large scale speech recognition systems, we must therefore explore modularity and incremental learning as design strategies to break up a large learning task into smaller subtasks. Breaking up a large task into subtasks to be tackled by individual black boxes interconnected in ad hoc arrangements, on the other hand, would mean to abandon one of the most attractive aspects of connectionism: the ability to perform complex constraint satisfaction in a massively parallel and interconnected fashion, in view of an overall optimal perfonnance goal. In this paper we demonstrate based on a set of experiments aimed at phoneme recognition that it is indeed possible to construct large neural networks incrementally by exploiting the hidden structure of smaller pretrained subcomponent 1 An extended version of this paper will also appear in the Proceedings of the 1989 International Conference on Acoustics, Speech and Signal Processing. Copyright: IEEE. Reprinted with pennission. 215 216 Waibel networks. 2. Small Phonemic Classes by Time-Delay Neural Networks In our previous work, we have proposed a Time-Delay Neural Network architecture (as shown on the left of Fig. 1 for B,D,G) as an approach to phoneme discrimination that achieves very high recognition scores [Waibel 89, Waibel 88a]. Its multilayer architecture, its shift-invariance and the time delayed connections of its units all contributed to its performance by allowing the net to develop complex, non-linear decision surfaces and insensitivity to misalignments and by incorporating contextual information into decision making (see [Waibel 89, Waibel 88a] for detailed analysis and discussion). It is trained by the back-propagation procedure [Rurnelhart 86] using shared weights for different time shifted positions of the net [Waibel 89 , Waibel 88a]. In spirit it has similarities to other models recently proposed [Watrous 88, Tank 87]. This network, however, had only been trained for the voiced stops B,D,G and we began our extensions by training similar networks for the other phonemic classes in our database. Intlgrltlon •• • ••••••• nlO •••••••••• t,n • . ••••••• 'N, • ••••••• •• • • • •••••• IJI' .. ....... """" .....•••. - ••••••••• III • • ••••• • • a&t ••••••••• UI --------""~-""--'-~ .. , IS ,,_""'"" 10 mS.c tr.me rate ~ n, · n "" . . , -... I "" ~ . , OutDut Llyt' Figure 1. The TDNN architecture: BOO-net (left), BooPTK-net (right) All phoneme tokens in our experiments were extracted using phonetic handlabels from a large vocabulary database of 5240 common Japanese words. Each word in the database was spoken in isolation by one male native Japanese speaker. All utterances were recorded in a sound proof booth and digitized at a 12 kHz sampling rate. The database was then split into a training set and a testing set of 2620 utterances each. A 150 msec range around a phoneme boundary was excised for each phoneme token and 16 mel scale fllterbank coefficients computed every 10 msec [Waibel 89, Waibel 88a]. The Consonant Recognition by Modular Construction 21 7 preprocessed training and testing data was then used to train or to evaluate our TDNNs' performance for various phoneme classes. For each class, TDNNs with an architecture similar to the BOO-net in Fig.l were trained. A total of seven nets aimed at the major coarse phonetic classes in Japanese were trained, including voiced stops B, D. G, voiceless stops P,T,I(, the nasals M, N and syllabic nasals, fricatives S, SR, R and Z, affricates CR, TS,liquids and glides R, W, Y and fmally the set of vowels A, I, U, E and O. Each of these nets was given between two and five phonemes to distinguish and the pertinent input data was presented for learning. Note, that each net was trained only within each respective coarse class and has no notion of phonemes from other classes yet. Evaluation of each net on test data within each of these subcategories revealed that an average rate of9S.S% can be achieved (see [WaibeISSb] for a more detailed tabulation of results). 3. Scaling TDNNs to Larger Phonemic Classes We have seen that TDNNs achieve superior recognition performance on difficult but small recognition tasks. To train these networlcs substantial computational resources were needed. This raises the question of how our networks could be extended to encompass all phonemes or handle speech recognition in general. To shed light on this question of scaling, we consider first the problem of extending our networks from the task of voiced stop consonant recognition (hence the BOO-task) to the task of distinguishing among all stop consonants (the BOOPTK-task). For a network aimed at the discrimination of the voiced stops (a BOO-net), approximately 6000 connections had to be trained over about SOO training tokens. An identical net (also with approximately 6000 connections2) can achieve discrimination among the voiceless stops (""P"", ""T"" and ""K""). To extend our networks to the recognition of all stops, i.e., the voiced and the unvoiced stops (B,D,G,P,T,K), a larger net is required. We have trained such a network for experimental purposes. To allow for the necessary number of features to develop we have given this net 20 units in the first hidden layer, 6 units in hidden layer 2 and 6 output units. On the right of Fig. 1 we show this net in actual operation with a ""G"" presented at its input. Eventually a high performance network was obtained that achieves 9S.3% correct recognition over a 1613- token BDGPTK-test database, but it took inordinate amounts of learning to arrive at the trained net (IS days on a 4 processor Alliant!). Although going from voiced stops to all stops is only a modest increase in task size, about IS,OOO connections had to be trained. To make matters worse, not only the number of connections should be increased with task size, but in general the amount of training data required for good generalization of a larger net has to be increased as well. Naturally, there are practical limits to the size of a training database, and more training data translates into even more learning time. Learning is further complicated by the increased complexity of the higher dimensional weightspace in large nets as well as the limited preciSion of our simulators. Despite progress towards faster learning algorithms [Haffner 88, Fahlman 88], it is clear that we cannot hope for one single monolithic network to be trained within reasonable time as we 2Note. that these are connettions over which a back-propagation pass is performed during each iteration. Since many of them share the same weights, only a small fraction (about SOO) of them are actually free pararneten. 218 Waibel increase size to handle larger and larger tasks. Moreover, requiring that all classes be considered and samples of each class be presented during training, is undesirable for practical reasons as we contemplate the design of large neural systems. Alternative ways to modularly construct and incrementally train such large neural systems must therefore be explored. 3.1. Experiments with Modularity Four experiments were performed to explore methodologies for constructing phonetic neural nets from smaller component subnets. As a task we used again stop consonant recognition (BooPTK) although other tasks have recently been explored with similar success (BOO and MNsN) [Waibel 88c]. As in the previous section we used a large database of 5240 common Japanese words spoken in isolation from which the testing an training tokens for the voiced stops (the BOO-set) and for the voiceless stops (the PTK- set) was extracted. Two separate TDNNs have been trained. On testing data the BOO-net used here performed 98.3% correct for the BDG-set and the PTK-net achieved 98.7% correct recognition for the PTK-set As a fIrst naive attempt we have now simply run a speech token from either set (i.e., B,D,G,P,T or K) through both a BOO-net and a PTK-net and selected the class with the highest activation from either net as the recognition result. As might have been expected (the component nets had only been trained for their respective classes), poor recognition performance (60.5%) resulted from the 6 class experiment. This is partially due to the inhibitory property of the TDNN that we have observed elsewhere [Waibel 89]. To combine the two networks more effectively, therefore, portions of the net had to be retrained. lOG ... •••• O""""tpul L'yt' ~~Q.I.~. "".t -=::::-::-:: ___ ~ MIdden I..,.., 1 .•..... .... .. ~!!I!·III· •• . .... i •••••••••••••• · ............. . ...... . . . .. . . . · ............. . · ............ . • •••• I ••••••••• •••••• ••••••••• ~I::~:~.::::::: .•.........•• ~: ...........•• ~~ .. . .......... . · ............. . ••••••••••••••• Figure 2. BDGPTK-net trained from hidden units from a Boo- and a PTK-net. We start by assuming that the fIrst hidden layer in either net already contains all the lower Consonant Recognition by Modular Construction 219 level acoustic phonetic features we need for proper identification of the stops and freeze the connections from the input layer (the speech data) to the first hidden layer's 8 units in the BOO-net and the 8 units in the PTK-neL Back-propagation learning is then performed only on the connections between these 16 (= 2 X 8) units in hidden layer 1 and hidden layer 2 and between hidden layer 2 and the combined BooPTK-net's output. This network is shown in Fig.2 with a ""G"" token presented as input. Only the higher layer connections had to be retrained (for about one day) in this case and the resulting network achieved a recognition performance of 98.1 % over the testing data. Combination of the two subnets has therefore yielded a good net although a slight performance degradation compared to the subnets was observed. This degradation could be explained by the increased complexity of the task. but also by the inability of this net to develop lower level acoustic-phonetic features in hidden layer 1. Such features may in fact be needed for discrimination between the two stop classes. in addition to the within- class features. In a third experiment. we therefore flrst train a separate fiNN to perform the voiced/unvoiced (V /UV) distinction between the Boo- and the PTK-task. The network has a very similar structure as the BOO-net. except that only four hidden units were used in hidden layer 1 and two in hidden layer 2 and at the output. This V/UV-net achieved better than 99% voiCed/unvoiced classification on the test data and its hidden units developed in the process are now used as additional features for the BooPTK-task. The connections from the input to the flrst hidden layer of the Boo-. the PTK- and the V/UV nets are frozen and only the connections that combine the 20 units in hidden layer 1 to the higher layers are retrained. Training of the V /UV -net and subsequent combination training took between one and two days. The resulting net was evaluated as before on our testing database and achieved a recognition score of 98.4% correct. i ~, ! ' "" ~t.g'''lan OutDut llyt' ' ... -. ..... ' .. __ .. . Frtt Fr .. ; \ .. ~ ___ ~_~_ MtddtnUl,."" Freel ',....... .:~: : :. : : . • • • , , Figure 3. Combination of a BDG-net and a PTK-net using 4 additional units in hidden layer 1 as free ""Connectionist Glue"". In the previous experiment, good results could be obtained by adding units that we believed to be the useful class distinctive features that were missing in our second experiment. In a fourth experiment. we have now examined an approach that allows for 220 Waibel the network to be free to discover any additional features that might be useful to merge the two component networks. In stead of previously training a class distinctive network. we now add four units to hidden layer 1. whose connections to the input are free to learn any missing discriminatory features to supplement the 16 frozen BOO and PTK features. We call these units the ""connectionist glue"" that we apply to merge two distinct networks into a new combined net. This network is shown in Fig.3. The hidden units of hidden layer 1 from the BOO-net are shown on the left and those from the PTK-net on the right. The connections from the moving input window to these units have been trained individually on Boo- and PTK-data. respectively. and -as before- remain fIxed during combination learning. In the middle on hidden layer 1 we show the 4 free ""Glue"" units. Combination learning now searches for an optimal combination of the existing Boo- and PTK-features and also supplements these by learning additional interclass discriminatory features. Combination retraining with ""glue"" required a two day training run. Performance evaluation of this network over the BDGPTK test database yielded a recognition rate of 98.4%. In addition to the techniques described so far. it may be useful to free all connections in a large modularly constructed network for an additional small amount of fine tuning. This has been done for the BooPTK-net shown in Fig.3 yielding some additional performance improvements. Each iteration of the full network is indeed very slow. but convergence is reached after only few additional tuning iterations. The resulting network fmally achieved (over testing data) a recognition score of 98.6%. 3.2. Steps for the Design of Large Scale Neural Nets Method bdg ptk bdgptk Individual TDNNs 98 .3~ 98.7 % TDNN:Max. ActlvatlOn GO . 5~ Reb-aiD BDGPTK 98.3 ~ Reb-aiD Combined Higher Layers 98.1 % Reb-aiD with VIUV-units 98 .4~ Reb-aiD with Glue 98 . 4~ All-Net Fine Tuning 98.6~ Table 3-1: From BOO to BDGPTK; Modular Scaling Methods. Table 3-1 summarizes the major results from our experiments. In the fIrst row it shows the recognition performance of the two initial TDNNs trained individually to perform the Boo- and the PTK-tasks. respectively. Underneath. we show the results from the various experiments described in the previous section. The results indicate, that larger TDNNs can indeed be trained incrementally. without requiring excessive amounts of training and without loss in performance. The total incremental training time was between one third and one half of a full monolithically trained net and the resulting Consonant Recognition by Modular Construction 221 networks appear to perform slightly better. Even more astonishingly, they appear to achieve performance as high as the subcomponent BDG- and PTK-nets alone. As a strategy for the efficient construction of larger networks we have found the following concepts to be extremely effective: modular,incremental learning, class distinctive learning, connectionist glue, partial and selective learning and all-netfine tuning. 4. Recognition of all Consonants The incremental learning techniques explored so far can now be applied to the design of networks capable of recognizing all consonants. 4.1. Network Architecture Outpu' La.,.., , ' I ' I \ I "" FilM. \ (Frtl) , , , HlGlclen Layer 1 , • Q G , T K II( H,N S S,K Z It VI Y , \ \ , I' .. H' \ (Fre., I \ \ \ Figure 4. Modular Construction of an All Consonant Network Our consonant TDNN (shown in Fig.4.1) was constructed modularly froHi networks aimed at the consonant subcategories, i.e., the BDG-, PTK-, MNsN-, SShHZ-, TsCh- and the RWY -tasks. Each of these nets had been trained before to discriminate between the consonants within each class. Hidden layers 1 and 2 were then extracted from these nets, i.e. their weights copied and frozen in a new combined consonant TDNN. In addition, an interclass discrimination net was trained that distinguishes between the consonant subclasses and thus hopefully provides missing featural information for interclass discrimination much like the V /UV network described in the previous section. The structure of this network was very similar to other subcategory TDNN s, except that we have allowed for 20 units in hidden layer 1 and 6 hidden units (one for each coarse consonant class) in hidden layer 2. The weights leading into hidden layers 1 and 2 were then also copied from this interclass discrimination net into the consonant network and frozen. Three connections were then established to each of the 18 consonant output categories (B,D,G,P,T,K,M,N,sN,S, Sh.H,Z,Ch,Ts,R,W and Y): one to connect an output 222 Waibel unit with the appropriate interclass discrimination unit in hidden layer 2, one with the appropriate intra class discrimination unit from hidden layer 2 of the corresponding subcategory net and one with the always activated threshold unit (not shown in Fig.4.1) The overall network architecture is shown in Fig.4.1 for the case of an incoming test token (e.g., a ""G""). For simplicity, Fig.4.1 shows only the hidden layers from the BDG-,PTK,SShHZ- and the inter-class discrimination nets. At the output, only the two connections leading to the correctly activated ""G"" -output unit are shown. Units and connections pertaining to the other subcategories as well as connections leading to the 17 other output units are omitted for clarity in Fig.4.1. All free weights were initialized with small random weights and then trained. 4.2. Results Consonants Task Recognition Rate (%) bdg 98.6 ptk 98.7 mnN 96.6 sshhz 99.3 chts 100.0 rwy 99.9 cons. class 96.7 All consonant TDNN 95.0 All-Net Fine Tuning 95.9 Table 4-1: Consonant Recognition Performance Results. Table 4.2 summarizes our results for the consonant recognition task. In the first 6 rows the recognition results (measured over the available test data in their respective sublasses) are given. The entry ""cons.class"" shows the performance of the interclass discrimination net in identifying the coarse phonemic subclass of an unknown token. 96.7% of all tokens were correctly categorized into one of the six consonant subclasses. Mter completion of combination learning the entire net was evaluated over 3061 consonant test tokens, and achieved a 95.0% recognition accuracy. All-net fme tuning was then performed by freeing up all connections in the network to allow for small additional adjustments in the interest of better overall performance. Mter completion of all-net fine tuning, the performance of the network then improved to 96.0% correct. To put these recognition results into perspective, we have compared these results with several other competing recognition techniques and found that our incrementally trained net compares favorably [Waibel 88b). Consonant Recognition by Modular Construction 223 5. Conclusion The serious problems associated with scaling smaller phonemic subcomponent networks to larger phonemic tasks are overcome by careful modular design. Modular design is achieved by several important strategies: selective and incremental learning of subcomponent tasks, exploitation of previously learned hidden structure, the application of connectionist glue or class distinctive features to allow for separate networks to ""grow"" together, partial training of portions of a larger net and finally, all-net fine tuning for making small additional adjustments in a large net Our findings suggest, that judicious application of a number of connectionist design techniques could lead to the successful design of high performance large scale connectionist speech recognition systems. References [Fahlman 88] Fahl man , S.E. An Empirical Study of Learning Speed in Back- Propagation Networks. Technical Report CMU-CS-88-162, Carnegie-Mellon University, June, 1988. [Haffner 88] Haffner, P., Waibel, A. and Shikano, K. Fast Back-Propagation Learning Methods for Neural Networks in Speech. In Proceedings of the Fall Meeting of the Acoustical Society of Japan. October, 1988. [Rumelhart 86] Rumelhart, D.E., Hinton, G.E. and Williams, R.J. Learning Internal Representations by Error Propagation. In McClelland, J L. and Rumelhart, D.E. (editor), Parallel Distributed Processing; Explorations in the Microstructure of Cognition, chapter 8, pages 318-362. MIT Press, Cambridge, MA, 1986. [Tank 87] Tank, D.W. and Hopfield, JJ. Neural Computation by Concentrating Information in Time. In Proceedings National Academy of Sciences, pages 1896-1900. April, 1987. [WaibeI88a] Waibel, A., Hanazawa, T., Hinton, G., Shikano, K. and Lang K. Phoneme Recognition: Neural Networks vs. Hidden Markov Models. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 8.S3.3. April, 1988. [Waibel 88b] Waibel, A., Sawai, H. and Shikano, K. Modularity and Scaling in Large Phonemic Neural Networks. Technical Report TR-I-0034, ATR Interpreting Telephony Research Laboratories, July, 1988. [Waibel 8Se] Waibel, A. Connectionist Glue: Modular Design of Neural Speech Systems. In Touretzky, D.S., Hinton, G.E. and Sejnowski, T J. (editors), Proceedings of the 1988 Connectionist Models Summer School. Morgan Kaufmann, 1988. [Waibel 89] Waibel, A., Hanazawa, T., Hinton, G., Shikano, K. and Lang K. Phoneme Recognition Using Time-Delay Neural Networks. IEEE, Transactions on Acoustics, Speech and Signal Processing, March, 1989. [Watrous 88] Watrous, R. Speech Recognition Using Connectionist Networks. PhD thesis, University of Pennsylvania, October, 1988.","[-0.1139325350522995, -0.09996306151151657, 0.03177526965737343, -0.05430414527654648, -0.082527756690979, 0.05992283672094345, 0.01683046482503414, -0.0835658460855484, 0.03358197957277298, -0.11482188105583191, 0.02265891246497631, -0.05262110009789467, -0.045582517981529236, 0.009244146756827831, -0.060051701962947845, -0.0414617657661438, 0.01720263995230198, 0.12701882421970367, -0.08157460391521454, -0.06554967910051346, 0.043525684624910355, 0.11119389533996582, 0.014358663000166416, 0.012806343846023083, 0.055534787476062775, 0.021361634135246277, -0.057357847690582275, 0.0003953193954657763, 0.08433511108160019, 0.03137555718421936, 0.055148277431726456, 0.0006597389583475888, 0.08535599708557129, 0.0313805490732193, -0.10296055674552917, 0.0014099465915933251, -0.016697941347956657, 0.025560587644577026, 0.004039553459733725, -0.025069206953048706, 0.012069940567016602, 0.02319703809916973, 0.012071961537003517, 0.00959097221493721, -0.011221547611057758, -0.06761142611503601, -0.008895212784409523, 0.007422294933348894, -0.02843993529677391, -0.014124943874776363, -0.0006284005357883871, 0.0031407447531819344, -0.034523140639066696, 0.0732947513461113, -0.0643034502863884, 0.03002006933093071, 0.005160980392247438, 0.10114846378564835, -0.11273767054080963, 0.05519871041178703, -0.05748838931322098, -0.0634990707039833, 0.00013225997099652886, -0.04511721804738045, -0.018115036189556122, 0.05640324950218201, -0.024777842685580254, 0.0009867169428616762, 0.06371369957923889, -0.00419396348297596, 0.026297718286514282, 0.08644089102745056, -0.05519970506429672, 0.04548809677362442, -0.03120712749660015, 0.027125433087348938, 0.09416543692350388, 0.008875695988535881, 0.038037125021219254, -0.031229054555296898, 0.05272248759865761, -0.04312294349074364, 0.08099067956209183, -0.009326769039034843, 0.06230468675494194, 0.008146936073899269, -0.06470031291246414, 0.0237728301435709, -0.11636598408222198, -0.06469007581472397, 0.024865729734301567, -0.060502562671899796, 0.02979232184588909, -0.0627196878194809, 0.03663690388202667, 0.0878821462392807, -0.00690123625099659, -0.01838015392422676, 0.04932669177651405, 0.06883600354194641, -0.062046319246292114, -0.008042090572416782, -0.025596456602215767, -0.023730454966425896, -0.023181309923529625, 0.010599442757666111, 0.022877944633364677, -0.0005877449875697494, 0.03429826349020004, -0.05595137178897858, -0.006005409173667431, -0.04242924600839615, 0.025786828249692917, -0.020638687536120415, 0.0172953549772501, -0.02034989558160305, -0.03196825459599495, -0.0059061553329229355, 0.07926512509584427, 0.0780777707695961, -0.04343957453966141, 0.02938747964799404, -0.08155135065317154, -0.00753994332626462, 0.055660080164670944, 0.012960686348378658, -0.08611983805894852, 2.8438898296626763e-33, 0.04011242091655731, 0.0060581485740840435, 0.006887826137244701, 0.010943695902824402, 0.01780068129301071, -0.0500449612736702, -0.017809057608246803, -0.013930145651102066, -0.002281069988384843, 0.0027221243362873793, -0.05230538174510002, -0.01164161879569292, -0.09430709481239319, 0.0002518553228583187, 0.07810597121715546, -0.01065976731479168, 0.009679489769041538, -0.011462699621915817, 0.02345798909664154, -0.10751300305128098, 0.06394331902265549, -0.04201248660683632, 0.09546040743589401, 0.016312701627612114, 0.07402107864618301, -0.025113243609666824, 0.0703563392162323, -0.07274843752384186, -0.0062652407214045525, 0.015432460233569145, -0.07377751916646957, 0.014695359393954277, -0.04935907572507858, -0.019240792840719223, 0.05011473968625069, -0.07416298240423203, 0.08124151825904846, 0.009214074350893497, 0.01663520187139511, -0.05290013924241066, -0.026689695194363594, -0.005866153631359339, -0.03146073967218399, 0.05096869543194771, -0.06878692656755447, -0.03520764410495758, 0.013106709346175194, 0.08270221203565598, -0.0004363081534393132, 0.02028837241232395, 0.038972679525613785, 0.05248778313398361, -0.08074399828910828, -0.015767861157655716, 0.052494291216135025, -0.036153629422187805, 0.029378017410635948, 0.024165309965610504, -0.035352401435375214, 0.04355704039335251, 0.015995249152183533, 0.03211119771003723, 0.008835393004119396, 0.12213356047868729, 0.005142322741448879, 0.019099824130535126, -0.09467492997646332, 0.011787796393036842, 0.08495098352432251, -0.03629085794091225, 0.0019614214543253183, -0.016843752935528755, 0.0508836954832077, -0.005887114442884922, 0.014141127467155457, 0.03297113999724388, 0.03068569116294384, -0.05801182985305786, -0.0448048859834671, 0.02494138292968273, -0.0009648914565332234, 0.007654872722923756, -0.05677903816103935, -0.01038532704114914, 0.03923674300312996, -0.04745151102542877, 0.05532831698656082, -0.060135368257761, 0.01880112662911415, -0.010197535157203674, -0.16629056632518768, 0.00667257234454155, 0.03386041149497032, 0.0004600861284416169, -0.03505396470427513, -3.5321125683813474e-33, -0.047619353979825974, 0.05886414274573326, -0.03421930596232414, 0.0020960289984941483, -0.0015778589295223355, 0.013425410725176334, -0.02665197290480137, 0.037965886294841766, -0.0630219578742981, 0.013639677315950394, -0.009097132831811905, 0.01904786005616188, 0.08011443167924881, -0.05197355896234512, -0.015674538910388947, 0.005377210211008787, 0.02982049621641636, 0.02525250054895878, 0.09923625737428665, 0.01808425411581993, 0.03004330024123192, 0.04634356498718262, -0.13246364891529083, 0.03401198238134384, -0.04829316586256027, -0.019270630553364754, -0.061406977474689484, 0.02512039802968502, 0.030807528644800186, 0.057197749614715576, -0.11236906051635742, -0.02287106215953827, -0.04087473079562187, 0.0064440155401825905, -0.006380236707627773, 0.08053591847419739, 0.030281277373433113, -0.025443844497203827, 0.05735422298312187, -0.006270076148211956, 0.0892270877957344, -0.08014770597219467, -0.006650926545262337, -0.06820584833621979, 0.03702440485358238, -0.10288561880588531, -0.05882396176457405, 0.03352520614862442, -0.07041113078594208, -0.0034922941122204065, 0.09768154472112656, -0.011602434329688549, 0.02791009470820427, 0.028084514662623405, -0.004499971400946379, 0.0019007163355126977, -0.0028271761257201433, -0.07199544459581375, 0.06038317084312439, 0.032402023673057556, -0.017224915325641632, -0.118022121489048, 0.04476743936538696, -0.0502135306596756, 0.005986184813082218, 0.038869474083185196, 0.024924717843532562, 0.007969220168888569, 0.1155075803399086, -0.02513565868139267, 0.04244183003902435, 0.021661756560206413, 0.03838331997394562, 0.050354935228824615, -0.023133816197514534, -0.056685350835323334, -0.0956125482916832, 0.018647022545337677, -0.015000143088400364, 0.01489158347249031, -0.04013334587216377, 0.008447523228824139, -0.015053973533213139, -0.01001034677028656, 0.08295571058988571, 0.0680525079369545, 0.03953941911458969, 0.03864534571766853, 0.09659125655889511, 0.047017671167850494, 0.02816399373114109, 0.10536408424377441, 0.06843911856412888, 0.040636613965034485, -0.03133019804954529, -4.1865522604211947e-08, 0.01995515637099743, 0.0026504292618483305, 0.00847434252500534, -0.03497070074081421, 0.06127562373876572, -0.14180125296115875, 0.012891455553472042, -0.04565165191888809, -0.006681516766548157, -0.015649598091840744, 0.11021789163351059, -0.017971040681004524, -0.004534238949418068, 0.01240906585007906, 0.02698604017496109, 0.01657375693321228, 0.014884802512824535, 0.007086002733558416, -0.01095573604106903, -0.05498974025249481, 0.08942198008298874, 0.058728158473968506, 0.03148890659213066, 0.03072289191186428, -0.05010222643613815, -0.10159919410943985, -0.07381997257471085, 0.007138119079172611, -0.02999500371515751, 0.020037245005369186, -0.08254610747098923, 0.0676841139793396, -0.0706990584731102, -0.016247093677520752, 0.03113183192908764, 0.008559859357774258, -0.028706619516015053, -0.05118550732731819, -0.058031003922224045, 0.03807218000292778, 0.02786763571202755, -0.044000931084156036, -0.0752541571855545, 0.033668261021375656, 0.06850215047597885, -0.11273571103811264, -0.011541422456502914, -0.07637084275484085, -0.03787614777684212, 0.04490451514720917, 0.009434892795979977, 0.0759812593460083, 0.00549662159755826, -0.009609295055270195, 0.026920443400740623, 0.02580888383090496, 0.0460665263235569, -0.06497424840927124, -0.06510503590106964, 0.051932625472545624, -0.04826079681515694, 0.05045405030250549, -0.062068648636341095, -0.036324091255664825]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\NeuralApproachforTVImageCompressionUsingaHopfieldTypeNetwork.pdf,Optimization,"OPTIMIZATION BY MEAN FIELD ANNEALING Griff Bilbro Reinhold Mann Thomas K. Miller ECE Dept. Eng. Physics and Math. Div. ECE Dept. NCSU Oak Ridge N atl. Lab. NCSU Raleigh, NC 27695 Oak Ridge, TN 37831 Raleigh, N C 27695 Wesley. E. Snyder David E. Van den Bout Mark White ECE Dept. ECE Dept. ECE Dept. NCSU NCSU NCSU Raleigh, NC 27695 Raleigh, NC 27695 Raleigh, NC 27695 ABSTRACT Nearly optimal solutions to many combinatorial problems can be found using stochastic simulated annealing. This paper extends the concept of simulated annealing from its original formulation as a Markov process to a new formulation based on mean field theory. Mean field annealing essentially replaces the discrete de- grees of freedom in simulated annealing with their average values as computed by the mean field approximation. The net result is that equilibrium at a given temperature is achieved 1-2 orders of magnitude faster than with simulated annealing. A general frame- work for the mean field annealing algorithm is derived, and its re- lationship to Hopfield networks is shown. The behavior of MFA is examined both analytically and experimentally for a generic combi- natorial optimization problem: graph bipartitioning. This analysis indicates the presence of critical temperatures which could be im- portant in improving the performance of neural networks. STOCHASTIC VERSUS MEAN FIELD In combinatorial optimization problems, an objective function or Hamiltonian, H(s), is presented which depends on a vector of interacting 3pim, S = {81,"" .,8N}, in some complex nonlinear way. Stochastic simulated annealing (SSA) (S. Kirk- patrick, C. Gelatt, and M. Vecchi (1983)) finds a global minimum of H by com- bining gradient descent with a random process. This combination allows, under certain conditions, choices of s which actually increa3e H, thus providing SSA with a mechanism for escaping from local minima. The frequency and severity of these uphill moves is reduced by slowly decreasing a parameter T (often referred to as the temperature) such that the system settles into a global optimum. Two conceptual operationo; are involved in simulated annealing: a thermodatic op- eration which schedules decreases in the temperature, and a relazation operation 91 92 Bilbro, et al which iteratively finds the equilibrium solution at the new temperature using the final state of the system at the previous temperature as a starting point. In SSA, re- laxation occurs by randomly altering components of s with a probability determined by both T and the change in H caused by each such operation. This corresponds to probabilistic transitions in a Markov chain. In mean field annealing (MFA), some aspects of the optimization problem are replaced with their means or averages from the underlying Markov chain (e.g. s is replaced with its average, (s)). As the tem- perature is decreased, the MFA algorithm updates these averages based on their values at the previous temperature. Because computation using the means attains equilibrium faster than using the corresponding Markov chain, MFA relaxes to a solution at each temperature much faster than does SSA, which leads to an overall decrease in computational effort. In this paper, we present the MFA formulation in the context of the familiar Ising Hamiltonian and discuss its relationship to Hopfield neural networks. Then the application of MFA to the problem of graph bipartitioning is discussed, where we have analytically and experimentally investigated the affect of temperature on the behavior of MFA and observed speedups of 50:1 over SSA. MFA AND HOPFIELD NETWORKS Optimization theory, like physics, often concerns itself with systems possessing a large number ofinteracting degrees offreedom. Physicists often simplify their prob- lems by using the mean field approzimation: a simple analytic approximation of the behavior of systems of particles or spins in thermal equilibrium. In a correspond- ing manner, arbitrary functions can be optimized by using an analytic version of stochastic simulated annealing based on a technique analogous to the mean field approximation. The derivation of MFA presented here uses the naive mean field (D. J. Thouless, P.W. Anderson, and R.G. Palmer (1977)) and starts with a simple Ising Hamiltonian of N spins coupled by a product interaction: L L """"' { Vi· = V.·i symmetry H(s) = ~Si + L..J'Vi;sis; where 'E {O '1} . t . s, , an eger spans. , i ;i:' Factoring H(s) shows the interaction between a spin s, and the rest of the system: H(s) = Si . (~ + 2 L Vi;S;) + L h""s"" + L L V"";s,,s; . (1) ;~, ""i:' ""i:i ;i:"".' The mean or effective field affecting s, is the average of its coefficient in (1): w, = (h, + 2 E;i:i Vi;s;) = ~ + 2 L Vi; (s;) = HI(Si)=l - HI(s,}=o' (2) ; I-i The last part of (2) shows that I for the Ising case, the mean field can be simply calculated from the difference in the Hamiltonian caused by changing (s,) from zero Optimization by Mean Field Annealing 93 1. Initialize spin averages and add noise: 8i = 1/2 + 6 Vi. 2. Perform this relaxation step until a fixed-point is found: a. Select a spin average (8,) at random from (s). h. Compute the mean field ~i = 14 + 2 E;;ti 'Vi; (8;). c. Compute the new spin average (8i) = {I + exp (~dT)} -1. 3. Decrease T and repeat step 2 until freezing occurs. Figure 1. The Mean Field Annealing Algorithm to one while holding the other spin averages constant. By taking the Boltzmann- weighted average of the state values, the spin average is found to be (3) Equilibrium is established at a given temperature when equations (2) and (3) hold for each spin. The MFA algorithm (Figure 1) begins at a high temperature where this fized-point is easy to determine. The fixed-point is tracked as T is lowered by iterating a relaxation step which uses the spin averages to calculate a new mean field that is then used to update the spin averages. As the temperature is lowered, the optimum solution is found as the limit of this sequence of fixed-points. The relationship of Hopfield neural networks to MFA becomes apparent if the re- laxation step in Figure 1 is recast in a parallel form in which the entire mean field vector partially moves towards its new state, and then all the spin averages are updated using ~nc1ll. As'Y -t 0, these difference equations become non-linear differential equations, dip, = h. + 2 ~ 11:'{8') - ip. 'Vi · dt ' L.,;'"" , I ; which are equivaleut to the equations of motion for the Hopfield network (J. J. Hopfield and D. W. Tank (1985», Vi, 94 Bilbro, et al provided we make Gi = Pi = 1 and use a sigmoidal transfer function 1 f( ui) = -1 -+-ex-p""""""( u-i /-:-T-""') • Thus, the evolution of a solution in a Hopfield network is a special case of the relaxation toward an equilibrium state effected by the MFA algorithm at a fixed temperature. THE GRAPH BIPARTITIONING PROBLEM Formally, a graph consists of a set of N nodes such that nodes 11i and n; are con- nected by an edge with weight ~; (which could be zero). The graph bipartitioning problem involves equally distributing the graph nodes across two bins, bo and bl , while minimizing the combined weight of the edges with endpoints in opposite bins. These two sub-objectives tend to frustrate one another in that the first goal is sat- isfied when the nodes are equally divided between the bins, but the second goal is met (trivially) by assigning all the nodes to a single bin. MEAN FIELD FORMULATION An optimal solution for the bipartitioning problem minimizes the Hamiltonian In the first term, each edge attracts adjacent nodes into the same bin with a force proportional to its weight. Counter balancing this attraction is .,., an amorphous repulsive force between all of the nodes which discourages them from clustering together. The average spin of a node 11i can be determined from its mean field: C)i = L)Vi; -.,.) ;¢i EXPERIl\1ENTAL RESULTS L 2(Vi; - ""')(8;} . ;¢i Table 1 compares the performance of the MFA algorithm of Figure 1 with SSA in terms of total optimization and computational effort for 100 trials on each of three example graphs. While the bipartitions found by SSA and MFA are nearly equivalent, MFA required as little as 2% of the number of iterations needed by SSA. The effect of the decrease in temperature upon the spin averages is depicted in Figure 2. At high tempera.tures the graph bipartition is maximally disordered, (i.e. (8i) R: i Vi), but as the system is cooled past a critical temperature, Te , each node Optimization by Mean Field Annealing 95 TABLE 1. Comparison of SSA and MFA on Graph Bipartitioning G1 G1 G1 Nodes/Edges 83/115 100/200 100/400 Solution Value (HMFA/ HSSA 0.762 1.078 1.030 RelaxatIOn Iterations (1M F A/ Iss A 0.187 0.063 0.019 1.0 0.8 0.6 <Si> 0.4 0.2 0.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 IOB#!I(T) Figure 2. The Effect of Decreasing Temperature on Spin Averages begins to move predominantly into one or the other of the two bins (as evidenced by the drift of the spin averages towards 1 or 0). The changes in the spin averages cause H to decrease rapidly in the vicinty of Te. To analyze the effect of temperature on the spin averages, the behavior of a cluster C of spins is idealized with the assumptions: 1. The repulsive force which balances the bin contents is negligible within C (7' = 0) compared to the attractive forces arising from the graph edgesj 2. The attractive force exerted by each edge is replaced with an average attractive force V = E, E; Vi; / E where E is the number of non-zero weighted edgesj 3. On average, each graph node is adjacent to e = 2E/N neighboring nodesj ( 4. The movement of the nodes in a cluster can be uniformly described by some deviation, u, such that ("") = (1 + u)/2. 96 Bilbro, et al Using this model, a cluster moves according to (4) The solution to (4) is a fixed point with (1' = 0 when T is high. This fixed point becomes unstable and the spins diverge from 1/2 when the temperature is lowered to the point where Solving shows that Tc = Ve/2, which agrees with our experiments and is within ±20% of those observed in (C. Peterson and J. R. Anderson (1987)). The point at which the nodes freeze into their respective bins can be found using (4) and assuming a worst-case situation in which a node is attracted by a single edge (i.e. e = 1). In this case, the spin deviation will cross an arbitrary threshold, (1', (usually set ±0.9), when V(1' Tf = . In(1 + (1't) - In(1 - (1't) A cooling scpedule is now needed which prescribes how many relaxation iterations, la, are required at each temperature to reach equilibrium as the system is annealed from Tc to Tf. Further analysis of (4) shows that Ia ex ITc/(Tc - T)I. Thus, more iterations are required to reach equilibrium around Tc than anywhere else, which agrees with observations made during our experiments. ,The affect of using fewer iterations at various temperatures was empirically studied using the following procedure: 1. Each spin average was initialized to 1/2 and a small amount of noise was added to break the symmetry of the problem. 2. An initial temperature Ti was imposed, and the mean field equations were iterated I times for each node. 3. After completing the iterations at 11, the temperature was quenched to near zero and the mean field equations were again iterated I times to saturate each node at one or zero. The results of applying this procedure to one of our example graphs with different values ofT, and I are shown in Figure 3. Selecting an initial temperature near Tc and performing sufficient iterations of the mean field equations (I ~ 40 in this case) gives final bipartitions that are usually near-opt'imum, while performing an insufficient number of iterations (I = 5 or I = 20) leads to poor solutions. However, even a large number of iterations will not compensate if T, is set so low that the initial convergence causes the graph to abruptly freeze into a local minimum. The highest Optimization by Mean Field Annealing 97 6.0 5.0 4.0 3.0 t POOR SOLUTIONS / ~ "" 2.0 1.0 - GOOD SOLUTIONS ~ -1.0 -0.5 0.0 0.5 IOHe(Ti) Figure 3. The Effect of Initial Temperature and Iterations on the Solution quality solutions are found when T, ~ Tc and a sufficient number of relaxations are performed, as shown in the traces for I = 40 and I = 90. This seems to perform as well as slow cooling and requires much less effort. Obviously, much of the structure of the optimal solution must be present after equilibrating at Te. Due to the equivalence we have shown between Hopfleld networks and MFA, this fact may be useful in tuning the gains in Hopfield networks to get better performance. CONCLUSIONS The concept of mean field annealing (MFA) has been introduced and compared to stochastic simulated annealing (SSA) which it closely resembles in both derivation and implementation. In the graph bipartitioning application, we saw the level of optimization achieved by MFA was comparable to that achieved by SSA, but 1-2 orders of magnitude fewer relaxation iterations were required. This speedup is achieved because the average values of the discrete degrees of freedom used by MFA relax to their equilibrium values much faster than the corresponding Markov chain employed in SSA. We have seen similar results when applying MFA to a other problems including N-way graph partitioning (D. E. Van den Bout and T. K. Miller III (1988)), restoration of range and luminance images (Griff Bilbro and Wesley Snyder (1988)), and image half toning (T. K. Miller III and D. E. Van den Bout (1989)). As was shown, the MFA algorithm can be formulated as a parallel iterative procedure, so it should also perform well in parallel processing environments. This has been verified by successfully porting MFA to a ZIP array processor, a 64-node 98 Bilbro, et al NCUBE hypercube computer, and a 10-processor Sequent Balance shared-memory multiprocessor with near-linear speedups in each case. In addition to the speed advantages of MFA, the fact that the system state is represented by continuous variables allows the use of simple analytic techniques to characterize the system dynamics. The dynamics of the MFA algorithm were examined for the problem of graph bipartitioning, revealing the existence of a critical temperature, Te , at which optimization begins to occur. It was also experimentally determined that MFA found better solutions when annealing began near Tc rather than at some lower temperature. Due to the correspondence shown between MFA and Hopfield networks, the critical temperature may be of use in setting the neural gains so that better solutions are found. Acknowledgements This work was partially supported by the North Carolina State University Center for Communications and Signal Processing and Computer Systems 'Laboratory, and by the Office of Basic Energy Sciences, and the Office of Technology Support Programs, U.S. Department of Energy, under contract No. DE-AC05-840R21400 with Martin Marietta Energy Systems, Inc. References Griff Bilbro and Wesley Snyder (1988) Image restoration by mean field annealing. In Advance, in Neural Network Information Proce""ing SYBteffll. D. E. Van den Bout and T. K. Miller III (1988) Graph partitioning using an- nealed neural networks. Submitted to IEEE Tran,. on Circuiu and SYBtem,. J. J. Hopfield and D. W. Tank (1985) Neural computation of decision in optimiza- tion problems. Biological Cybernetic"" 52, 141-152. T. K. Miller III and D. E. Van den Bout (1989) Image halftoning by mean field annealing. Submitted to ICNN'89. S. Kirkpatrick, C. Gelatt, and M. Vecchi (1983) Optimization by simulated an- nealing. Science, 220(4598),671-680. C. Peterson and J. R. Anderson (1987) Neural Network, and NP-complete Opti- mization Problem,: a Performance Study on the Graph Bi,ection Problem. Tech- nical Report MCC-EI-287-87, MCC. D. J. Thouless, P.'¥l. Anderson, and R.G. Palmer (1977) Solution of 'solvable model of a spin glass'. Phil. Mag., 35(3), 593-601.","[0.022961754351854324, 0.028543809428811073, 0.01642109453678131, -0.0525742806494236, -0.03951022028923035, -0.0947127640247345, 0.015277623198926449, 0.005366795230656862, 0.022260798141360283, 0.02407332882285118, -0.07900627702474594, 0.010300779715180397, 0.052118994295597076, -0.05187837779521942, -0.034456148743629456, 0.024905895814299583, -0.014927443116903305, 0.02639014646410942, -0.013525711372494698, -0.04864272102713585, -0.0021705159451812506, 0.011855226941406727, -0.03494274988770485, -0.032089781016111374, -0.04900812730193138, 0.0007708012126386166, -0.02008313685655594, 0.03233246132731438, -0.0674925148487091, 0.02384534664452076, -0.03955167159438133, 0.05857324227690697, 0.07046229392290115, -0.009144269861280918, 0.03724989295005798, 0.026625247672200203, -0.12011276930570602, -0.00784168392419815, -0.062141090631484985, 0.07106151431798935, -0.07868912816047668, 0.02082759141921997, 0.05741816759109497, -0.001952808233909309, -0.014837090857326984, -0.009676272980868816, 0.042859725654125214, -0.036639776080846786, -0.034933902323246, -0.07234758138656616, -0.010707849636673927, 0.07889312505722046, -0.05475318059325218, 0.02234606072306633, 0.03856426477432251, 0.03423671796917915, -0.0007273883675225079, -0.048024799674749374, -0.013234496116638184, 0.0590963251888752, 0.04393652454018593, -0.07666568458080292, 0.07773009687662125, -0.04316629841923714, 0.06019885092973709, 0.025727856904268265, 0.0020076772198081017, 0.0034475168213248253, -0.0003801927377935499, -0.07938790321350098, 0.05040697008371353, 0.024547742679715157, -0.07895432412624359, -0.07140176743268967, 0.011569413356482983, 0.077682726085186, 0.09318037331104279, 0.0716024711728096, 0.030854538083076477, -0.0022318544797599316, -0.07967846095561981, 0.020070185884833336, 0.028347620740532875, -0.09108457714319229, -0.03427105024456978, -0.03120889514684677, -0.00804199744015932, 0.018000204116106033, 0.14054948091506958, -0.01740339957177639, 0.030214691534638405, -0.026904132217168808, -0.10220634192228317, -0.011528746224939823, 0.019081035628914833, 0.03921269252896309, 0.018482139334082603, -0.03635828197002411, 0.05260518193244934, -0.01400882937014103, 0.03782522305846214, -0.018521424382925034, 0.09972138702869415, -0.019874686375260353, 0.013307875953614712, -0.06392794102430344, 0.09338787943124771, 0.03316401690244675, -0.0671166256070137, -0.07442884892225266, -0.025932220742106438, 0.06295813620090485, 0.05101136490702629, 0.02833823300898075, -0.061085402965545654, 0.039502888917922974, 0.06091976910829544, -0.0002378615754423663, 0.015394366346299648, 0.03206532076001167, -0.019730573520064354, 0.02241055853664875, 0.002306499984115362, 0.027433574199676514, -0.002264939248561859, 0.040505990386009216, 0.018069962039589882, 6.978306050609186e-33, -0.0655733048915863, 0.05391404777765274, 0.08877898752689362, -0.04356660693883896, 0.1081424131989479, -0.020271986722946167, 0.0466068796813488, -0.02394849620759487, 0.02680680714547634, -0.01896554045379162, -0.01628836803138256, -0.0115662831813097, 0.01979980804026127, -0.07282531261444092, 0.03170585632324219, -0.0839567631483078, 0.011258113197982311, 0.02780941128730774, 0.0019389475928619504, -0.047764722257852554, 0.07400570809841156, -0.03273114189505577, 0.036498118191957474, 0.033950284123420715, -0.020426839590072632, -0.0071907746605575085, 0.02548903413116932, -0.06315502524375916, -0.03739730641245842, -0.01111444178968668, 0.012120317667722702, -0.03651341050863266, -0.014820687472820282, 0.0356234610080719, 0.01888106018304825, -0.011783591471612453, 0.021197356283664703, 0.020488981157541275, -0.008088848553597927, -0.061060935258865356, 0.04449891299009323, 0.0037493621930480003, 0.1288238912820816, 0.009180165827274323, -0.047538965940475464, -0.039993539452552795, 0.07195385545492172, 0.06146889179944992, 0.019119005650281906, 0.0072087314911186695, -0.00569341704249382, -0.015515957027673721, 0.06926557421684265, 0.03707662224769592, 0.08165683597326279, 0.07201843708753586, 0.030602719634771347, -0.007550934795290232, 0.011393521912395954, 0.06754781305789948, -0.005714991595596075, -0.016991114243865013, 0.02531435526907444, -0.042761534452438354, 0.10381925106048584, 0.07064828276634216, -0.014772174879908562, 0.04710351675748825, 0.09047748148441315, -0.040800537914037704, 0.04564287140965462, -0.013956730253994465, 0.004826676566153765, -0.06290927529335022, -0.0007375548011623323, 0.005747469142079353, 0.041336096823215485, -0.030283521860837936, -0.008731158450245857, -0.03127041459083557, -0.05288210138678551, 0.13872690498828888, -0.12611252069473267, -0.09883397072553635, 0.0026258232537657022, -0.07632361352443695, 0.005936876405030489, -0.0012398144463077188, -0.04057454690337181, -0.08014705032110214, 0.026553496718406677, 0.01961890421807766, 0.024143533781170845, 0.050837963819503784, -0.002749594859778881, -6.42116360530567e-33, -0.07471191883087158, -0.09101175516843796, 0.056331176310777664, 0.01314790640026331, 0.056202154606580734, -0.013129306957125664, -0.004593030549585819, -0.06327419728040695, 0.0957493931055069, -0.07709312438964844, -0.04782726615667343, -0.007377427536994219, -0.041736435145139694, -0.029793059453368187, 0.03535934165120125, 0.023465923964977264, 0.009469587355852127, -0.011679451912641525, 0.08914697170257568, 0.018789634108543396, -0.009176596067845821, 0.07295342534780502, -0.08136646449565887, -0.010745353996753693, -0.05065600574016571, -0.010317512787878513, 0.010223248973488808, 0.04876274988055229, -0.005554412491619587, 0.027899786829948425, 0.02644805982708931, 0.019873157143592834, -0.12778829038143158, 0.1542944461107254, -0.010562398470938206, -0.0008220613817684352, 0.06776617467403412, 0.12741167843341827, 0.044808924198150635, -0.036390382796525955, -0.04375392943620682, -0.004494776949286461, -0.09043291956186295, -0.03050716407597065, 0.0797884613275528, 0.0019623131956905127, 0.017009355127811432, -0.022717755287885666, 0.036066003143787384, 0.012143840081989765, -0.05265888571739197, -0.04130139946937561, -0.05730615556240082, 0.0011735283769667149, -0.05148670822381973, 0.034853748977184296, -0.07897669076919556, 0.04660499468445778, -0.00474444217979908, -0.04719596356153488, -0.007561336271464825, -0.06270412355661392, -0.041756514459848404, 0.012606573291122913, -0.06922980397939682, -0.054797206073999405, -0.08643317967653275, -0.03214742988348007, -0.03410336747765541, -0.04642413929104805, -0.07523279637098312, 0.03123958222568035, 0.032035812735557556, -0.015497339889407158, -0.0023741230834275484, -0.0098665626719594, 0.07739239931106567, 0.031874995678663254, 0.05495424568653107, -0.05089429020881653, -0.06946715712547302, 0.0440911129117012, -0.02238399162888527, 0.007050773128867149, 0.014084186404943466, -0.04010016843676567, 0.07284733653068542, -0.043126437813043594, 0.01745932549238205, -0.0983738824725151, -0.007251909002661705, -0.04497836157679558, 0.07659073919057846, 0.026386547833681107, -0.04370869696140289, -5.165860628153496e-08, 0.004230963997542858, -0.009828096255660057, 0.03858397528529167, -0.050133757293224335, 0.17754144966602325, 0.0341605618596077, -0.03295838460326195, 0.028361855074763298, 0.0032024981919676065, -0.12643945217132568, 0.045024897903203964, -0.012729874812066555, -0.004742561839520931, 0.03766017407178879, -0.0481785349547863, 0.06646214425563812, -0.054415564984083176, -0.08649783581495285, 0.03572876751422882, 0.005444811191409826, -0.01586139015853405, -0.036020565778017044, 0.03421047329902649, 0.05257093533873558, 0.02297195792198181, -0.051558010280132294, 0.017679488286376, -0.09612054377794266, 0.07645715773105621, 0.028091825544834137, -0.03666327893733978, -0.0332803912460804, 0.13597267866134644, 0.05492743104696274, 0.06591495126485825, -0.02086719684302807, -0.011087585240602493, 0.029076701030135155, -0.0648137629032135, -0.04102388024330139, 0.010128211230039597, 0.03380599245429039, 0.03233567252755165, -0.09518106281757355, 0.08416987955570221, -0.016196278855204582, -0.011762132868170738, 0.047223128378391266, 0.07369012385606766, 0.004534316249191761, 0.01902223378419876, -0.0004984439583495259, 0.0005743028013966978, 0.012984936125576496, 0.023655962198972702, 0.014759969897568226, 0.004693339578807354, -0.04008450731635094, 0.12391560524702072, 0.039451178163290024, -0.07126642018556595, 0.02606952376663685, -0.09078477323055267, -0.08117199689149857]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\NeuralArchitecture.pdf,Optimization,"634 ON THE K-WINNERS-TAKE-ALL NETWORK E. Majani Jet Propulsion Laboratory California Institute of Technology R. Erlanson, Y. Abu-Mostafa Department of Electrical Engineering California Institute of Technology ABSTRACT We present and rigorously analyze a generalization of the Winner- Take-All Network: the K-Winners-Take-All Network. This net- work identifies the K largest of a set of N real numbers. The network model used is the continuous Hopfield model. I - INTRODUCTION The Winner-Take-All Network is a network which identifies the largest of N real numbers. Winner-Take-All Networks have been developed using various neural networks models (Grossberg-73, Lippman-87, Feldman-82, Lazzaro-89). We present here a generalization of the Winner-Take-All Network: the K-Winners-Take-All (KWTA) Network. The KWTA Network identifies the K largest of N real numbers. The neural network model we use throughout the paper is the continuous Hopfield network model (Hopfield-84). If the states of the N nodes are initialized to the N real numbers, then, if the gain of the sigmoid is large enough, the network converges to the state with K positive real numbers in the positions of the nodes with the K largest initial states, and N - K negative real numbers everywhere else. Consider the following example: N = 4, K = 2. There are 6 = (~) stable states:(++-_)T, (+_+_)T, (+--+)T, ( __ ++)T, (_+_+)T, and (_++_)T. If the initial state of the network is (0.3, -0.4, 0.7, O.l)T, then the network will converge to (Vi,V2,V3,v4)T where Vi> 0, V2 < 0, V3 > 0, V4 < 0 ((+ _ +_)T). In Section II, we define the KWTA Network (connection weights, external inputs). In Section III, we analyze the equilibrium states and in Section IV, we identify all the stable equilibrium states of the KWTA Network. In Section V, we describe the dynamics of the KWTA Network. In Section VI, we give two important examples of the KWTA Network and comment on an alternate implementation of the KWTA Network. On the K-Winners-Take-All Network 635 II - THE K-WINNERS-TAKE-ALL NETWORK The continuous Hopfield network model (Hopfield-84) (also known as the Grossberg additive model (Grossberg-88)), is characterized by a system of first order differen- tial equations which governs the evolution of the state of the network (i = 1, .. . , N) : The sigmoid function g(u) is defined by: g(u) = f(G· u), where G > 0 is the gain of the sigmoid, and f(u) is defined by: 1. ""f/u, 0 < f'(u) < f'(O) = 1, 2. limu .... +oo f( u) = 1, 3. limu .... -oo f( u) = -l. The KWTA Network is characterized by mutually inhibitory interconnections Taj = -1 for i ¥= j, a self connection Tai = a, (Ial < 1) and'an external input (identical for every node) which depends on the number K of winners desired and the size of the network N : ti = 2K - N. The differential equations for the KWTA Network are therefore: for all i, Cd~i = -Aui + (a + l)g(ui) - (tg(uj ) - t) , (1) J=l where A = N - 1 + lal, -1 < a < +1, and t = 2K - N. Let us now study the equilibrium states of the dynamical system defined in (1). We already know from previous work (Hopfield-84) that the network is guaranteed to converge to a stable equilibrium state if the connection matrix (T) is symmetric (and it is here). III - EQUILIBRIUM STATES OF THE NETWORK The equilibrium states u· of the KWTA network are defined by for all i, dUi - 0 dt - , I.e., for all i, A (E. g(u~) - (2K - N)) g(u'!') = --u'!' + J J • I a+1 I a+1 (2) Let us now develop necessary conditions for a state u· to be an equilibrium state of the network. Theorem 1: For a given equilibrium state u·, every component ui of u· can be one of at most three distinct values. 636 Majani, Erlanson and Abu-Mostafa Proof of Theorem 1. If we look at equation (2), we see that the last term of the righthandside expression is independent of i; let us denote this term by H(u*). Therefore, the components ut of the equilibrium state u* must be solutions of the equation: g(ui) = _A_u; + H(u*). a+1 Since the sigmoid function g(u) is monotone increasing and A/(a + 1) > 0, then the sigmoid and the line a~l ut + H(u*) intersect in at least one point and at most three (see Figure 1). Note that the constant H(u*) can be different for different equilibrium states u*. • The following theorem shows that the sum of the node outputs is constrained to being close to 2K - N, as desired. Theorem 2: If u* is an equilibrium state of (1), then we have: N (a+ 1)maxg(ui) < '"" g(uJ~) -2K +N < (a+ 1) min g(ui). (3) u~<o L..J u'!'>o • j=l • Proof of Theorem 2. Let us rewrite equation (2) in the following way: Aut = (a + 1)g(ui) - (Eg(uj) - (2K - N)). j Since ut and g( un are of the same sign, the term (Lj g( un - (2K - N)) can neither be too large (if ut > 0) nor too low (if ui < 0). Therefore, we must have { (Lj g(uj) - (2K - N)) < (a + 1)g(un, (Lj g(uj) - (2K - N)) > (a + 1)g(ut), which yields (3). for all ut > 0, for all ut < 0, • Theorem 1 states that the components of an equilibrium state can only be one of at most three distinct values. We will distinguish between two types of equilibrium states, for the purposes of our analysis: those which have one or more components ut such that g'( un > a~l' which we categorize as type I, and those which do not (type II). We will show in the next section that for a gain G large enough, no equilibrium state of type II is stable. On the K-Winners-Take-All Network 637 IV - ASYMPTOTIC STABILITY OF EQUILIBRIUM STATES We will first derive a necessary condition for an equilibrium state of (1) to be asymptotically stable. Then we will find the stable equilibrium states of the KWTA Network. IV-I. A NECESSARY CONDITION FOR ASYMPTOTIC STABILITY An important necessary condition for asymptotic stability is given in the following theorem. Theorem 3: Given any asymptotically stable equilibrium state u*, at most one of the components ut of u* may satisfy: '( *) A 9 u· > --. , - a+ 1 Proof of Theorem 3. Theorem 3 is obtained by proving the following three lemmas. Lemma 1: Given any asymptotically stable equilibrium state u*, we always have for all i and j such that i # j : g'(u~) + g'(u~) Ja 2 (g'(un - g'(ujn 2 + 4g'(ung'(uj) A> a '2 J + 2 . (4) Proof of Lemma l. System (1) can be linearized around any equilibrium state u* : d(u ~ u*) ~ L(u*)(u _ u*), where L(u*) = T· diag (g'(ui), ... ,g'( uN» - AI. A necessary and sufficient condition for the asymptotic stability of u* is for L(u*) to be negative definite. A necessary condition for L(u*) to be negative definite is for all 2 X 2 matrices Lij(U*) of the type * (ag'(u~)-A -g'(U~») Lij(U ) = -g,'(ut) ag'(uj):'- A ' (i # j) to be negative definite. This results from an infinitesimal perturbation of compo- nents i and j only. Any matrix Lij (u*) has two real eigenvalues. Since the largest one has to be negative, we obtain: ~ (ag'(ui) - A + ag'(uj) - A + Ja 2 (g'(ut) - g'(ujn 2 + 49'(Ut)g'(Uj») < 0 .• 638 Majani, Erlan80n and Abu-Mostafa Lemma 2: Equation (4) implies: Proof of Lemma 2. min (g'(u:),g'(u1)) < 2-1 . a+ Consider the function h of three variables: (5) , • , * _ g'(u;) + g'(u;) va2 (g'(u;) - g'(u;))2 + 4g'(u;)g'(uj) h (a,g (ua),g (Uj)) - a 2 + 2 . If we differentiate h with respect to its third variable g'(uj), we obtain: {)h (a, g'(ut) , g'(uj)) = ~ + a2g'(uj) + (2 - a2)g'(ut) {)g'(uj) 2 2va2 (g'(un-g'(uj))2 +4g'(ung'(uj) which can be shown to be positive if and only if a > -1. But since lal < 1, then if g'(u;) < g'(uj) (without loss of generality), we have: h (a,g'(ui),g'(u1)) > h(a,g'(ui),g'(ui)) = (a+ 1)g'(ut), which, with (4), yields: which yields Lemma 2. Lemma 3: If for all i # j, '( *) A 9 Us < --1' a+ min (g'(ut),g'(u1)) < 2-1, a+ then there can be at most one ui such that: Proof of Lemma 3. A g'(u~) > --. I - a+ 1 • Let us assume there exists a pair (ui, uj) with i # j such that g'( ut) > 0;1 and g'(uj) > 0;1' then (5) would be violated. I On the K-Winners-Take-All Network 639 IV-2. STABLE EQUILmRIUM STATES From Theorem 3, all stable equilibrium states of type I have exactly one component , (at least one and at most one) such that g' ( ,) ~ 0; l' Let N + be the number of components a with g'(a) < 0;1 and a > 0, and let N_ be the number of components (3 with g'(f3) < 0;1 and f3 < 0 (note that N+ + N_ + 1 = N). For a large enough gain G, g(a) and g(f3) can be made arbitrarily close to +1 and -1 respectively. Using Theorem 2, and assuming a large enough gain, we obtain: -1 < N + - K < O. N + and K being integers, there is therefore no stable equilibrium state of type I. For the equilibrium states of type II, we have for all i, ut = a(> 0) or f3( < 0) where g'(a) < 0~1 and g'(f3) < 0;1' For a large enough gain, g(a) and g(f3) can be made arbitrarily close to +1 and -1 respectively. Using theorem 2 and assuming a large enough gain, we obtain: -(a + 1) < 2(N+ - K) < (a + 1), which yields N+ = K. Let us now summarize our results in the following theorem: Theorem 4: For a large enough gain, the only possible asymptotically stable equilibrium states u· of (1) must have K components equal to a > 0 and N - K components equal to f3 < 0, with { ( ) -.....L + K(g(a)-g(p)-2)+N(1+g(P» g a - 0+1 a 0+1 , g({3) - ...Lf3 + K(g(a)-g(p)-2)+N(1+g(,8» - 0+1 0+1 • (7) Since we are guaranteed to have at least one stable equilibrium state (Hopfield-84), and since any state whose components are a permutation of the components of a stable equilibrium state, is clearly a stable equilibrium state, then we have: Theorem 5: There exist at least (~) stable equilibrium states as defined in Theo- rem 4. They correspond to the (~) different states obtained by the N! permutations of one stable state with K positive components and N - K positive components. v - THE DYNAMICS OF THE KWTA NETWORK Now that we know the characteristics of the stable equilibrium states of the KWTA Network, we need to show that the KWTA Network will converge to the stable state which has a > 0 in the positions of the K largest initial components. This can be seen clearly by observing that for all i ;/; j : d(u' - u·) C 'dt J =.>t(ui- uj)+(a+1)(g(Ui)-g(Uj». If at some time T, ui(T) = uj(T), then one can show that Vt, Ui(t) = Uj(t). Therefore, for all i ;/; j, Ui(t) - Uj(t) always keeps the same sign. This leads to the following theorem. 640 Majani, Erlan80n and Abu-Mostafa Theorem 6: (Preservation of order) For all nodes i # j, We shall now summarize the results of the last two sections. Theorem 7: Given an initial state u-(O) and a gain G large enough, the KWTA Network will converge to a stable equilibrium state with K components equal to a positive real number (Q > 0) in the positions of the K largest initial components, and N - K components equal to a negative real number (13 < 0) in all other N - K positions. This can be derived directly from Theorems 4, 5 and 6: we know the form of all stable equilibrium states, the order of the initial node states is preserved through time, and there is guaranteed convergence to an equilibrium state. VI - DISCUSSION The well-known Winner-Take-All Network is obtained by setting K to 1. The N/2-Winners-Take-All Network, given a set gf N real numbers, identifies which numbers are above or below the mediaIl~ This task is slightly more complex com- putationally (~ O(N log(N» than that of the Winner-Take-All (~ O(N». The number of stable states is much larger, ( N) 2N N/2 ~ J21rN' i.e., asymptotically exponential in the size of the network. Although the number of connection weights is N2, there exists an alternate imple- mentation of the KWTA Network which has O(N) connections (see Figure 2). The sum of the outputs of all nodes and the external input is computed, then negated and fed back to all the nodes. In addition, a positive self-connection (a + 1) is needed at every node. The analysis was done for a ""large enough"" gain G. In practice, the critical value of Gis a~i for the N/2-Winners-Take-All Network, and slightly higher for K # N/2. Also, the analysis was done for an arbitrary value of the self-connection weight a (Ial < 1). In general, if a is close to +1, this will lead to faster convergence and a smaller value of the critical gain than if a is close to -1. On the K-Winners-Take-All Network 641 VII - CONCLUSION The KWTA Network lets all nodes compete until the desired number of winners (K) is obtained. The competition is ibatained by using mutual inhibition between all nodes, while the number of winners K is selected by setting all external inputs to 2K - N. This paper illustrates the capability of the continuous Hopfield Network to solve exactly an interesting decision problem, i.e., identifying the K largest of N real numbers. Acknowledgments The authors would like to thank John Hopfield and Stephen DeWeerth from the California Institute of Technology and Marvin Perlman from the Jet Propulsion Laboratory for insightful discussions about material presented in this paper. Part of the research described in this paper was performed at the Jet Propulsion Laboratory under contract with NASA. References J .A. Feldman, D.H. Ballard, ""Connectionist Models and their properties,"" Cognitive Science, Vol. 6, pp. 205-254, 1982 S. Grossberg, ""Contour Enhancement, Short Term Memory, and Constancies in Reverberating Neural Networks,"" Studies in Applied Mathematics, Vol. LII (52), No.3, pp. 213-257, September 1973 S. Grossberg, ""Non-Linear Neural Networks: Principles, Mechanisms, and Archi- tectures,"" Neural Networks, Vol. 1, pp. 17-61, 1988 J.J. Hopfield, ""Neurons with graded response have collective computational prop- erties like those of two-state neurons,"" Proc. Natl. Acad. Sci. USA, Vol. 81, pp. 3088-3092, May 1984 J. Lazzaro, S. Ryckebusch, M.A. Mahovald, C.A. Mead, ""Winner-Take-All Networks of O(N) Complexity,"" in this volume, 1989 R.P. Lippman, B. Gold, M.L. Malpass, ""A Comparison of Hamming and Hopfield Neural Nets for Pattern Classification,"" MIT Lincoln Lab. Tech. Rep. TR-769, 21 May 1987 642 Majani, Erlanson and Abu-Mostafa u ,1 / , Fj gure 1; I ntersecti on of si gmoj d and line, a+1 N-2K Figure 2; An Implementation of the KWTA Network,","[-0.047974638640880585, -0.05243142321705818, -0.03719427436590195, -0.03778369352221489, -0.03677774965763092, 0.048217546194791794, 0.01980563811957836, -0.03520100191235542, 0.014833091758191586, -0.02167409099638462, -0.06591419130563736, 0.0779348686337471, 0.003595238085836172, 0.022205447778105736, -0.12363933026790619, 0.040296684950590134, 0.020949997007846832, 0.031794216483831406, -0.05897311493754387, -0.028920182958245277, 0.014247936196625233, 0.00727701373398304, -0.05842992290854454, -0.010112856514751911, 0.037621237337589264, -0.02138236165046692, -0.012450851500034332, 0.02479715459048748, -0.018942618742585182, -0.061185795813798904, -0.028528302907943726, -0.02128290943801403, 0.05676883086562157, 0.037729766219854355, -0.04439187049865723, -0.02507902681827545, -0.12965647876262665, 0.023221975192427635, 0.028360599651932716, 0.045471277087926865, 0.014592068269848824, -0.032011281698942184, 0.03339724987745285, 0.06843125820159912, 0.06328330934047699, 0.015318329446017742, -0.05700208246707916, -0.03302638977766037, 0.07573355734348297, -0.004338116850703955, -0.09273461997509003, 0.01623663492500782, -0.002374813659116626, 0.07349113374948502, 0.053292904049158096, 0.023930134251713753, -0.032569754868745804, -0.015173997730016708, -0.02262568660080433, 0.009099099785089493, 0.005758422892540693, -0.01995626650750637, -0.01659415476024151, -0.028458938002586365, -0.0014879348454996943, 0.022485727444291115, -0.03204772248864174, 0.06838370114564896, 0.03431091085076332, 0.007961777970194817, 0.0800577849149704, 0.006592284422367811, -0.09000334143638611, 0.03669111058115959, 0.09012865275144577, 0.0345752127468586, 0.06275828927755356, 0.09157373011112213, 0.04804936796426773, 0.04086644947528839, -0.010790575295686722, 0.018328525125980377, -0.062297385185956955, -0.08345958590507507, 0.04536260664463043, 0.006854198407381773, -0.029592106118798256, 0.05922148376703262, 0.12964873015880585, -0.0047182198613882065, -0.10968528687953949, 0.03537902608513832, 0.02282167784869671, -0.04261859878897667, 0.026103947311639786, 0.04540010169148445, -0.009378124959766865, -0.07202984392642975, 0.014118012972176075, 0.06509236246347427, 0.0024370967876166105, -0.015001707710325718, 0.07261310517787933, -0.09399701654911041, 0.10139870643615723, 0.000670293637085706, 0.08768386393785477, -0.0007898235344327986, 0.038279030472040176, -0.14136604964733124, -0.06780942529439926, -0.004629561211913824, -0.06639178097248077, 0.01050477009266615, 0.026919925585389137, 0.022057214751839638, -0.016178762540221214, 0.017966164276003838, -0.013795111328363419, 0.02456621825695038, -0.11291663348674774, -0.0005404191906563938, -0.024048781022429466, 0.048396676778793335, -0.02796499989926815, 0.0413929782807827, -0.06193452328443527, -1.9496010385726796e-36, -0.09196952730417252, 0.016125671565532684, 0.03889654576778412, -0.016222642734646797, 0.008980883285403252, -0.00240585976280272, -0.018262630328536034, 0.012388059869408607, 0.02023874782025814, -0.01116381585597992, -0.1437571793794632, 0.10087086260318756, -0.02346613258123398, 0.022078119218349457, 0.07223282754421234, -0.024797262623906136, 0.003798414720222354, -0.08217290788888931, 0.03980032354593277, -0.10961799323558807, 0.11435750126838684, -0.05410230532288551, -0.01687832921743393, 0.0036384868435561657, -0.009511278010904789, -0.04689693823456764, 0.010320067405700684, 0.012025898322463036, -0.12901893258094788, 0.023713601753115654, -0.04341474175453186, 0.004447889514267445, -0.04124685376882553, 0.01454256009310484, -0.013529752381145954, -0.027600228786468506, -0.009388172999024391, 0.025235949084162712, 0.05287062004208565, 0.012776286341249943, -0.1313161849975586, 0.008086254820227623, -0.04106874763965607, 0.030324600636959076, -0.08119971305131912, -0.03935210406780243, -0.0014527499442920089, 0.04741528630256653, 0.022421376779675484, 0.0213400200009346, -0.04928094521164894, -0.041818052530288696, 0.04020293056964874, -0.05384974554181099, 0.01566353067755699, 0.029821358621120453, 0.09470462054014206, 0.09462463855743408, 0.01320971641689539, 0.0645102709531784, -0.042982980608940125, 0.008521227166056633, 0.023197056725621223, 0.05149585381150246, 0.00564847607165575, 0.06532757729291916, -0.08143148571252823, -0.0039158170111477375, 0.08775562793016434, -0.044801220297813416, -0.005590466316789389, 0.01901533268392086, -0.04558604583144188, -0.08388451486825943, 0.07518230378627777, -0.029957959428429604, -0.012001074850559235, -0.0591505728662014, -0.04379774257540703, 0.0929393619298935, -0.06943817436695099, 0.09662342071533203, -0.03056987002491951, -0.07128611952066422, -0.014658092521131039, 0.02638622559607029, 0.017514051869511604, -0.018132291734218597, 0.029926840215921402, -0.02455045096576214, -0.06766923516988754, 0.022912055253982544, 0.05848914384841919, 0.022771408781409264, -0.023034067824482918, -4.298635904163253e-34, -0.021753570064902306, 0.04636256769299507, -0.002168213715776801, 0.034162260591983795, -0.006973949261009693, 0.015460862778127193, -0.0027869343757629395, -0.00951852835714817, -0.013069753535091877, 0.06308995187282562, 0.014027370139956474, 0.03136327862739563, 0.07497237622737885, -0.061977092176675797, 0.06908861547708511, -0.061951685696840286, -0.02465575747191906, -0.04692428559064865, 0.0323241651058197, -0.031106239184737206, 0.019933626055717468, 0.06651496887207031, -0.12356970459222794, -0.015227020718157291, -0.022220656275749207, -0.014954178594052792, -0.007509952411055565, 0.06459490954875946, 0.004397565498948097, 0.02379741333425045, 0.012147772125899792, -0.046824946999549866, -0.09789227694272995, 0.10549468547105789, 0.07667897641658783, 0.059028640389442444, 0.09409809112548828, 0.08911249041557312, -0.002939143218100071, 0.013189851306378841, 0.030776862055063248, -0.1112082302570343, -0.07000298798084259, 0.07121767103672028, -0.003474507248029113, -0.024190856143832207, 0.0012903566239401698, -0.00430782837793231, -0.048124633729457855, 0.010449888184666634, 0.0028193616308271885, 0.0008748910622671247, -0.018389500677585602, 0.042844463139772415, -0.008240308612585068, 0.07395078986883163, -0.0026090163737535477, 0.04992265626788139, 0.0048300884664058685, -0.0139190424233675, 0.008289171382784843, -0.14792098104953766, -0.02785559743642807, 0.08450841903686523, -0.08697561919689178, -0.07496370375156403, -0.029537327587604523, 0.10315574705600739, -0.009927930310368538, 0.009708942845463753, -0.05045733600854874, 0.07734283804893494, 0.046988651156425476, 0.036859676241874695, -0.14592398703098297, -0.04599519819021225, 0.03132181987166405, 0.07496801018714905, 0.0782637670636177, 0.015518011525273323, -0.0246701892465353, 0.04622384533286095, 0.016748983412981033, 0.0028336162213236094, 0.10509070754051208, -0.0059021650813519955, 0.04937862977385521, -0.017066024243831635, 0.047734763473272324, -0.06068558245897293, 0.012860661372542381, 0.00026079907547682524, -0.05041174218058586, 0.005009833723306656, -0.046948276460170746, -4.5955250271845216e-08, -0.02306586503982544, 0.04234526678919792, 0.02174752578139305, 0.024049922823905945, 0.11964979767799377, -0.05500410497188568, 0.0014302334748208523, 0.038556694984436035, -0.03763855621218681, -0.017992889508605003, 0.09423364698886871, 0.05438339710235596, -0.06341380625963211, 0.005114354658871889, -0.039053384214639664, -0.00044229949708096683, -0.022695820778608322, -0.03303278982639313, 0.055861253291368484, 0.017134925350546837, 0.029877375811338425, -0.014951360411942005, 0.004629218485206366, 0.04718096926808357, 0.0759892389178276, -0.023538127541542053, -0.07092484086751938, -0.008065297268331051, 0.01149983610957861, 0.048718106001615524, 0.015061495825648308, -0.013380547054111958, 0.01725192181766033, 0.026999883353710175, 0.06229595094919205, 0.1063004806637764, -0.01711888797581196, 0.03813255950808525, -0.032142896205186844, -0.014914087951183319, -0.05424695834517479, 0.010694168508052826, -0.00950651429593563, -0.04278416186571121, -0.040778957307338715, -0.010242126882076263, -0.02028830163180828, -0.07321973145008087, 0.08912697434425354, 0.005414189305156469, 0.010382573120296001, 0.034905485808849335, -0.04958260431885719, 0.025191709399223328, 0.018535347655415535, -0.023592114448547363, -0.012045444920659065, -0.10805395245552063, 0.010287138633430004, 0.02251335233449936, 0.0009428065968677402, 0.048224832862615585, -0.05571196973323822, 0.0546090342104435]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\NeuralControlofSensoryAcquisitionTheVestibuloOcularReflex.pdf,Computer Vision,"410 NEURAL CONTROL OF SENSORY ACQUISITION: THE VESTIBULO-OCULAR REFLEX. Michael G. Paulin, Mark E. Nelson and James M. Bower Division of Biology California Institute of Technology Pasadena, CA 91125 ABSTRACT We present a new hypothesis that the cerebellum plays a key role in ac- tively controlling the acquisition of sensory infonnation by the nervous system. In this paper we explore this idea by examining the function of a simple cerebellar-related behavior, the vestibula-ocular reflex or VOR, in which eye movements are generated to minimize image slip on the retina during rapid head movements. Considering this system from the point of view of statistical estimation theory, our results sug- gest that the transfer function of the VOR, often regarded as a static or slowly modifiable feature of the system, should actually be continu- ously and rapidly changed during head movements. We further suggest that these changes are under the direct control of the cerebellar cortex and propose experiments to test this hypothesis. 1. INTRODUCTION A major thrust of research in our laboratory involves exploring the way in which the nervous system actively controls the acquisition of infonnation about the outside world. This emphasis is founded on our suspicion that the principal role of the cerebellum, through its influence on motor systems, is to monitor and optimize the quality of sensory information entering the brain. To explore this question, we have undertaken an investi- gation of the simplest example of a cerebellar-related motor activity that results in improved sensory inputs, the vestibulo-ocular reflex (VOR). This reflex is responsible for moving the eyes to compensate for rapid head movements to prevent retinal image slip which would otherwise significantly degrade visual acuity (Carpenter, 1977). 2. VESTIBULO-OCULAR REFLEX (VOR) The VOR relies on the vestibular apparatus of the inner ear which is an inertial sensor that detects movements of the head. Vestibular output caused by head movements give rise to compensatory eye movements through an anatomically well described neural pathway in the brain stem (for a review see Ito, 1984). Visual feedback also makes an important contribution to compensatory eye movements during slow head movements, Neural Control of Sensory Acquisition 411 but during rapid head movements with frequency components greater than about 1Hz, the vestibular component dominates (Carpenter, 1977). A simple analysis of the image stabilization problem indicates that during head rotation in a single plane, the eyes should be made to rotate at equal velocity in the opposite direction. This implies that, in a simple feedforward control model, the VOR transfer function should have unity gain and a 1800 phase shift. This would assure stabilized reti- nal images of distant objects. It turns out, however, that actual measurements reveal the situation is not this simple. Furman, O'Leary and Wolfe (1982), for example, found that the monkey VOR has approximately unity gain and 1800 phase shift only in a narrow fre- quency band around 2Hz. At 4Hz the gain is too high by a factor of about 30% (fig. 1). 1.2 ~ -< C) 1.0 0.8 ,...., I til f f1d1t H f~ff 1\ j ~ 5 0 I - Hf lIJ (f) ~O n.. -5 2 3 4 5 2 3 4 5 FREQUENCY (Hz) FREQUENCY (Hz) Figure 1: Bode gain and phase plots for the transfer function of the horizontal component of the VOR of the alert Rhesus monkey at high frequencies (Data from Furman et al. (1982». Given the expectation of unity gain, one might be tempted to conclude from the monkey data that the VOR simply does not perform well at high frequencies. But 4Hz is not a very high frequency for head movements, and perhaps it is not the VOR which is performing poorly, but the simplified analysis using classical control theory. In this paper, we argue that the VOR uses a more sophisticated strategy and that the ""excessive"" gain in the system seen at higher frequencies actually improves VOR performance. 3. OPTIMAL ESTIMATION In order to understand the discrepancy between the predictions of simple control theory models and measured VOR dynamics, we believe it is necessary to take into account more of the real world conditions under which the VOR operates. Examples include noisy head velocity measurements, conduction delays and multiple, possibly conflicting, measurements of head velocity, acceleration, muscle contractions, etc., generated by different sensory modalities. The mathematical framework that is appropriate for analyz- 412 Paulin, Nelson and Bower ing problems of this kind is stochastic state-space dynamical systems theory (Davis and Vinter, 1985). This framework is an extension of classical linear dynamical systems the- ory that accommodates multiple inputs and outputs, nonlinearities, time-varying dynam- ics, noise and delays. One area of application of the state space theory has been in target tracking, where the basic principle involves using knowledge of the dynamics of a target to estimate its most probable trajectory given imprecise data. The VOR can be viewed as a target tracking system whose target is the ""world"", which moves in head coordinates. We have reexamined the VOR from this point of view. The Basic VOR. To begin our analysis of the VOR we have modeled the eye-head-neck system as a damped inverted pendulum with linear restoring forces (fig. 2) where the model system is driven by random (Gaussian white) torque. Within this model, we want to predict the correct compensatory ""eye"" movements during ""head"" movements to stabilize the direction in which the eye is pointing. Figure 2 shows the amplitude spectrum of head velocity for this model. In this case, the parameters of the model result in a system that has a natural resonance in the range of 1 to 2 Hz and attenuates higher frequencies. 20 • • ••• SIS z, s. u • '. • •• i. I 0.1 1.0 FREQUENCY Figure 2: Amplitude spectrum of model head velocity. We provide noisy measurements of ""head"" velocity and then ask what transfer function, or filter, will give the most accurate ""eye"" movement compensation? This is an estima- tion problem and, for Gaussian measurement error, the solution was discovered by Kalman and Bucy (1961). The optimal fIlter or estimator is often called the Kalman- Bucy filter. The gain and phase plots of the optimal filter for tracking movements of the inverted pendulum model are shown in figure 3. It can be seen that the gain of the opti- mal estimator for this system peaks near the maximum in the spectrum of ""head-neck"" velocity (fig. 2). This is a general feature of optimal filters. Accordingly, to accurately compensate for head movement in this system, the VOR would need to have a frequency dependent gain. - ~ 20 - ~ 0 0 5 ~ -20 ~ Neural Control of Sensory Acquisition 413 - bO 0 So j ~ tI) ~90 0.1 1.0 10.0 .0 Figure 3: Bode gain plot (left) and phase plot (right) of an optimal estimator for tracking the inverted pendulum using noisy data. Time Varying dynamics and the VOR So far we have considered our model for VOR optimization only in the simple case of a constant head-neck velocity power spectrum. Under natural conditions, however, this spectrum would be expected to change. For example, when gait changes from walking to running, corresponding changes in the VOR transfer function would be necessary to maintain optimal performance. To explore this, we added a second inverted pendulum to our model to simulate body dynamics. We simulated changes in gait by changing the resonant frequency of the trunk. Figure 4 compares the spectra of head-neck velocity with two different trunk parameters. As in the previous example, we then computed transfer functions of the optimal filters for estimating head velocity from noisy measure- ments in these two cases. The gain and phase characteristics of these filters are also shown in Figure 5. These plots demonstrate that significant changes in the transfer function of the VOR would be necessary to maintain visual acuity in our model system under these different conditions. Of course, in the real situation head-neck dynamics will change rapidly and continuously with changes in gait, posture, substrate, etc. requiring rapid continuous changes in VOR dynamics rather than the simple switch implied here. HEAD - ~ 20 - ~ § 0 ....... ----~~-~ 5 ~ ~ -20 0.1 1.0 10.0 FREQUENCY Figure 4: Head velocity spectrum during ""walking"" (light) and ""running"" (heavy). 414 Paulin, Nelson and Bower - ~ 0 .8 - ~ 1----""""'::3I~ § 5 ~ -20 • • •• , Ci. D •• .1 1.0 10.0 .1 1.0 10.0 Figure 5: Bode gain plots (left) and phase plots (right) for optimal estimators of head angular velocity during ""walking"" (light) and ""running"" (heavy). 4. SIGNIFICANCE TO THE REAL VOR Our results show that the optimal VOR transfer function requires a frequency dependent gain to accurately adjust to a wide range of head movements under real world conditions. Thus, the deviations from unity gain seen in actual measurements of the VOR may not represent poor, but rather optimal, performance. Our modeling similarly suggests that several other experimental results can be reinterpreted. For example, localized peaks or valleys in the VOR gain function can be induced experimentally through prolonged sinu- soidal oscillations of subjects wearing magnifying or reducing lenses. However, this ""frequency selectivity"" is not thought to occur naturally and has been interpreted to im- ply the existence of frequency selective channels in the VOR control network (Lisberger, Miles and Optican, 1983). In our view there is no real distinction between this phenom- enon and the ""excessive"" gain in normal monkey VOR; in each case the VOR optimizes its response for the particular task which it has to solve. This is testable. If we are cor- rect, then frequency selective gain changes will occur following prolonged narrow-band rotation in the light without wearing lenses. In the classical framework there is no reason for any gain changes to occur in this situation. Another phenomenon which has been observed experimentally and that the current modeling sheds new light on is referred to as ""pattern storage"". After single-frequency sinusoidal oscillation on a turntable in the light for several hours, rabbits will continue to produce oscillatory eye movements when the lights are extinguished and the turntable stops. Trained rabbits also produce eye oscillations at the training frequency when oscil- lated in the dark at a different frequency (Collewijn, 1985). In this case the sinusoidal pattern seems to be ""stored"" in the nervous system. However, the effect is naturally ac- counted for by our optimal estimator hypothesis without relying on an explicit ""pattern storage mechanism"". An optimal estimator works by matching its dynamics to the dynamics of the signal generator, and in effect it tries to force an internal model to mimic the signal generator by comparing actual and expected patterns of sensory inputs. When Neural Control of Sensory Acquisition 415 no data is available, or the data is thought to be very unreliable, an optimal estimator relies completely, or almost completely, on the model. In cases where the signal is pat- terned the estimator will behave as though it had memorized the pattern. Thus, if we hypothesize that the VOR is an optimal estimator we do not need an extra hypothesis to explain pattern storage. Again, our hypothesis is testable. If we are correct, then repeat- ing the pattern storage experiments using rotational velocity waveforms obtained by driv- ing a frequency-tuned oscillator with Gaussian white noise will produce identical dynam- ical effects in the VOR. There is no sinusoidal pattern in the stimulus, but we predict that the rabbits can be induced to generate sinusoidal eye movements in the dark after this training. The modeling results shown in figures 4 and 5 represent an extension of our ideas into the area of gait (or more generally ""context"") dependent changes in VOR which has not been considered very much in VOR research. In fact, VOR experimental paradigms, in general, are explicitly set up to produce the most stable VOR dynamics possible. Accordingly, little work has been done to quantify the short term changes in VOR dynamics that must occur in response to changes in effective head-neck dynamics. Ex- periments of this type would be valuable and are no more difficult technically than experiments which have already been done. For example, training an animal on a turn- table which can be driven randomly with two distinct velocity power spectra, i.e. two ""gaits"", and providing the animal with external cues to indicate the gait would, we predict, result in an animal that could use the cues to switch its VOR dynamics. A more difficult but also more compelling demonstration would be to test VOR dynamics with impulsive head accelerations in different natural situations, using an unrestrained animal. s. SENSOR FUSION AND PREDICTION To this point, we have discussed compensatory eye movements by treating the VOR as a single input, single output system. This allowed us to concentrate on a particular aspect of VOR control: tracking a time-varying dynamical system (the head) using noisy data. In reality there are a number of other factors which make control of compensatory eye movements a somewhat more complex task than it appears to be when it is modeled using classical control theory. For example, a variety of vestibular as well as non-vestib- ular signals (e.g. visual, proprioceptive) relating to head movements are transmitted to the compensatory eye movement control network (Ito, 1984). This gives rise to a ""sen- sor fusion"" problem where data from different sources must be combined. The optimal solution to this problem for a multiple input - multiple output, time-varying linear, sto- chastic system is also given by the Kalman-Bucy filter (Davis and Vinter, 1985). Borah, Young and Curry (1988) have demonstrated that a Kalman-Bucy filter model of visual- vestibular sensor fusion is able to account for visual-vestibular interactions in motion perception. Oman (1982) has also developed a Kalman-Bucy filter model of visual- vestibular interactions. Their results show that the optimal estimation approach is useful 416 Paulin, Nelson and Bower for analyzing multivariate aspects of compensatory eye movement control, and comple- ment our analysis of dynamical aspects. Another set of problems arises in the VOR because of small time delays in neural trans- mission and muscle activation. To optimize its response, the mammalian VOR needs to make up for these delays by predicting head movements about lOmsec in advance (ret). Once the dynamics of the signal generator have been identified, prediction can be per- formed using model-based estimation (Davis and Vinter, 1985). A neural analog of a Taylor series expansion has also been proposed as a model of prediction in the VOR (pellionisz and LUnas, 1979), but this mec.hanism is extremely sensitive to noise in the data and was abandoned as a practical technique for general signal prediction several de- cades ago in favor of model-based techniques (Wiener, 1948). The later approach may be more appropriate for analyzing neural mechanisms of prediction (Arbib and Amari, 1985). An elementary description of optimal estimation theory for target tracking, and its possible relation to cerebellar function, is given by Paulin (1988). 6. ROLE OF CEREBELLAR CORTEX IN VOR CONTROL To this point we have presented a novel characterization of the problem of compensatory eye movement control without considering the physical circuitry which implements the behavior. However, there are two parts to the optimal estimation problem. At each in- stant it is necessary to (a) filter the data using the optimal transfer function to drive the desired response and (b) determine what transfer function is optimal at that instant and adjust the filtering network accordingly. The first problem is fairly straightforward, and existing models of VOR demonstrate how a network of neurons based on known brains tern circuitry can implement a particular transfer function (Cannon and Robinson, 1985). The second problem is more difficult because requires continuous monitoring of the context in which head movements occur using a variety of sources of relevant data to tune the optimal filter for that context. We speculate that the cerebellar cortex performs this task. First, the cortex of the vestibulo-cerebellum is in a position to mflke the required compu- , tation, since it receives detailed information from multiple sensory modalities that provide information on the state of the motor system (Ito, 1985). Second, the cerebellum projects to and appears to modulate the brain stem compensatory eye movement control network (Mackay and Murphy, 1979). We predict that the cerebellar cortex is necessary to produce rapid, context-dependent optimal state dependent changes in VOR transfer function which we have discussed. This speculation can be tested with turntable experi- ments similar to those described in section 4 above in the presence and absence of the cerebellar cortex. Neural Control of Sensory Acquisition 417 7. THE GENERAL FUNCTION OF CEREBELLAR CORTEX According to our hypothesis, the cerebellar cortex is required for making optimal com- pensatory eye movements during head movements. This is accomplished by continuous- ly modifying the dynamics of the underlying control network in the brainstem, based on current sensory information. The function of the cerebellar cortex in this case can then be seen in a larger context as using primary sensory information (vestibular, visual) to coordinate the use of a motor system (the extraoccular eye muscles) to position a sensory array (the retina) to optimize the quality of sensory information available to the brain. We believe that this is the role played by the rest of the cerebellum for other sensory systems. Thus, we suspect that the hemispheres of the rat cerebellum, with their peri-oral tactile input (Bower et al., 1983), are involved in controlling the optimal use of these tactile surfaces in sensory exploration through the control of facial musculature. Similarly, the hemispheres of the primate cerebellum, which have hand and finger tactile inputs (Ito, 1984), may be involved in an analogous exploratory task in primates. These tactile sensory-motor systems are difficult to analyze, and we are currently studying a functionally analogous but more accessible model system, the electric sense of weakly electric fish (cf Rasnow et al., this volume). 8.CONCLUSION Our view of the cerebellum assigns it an important dynamic role which contrasts markedly with the more limited role it was assumed to have in the past as a learning device (Marr, 1969; Albus, 1971; Robinson, 1976). There is evidence that cerebellar cortex has some learning abilities (Ito, 1984), but it is recognized that cerebellar cortex has an important dynamic role in motor control. However, there are widely differing opinions as to the nature of that role (Ito, 1985; Miles and Lisberger, 1981; Pellionisz and Llinas, 1979). Our proposal, that the VOR is a neural analog of an optimal estimator and that the cerebellar cortex monitors context and sets reflex dynamics accordingly, should not be interpreted as a claim that the nervous system actually implements the computations which are involved in applied optimal estimation, such as the Kalman- Bucy filter. Understanding the neural basis of cerebellar function will require the combined power of a number of experimental, theoretical and modeling approaches (cf Wilson et al., this volume). We believe that analyses of the kind presented here have an important role in characterizing behaviors controlled by the cerebellum. Acknowledgments This work was supported by the NIH (BNS 22205), the NSF (EET-8700064), and the Joseph Drown Foundation. References Arbib M.A. and Amari S. 1985. Sensori-moto Transformations in the Brain (with a cri- tique of the tensor theory of the cerebellum). J. Theor. BioI. 112:123-155 418 Paulin, Nelson and Bower Borah J., Young L.R. and Curry, R.E. 1988. Optimal Estimator Model for Human Spatial Orientation. In: Proc. N.Y. Acad. Sci. B. Cohen and V. Henn (eds.). In Press. Bower lM. and Woolston D.C. 1983. The Vertical Organization of Cerebellar Cortex. J. Nemophysiol. 49: 745-766. Carpenter R.H.S. 1977. Movements of the Eyes. Pion, London. Davis M.B.A. and Vinter R.B. 1985. Stochastic Modelling and Control. Chapman and Hall, NY. Funnan J.M., O'Leary D.P. and Wolfe lW. 1982. Dynamic Range of the Frequency Response of the Horizontal Vestibulo-Ocular Reflex of the Alert Rhesus Monkey. Acta Otolaryngol. 93: 81 Ito, M. 1984. The Cerebellum and Neural Control. Raven Press, NY. Kalman R.E. 1960. A New Approach to Linear Filtering and Prediction Problems. l Basic Eng., March 1960. Kalman R.E. and Bucy R.S. 1961. New Results in Linear Filtering and Prediction Theory. 1. Basic Eng., March 1961. Lisberger, S.G. 1988. The Nemal Basis for Learning of Simple Motor Skills. Science, 242:728- 735. Lisberger S.G. , Miles F.A. and Optican L.M. 1983. Frequency Selective Adaptation: Evidence for Channels in the Vestibulo-Ocular Reflex. J. Neurosci. 3:1234-1244 Mackay W.A. and Murphy J.T. 1979. Cerebellar Modulation of reflex Gain. Prog. Neurobiol. 13:361-417. Oman C.M. 1982. A heuristic mathematical Model for the Dynamics of Sensory Conflict and Motion Sickness. Acta Oto-Laryngol. S392. Paulin M.G. 1988. A Kalman Filter Model of the Cerebellum. In: Dynamic Interactions in Nemal Networks: Models and Data. M. Arbib and S. Amari (eds). Springer-Verlag, NY. pp239-261. Pellionisz A. and Llinas R. 1979. Brain Modelling by Tensor Network Theory and Computer Simulation. The Cerebellum: Distributed Processor for Predictive Coordination. Nemoscience 4:323-348. Robinson D.A. 1976. Adaptive Control of the Vestibulo-Ocular Reflex by the Cerebellum. J. Nemophys.36:954-969. Robinson D.A. 1981. The Use of Control Systems Analysis in the Neurophysiology of Eye Movements. Ann. Rev. Neurosci. 4:463-503. Wiener, N. 1948. Cybernetics: Communication and Control in the Animal and the Machine. MIT Press, Boston.","[-0.014307421632111073, -0.09350201487541199, 0.06203974783420563, -0.0404861681163311, -0.03965672478079796, 0.049321357160806656, 0.08664601296186447, 0.005427953787147999, 0.07755957543849945, 0.028302643448114395, 0.11324754357337952, -0.11539094895124435, 0.04910774901509285, -0.023112008348107338, -0.036250244826078415, -0.052125874906778336, 0.04856451600790024, 0.04562653601169586, -0.03977802023291588, 0.012071985751390457, -0.011039464734494686, -0.02430710755288601, -0.019709333777427673, -0.01778826117515564, -0.02770874835550785, -0.0018339507514610887, -0.04525464400649071, -0.03544685244560242, -0.002172839595004916, -0.10616639256477356, 0.03143590688705444, -0.035913534462451935, -0.03567541018128395, -0.016602415591478348, -0.07837403565645218, 0.0007516879122704268, -0.038668833673000336, -0.0546787865459919, -0.03328152745962143, 0.049886252731084824, -0.07207714021205902, 0.005043004639446735, -0.03989546000957489, -0.0015553091652691364, -0.0008927616290748119, 0.16032098233699799, 0.10071247816085815, 0.03730934113264084, -0.07635302096605301, -0.0019200023962184787, -0.06939875334501266, -0.059479206800460815, -0.014266285113990307, -0.01871129311621189, -0.0035355554427951574, 0.11011350899934769, 0.007903431542217731, 0.011381597258150578, -0.0245225727558136, 0.030613817274570465, -0.023950127884745598, -0.06480594724416733, -0.004863877315074205, 0.000194466658285819, -0.0012731142342090607, 0.038189053535461426, 0.022311681881546974, -0.06321864575147629, 0.013323817402124405, -0.06084052845835686, -0.0051780035719275475, -0.0009597282041795552, 0.006557278800755739, -0.006977163255214691, 0.014666149392724037, -0.06787772476673126, 0.023307673633098602, -0.04900003969669342, 0.08924875408411026, -0.05617943033576012, 0.06475172191858292, 0.07644156366586685, -0.0030986161436885595, -0.01215932983905077, 0.03278079256415367, 0.005135275889188051, -0.018221333622932434, 0.04518624022603035, -0.003316618734970689, -0.0057777222245931625, -0.004824122413992882, -0.08493123203516006, -0.18252333998680115, -0.028774823993444443, 0.014473202638328075, -0.039615415036678314, 0.007476265542209148, 0.019310077652335167, 0.04130937159061432, -0.04573722183704376, 0.08156713843345642, -0.01141334418207407, 0.028080515563488007, 0.08828681707382202, -0.007495582569390535, -0.015591160394251347, 0.12603293359279633, 0.03375356271862984, -0.01774020306766033, 0.017608938738703728, -0.05502600967884064, -0.011994898319244385, -0.04901774600148201, 0.014607035554945469, 4.66939527541399e-05, -0.07749231904745102, -0.04382513836026192, 0.028336362913250923, 0.04282102733850479, -0.03462015092372894, 0.058176230639219284, -0.02324662171304226, -0.026968104764819145, -0.05235449969768524, 0.04257054999470711, -0.007099064067006111, -0.09978358447551727, 1.771257112009093e-33, -0.02426179312169552, -0.021050116047263145, -0.008860893547534943, -0.0019510925048962235, -0.003946691285818815, -0.03791270777583122, -0.013638869859278202, 0.037117794156074524, 0.024240434169769287, -0.005081170704215765, -0.03512120246887207, -0.025873342528939247, -0.041062645614147186, 0.10468646138906479, 0.055046197026968, -0.015675313770771027, -0.04990394040942192, 0.039269354194402695, -0.01689029298722744, -0.07289278507232666, 0.043396495282649994, -0.0033190848771482706, 0.04810596629977226, -0.022315865382552147, 0.019530100747942924, 0.002996192080900073, -0.06159551441669464, 0.009665153920650482, -0.017605895176529884, 0.008228009566664696, -0.007969388738274574, -0.011602777056396008, -0.07067249715328217, -0.08085211366415024, 0.033575281500816345, 0.0012828566832467914, 0.10913021117448807, -0.008102253079414368, 0.005411933641880751, 0.05718245729804039, 0.0071427286602556705, 0.0830063447356224, -0.017640992999076843, -0.03601012006402016, -0.02035743184387684, 0.01800697296857834, -0.05717538297176361, 0.042344335466623306, -0.022182678803801537, -0.037673089653253555, 0.05464809387922287, -0.06556832790374756, 0.007819486781954765, -0.10814154893159866, 0.04180952534079552, 0.0720166265964508, -0.1481562852859497, -0.002614966593682766, -0.08781807124614716, -0.014561409130692482, -0.022289426997303963, 0.05249711498618126, 0.04662226140499115, -0.01828201860189438, 0.0464322566986084, 0.0853709727525711, -0.04250035062432289, 0.008175787515938282, 0.0003694749320857227, -0.05275457352399826, -0.05478522554039955, 0.03226633369922638, -0.020785165950655937, -0.05327777937054634, 0.07336252182722092, 0.033841777592897415, -0.027845235541462898, -0.0033297871705144644, -0.0238488856703043, -0.037448082119226456, 0.0424029715359211, 0.04584495350718498, 0.09546227753162384, 0.028369510546326637, 0.06599733233451843, -0.005267851520329714, 0.0503876768052578, -0.011873142793774605, -0.059888385236263275, 0.045513592660427094, 0.08966321498155594, 0.03446202725172043, -0.032904863357543945, -0.051043737679719925, -0.029864370822906494, -4.042809027831967e-33, -0.02209935151040554, -0.007957398891448975, 0.04376234859228134, 0.02394481934607029, -0.05019142106175423, 0.05915077403187752, -0.005074432585388422, 0.005225988104939461, -0.03517036885023117, -0.09354279935359955, -0.0346430167555809, 0.04902422055602074, -0.028072062879800797, 0.013648664578795433, 0.003133031539618969, -0.026776820421218872, -0.03774701803922653, 0.09284485131502151, -0.04301993176341057, -0.026898093521595, -0.004231862723827362, 0.011921025812625885, -0.01566937565803528, -0.04090871661901474, -0.030350979417562485, 0.05425601452589035, -0.03627821430563927, 0.07400407642126083, 0.005182455759495497, -0.002826054347679019, -0.08074165135622025, 0.04371514171361923, -0.014653620310127735, 0.004723940510302782, -0.018933691084384918, 0.006174510344862938, -0.00830757711082697, 0.039189647883176804, -0.08302643895149231, 0.08879690617322922, 0.03102727420628071, 0.03708978742361069, 0.05459517240524292, -0.08889377117156982, 0.04266120865941048, -0.06064169108867645, 0.011218461208045483, -0.04075481370091438, -0.07368767261505127, 0.03273128718137741, -0.0038303930778056383, -0.006957450415939093, -0.07998454570770264, -0.0752970278263092, -0.05475626885890961, 0.0940626859664917, -0.0012277299538254738, -0.03040100261569023, 0.09273122996091843, -0.028017880395054817, -0.007333056069910526, -0.030024198815226555, -0.07582984119653702, 0.05139540135860443, 0.07150271534919739, 0.10755808651447296, -0.01395976822823286, 0.0014620021684095263, 0.1378512680530548, -0.01711284928023815, 0.06680713593959808, 0.02924736961722374, -0.004657190293073654, -0.0540558360517025, 0.053417008370161057, 0.05878979340195656, -0.031634774059057236, 0.007187674753367901, -0.023617127910256386, 0.07422756403684616, -0.036349840462207794, -0.04182688146829605, 0.10604575276374817, -0.026353638619184494, 0.005523156374692917, 0.04386931285262108, -0.028131870552897453, -0.044123195111751556, 0.013820452615618706, -0.08709309250116348, -0.04331471771001816, -0.03423401340842247, 0.04821658506989479, -0.02699078805744648, 0.0457354374229908, -5.132491764925362e-08, -0.05193772539496422, 0.04619956389069557, 0.04754638671875, 0.030317863449454308, -0.005793162155896425, 0.024517573416233063, -0.006620808504521847, 0.008619937114417553, -0.07553403079509735, -0.045229945331811905, 0.01995743438601494, 0.04036549851298332, 0.08685313910245895, 0.061340536922216415, 0.04172457382082939, 0.025482261553406715, 0.05174072086811066, 0.02475809119641781, -0.033422209322452545, -0.059913311153650284, 0.06329391151666641, -0.019920550286769867, 0.03665957599878311, 0.07239729911088943, 0.04651789739727974, -0.05163171887397766, -0.04431229457259178, 0.10456333309412003, -0.005899939686059952, -0.024240516126155853, 0.07163983583450317, 0.06469788402318954, -0.014382908120751381, -0.08499577641487122, 0.019773975014686584, -0.023476433008909225, 0.014577408321201801, -0.07085103541612625, 0.015027681365609169, 0.05398065969347954, -0.027792133390903473, -0.006121453363448381, -0.026717644184827805, 0.08196444064378738, 0.05857847258448601, 0.015390141867101192, 0.13096459209918976, -0.11618684977293015, -0.047592610120773315, -0.06151944771409035, 0.01238265074789524, 0.08392705768346786, 0.03197718411684036, 0.10040464997291565, -0.046642206609249115, -0.01028447225689888, 0.016280492767691612, -0.031985096633434296, -0.11144601553678513, -0.0032718388829380274, 0.038432467728853226, 0.060164209455251694, -0.08541432023048401, -0.08152790367603302]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\NeuralNetReceiversinMultipleAccessCommunications.pdf,Deep Learning,"Connectionist Learning of Expert Preferences by Comparison Training Gerald Tesauro IBl\f Thomas.1. '''atson Rcsearc11 Centcr PO Box 704, Yorktown Heights, NY 10598 USA Abstract A new training paradigm, caned the ""eomparison pa.radigm,"" is introduced for tasks in which a. network must learn to choose a prdcrred pattern from a set of n alternatives, based on examplcs of Imma.n expert prderences. In this pa.radigm, the inpu t to the network consists of t.wo uf the n alterna tives, and the trained output is the expert's judgement of which pa.ttern is better. This para.digm is applied to the lea,rning of hackgammon, a difficult board ga.me in wllieh the expert selects a move from a. set, of legal mm·es. \Vith compa.rison training, much higher levels of performance can hc a .chiew~d, with networks that are much smaller, and with coding sehemes t.hat are much simpler and easier to understand. Furthermorf', it is possible to set up the network so tha.t it always produces consisten t rank-orderings. 1. Introduction There is now widespread interest, in tlH~ use of conncctillnist networks fllr real- world practical problem solving. The principal areas of applica.tion which have been studied so far involvc rela tiv('ly low-level signal processing and pattern recognition t.asks. However, eOllllectionist networks might also he useful in higher-level tasks whkh a.re curr('nt.l~· tackled hy cxprrt systems a.nd knowledge engineering approadl(,s [2]. In this pa per, 'vc considcr problem domains in which tlte expert is givcn a s('t of 71. alt.enulti,·es as input (71. may be either sma.II or large), and mnst select. t.}l<' m()st dC'sirtlhl(' ()T most prderable alternative. This type of task occurs rep('atccUy throughout the domains of politics, business, eeonomics, mcdicine, a.nd many ot hers. \Vhcthcr it is choosing a fureign-policy option, a. w('apons cont,rador, a course of trea.tment for a disease, or simply what to have for dinner, prohlems requiring choice are constan tly being faced and 801\'(\(1 by 111lma.n experts. How might a. learning system snch as a. cOlllH,ctionist network be set up to learn to ma.ke such choices from human expert cxampI~s? The immediately obvious a,pproa,ch is to train the network to produce a numerical output 99 100 Tesauro ""score"" for ea,ch input alterna.tive. To make a, choice, then, une would have the network score each a.lterna,tive, and select t.he alterna tive with the high- est score. Since the lea,rning system learns from examples, it. seems logical to train the network on a da.ta base of examples in which a, human expert has entered a numerical score for ea,ch possible choice. Howc\'cr, there are two major problems with such a,n a,pproa,ch. First, in many domains in which n is large, it would be tremendously time-consuming for the expert to create a, data base in which each individual a.lternative has been painstaking evalu- a,ted, even the vast number of obviously ba,d alterna.tives which are not even worth considering. (It is importa.nt for the network t,o sec examples of ba,d alternatives, otherwise it would tend t.o produce high scores for everything.) l\1:ore importa.ntly, in ma,ny domains human experts do not think in terms of absolute scoring functions, and it would thus be extremely difficult to create training data containing a,hsolute scores, because such scoring is alien to the expert's wa,y of thinking a,hout the problem. Inst.cad, the most natural way to make training data is simply tu record the expcrt in adinn, i.e., for ea,eh problem situation, record each of the tlltcrna,th'cs hc htld t.o choose from, a,nd record which one he actually selected. . For these reasons, we advoca.te teaching the network to compare pairs of alterna.tives, rather than scoring individual aIterna.tivcs. In ot.her words, the input should be two of the set of n alternatives, and thc output should be a 1 or 0 depending on which of the two alterna.tives is hetter. From a set of recorded huma,n expert preferences, one can then teach thc network that the expert's choice is better than all other a.ltcrna tivcs. One potential concern raised by this approach is tha t, in performance mode a,fter the network is trained, it might he neccssary t.o mtlke n2 comparisons to select the best alterna.tive, whereas only n individual scor('s tlrc needed in the other approa,ch. However, the network can select the hcst alterna.tive with only n compa.risons by going through the list of altern a tives in ordcr, a.nd compa,ring the current alternative wHh thc b('st ait.erntltive secn so fa.r. If the current alterna.tive is better, it becomcs the ncw 1>('st altcrnative, and if it is worse, it is rejected. Anot.hcr poten tial COIlcern is tha t tI. network which only knows how t.o compare might not produce a consist('nt rank-ordering, i.e., it might sa.y that alternativc a is better t.han h, b is hdtN t.han c, and c is better than a, and then one do('s not know which alt('rnative to select. However, we shall see la.ter that it is possiblc to gllartlutc'c (,(lllsist('ncy with a constrained a.rchitecture which forccs t.he network to c:()mc~ IIp wit.h absolute numerical scores for individual alterna tives. In the following, we shall exa.minc the applic~tI . ti()n of t.he comparison train- ing pa.radigm to the ga.me of backgammon, as considC'ra hIe cxperience ha.s already been obtained in this domain. In previous papC'rs [7,6]' a network was described which lea.rned to pla.y ba,ckgammon from an expert data, base, using the so-called ""back-proptl,ga.tion"" learning rule [5J. In that system, the network was trained to score individual moyes. In other words, the input Connectionist Learning of Expert Preferences 101 consists of a move (drfined by the initi::ll position bdnre the move a.nd the final position after the move), and the desired output is a. real number indi- ca.ting the strength of the move. Henceforth we shall refer to this training paradigm as the ""rcla.t.ive score"" par::ldigm. 'Vhilc this approach produced considerable success, it had a. number of serious limitations. \Ve sha.ll see that the compa.rison paradigm solyes one of the most important limita.tions of t.he previous a.pproach, with the rrsult tha t the overall performance of Ute network is much bet ter, the llum her of connedions required is greatly reduced, a.nd. the network's input coding scheme is much simpler and easier to understa.nd. 2. Previous backgammon networks In [7], a network was descrihed which learnrd tl) pla~· fairly good ba.ckga.m- mon by ba.ck-propa.gation learning of a. large expert t.raining set, using the rcla,tive score paradigm describc(l pr(,yiollsly. After tr::lining, the network was tested both by measuring its prrformance on ::I t('st sct. of positions not used in training, and hy a,ctual game pl::l)' against hunums ::Ind cOllventional computer programs. The best net.work was ablr to drfeat. Sun :Microsys- terns' Gammontool, Ute best available commercial program, hy a sllbsta.ntial ma.rgin, but it was still fa.r from human expert-level performance. The basic conclusion of [7] was that it was possihle to achieve decent levels of performance by this network learning procedure, but it, was not a.n ea.sy matter, and required suhst.antial hum::ln intervention. The choice of a. coding scheme for the inpnt informa.t.ion, for rxamplr, was fonn(l to be an extremely important issue. The hest coding SclH'IrleS contained a. great deal of doma.in- specific information. The best, encoding of the ""raw"" board iuforma.tion was in terms of concepts that 111lman exprrts use to describe loc::II tra.llsitions, such as ""slotting,"" ""stripping,"" et.c.. Also, a few ""pre-colllputrd features"" were required in addition t.o the raw board inform::lt.ion. Thus it ,,·as necessary to be a domain expert in order to clpsign a suit,a hIe ndwork I.oding sdteme, a.nd it seemed tha.t the only way t.o discov('r thr hrst cOlling scheme was by painstaking trial and error. This ""'::IS som('\dtat. disappoiut,ing, as it was hoped tha.t the net\vork lea rIling l)foc('(illre \y()ul<l alit lima t ically produl.e an expert ba.ckgammon net.work with little or no human ('ffort. 3. Comparison paradigul network set-up 111 the sta.nd.ard pra.ctice of ha.c:k-propag::lt.inn, a compfHisoIt paradigm net- work would ha.ve a.n input layer, (lIlC ()r morr layrTs of hidden nnits, and a.n output layer, wit.h full connectivity hdwcrn adjacell t l::lyers. The input. layer would represent, two final board posit.ions a and '"" and the out.put layer would ha.ve just a single unit t.o represent which board position was better. The 102 Tesauro teacher signal for the output unit would be a, 1 if hoard position (l was better than b, a,nd a 0 if b was better tha.n a. The proposed compa,rison pa.ra.digm network ""'ould m;ercome the limita,t.ion of only being able to consider individual moves in isolation, without knO\vl- edge of what other a.lt.erna,tives arc available. In addit.ion, t.he sophisticated coding scheme that was developed to encode transition informa tion would not be needed, since compa,risons could be based solely on the final board sta.tes. The comparison approa.ch offers grca ter sensitivity in dist.inguishing between close alternatives, a,nd as stated prc""iously, it corrcsp onds more closely to the actual form of human expert knowledge. These advantages a,re formida,ble, hut. t.here are some import.ant. problems with the a.pproach as curren t.ly descrihed. One technical prohlem is tlta,t the learning is significantly slower. This is beca,use 271, comparisons per t.ra.ining position a,re prcsen ted to the network, where 11. '"" 20, wh('rC'tls in tllC relative score approach, only about 3-4 moves per position would he presented. It was therefore necessary to develop a, number of technical tricks to increase the speed of the simulator code for t.hi.s specific a.pplica tion (to he described in a future publication). A more fundamental problem with the a.pproa(""h, however, is the issue of con- sistency of network comparisons. Two properties arc required for complete consistency: (1) The comparison between any two positions mllst be unam- biguous, i.e., if the network says that. a is bett('r t.han b when a is presented on the left a,nd b on the right, it !tact hetter say that. a is hetter than b if a is on the right and b is on the left. One ca.n show tha t t.his requircs t.he network's output to exaetly invert whenever Ule input. hO(lrd positions are swapped. (2) The compa,risons must he transitire, as alluded to previously, i.e., if a is judged better than h, and 1J is judged better than (', the network lta,d better judge a to be better than c. Sta.ndard unconstrained networks hayc no gUll ra n tee flf sa t.isfying eit.her of these properties. After some thought, howe\""('I, one realizc's that the output inversion symmetry can be enforcC'd by a symmetry rc'ltl tion amongst the weight.s in the network, and that the transiti""ity ancl rank-OHler consist.ency can be guaranteed by separability in the ardtitC'dure, as illllstratC'd in Figurc 1. Here we see tlla,t t.his network really consists of t.wo half-networks, one of which is only conceIllcd with t}w evaluat.ion ""fhoard positi.on (I, and the ot.her of which is concerned only wit.h t.h~ C',""alua t.iol\ of hOH.T() posit.inn b. (Duc to the indicated symmetry relation, nne neC'ds only store oue half-network in the simulator code.) Each half-network mrly have on(' or morc' lay('rs of hidden units, but as long as they a,re not cross-coupled, t.he C'valuat.ion of each of the two input boa,rd positions is hoilC'd down to a single rC'al number. Since real numbers always ra,nk-order consist.en tly, t.he ll('t\,'ork's compa,risons a,re al ways com;is ten t. Connectionist Learning of Expert Preferences 103 final position (a) final position (b) Figure 1: A network design for cOlTlpari~()n trailling wit h gnarallteed consis- tency of comparisons. \Veight groHps han~ ~YlTlmet.ry rc1ati()ns W 1 = W 2 and W 3 == - W 4, which ensures I hat the outPllt cxact.Jy in\Trts upon swap- ping positions in the input arraJ'. Separation or the hidden unils cOllrlenses the evaluation of each final hoard position into a single real IIIIJTlhcr, thus ensuring transitivity. An importa.nt a.dded benefit of t.his scheme is that an a.hsoillte hoard eval- ua.tion fundion is obtained in each half-network. This means t.hat the net- wurk, to the extcnt that it.s cvaluation fllnct.ioll is ac-cnrat.e, has an intrinsic understanding of a. given posit.ion, as opposed to HH'rely heing a hIe to de- tect features which correspond t.o good moves. As has heen emphasil':cd by Berliner [1], an intrinsic uudr.rst.a.llding of the position is crucial for play at the highest levels, a.n<l for use of t,he dOllhling cnl)('. Thnf-:, this a.pproach can serve as the ba.sis for fut.ure progress, whNeas thr previous approach of scoring moves was doomed eventually to run into a cirad rud. 4. Results of comparison training The training procedure for the comparison paradigm nC't.work was as follows: Networks """"'ere set up with 289 input. units which enc(l(le a (ic'scription of sin- gle final boa.rd position, varying nnmlH'rs (If hidden units, and a single output unit. The training data was taken from a set (If 40n gamrs ill which the au- thor played both sides. This <ia ta sd con hlins a recording of t.he a.n t,hor's preferred move for each position, and no ot.her comments. The engaged posi- tions in the da.ta set weIe selected out, (discngflged ra.cing I)()sit.ions were not studied) and divided int.o five cat<'gories: hearoff, bearin, opponent bearoff, opponent b~a.rin, a.nd a default ca t.egnry ('overing eVPlything else. In each 104 Tesauro Type of n.SP net, CP net _t~e_st_s~e~t _____ (~6_5_1-_1_2-_1~)~(_289-1)_ bcaroff .82 .83 hearin .54 .60 opp. bea.roff· .56 .54 opp. bearin .60 .66 other .58 .65 Table 1: Performance of neLs of indicated size on respedin~ test. sets, as mea- sllred by fraction of positions for which TIel agrees wit h lmTllrlll expert choice of best move. HSP: relative score paradigm, CP: comparison paradigm. category, 200 positions chosen (I.t. r(1UdOIU were set. (lside to he us('d as test- ing <lat(l.; tIte remaining da ta. (a bon t. 1000 positions in ('(I eh category except t.he dcfa.lllt category, for which a.bout 4000 positions were used) was used to tra.iu networks which spedalil':ed in each category. The learning algorithm llsed was st.a.nd(l.rd back-propagation with IIWInelltllIn a.nd without weight decay. Performa,nce after tr(lining is summaril':ed in Ta.hl('s 1 and 2. Ta.hle 1 gives the performance of each specialist network on the appropriate set of test positions. Results for the comparison par(ldigm nebYllrk are shown for net- works without hidden units, because it was fouud that the (lddition of hidden units did not improve the performance. (This is discussed in the following section.) \Ve contra.st. these results with result.s of tr(lining networks in the rela.tive score pa,radigm on the same tr(lilling da.t(l sds. ""~e see in Ta hIe 1 tha.t for the bearoff and oppon('nt. hearofi' speeialists, there is only a small cha.nge in perform(lnce under t.he contpa.rison p(lradigm. Fllr the hea.rin and opponcnt. bcarin specialists, t,herr is an improv('ment. in IH'rformance of a.bout 6 percenta.ge poin ts in each casco For t.his pa.rtkular applica tion, this is a. ""ery substantial improvement in perfoIIuance. How('y<'I, th(~ most, import(lnt find- ing is for the default ca tegory, which is much l(lrgcr and mor(' difficult than any of the specialist ea.t.egories. Th(' d('f(lult network's prrformance is the key factor in determining the syst('m's IIvrr(lll g(lme prrfOrIll(lllCe. 'Vit.h cum- pa.rison t.rainillg, we find an improvc'Ineur. in perform(lll('e from 58% t.o 65%. Given the sil':e and diffkllIt.y of t,his ca t('gory, t his ('(Ill onl)- he desnibcd as a. huge improvement, in performance, and is all t.h(' mllre rem ark(l hIe when nne considers th(lt. t,he comp(lrison p(I.radigm net has only 30() w('ights, (IS opposed to 8000 weight,s for the relative score paradigm net. Next, a combined game-playing s)-stC'ln was s('t. up llsing t.hr the specialist. nets for all engaged posit.ions. (The G(llTlmontool evaluat.ion function was called for ra.cing posit.ions.) Results are given in T(lble 2. Ag(linst, Gammon- tool itself, the pcrforma.nce und('r thr comparison p(lr(l(ligm improves from 59% to 64%. Against the a.uthor (and tcachE'r), the pC'rforma.nce improves from a.n estimat,ed 35% (since the nsp net.s (Ire so hig and slow, a.ccnra,te Opponent Ga,mmon tool TesauIO Connectionist Learning of Expert Preferences 105 RSP nets .59 (500 games) .35 (l00 ga,mes) CP nets .64 (2000 games) .42 (400 games) Table 2: Game-playing performance of composite network systems against Gammontool and against the author, as measured by fractioTl of games won, without counting gammons or backgammons. statistics could not be obtained) to about 42%. Qualitatively, one notices a subst.a,ntial overall improvement in the new net- work's level of pla,y. But what, is most, striking is the nct.work's In)lst case be- ha,vior. The previous relative-score network ha.d particularly bad worst-case beha,vior: about once every other ga,me, the network would make an atro- cious blunder wltich would seriously jeopardize its dlances of winning that ga,me [6]. An alarming fradion of these blunders ,,,,ere seemingly random and could not be logically explained. The new comparison para,digm network's wOIst-ca.se behavior is vastly improvcd in this rega,rd. The frcquency and severity of its mistakes are significantly reducE!d, hut more imp(lrtantly, its mista,kes a.re understandable. (Some of the improvement in this respect may be dne to the elimination of the noisy teacher signal descrihed in [7].) 5. Conclusions \Ve have seen that, in the domain of backgammon, the int.roduct.ion of the comparison training pa.ra,digm has result.ed in networks whkh perform much better, with vastly reduced numbers Ilfweights, a.ud with input. coding schemes tha.t a.re much simpler and easier t.o understand. It was surprising that snch high performa.nce could be obtained in ""perceptrou"" networks, i.E!., networks withou t hidden units. This reminds us t.hat. one should not. summarily dis- miss perceptrons as uninteresting or unworthy of study hecause they arc only ca.pable of lea.rning linearly separable funct.ions [3]. A su hst.an tial component of many difficult real-world problems may lie in the liu('arly separable spec- trum, and thus it makes sense to try perceptIons at least as a first attempt. It was also surprising tha.t the use (If hidden units in the comparison- trained networks does not improvc the pcrformanc('. This is nn('xplained, a.nd is t.he subject of current resea.rch. It is, however, not without precedent: in at least one other real-world application [4], it has heen found tha.t networks with hidden units do not pcrform any bettE'r than netwllrks without hidden units. ?vIore generally, one might conclude t.ha.t, in training a. neural network (or indeed a.ny learning system) from human expert. E'xamples in a cumplex do- main, there should he a. good match hetween Ute na t,ural form of the expert's knowledge and the method by which the net.work is trained. For domains in which the expert must seh~ct a preferred alt.crnative from a set of alternatives, 106 Tesauro the expert naturally thinks in terms of comparisons a.mongst the top few a.l- terna.tives, and the compa.rison paradigm proposed here takes advantage of that fact. It would be possihle in principle to train a network using absolute evaluations, hut the crea.tion of sueh a. training set might he too difficult to underta.ke on a large scale. If the above discussion is coned, then the comparisou pa.ra.digm should be useful in ot,her applications involving expert choice, and in other lea.rning syst,ems besides connectionist networks. Typically expert systems a.re hand- crafted by knowledge engineers, ra.ther than learned from human expert ex- amples; however, there has recently been some interest in sl1pervis(~d lea.rning approa.ches. It will be interesting to see if the compa.rison paradigm proves to be useful when supervised lea.rning procedures are applied t,o otller domains involving expert choice. In using the compa.rison paradigm, it. will be impor- tant to ha.ve some way to gua.ra.nt,ee that the syst,em's comparisons will be unambiguous and t,ra.nsitive. For feed-forward networks, it was shown in this pa.per how to gua.rantee this using symmetric, separa.t.ed nrtworks; it should be possible to impose similar constraints Oil otll<'r learning systems to enforce consistency. References [l} H. Berliner, ""On the construction or c\""aluation runctions for large domains,"" Proc. of IleA T (1979) 53--55. [2] S. 1. Gallant, ""Conncctionist r.xpert systems,"" Comm. l\eM 31, 152 --169 (1988). P} M. Minsky and S. Papcrt, Pcrceptrons, MIT Press, Camhridge ~fA (1969). [,1] N. Qian and T. J. Sejnowski, ""Predicting the' secondary slrucl,nre of globular proteins using neural nctwork models,"" .1. Mol. Bioi. 202, R(jfi 8R,1 (1988). [5] D. E. Rumelarl and J. L. ~lcClellanrl (cds.), Parallel J>i.<;frifllltecl Processing: Explorations in I.Ire Microsirucl.lIl'r or Cogn;t,;on, Vt)ls. I and 2, l\f1T Prrss, Cambrioge MA (1986). [6} G. Tesauro, ""Neural network ddral.s creal.or in hack~annl1(ln match."" Univ. of ll1inoiR, Center for Complex S~' strll1s Technical Heporl. CCSl1-8R-6 (1988). [71 G. Tesauro and T. J. Sejnowski, ""/\ parallel nei.work that ]rarns to play backgammon,"" A rtificialTntell;gmrcc, in press (H)89).","[-0.028837572783231735, -0.0646638497710228, -0.049333468079566956, -0.03752001374959946, -0.0810503140091896, -0.023652955889701843, 0.03957105800509453, 0.1063484475016594, -0.043964460492134094, 0.05429892614483833, -0.02802260033786297, 0.04472540691494942, 0.0447465218603611, -0.026646915823221207, -0.038784392178058624, 0.06298764050006866, 0.051633600145578384, -0.0009403282310813665, -0.04972873255610466, -0.017643623054027557, -0.05035422742366791, -0.07231085002422333, -0.030345313251018524, -0.08288133889436722, 0.0036425869911909103, 0.0045090410858392715, -0.005728603806346655, 0.04654888063669205, 0.042896006256341934, -0.03748149424791336, -0.004418413620442152, 0.06880327314138412, 0.04210168123245239, 0.012914340943098068, -0.0603877492249012, 0.048019878566265106, -0.08543660491704941, 0.03299388661980629, 0.004043153021484613, 0.07121418416500092, -0.03685154393315315, 0.02438468113541603, 0.012427061796188354, 0.014984280802309513, -0.015657737851142883, 0.06403838843107224, -0.07411354780197144, 0.031568631529808044, -0.07380350679159164, -0.0451168566942215, -0.12844549119472504, 0.009558550082147121, -0.026322565972805023, -0.000798282737378031, -0.0007829460664652288, 0.013261578045785427, -0.012235506437718868, 0.044215232133865356, -0.04396475851535797, -0.0023026035632938147, -0.002737092087045312, -0.027286002412438393, -0.11487434804439545, -0.01945468783378601, 0.06367127597332001, -0.028102772310376167, 0.012906254269182682, 0.031534697860479355, 0.005718540865927935, -0.07414303719997406, 0.03163384646177292, 0.0019529780838638544, -0.11039923131465912, 0.06522151082754135, 0.06188643351197243, 0.017568960785865784, 0.0829431489109993, 0.02642662078142166, 0.02636612392961979, -0.03610893338918686, 0.0031425466295331717, 0.10263065248727798, -0.027330029755830765, -0.022621983662247658, 0.017301635816693306, -0.05296019837260246, -0.0874316617846489, 0.01688229851424694, 0.11032643169164658, -0.013545111753046513, 0.05102010816335678, -0.009229842573404312, -0.02273494563996792, 0.007178375497460365, 0.09356480836868286, 0.08249914646148682, -0.011377000249922276, 0.008897317573428154, -0.04975728318095207, 0.10633018612861633, -0.0015209353296086192, -0.02980891428887844, -0.08833930641412735, -0.029148157685995102, -0.016252940520644188, 0.031053144484758377, 0.03929295390844345, 0.005433737765997648, 0.1004846841096878, -0.02365855872631073, -0.05269536375999451, 0.016751045361161232, -0.035127706825733185, -0.052899282425642014, -0.04475948214530945, 0.010616477578878403, -0.02097322978079319, 0.0862661749124527, 0.0432225801050663, 0.09243264049291611, -0.04707074537873268, -0.021745342761278152, -0.01322281826287508, 0.00894184224307537, 0.055562861263751984, -0.012497507967054844, -0.048214249312877655, 6.848723961478353e-33, -0.011683328077197075, 0.018828388303518295, -0.05798887833952904, -0.030909564346075058, 0.06527120620012283, 0.010020178742706776, 0.029202492907643318, -0.058281466364860535, 0.003992503043264151, -0.005807074718177319, -0.09871012717485428, 0.035361066460609436, 0.002167622558772564, 0.038769323378801346, 0.08819670230150223, -0.04390560835599899, 0.01671440154314041, -0.027992146089673042, -0.055215902626514435, -0.01764357089996338, 0.07033368945121765, -0.041976746171712875, 0.041337307542562485, -0.03729528561234474, -0.02076348289847374, -0.05339530110359192, 0.019792916253209114, 0.020846543833613396, 0.04980214312672615, 0.018429050222039223, -0.011995927430689335, -0.05299468711018562, -0.02487466298043728, 0.003911982290446758, 0.0546736977994442, -0.008738867938518524, 0.05303710699081421, -0.04677150025963783, 0.003811160335317254, -0.06319653987884521, -0.0832756757736206, 0.0009126840741373599, 0.028944768011569977, 0.03345523402094841, -0.02481698989868164, 0.006868784315884113, 0.009316368028521538, -0.007367987651377916, -0.09440924972295761, -0.031040918081998825, -0.021074090152978897, -0.006123634986579418, 0.01844797655940056, -0.06835713237524033, 0.0823267251253128, 0.022589698433876038, 0.022732358425855637, 0.10979591310024261, 0.07970108091831207, 0.08975569158792496, -0.009871447458863258, 0.06493708491325378, 0.02705538645386696, 0.07691414654254913, 0.042642395943403244, 0.04143952205777168, -0.08448678255081177, -0.0392334870994091, 0.11745902895927429, -0.04215046018362045, -0.051382556557655334, 0.039920296519994736, -0.005941077601164579, -0.027582990005612373, 0.017019907012581825, 0.015825295820832253, -0.005568049382418394, 0.04475525766611099, -0.02323146164417267, -0.0026296996511518955, -0.061445433646440506, 0.10269119590520859, -0.10507092624902725, -0.04273580014705658, 0.0010450530098751187, -0.003266974352300167, 0.051903534680604935, -0.07132534682750702, 0.06906523555517197, 0.07786700874567032, -0.0034941446501761675, 0.013268176466226578, -0.04925445467233658, 0.052839718759059906, 0.04405500739812851, -4.914640127058241e-33, -0.09053362160921097, -0.003650394268333912, -0.07518452405929565, 0.06580917537212372, -0.0068304226733744144, -0.055985815823078156, -0.01323242299258709, -0.06036074832081795, 0.004229087382555008, -0.05255327373743057, -0.023161914199590683, -0.01143098995089531, -0.003381699090823531, -0.034852396696805954, 0.024351727217435837, -0.0654328316450119, -0.04711739346385002, 0.013317129574716091, 0.01809459552168846, 0.01383132766932249, 0.04331030324101448, 0.0799483209848404, -0.14799772202968597, -0.02112545445561409, 0.0036417520605027676, -0.002307640155777335, -0.0708678737282753, 0.09300215542316437, -0.033820632845163345, 0.04491094499826431, -0.012339325621724129, -0.028934111818671227, -0.04203196242451668, 0.06231645867228508, 0.009184161201119423, 0.11223401129245758, 0.05980440229177475, 0.049801670014858246, -0.038781724870204926, 0.07648889720439911, 0.02654099091887474, 0.0008015334606170654, -0.029408924281597137, -0.01675812155008316, -0.023369798436760902, -0.020401205867528915, -0.07244811207056046, -0.05535881593823433, 0.0048859454691410065, -0.006676477380096912, 0.07007577270269394, 0.02443113923072815, 0.04442496597766876, -0.03909963369369507, -0.04689157009124756, 0.023052502423524857, 0.03382853791117668, -0.03402591496706009, -0.008211583830416203, 0.002950641792267561, 0.09041962772607803, -0.08015777170658112, -0.04296967014670372, 0.12063493579626083, 0.06124954670667648, 0.03049653209745884, -0.07493066042661667, 0.053148236125707626, 0.04546559974551201, 0.003768032183870673, -0.04946715757250786, -0.022251741960644722, 0.026286518201231956, -0.09519760310649872, 0.041244473308324814, 0.0016976564656943083, -0.04039900377392769, 0.05850224569439888, -0.05357985571026802, 0.014971855096518993, -0.07011765986680984, 0.005377165041863918, -0.041467297822237015, 0.03319366276264191, 0.04668498411774635, 0.1131899505853653, 0.07957271486520767, 0.03305111080408096, 0.03836682438850403, -0.0791897177696228, 0.017048710957169533, -0.005806904286146164, 0.051826149225234985, 0.008660086430609226, -0.043082352727651596, -5.874993647125848e-08, -0.06143262982368469, -0.09049224108457565, 0.017351076006889343, 0.057317234575748444, 0.1218232735991478, 0.0030498988926410675, -0.06261401623487473, -0.021446678787469864, -0.09534727036952972, 0.0858847051858902, 0.04435122385621071, 0.05215556174516678, -0.09209243208169937, -0.006412279326468706, 0.09895551949739456, 0.03207669034600258, 0.02280808985233307, 0.011844372376799583, -0.04984181001782417, 0.0408882200717926, 0.13750404119491577, 0.0010489043779671192, -0.007492860313504934, 0.06116513907909393, -0.014107951894402504, -0.07187297195196152, -0.009428300894796848, 0.04225558415055275, -0.06767670810222626, 0.09317364543676376, -0.026307491585612297, -0.010679518803954124, 0.06449637562036514, -0.004039029125124216, 0.009359240531921387, 0.06461665034294128, 0.012565983459353447, -0.0006949133239686489, -0.05709743872284889, 0.03701288625597954, -0.037036359310150146, -0.041378531605005264, 0.016441449522972107, 0.035705722868442535, 0.0013898226898163557, -0.040885332971811295, -0.01269562914967537, -0.0942198857665062, 0.031606525182724, -0.07236230373382568, 0.01913992501795292, 0.016525527462363243, -0.004533284343779087, -0.02408347651362419, 0.08348297327756882, -0.022777900099754333, 0.029738489538431168, -0.033750779926776886, -0.0769665464758873, 0.0665825828909874, -0.040139757096767426, 0.08995910733938217, -0.00237559387460351, -0.009314531460404396]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\NeuralNetworkRecognizerforHandWrittenZipCodeDigits.pdf,Computer Vision,"728 DIGITAL REALISATION OF SELF-ORGANISING MAPS Nigel M. Allinson M~rtin J. Johnson Department of Electronics University of York York Y015DD England ABSTRACT Kevin J. Moon A digital realisation of two-dimensional self-organising feature maps is presented. The method is based on subspace classification using an n-tuple technique. Weight vector approximation and orthogonal projections to produce a winner- takes-all network are also discussed. Over one million effective binary weights can be applied in 25ms using a conventional microcomputer. Details of a number of image recognition tasks, including character recognition and object centring, are described. INTRODUCTION Background The overall aim of our work is to develop fast and flexible systems for image recognition, usually for commercial inspection tasks. There is an urgent need for automatic learning systems in such applications, since at present most systems employ heuristic classification techniques. This approach requires an extensive development effort for each new application, which exaggerates implementation costs; and for many tasks, there are no clearly defined features which can be employed for classification. Enquiring of a human expert will often only produce ""good"" and ""bad"" examples of each class and not the underlying strategies which he may employ. Our approach is to model in a quite abstract way the perceptual networks found in the mammalian brain for vision. A back-propagation network could be employed to generalise about the input pattern space, and it would find some useful representations. However, there are many difficulties with this approach, since the network structure assumes nothing about the input space and it can be difficult to bound complicated feature clusters using hyperplanes. The mammalian brain is a layered structure, and so another model may be proposed which involves the application of many two-dimensional feature maps. Each map takes information from the output of the preceding one and performs some type of clustering analysis in order to reduce the dimensionality of the input information. For successful recognition, similar patterns must be topologically close so that Digi tal Realisation of Self-Organising Maps 729 novel patterns are in the same general area of the feature map as the class they are most like. There is therefore a need for both global and local ordering processes within the feature map. The process of global ordering in a topological map is termed, by Kohonen (1984), as self-organisation. It Is important to realize that all feedforward networks perform only one function, namely the labelling of areas in a pattern space. This paper concentrates on a technique for realising large, fast, two-dimensional feature maps using a purely digital implementation. Figure 1. Unbounded Feature Map of Local Edges Self Organisation Global ordering needs to adapt the entire neural map, but local ordering needs only local information. Once the optimum global organisation has been found, then only more localised ordering can improve the topological organisation. This process is the basis of the Kohonen clustering algorithm, where the specified area 730 Johnson, Allinson and Moon of adaption decreases with time to give an increasing local ordering. It has been shown that this approach gives optimal ordering at global and local levels (Oja, 1983). It may be considered as a dimensionality reduction algorithm, and can be used as a vector quantiser. Although Kohonen's self-organising feature maps have been successfully applied to speech recognition (Kohonen, 1988; Tattersall et aI., 1988), there has been little Investigation in their application for image recognition. Such feature maps can be used to extract various image primitives, such as textures, localised edges and terminations, at various scales of representations (Johnson and Allinson, 1988). As a simple example, a test image of concentric circles is employed to construct a small feature map of localised edges (Figure 1). The distance measure used is the normalised dot product since in general magnitude information is unimportant. Under these conditions, each neuron output can be considered a similarity measure of the directions between the input pattern and the synaptic weight vector. This map shows that similar edges have been grouped together and that inverses are as far from each other as possible. DIGITAL IMPLEMENTATION Sub-Space Classification Although a conventional serial computer is normally thought of as only performing one operation at a time, there is a task which it can successfully perform involving parallel computation. The action of addressing memory can be thought of as a hi&JhlY parallel process, since it involves the comparison of a word, W, with a set ~ 2 others where N is the number of bits in W. It is, in effect, performing 2 parallel computations - each being a single match. This can be exploited to speed up the simulation of a network by using a conversion between conventional pattern space labelling and binary addressing. Figure 2 shows how the labelling of two-dimensional pattern space is equivalent to the partitioning of the same space by the decision regions of a multiple layer perceptron. If each quantised part of the space is labelled with a number for each class then all that is necessary is for the pattern to be used as an address to give the stored label (i.e. the response) for each class. These labels may form a cluster of any shape and so multiple layers are not required to combine regions. The apparent flaw in the above suggestion is that for anything other than a trivial problem, the labelling of every part of pattern space is impractical. For example a 32 x 32 input vector would require a memory of 21024 words per unit! What is needed is a coding system which uses some basic assumptions about patterns in order to reduce the memory requirements. One assumption which can be made is that patterns will cI uster together into various classes. As early as 1959, a method known as the n-tuple technique was used for pattern recognition (Bledsoe and Browning, 1959). This technique takes a number of subspaces of the pattern x2 l Digital Realisation of Self-Organising Maps 731 PERCEPTRON ~=C1 =c2 I' I,' ,I,' ~I,J::-I' 1.-1-1·' 1' 1· I·' I· ~~t:' ~~~ I.;:I~'I~I-::I ' I, I· ~ ~~ ~~ I~r.-r~ I .· ~ .• "" I""~ ~~~ ~~~~ ~ I.' i' I, I.' . ~ ~ I~. , ... 010 I. ' .' I, ~ 1:\ , I· • .t It I-. , ~~ ~~ , ' • I· ,- II • I_ i "" "" 1--1' ~~ ~ • • ~'I 1'1 • '. , 1,' 1' ~ ~ .' f .ltl II • . ' I' • ~ . . ~. 1411. - I,' , I"" I,' I- I. •• Itili • 1.'1, I· i· - It It 11111. ·1· 1·1·1- • = Class 1 0 = Class 2 c1/c2 x1 x2 LABELING The labeling of a quantized subspace is equivalent to the partitioning of pattern space by the multi-layer perceptron. Figure 2. Comparison of Perceptron and Sub-Space Classification space and uses the sum of the resultant labels as the overall response. This gives a set of much smaller memories and inherent in the coding method is that similar patterns will have identical labels. For example, assume a 16 bit pattern - 0101101001010100. Taking a four-bit sample from this, say bits 0-3, giving 0100. This can be used to address a 16 word memory to produce a single bit. If this bit is set to 1, then it is in effect labelling all patterns with 0100 as their first four bits; that is 4096 patterns of the form xxxxxxxxxxxx0100. Taking a second sample, namely bits 4-7 (0101). This labels xxxxxxxx0101xxxx patterns, but when added to the first sample there will be 256 patterns labelled twice (namely, xxxxxxxx01010100) and 7936 (Le. 8192-256) labelled once. The third four-bit sample produces 16 patterns (namely, 732 Johnson, Allinson and Moon xxx(101001010100) labelled three times. The fourth sample produces only one pattem 0101101001010100, which has been labelled four times. If an input pattern is applied which differs from this by one bit, then this will now be labelled three times by the samples; if it differs by two bits, it will either be labelled two or three times depending on whether the changes were in the same four-bit sample or not. Thus a distance measure is implicit in the coding method and reflects the assumed clustering of patterns. Applying this approach to the earlier problem of a 32 x 32 binary input vector and taking 128 eight-bit samples results in a distance measure between 0 and 128 and uses 32K bits of memory per unit. Weight Vector Approximation It is possible to make an estimate of the approximate weight vector for a particular sample from the bit table. For simplicity, consider a binary image from which t samples are taken to form a word, w, where t-1 w = xo + 2x1 + .... + 2 ~-1 This word can be used to address a vector W. Every bit in W[b] which is 1 either increases the weight vector probability where the respective bit in the address is set, or decreases if it is clear. Hence, if BIT [w,i] is the ith bit of wand A[i] is the contents of the memory {O, 1} then, 2t-1 W[b] = E A[i] (2 BIT(b,i) -1) i = 0 This represents an approximate measure of the weight element. Table 1 demonstrates the principle for a four-bit sample memory. Given randomly distributed inputs this binary vector is equivalent to the weight vector [2, 4, 0, -2]. If there is a large number of set bits in the memory for a particular unit then that will always give a high response - that is, it will become saturated. However, if there are too few bits set, this unit will not rfiSpond strongly to a general set of patterns. The number of bits must, therefore, be fixed at the start of training, distributed randomly within the memory and only redistribution of these bits allowed. Set bits could be taken from any other sample, but some samples will be more important than others. The proportion of 1's in an image should not be used as a measure, otherwise large uniform regions will be more significant than the pattern detail. This is a form of magnitude independent operation similar to the use of the normalised dot product applied in the analogue approach and so bits may only be moved from addresses with the same number of set bits as the current address. Digital Realisation of Self-Organising Maps TABLE 1. Weight Vector Approximation Address Weight change Address Weight change X3 x2 x, Xo A W3 W2 W, W x3 x2 x, Xo A W3 W2 W, Wo 0 0 0 0 0 0 1 0 0 0 1 + - 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 a 0 1 1 1 + + 1 0 1 1 0 0 1 0 0 1 + - 1 1 0 0 1 + + - 0 1 0 1 0 1 1 0 1 1 + + - + 0 1 1 0 1 + + - 1 1 1 0 1 + + + - 0 1 1 1 0 1 1 1 1 1 + + + + Equivalent weight vector 2 4 0-2 Orthogonal Projections In order to speed up the simulation further, instead of representing each unit by a single bit in memory, each unit can be represented by a combination of bits. Hence many calculations can be effectively computed in parallel. The number of units which require a 1 for a particular sample will always be relatively small, and hence these can be coded. The coding method employed is to split the binary word, W, into x and y fields. These projection fields address a two dimensional map and so provide a fast technique of approximating the true content of the memory. The x bits are summed separately to the y bits, and together they give a good estimate of the unit co-ordinates with the most bits set in x and in y. This map becomes, in effect, a winner-takes-all network. The reducing neighbourhood of adaption employed in the Kohonen algorithm can also be readily incorporated by applying an overall mask to this map during the training phase. Though only this output map is required during normal application of the system to image recognition tasks, it is possible to reconstruct the distribution of the two- dimensional weight vectors. Figure 3, using the technique illustrated in Table 1, shows this weight vector map for the concentric circle test image applied 733 734 Johnson, Allinson and Moon Figure 3. Reconstructed Feature Map of Local Edges previously in the conventional analogue approach. This is a small digitised map containing 32 x 32 elements each with 16 x 16 input units and can be applied, using a general purpose desktop microcomputer running at 4 mips, in a few milliseconds. APPLICATION EXAMPLES Character Recognition Though a long term objective remains the development of general purpose computer vision systems, with many layers of interacting feature maps together with suitable pre- and post-processing, many commercial tasks require decisions based on a constricted range of objects - that is their perceptual set is severely limited. However, ease of training and speed of application are paramount. An example of such an application involves the recognition of characters. Figures 4 and 5 show an input pattern of hand-drawn A's and B's. The network, using the above digital technique, was given no information concerning the input image and the input window of 32 x 32 pixels was placed randomly on the image. ,The network took less than one minute to adapt and can be applied in 25 ms. This network is a 32 x 32 feature map of 32 x 32 elements, thus giving over one million effective weights. The output map forms two distinct clusters, one for A's in the top right corner of the map (Figure 4), and one for B's in the bottom left corner (Figure 5). If further characters are introduced in the input image then the output map will, during the training phase, self-organise to incorporate them. Digital Realisation of Self-Organising Maps 735 Figure 4. Trained Network Response for 'A' in Input Window Figure 5. Trained Network Response for 'B' in Input Window 736 Johnson, Allinson and Moon Corrupted Images Once the maximum response from the map is known, then the parts of the input window which caused it can be reconstructed to provide a form of ideal input pattern. The reconstructed input pattern is shown in the figures beneath the input image. This reconstruction can be employed to recognise occuluded patterns or to eliminate noise in subsequent input images. Figure 6. Trained Network Response for Corrupted 'A' in Input Window. Reconstructed Input Pattern Shown Below Test Image Figure 6 shows the response of the network, trained on the input image of Figures 4 and 5, to a corrupted image of A's and B's. It has still managed to recognise the input character as an A, but the reconstructed version shows that the extra noise has been eliminated. Object Centring The centering of an object within the input window permits the application of conformant mapping strategies, such as polar exponential grids, to be applied which yields scale and rotation invariant recognition. The same network as employed in the previous example was used, but a target position for the maximum network response was specified and the network was adapted half-way between this and the actual maximum response location. Digital Realisation of Self-Organising Maps 737 Figure 7. Trained Network Response for Off-Centred Character. Input Window is Low-Pass Filtered as shown. Figure 7 shows such a network. When the response is in the centre of the output map then an input object (character) is centred in the recognition window. In the example shown, there is an off-centred response of the trained network for an off- centred character. This deviation is used to change the position of the input window. Once centering has been achieved, object recognition can occur. CONCLUSIONS The application of unsupervised feature maps for image recognition has been demonstrated. The digital realisation technique permits the application of large maps. which can be applied in real time using conventional microcomputers. The use of orthogonal projections to give a winner-take-all network reduces memorY requirements by approximately 3D-fold and gives a computational cost of O(n 1/2), where n is the number of elements in the map. The general approach can be applied in any form of feedforward neural network. Acknowledgements This work has been supported by the I nnovation and Research Priming Fund of the University of York. 738 Johnson, Allinson and Moon References W. W. Bledsoe and I. Browning. Pattern Recognition and Reading by Machine. Proc. East. Joint Compo Conf., 225-232 (1959). M. J. Johnson and N. M. Allinson. An Advanced Neural Network for Visual Pattern Recognition. Proc. UKIT 88, Swansea, 296-299 (1988). T. Kohonen. Self Organization and Associative Memory. Springer-Vertag, Bertin (1984). T. Kohonen. The 'Neural' Phonetic Typewriter. Computer21,11-22 (1988). E. Oja. Subspace Methods of Pattern Recognition. Research Studies Press, Letchworth (1983). G. D. Tattersall, P. W. Linford and R. Linggard. Neural Arrays for Speech Recognition. Br. Telecom Techno/. J. Q. 140-163 (1988).","[0.01939236931502819, 0.016272153705358505, 0.01853283680975437, -0.08629073202610016, -0.06011446565389633, 3.4674965718295425e-05, 0.02517460100352764, -0.010772253386676311, -0.10690465569496155, -0.022327866405248642, 0.008672119118273258, -0.019327273592352867, 0.06660936772823334, 0.03223855048418045, -0.10039679706096649, -0.054860956966876984, -0.008923981338739395, 0.06003377586603165, -0.028067398816347122, -0.029872655868530273, 0.005111544393002987, -0.04976457729935646, -0.022071881219744682, -0.028297998011112213, 0.0011639406438916922, 0.051912687718868256, 0.037186600267887115, -0.025073261931538582, -0.026932070031762123, -0.08679892122745514, 0.05174911394715309, -0.038027651607990265, 0.06384862959384918, -0.008753059431910515, -0.00923803262412548, 0.03441286459565163, -0.015459952875971794, 0.02042386867105961, -0.09166871011257172, -0.046338148415088654, -0.03596482053399086, -0.0393647775053978, -0.012536055408418179, 0.004288546275347471, 0.10371065139770508, 0.12359198182821274, 0.04028553143143654, 0.007206704001873732, 0.058080025017261505, -0.01759154535830021, -0.07256373018026352, 0.03814677521586418, -0.05016731470823288, 0.04956354573369026, -0.06613364815711975, -0.002562914974987507, 0.05922743305563927, -0.03518397733569145, -0.0062857759185135365, -0.020916081964969635, 0.04335956647992134, -0.034517716616392136, 0.09101059287786484, 0.0074180918745696545, 0.055868010967969894, 0.034251656383275986, 0.009042627178132534, 0.023406604304909706, 0.057360485196113586, -0.0597161129117012, 0.05542217195034027, 0.01481037586927414, 0.004402088932693005, 0.012460743077099323, 0.025733821094036102, 0.009374932385981083, -0.006174859590828419, -0.012563789263367653, 0.06080141291022301, 0.0001824033970478922, -0.04510458931326866, 0.007978744804859161, 0.0163197610527277, 0.01834605447947979, 0.04206283017992973, 0.031193271279335022, -0.11271058768033981, -0.005065970588475466, -0.020536886528134346, 0.01283272448927164, -0.008995971642434597, -0.05306186527013779, -0.014591776765882969, -0.08250583708286285, 0.033975593745708466, -0.07579921185970306, 0.06261860579252243, -0.015053769573569298, -0.028999771922826767, 0.04932010918855667, 0.0059674461372196674, -0.04450340196490288, 0.053730789572000504, -0.012437558732926846, 0.07124098390340805, 0.037802089005708694, 0.029959458857774734, -0.056616753339767456, 0.08946343511343002, -0.17905215919017792, -0.02724229358136654, -0.0947563499212265, -0.12006063759326935, -0.05748985335230827, 0.008756387047469616, -0.02312380075454712, 0.0083844605833292, 0.07447127252817154, 0.040427107363939285, -0.00932695809751749, -0.03929302096366882, -0.05120590329170227, 0.01241283304989338, -0.009692295454442501, 0.07189461588859558, 0.00786686409264803, -0.13733844459056854, 5.5977874852462446e-33, -0.018030697479844093, 0.07322610169649124, -0.027242278680205345, -0.017819201573729515, -0.0009272147435694933, -0.04172256961464882, -0.004005420953035355, 0.08019157499074936, 0.04917193204164505, 0.027346407994627953, 0.028669200837612152, 0.056768350303173065, -0.006286518648266792, 0.12426289916038513, 0.12344567477703094, 0.022960683330893517, 0.03941380977630615, 0.005033901892602444, 0.0277359988540411, -0.10139556974172592, -0.04446453973650932, -0.04556472972035408, 0.02504764124751091, -0.04544970020651817, 0.024107856675982475, -0.01381612103432417, 0.01980016939342022, -0.02608124166727066, 0.009214375168085098, 0.002830322366207838, 0.020235558971762657, 0.024022577330470085, 0.006791223771870136, -0.01166999340057373, 0.015246565453708172, -0.07042627781629562, -0.03522932529449463, 0.038145266473293304, 0.07570984959602356, 0.03263462334871292, 0.006159872282296419, -0.015465723350644112, 0.022218165919184685, -0.0769343450665474, -0.05095464736223221, 0.04500925913453102, 0.010306626558303833, 0.09137368947267532, -0.030439449474215508, 0.024786056950688362, 0.031693652272224426, 0.017910463735461235, -0.09718712419271469, -0.0021923743188381195, 0.03423471376299858, 0.00901600532233715, -0.01329402718693018, 0.0632968544960022, 0.043773382902145386, 0.09049811214208603, -0.01588168926537037, 0.06004792079329491, 0.021403862163424492, 0.09598873555660248, 0.005150167737156153, -0.09480281174182892, 0.08967119455337524, 0.008659453131258488, -0.0013666065642610192, -0.0036557333078235388, -0.07685302197933197, 0.021688595414161682, 0.005309918895363808, -0.08492831885814667, 0.05768924951553345, 0.007060471922159195, 0.048167530447244644, -0.06860911101102829, -0.09839330613613129, -0.02347794733941555, -0.040900032967329025, 0.0564168281853199, 0.046507351100444794, -0.07501884549856186, -0.04744065925478935, 0.009477293118834496, 0.07399363070726395, -0.07551831752061844, -0.007309858221560717, 0.01457168348133564, -0.054133493453264236, 0.0712168738245964, -0.04697295278310776, 0.11959215998649597, -0.04839788377285004, -5.129380169750426e-33, -0.04600519314408302, 0.04094556346535683, -0.007934554480016232, 0.006096828728914261, 0.04784692823886871, -0.08467990905046463, 0.009035106748342514, 0.0030309506691992283, -0.031713876873254776, 0.034790314733982086, -0.051651593297719955, 0.01123632863163948, 0.028875403106212616, -0.002364247338846326, -0.027171630412340164, 0.025150002911686897, -0.03943049535155296, 0.008368400856852531, 0.03686533123254776, 0.018167920410633087, -0.05449013411998749, 0.08831211924552917, -0.06217615678906441, 0.01157488115131855, -0.11731856316328049, 0.06239967793226242, -0.00018454341625329107, 0.01166185550391674, 0.061469752341508865, 0.05759416148066521, -0.05095291882753372, -0.03820260241627693, -0.03338516876101494, 0.014690619893372059, -0.0055787451565265656, -0.013570889830589294, 0.004655353259295225, -0.013373132795095444, 0.02992434985935688, 0.07433869689702988, 0.02435171976685524, 0.04524082690477371, -0.10316066443920135, -0.11020659655332565, -0.047471094876527786, -0.11045080423355103, -0.02010730654001236, 0.09371685236692429, 0.01220130268484354, 0.09645000845193863, -0.02852208912372589, -0.006645532790571451, -0.056844424456357956, -0.0017029691953212023, 0.018138404935598373, 0.060021139681339264, -0.01826798915863037, 0.036261674016714096, 0.05609484389424324, 0.04064984992146492, -0.10250195860862732, -0.09579010307788849, 0.06337347626686096, 0.02606462687253952, -0.01923467218875885, -0.045084137469530106, 0.009155157022178173, 0.018577875569462776, -0.06583818793296814, 0.07917708903551102, -0.001226138207130134, 0.013391145505011082, 0.023539571091532707, -0.06637541204690933, -0.04674408212304115, -0.04645857587456703, -0.015333162620663643, -0.015290467068552971, 0.05965087562799454, -0.016201762482523918, -0.060712751001119614, -0.04778413474559784, 0.018274880945682526, 0.10493926703929901, 0.017878485843539238, 0.09268965572118759, 0.0467948280274868, 0.00901161227375269, 0.07685976475477219, -0.006562158465385437, 0.10612735897302628, 0.09038510173559189, -0.01272172573953867, 0.0970219299197197, -0.04828327149152756, -5.351076026727242e-08, -0.06988673657178879, -0.04995090141892433, 0.05483810231089592, -0.016814421862363815, 0.06032753363251686, -0.047291189432144165, 0.06439893692731857, 0.05008980631828308, -0.03587914630770683, -0.0272075105458498, 0.0632631778717041, -0.0489862821996212, -0.05837582051753998, -0.043858930468559265, 0.0033078028354793787, 0.024914022535085678, -0.034249112010002136, 0.04347795993089676, -0.042519956827163696, 0.009288660250604153, 0.06391491740942001, -0.062128592282533646, 0.02183574065566063, 0.027349917218089104, -0.055770326405763626, -0.02554473839700222, -0.03128691390156746, -0.024922257289290428, 0.026436597108840942, 0.059508901089429855, -0.0367683544754982, 0.061602793633937836, 0.0371134951710701, -0.01999487727880478, 0.08355572074651718, -0.03813067078590393, 0.020713653415441513, 0.02686002105474472, -0.08660589903593063, -0.02987716533243656, 0.010495075024664402, 0.05804566666483879, -0.00890316627919674, 0.024118194356560707, 0.07897384464740753, -0.025015149265527725, 0.07405068725347519, -0.07367660105228424, -0.007140136789530516, -0.00782493781298399, -0.03597015142440796, 0.016303502023220062, -0.019429566338658333, 0.0450008399784565, 0.027918564155697823, 0.05517084151506424, 0.04216339811682701, -0.0490400455892086, 0.048967476934194565, 0.09304775297641754, -0.05605032294988632, -0.01823636144399643, -0.07000250369310379, -0.038603559136390686]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\NeuralNetworkStarPatternRecognitionforSpacecraftAttitudeDeterminationandControl.pdf,Reinforcement Learning,"GEMINI: GRADIENT ESTIMATION THROUGH MATRIX INVERSION AFTER NOISE INJECTION Yann Le Cun 1 Conrad C. Galland and Geoffrey E. Hinton Department of Computer Science University of Toronto 10 King's College Rd Toronto, Ontario M5S 1A4 Canada ABSTRACT Learning procedures that measure how random perturbations of unit ac- tivities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart, Hinton and Williams, 1986) which compute how changes in activities af- fect the output error are much more efficient, but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks, which shares many of the implementation advantages of correlational reinforce- ment procedures but is more efficient. GEMINI injects noise only at the first hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change, thereby obtaining the error-derivatives. No back-propagation is involved, thus allowing un- known non-linearities in the system. Two simulations demonstrate the effectiveness of GEMINI. OVERVIEW Reinforcement learning procedures typically measure the effects of changes in lo- cal variables on a global reinforcement signal in order to determine sensible weight changes. This measurement does not require the connections to be used backwards (as in back-propagation), but it is inefficient when more than a few units are in- volved. Either the units must be perturbed one at a time, or, if they are perturbed simultaneously, the noise from all the other units must be averaged away over a large number of samples in order to achieve a reasonable signal to noise ratio. So reinforcement learning is much less efficient than back-propagation (BP) but much easier to implement in hardware. GEMINI is a hybrid procedure which retains many of the implementation advan- tages of reinforcement learning but eliminates some of the inefficiency. GEMINI uses the squared difference between the desired and actual output vectors as a reinforcement signal. It injects random noise at the first hidden layer only, caus- ing correlated noise at later layers. If the noise is sufficiently small, the resultant 1 First Author's present address: Room 4G-332, AT&T Bell Laboratories, Crawfords Corner Rd, Holmdel, NJ 07733 141 142 Le Cun, Galland and Hinton change in the reinforcement signal is a linear function of the noise vector at any given layer. A matrix inversion procedure implemented separately at each hidden layer then determines how small changes in the activities of units in the layer affect the reinforcement signal. This matrix inversi?n gives a much more accurate esti- mate of the error-derivatives than simply averaging away the effects of noise and, unlike the averaging approach, it can be used when the noise is correlated. The matrix inversion at each layer can be performed iteratively by a local linear network that ""learns"" to predict the change in reinforcement from the noise vector at that layer. For each input vector, one ordinary forward pass is performed, followed by a number of forward passes each with a small amount of noise added to the total inputs of the first hidden layer. After each forward pass, one iteration of an LMS training procedure is run at each hidden layer in order to improve the estimate of the error-derivatives in that layer. The number of iterations required is comparable to the width of the largest hidden layer. In order to avoid singularities in the matrix inversion procedure, it is necessary for each layer to have fewer units than th~ preceding one. In this hybrid approach, the computations that relate the perturbation vectors to the reinforcement signal are all local to a layer. There is no detailed back- propagation of information, so that GEMINI is more amenable to optical or elec- tronic implementations than BP. The additional time needed to run the gradient- estimating inner loop, may be offset by the fact that only forward propagation is required, so this can be made very efficient (e.g. by using analog or optical hard- ware). TECHNIQUES FOR GRADIENT ESTIMATION The most obvious way to measure the derivative of the cost function w.r.t the weights is to perturb the weights one at a time, for each input vector, and to measure the effect that each weight perturbation has on the cost function, C. The advantage of this technique is that it makes very few assumptions about the way the network computes its output. It is possible to use far fewer perturbations (Barto and Anandan, 1985) if we are using ""quasi-linear"" units in which the output, Yi, of unit i is a smooth non-linear function, I, of'its total input, Xi, and the total input is a linear function of the incoming weights, Wij and the activities, Yi, of units in the layer below: Xi = L WijYj i Instead of perturbing the weights, we perturb the total input, Xi, received by each unit, in order to measure 8C / 8Xi . Once this derivative is known it is easy to derive 8C / 8Wij for each of the unit's incoming weights by performing a simple local compu tation: 8C 8C __ -_yo 8W ij - 8Xi J If the units are perturbed one at a time, we can approximate 8C / 8Xi by GEMINI 143 where 6C is the variation of the cost function induced by a perturbation 6Xi of the total input to unit i. This method is more efficient than perturbing the weights directly, but it still requires as many forward passes as there are hidden units. Reducing the number of perturbations required If the network has a layered, feed-forward, architecture the state of any single layer completely determines the output. This makes it possible to reduce the number of required perturbations and forward passes still further. Perturbing units in the first hidden layer will induce perturbations at the following layers, and we can use these induced perturbations to compute the gradients for these layers. However, since many of the units in a typical hidden layer will be perturbed simultaneously, and since these induced perturbations will generally be correlated, it is necessary to do some local computation within each layer in order to solve the credit assignment problem of deciding how much of the change in the final cost function to attribute to each of the simultaneous perturbations within the layer. This local computation is relatively simple. Let x(k) be the vector of total inputs to units in layer k. Let 6xt(k) be the perturbation vector of layer k at time t. It does not matter for the following analysis whether the perturbations are directly caused (in the first hidden layer) or are induced. For a given state of the network, we have: To compute the gradient w.r.t. layer k we must solve the following system for g,c t = 1. .. P where P is the number of perturbations. Unless P is equal to the number of units in layer k, and the perturbation vectors are linearly independent, this system will be over- or under-determined. In some network architectures it is impossible to induce nl linearly independent perturbation vectors in a hidden layer, I containing nl units. This happens when one of the preceding hidden layers, k, contains fewer units because the perturbation vectors induced by a layer with nk units on the following layer generate at most nk independent directions. So to avoid having to solve an under-determined system, we require ""convergent"" networks in which each hidden layer has no mbre units than the preceding layer. Using a Special Unit to Allocate Credit within a Layer Instead of directly solving for the 8C/8xi within each layer, we can solve the same system iteratively by minimizing: E = I)6Ct - gf6xt(k))2 t 144 Le Cun, Galland and Hinton D D o o o input layer Figure 1: A GEMINI network. linear unit linear unit This can be done by a special unit whose inputs are the perturbations of layer k and whose desired output is the resulting perturbation of the cost function 6C (figure 1). When the LMS algorithm is used, the weight vector gk of this special unit converges to the gradient of C with respect to the vector of total inputs x(k). If the components of the perturbation vector are uncorrelated, the convergence will be fast and the number of iterations required should be of the order of the the number of units in the layer. Each time a new input vector is presented to the main network, the ""inner-loop"" minimization process that estimates the 8C / 8Xi must be re-initialized by setting the weights of the special units to zero or by reloading approximately correct weights from a table that associates estimates of the 8C / 8Xi with each input vector. Summary of the Gemini Algorithm 1. Present an input pattern and compute the network state by forward propagation. 2. Present a desired output and evaluate the cost function. 3. Re-initialize the weights of the special units. 4. Repeat until convergence: (a) Perturb the first hidden layer and propagate forward. (b) Measure the induced perturbations in other layers and the output cost function. (c) At each layer apply one step of the LMS rule on the special units to minimize the error between the predicted cost variation and the actual variation. 5. Use the weights of the special units (the estimates of 8C /8Xi ) to compute the weight changes of the main network. 6. Update the weights of the main network. GEMINI 145 A TEST EXAMPLE: CHARACTER RECOGNITION The GEMINI procedure was tested on a simple classification task using a network with two hidden layers. The input layer represented a 16 by 16 binary image of a handwritten digit. The first hidden layer was an 8 by 8 array of units that were locally connected to the input layer in the following way: Each hidden unit connected to a 3 by 3 ""receptive field"" of input units and the centers of these receptive fields were spaced two ""pixels"" apart horizontally and vertically. To avoid boundary effects we used wraparound which is unrealistic for real image processing. The second hidden layer was a 4 by 4 array of units each of which was connected to a 5 by 5 receptive field in the previous hidden layer. The centers of these receptive fields were spaced two pixels apart. Finally the output layer contained 10 units, one for each digit, and was fully connected to the second hidden layer. The network contained 1226 weights and biases. The sigmoid function used at each node was of the form f( x) = stanh( mx) with m = 2/3 and s = 1.716, thus f was odd, and had the property that f(l) = 1 (LeCun, 1987). The training set was composed of 6 handwritten exemplars of each of the 10 digits. It should be emphasized that this task is simple (it is linearly separable), and the network has considerably more weights than are required for this problem. Experiments were performed with 64 perturbations in the gradient estimation inner loop. Therefore, assuming that the perturbation vectors were linearly inde- pendent, the linear system associated with the first hidden layer was not under- constrained 2. Since a stochastic gradient procedure was used with a single sweep through the training set, the solution was only a rough approximation, though con- vergence was facilitated by the fact that the components of the perturbations were statistically independent. The linear systems associated with the second hidden layer and the output layer were almost certainly overconstrained 3, so we expected to obtain a better estimate of the gradient for these layers than for the first one. The perturbations injected at the first hidden layer were independent random numbers with a zero-mean gaussian distribution and standard deviation of 0.1. The minimization procedure used for gradient estimation was not a pure LMS, but a pseudo-newton method that used a diagonal approximation to the matrix of second derivatives which scales the learning rates for each link independently (Le Cun, 1987; Becker and Le Cun, 1988). In our case, the update rule for a gradient estimate coefficient was where a'[ is an estimate of the variance of the perturbation for unit i. In the simulations TJ was equal to 0.02 for the first hidden layer, 0.03 for the second hidden layer, and 0.05 for the output layer. Although there was no real need for it, the gradient associated-with the output units was estimated using GEMINI so that we could evaluate the accuracy of gradient estimates far away from the noise-injection 2Jt may have been overconstrained since the actual relation between the perturbation and variation of the cost function is usually non-linear for finite perturbations 3This depends on the degeneracy of the weight matrices 146 Le Cun, Galland and Hinton B.1 B +---~--~--~---r---r---+---+--~--~---4~~ 12 Figure 2: The mean squared error as a function of the number of sweeps through the training set for GEMINI (top curve) and BP (bottom curve). layer. The learning rates for the main network, fi, had different values for each unit and were equal to 0.1 divided by the fan-in of the unit. Figure 2 shows the relative learning rates of BP and GEMINI. The two runs were started from the same initial conditions. Although the learning curve for GEMINI is consistently above the one for BP and is more irregular, the rate of decrease of the two curves is similar. The 60 patterns are all correctly classified after 10 passes through the training set for regular BP, and after 11 passes for GEMINI. In the experiments, the direction of the estimated gradient for a single pattern was within about 20 degrees of the true gradient for the output layer and the second hidden layer, and within 50 degrees for the first hidden layer. Even with such inaccuracies in the gradient direction, the procedure still converged at a reasonable rate. LEARNING TO CONTROL A SIMPLE ROBOT ARM In contrast to the digit recognition task, the robot arm control task considered here is particularily suited to the GEMINI procedure because it contains a non- linearity which is unknown to the network. In this simulation, a network with 2 input units, a first hidden layer with 8 units, a second with 4 units, and an output layer with 2 units is used to control a simulated arm with two angular degrees of freedom. The problem is to train the network to receive x, y coordinates encoded on the two input units and produce two angles encoded on the output units which would place the end of the arm on the desired input point (figure 3). The units use the same input-output function as in the digit recognition example. Cost. (Euclidean disl. 10 adual point? t ROBOT ARM ""unknown"" non-lIne.rlty t 91 92 00 o oto 0 000 O~O 0 0 0 00 (a) Jl y GEMINI 147 (x,y) (b) Figure 3: (a) The network trained with the GEMINI procedure, and (b) the 2-D arm controlled by the network. Each point in the training set is successively applied to the inputs and the resultant output angles determined. The training points are chosen so that the code for the output angles exploits most of the sigmoid input-output curve while avoiding the extreme ends. The ""unknown"" non-linearity is essentially the robot arm, which takes the joint angles as input and then ""outputs"" the resulting hand coordinates by positioning itself accordingly. The cost function, C, is taken as the square of the Euclidean distance from this point to the desired point. In the simulation, this distance is determined using the appropriate trigonometric relations: where al and a2 are the lengths of the two components of the arm. Although this non-linearity is not actually unknown, analytical derivative calculation can be difficult in many real world applications, and so it is interesting to explore the possibility of a control system that can learn without it. It is found that the minimum number of iterations of the LMS inner loop search needed to obtain good estimates ofthe gradients when compared to values calculated by back-propagation is between 2 and 3 times the number of units in the first hidden layer (figure 4). For this particular kind of problem, the process can be sped up significantly by using the following two modifications. The same training vector can be applied to the inputs and the weights changed repeatedly until the actual output is within a certain radius of the desired output. The gradient estimates are kept between these weight updates, thereby reducing the number of inner loop 148 Le Cun, Galland and Hinton Figure 4: Gradients of the units in all non-input layers, determined (a) by the GEMINI procedure after 24 iterations of the gradient estimating inner loop, and (b) through analytical calculation. The size of the black and white squares indicates the magnitude of negative and positive error gradients respectively. iterations needed at each step. The second modification requires that the arm be made to move continuously through 2-D space by using an appropriately ordered training set. The state of the network changes slowly as a result, leading to a slowly varying gradient. Thus, if the gradient estimate is not reset between successive input vectors, it can track the real gradient, allowing the number of iterations per gradient estimate to be reduced to as little as 5 in this particular network. The results of simulations using training sets of closely spaced points in the first quadrant show that GEMINI is capable of training this network to correctly orient the simulated arm, with significantly improved learning efficiency when the above two modifications are employed. Details of these simulation results and the param- eters used to obtain them are given in (Galland, Hinton, and Le Cun, 1989). Acknowledgements This research was funded by grants from the Ontario Information Technology Research Center, the Fyssen Foundation, and the National Science and Engineer- ing Research Council. Geoffrey Hinton is a fellow of the Canadian Institute for Advanced Research. References A. G. Barto and P. Anandan (1985) Pattern recognizing stochastic learning au- tomata. IEEE Transactions on Systems, Man and Cybernetics, 15, 360-375. S. Becker and Y. Le Cun (1988) Improving the convergence of back-propagation learning with second order methods. In Touretzky, D. S., Hinton, G. E. and Se- jnowski, T. J., editors, Proceedings of the 1988 Connectionist Summer School, Mor- gan Kauffman: Los Altos, CA. C. C. Galland, G. E. Hinton and Y. Le Cun (1989) Technical Report, in preparation. Y. Le Cun (1987) Modeles Connexionnistes de l'Apprentissage. Doctoral thesis, University of Paris, 6. D. E. Rumelhart, G. E. Hinton, and R. J. Williams (1986) Learning internal repre- sentations by back-propagating errors. Nature, 323, 533-536.","[-0.1009606122970581, -0.08166833221912384, 0.045250795781612396, 0.06454446911811829, -0.050230056047439575, -0.02958468161523342, -0.020144876092672348, -0.06524618715047836, -0.00693770544603467, 0.018511418253183365, -0.05138777941465378, 0.07671873271465302, 0.05534444376826286, -0.11186373233795166, -0.06938410550355911, 0.05537242814898491, 0.08769209682941437, 0.08653691411018372, -0.019184354692697525, -0.058357078582048416, -0.08091527223587036, -0.015238638035953045, -0.0122615285217762, 0.06780547648668289, 0.015172543935477734, 0.026326868683099747, -0.03998600319027901, -0.003721249522641301, -0.0295262448489666, -0.04108164831995964, 0.06203031539916992, -0.013658519834280014, -0.01738692633807659, -0.051859039813280106, -0.17268230020999908, 0.014593028463423252, -0.04988064244389534, -0.03783334046602249, -0.0014419317012652755, 0.0029788841493427753, -0.0011823932873085141, 0.04229936748743057, -0.017428621649742126, 0.0715646743774414, 0.06808773428201675, -0.07022318989038467, 0.06251376122236252, -0.09435372054576874, -0.007606926374137402, 0.03369993343949318, 0.05664438009262085, 0.003438575891777873, -0.008754376322031021, -0.037967994809150696, 0.05004191771149635, 0.06763460487127304, 0.045440904796123505, 0.03117687627673149, -0.04249407723546028, 0.07385638356208801, 0.009307779371738434, -0.060663800686597824, -0.05903080105781555, -0.007943164557218552, 0.021398669108748436, -0.03448541834950447, -0.007433698512613773, 0.048154741525650024, 0.05111462250351906, -0.028283074498176575, -0.06030029058456421, 0.022281181067228317, -0.023693149909377098, 0.01207069493830204, 0.037580832839012146, 0.047849010676145554, 0.11524751782417297, 0.018471403047442436, 0.06155477464199066, -0.05773796886205673, 0.053033895790576935, 0.03742116689682007, -0.03931406885385513, -0.09581460058689117, 0.07950939238071442, -0.018082749098539352, -0.04909583553671837, 0.03995293006300926, 0.02248837985098362, -0.044843826442956924, 0.05747933313250542, 0.00016367834177799523, -0.1166953444480896, 0.01599223166704178, 0.038190457969903946, -0.04484478011727333, -0.025398755446076393, -0.10109483450651169, 0.007655062712728977, 0.12158466130495071, 0.05334433540701866, 0.03053944930434227, -0.023932894691824913, 0.033667661249637604, -0.047369372099637985, 0.020772863179445267, 0.057337045669555664, 0.022228337824344635, -0.01048561092466116, -0.03336323797702789, -0.0657738447189331, 0.028592465445399284, -0.03891918435692787, 0.01518032792955637, -0.025563949719071388, -0.015196137130260468, 0.047304779291152954, -0.0039072902873158455, -0.004217160865664482, 0.06542639434337616, -0.0584646537899971, -0.07479378581047058, 0.05515888333320618, 0.039718061685562134, -0.002424956299364567, 0.02135666087269783, -0.01916649006307125, 6.097787313446366e-33, -0.02542939968407154, 0.029994279146194458, -0.011604683473706245, -0.0020481215324252844, -0.008727048523724079, -0.007008002605289221, 0.07342720031738281, -0.06011322885751724, 0.03264227509498596, 0.03162355720996857, 0.010603997856378555, 0.06852099299430847, -0.019727643579244614, 0.07372629642486572, 0.08265379816293716, -0.02017739787697792, 0.004410708788782358, -0.028095131739974022, -0.012845508754253387, -0.07701143622398376, 0.016987599432468414, -0.07729579508304596, -0.03425201028585434, -0.02385280467569828, 0.08707506209611893, 0.032283373177051544, 0.05037633329629898, 0.08935374021530151, -0.05818531662225723, 0.028202196583151817, 0.08205914497375488, 0.0231071338057518, -0.03908471763134003, 0.015009826980531216, 0.0407952219247818, -0.019667839631438255, 0.017239345237612724, -0.02136233076453209, 0.01464229915291071, -0.03804637864232063, 0.0007620175019837916, -0.022291099652647972, 0.0056750355288386345, -0.012922396883368492, -0.06261999160051346, -0.040880318731069565, 0.01852826215326786, 0.027892325073480606, 0.0011156453983858228, -0.0815357193350792, -0.0013912023277953267, 0.01946445368230343, -0.0663115456700325, -0.0484059602022171, 0.024912213906645775, 0.03009069710969925, 0.0390094518661499, 0.028569845482707024, 0.06687706708908081, 0.06843674182891846, -0.034308623522520065, -0.022818101570010185, -0.024059589952230453, -0.03916832432150841, -0.04404135048389435, 0.13356399536132812, -0.03384935483336449, 0.0326247476041317, -0.005725223571062088, -0.019306257367134094, 0.047739703208208084, 0.06189830228686333, -0.04155195131897926, -0.07341987639665604, 0.08393372595310211, -0.006238420493900776, -0.08305847644805908, -0.021782491356134415, -0.03013790026307106, -0.0450962632894516, -0.09115051478147507, -0.004143203143030405, -0.012830683961510658, 0.054559413343667984, -0.01859167404472828, 0.02786262147128582, 0.014342783018946648, 0.004255085717886686, -0.029454922303557396, 0.017074892297387123, -0.007862627506256104, 0.043525416404008865, 0.07783757150173187, 0.013230250217020512, -0.009408787824213505, -5.736131782736504e-33, -0.09899463504552841, 0.05209128186106682, -0.03649704158306122, -0.010025824420154095, -0.027515660971403122, -0.034618377685546875, 0.03033977374434471, 0.0001522040256531909, -0.015839701518416405, -0.009565034881234169, -0.07641709595918655, 0.017289869487285614, -0.09480632841587067, -0.021906083449721336, 0.031487837433815, 0.028958840295672417, -0.09186466038227081, -0.00785170216113329, 0.044927455484867096, 0.009129161946475506, 0.032423049211502075, 0.08145999908447266, 0.011159403249621391, -0.017874183133244514, -0.009107216261327267, -0.05629672482609749, -0.010085991583764553, 0.11359281837940216, 0.05540679022669792, -0.001987897092476487, -0.07725677639245987, -0.017388952895998955, -0.0014759382465854287, 0.07287514954805374, 0.030341630801558495, 0.07804527878761292, 0.0835815891623497, 0.04475065693259239, -0.0839104950428009, -0.01522193942219019, 0.0035690583754330873, 0.03247324004769325, -0.026743745431303978, -0.025783786550164223, 0.051834531128406525, -0.00323682464659214, -0.08350349962711334, 0.029989896342158318, -0.048588044941425323, 0.024626940488815308, 0.032442230731248856, -0.014448003843426704, -0.010328679345548153, 0.039651211351156235, -0.023662088438868523, 0.032522574067115784, 0.04794872924685478, 0.03361200541257858, 0.09194877743721008, -0.00314431544393301, -0.051584430038928986, -0.08676424622535706, -0.0496530644595623, -0.02859591133892536, -0.007340705022215843, 0.03728596493601799, 0.01090951170772314, 0.024159466847777367, 0.07248659431934357, 0.06632248312234879, 0.06059025600552559, 0.0728214755654335, 0.04640709608793259, -0.032891079783439636, 0.039306748658418655, 0.008242129348218441, -0.01244895439594984, -0.10249300301074982, -0.04778045788407326, 0.008857841603457928, -0.06468697637319565, -0.015008287504315376, 0.0536174513399601, 0.074759840965271, -0.00018736449419520795, 0.022586628794670105, 0.05258126184344292, 0.014313087798655033, -0.02015862427651882, -0.05117524415254593, -0.030803536996245384, 0.03363791108131409, 0.029444128274917603, -0.04323841258883476, -0.0073741828091442585, -6.048072265230076e-08, -0.08087117224931717, -0.0007776992861181498, 0.008977266028523445, 0.02506665699183941, 0.049601100385189056, 0.006949198432266712, 0.08277770131826401, -0.05306845158338547, -0.00520795863121748, -0.06632551550865173, 0.013829500414431095, 0.015854066237807274, 0.04474339634180069, -0.05954526737332344, -0.04257878661155701, 0.04206085950136185, 0.02277880720794201, 0.0060338242910802364, -0.0027130255475640297, -0.01687539927661419, 0.025898965075612068, 0.0954166129231453, 0.01389180775731802, 0.03320062533020973, 0.09011262655258179, -0.08874884992837906, -0.019437851384282112, 0.10676953941583633, 0.0597335547208786, -0.013306771405041218, -0.0034517724998295307, -0.06397638469934464, 0.11891067028045654, 0.00017088754975702614, 0.02201281301677227, 0.11151480674743652, 0.09685533493757248, -0.029834603890776634, 0.05379148945212364, 0.05110067501664162, -0.10451851785182953, -0.05697036534547806, 0.04872944578528404, -0.016461262479424477, -0.026474792510271072, -0.039909131824970245, -0.06555338948965073, -0.11628765612840652, 0.018574316054582596, -0.08766449242830276, 0.12147806584835052, 0.003935530781745911, -0.051065221428871155, 0.02003921940922737, 0.06989014148712158, -0.06710445880889893, -0.03041485697031021, -0.10924522578716278, 0.02126135490834713, 0.08218469470739365, -0.03904012218117714, -0.007265038788318634, -0.09155892580747604, -0.09236471354961395]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\NeuralNetworksthatLearntoDiscriminateSimilarKanjiCharacters.pdf,Computer Vision,"384 MODELING SMALL OSCILLATING BIOLOGICAL NETWORKS IN ANALOG VLSI Sylvie Ryckebusch, James M. Bower, and Carver Mead California Instit ute of Technology Pasadena, CA 91125 ABSTRACT We have used analog VLSI technology to model a class of small os- cillating biological neural circuits known as central pattern gener- ators (CPG). These circuits generate rhythmic patterns of activity which drive locomotor behaviour in the animal. We have designed, fabricated, and tested a model neuron circuit which relies on many of the same mechanisms as a biological central pattern generator neuron, such as delays and internal feedback. We show that this neuron can be used to build several small circuits based on known biological CPG circuits, and that these circuits produce patterns of output which are very similar to the observed biological patterns. To date, researchers in applied neural networks have tended to focus on mam- malian systems as the primary source of potentially useful biological information. However, invertebrate systems may represent a source of ideas in many ways more appropriate, given current levels of engineering sophistication in building neural-like systems, and given the state of biological understanding of mammalian circuits. In- vertebrate systems are based on orders of magnitude smaller numbers of neurons than are mammalian systems. The networks we will consider here, for example, are composed of about a dozen neurons, which is well within the demonstrated capabilities of current hardware fabrication techniques. Furthermore, since much more detailed structural information is available about these systems than for most systems in higher animals, insights can be guided by real information rather than by guesswork. Finally, even though they are constructed of small numbers of neurons, these networks have numerous interesting and potentially even useful properties. CENTRAL PATTERN GENERATORS Of all the invertebrate neural networks currently being investigated by neurobi- ologists, the class of networks known as central pattern generators (CPGs) may be especially worthy of attention. A CPG is responsible for generating oscillatory neural activity that governs specific patterns of motor output, and can generate its pattern of activity when isolated from its normal neuronal inputs. This prop- Modeling Small Oscillating Biological Networks 385 erty, which greatly facilitates experiments, has enabled biologists to describe several CPGs in detail at the cellular and synaptic level. These networks have been found in all animals, but have been extensively studied in invertebrates [Selverston, 1985]. We chose to model several small CPG networks using analog VLSI technology. Our model differs from most computer simulation models of biological networks [Wilson and Bower, in press] in that we did not attempt to model the details of the individual ionic currents, nor did we attempt to model each known connection in the networks. Rather, our aim was to determine the basic functionality of a set of CPG networks by modeling them as the minimum set of connections required to reproduce output qualitatively similar to that produced by the real network under certain conditions. MODELING CPG NEURONS The basic building block for our model is a general purpose CPG neuron circuit. This circuit, shown in Figure 1, is our model for a typical neuron found in central pattern generators, and contains some of the essential elements of real biological neurons. Like real neurons, this model integrates current and uses positive feedback to output a train of pulses, or action potentials, whose frequency depends on the magnitude of the current input. The part of the circuit which generates these pulses is shown in Figure 2a [Mead, 19891. The second element in the CPG neuron circuit is the synapse. In Figure 1, each pair of transistors functions as a synapse. The p-well transistors are. excitatory synapses, whereas the n-well transistors are inhibitory synapses. One of the transistors in the pair sets the strength of the synapse, while the other transistor is the input of the synapse. Each CPG neuron has four different synapses. The third element of our model CPG neuron involves temporal delays. Delays are an essential element in the function of CPGs, and biology has evolved many different mechanisms to introduce delays into neural networks. The membrane capacitance of the cell body, different rates of chemical reactions, and axonal transmission are just a few of the mechanisms which have time constants associated with them. In our model we have included synaptic delay as the principle source of delay in the network. This is modeled as an RC delay, implemented by the follower-integrator circuit shown in Figure 2b [Mead, 19891. The time constant of the delay is a function of the conductance of the amplifier, set by the bias G. A multiple time constant delay line is formed by cascading several of these elements. Our neuron circuit uses a delay line with three time constants. The synapses which are before the delay element are slow synapses, whereas the undelayed synapses are fa.st synapses. We fabricated the circuit shown in Figure 1 using CMOS, VLSI technology. Several of these circuits were put on each chip, with all of the inputs and controls going out to pads, so that these cells could be externally connected to form the network of interest. 386 Ryckebusch, Bower, and Mead slow excitation -t G Figure 1. The CPG neuron circuit. r I- Pulse Length ~ (a) Yout. pulse length Yout. -111 0 -QJ- (b) Figure 2. (a). The neuron spike-generating circuit. (b). The follower-integrater circuit. Each delay box 0 contains a delay line formed by three follower-integrater circuits. The Endogenous Bursting Neuron One type of cell which has been found to play an important role in many oscilla- tory circuits is the endogenous bursting neuron. This type of cell has an intrinsic oscillatory membrane potential, enabling it to produce bursts of action potentials at rhythmic intervals. These cells have been shown to act both as external ""pace- makers"" which set the rhythm for the CPG, or as an integral part of a central pattern generator. Figure 3a shows the output from a biological endogenous burst- ing neuron. Figure 3b demonstrates how we can configure our CPG neuron to be an endogenous bursting neuron. The delay element in the cell must have three time constants in order for this circuit to oscillate stably. Note that in the circuit, the Modeling Small Oscillating Biological Networks 387 cell has internal negative feedback. Since real neurons don't actually make synaptic connections onto themselves, this connection should be thought of as representing an internal molecular or ionic mechanism which results in feedback within the cell. 4 mV AS 1 sec (a) (b) (c) Figure 3. (a). The output from the AB cell in the lobster stomatogastric ganglion CPG [Eisen and Marder, 1982]. This cell is known to burst endogenously. (b). The CPG neuron circuit configured as an endogenous bursting neuron and (c) the output from this circuit. Postinhibitory Rebound A neuron configured to be an endogenous burster also exhibits another property common to many neurons, including many CPG neurons. This property, illustrated in Figures 4a and 4b, is known as postinhibitory rebound (PIR). Neurons with this property display increased excitation for a certain period of time following the release of an inhibitory influence. This property is a useful one for central pattern generator neurons to have, because it enables patterns of oscillations to be reset following the release of inhibition. 388 Ryckebusch, Bower and Mead -(a) (b) I N' "" ... .. I .. • ' LA 4 t •••• I ........ (c) Figure 4. (a) The output of a ganglion cell of the mudpuppy retina exhibiting postinhibitory rebound [Miller and Dacheux, 19761. The bar under the trace indi- cates the duration of the inhibition. (b) To exhibit PIR in the CPG neuron circuit, we inhibit ,the cell with the square pulse shown in (c). When the inhibition is released, the circuit outputs a brief burst of pulses. MODELING CENTRAL PATTERN GENERATORS The Lobster Stomatogastric Ganglion The stomatogastric ganglion is a CPG which controls the movement of the teeth in the lobster's stomach. This network is relatively complex, and we have only modeled the relationships between two of the neurons in the CPG (the PD and LP cells) which have a kind of interaction found in many CPGs known as reciprocal inhibition (Figure Sa). In this case, each cell inhibits the other, which produces a pattern of output in which the cells fire alternatively (Figure Sb). Note that in the absence of external input, a mechanism such as postinhibitory rebound must exist in order for a cell to begin firing again once it has been released from inhibition. (b) Modeling Small Oscillating Biological Networks 389 (a) (d) (c) _ 120 ~rrN Figure 5. (a) Output from the PD and LP cells in the lobster stomatogastric ganglion [Miller and Selverston, 1985]. (c) and (d) demonstrate reciprocal inhibition with two CPG neuron circuits. The Locust Flight CPG A CPG has been shown to play an important role in producing the motor pattern for flight in the locust [Robertson and Pearson, 19851. Two of the cells in the CPG, the 301 and 501 cells, fire bursts of action potentials as shown in Figure 6a. The 301 cell is active when the wings of the locust are elevated, whereas the 501 cell is active when the wings are depressed. The phase relationship between the two cells is very similar to the reciprocal inhibition pattern just discussed, but the circuit that produces this pattern is quite different. The connections between these two cells are shown in Figure 6b. The 301 cell makes a delayed excitatory connection onto the 501 cell, and the 501 cell makes fast inhibitory contact with the 301 cell. Therefore, the 301 cell begins to fire, and after some delay, the 501 cell is activated. When the 501 cell begins to fire, it immediately shuts off the 301 cell. Since the 501 cell is no longer receiving excitatory input, it will eventually stop firing, releasing the 301 cell from inhibition. The cycle then repeats. This same circuit has been reproduced with our model in Figures 6c and 6d. 390 Ryckebusch, Bower and Mead • ---.J50mv ~ 100ms \5'~'1 I--~ 501 301~~~~=-~~~~~~ __ ~ __ ~~ Dl (a) (b) 301 301 CELL SOl CELL (e) (d) Figure 6. (a) The 301 and 501 cells in the locust flight CPG [Robertson and Pearson, 19851. (b) Simultaneous intracellular recordings of 301 and 501 during flight. (c) The model circuit and (d) its output. The Tritonia Swim CPG One of the best studied central pattern generators is the CPG which controls the swimming in the small marine mollusc Tritonia. This CPG was studied in great detail by Peter Getting and his colleagues at the University of Iowa, and it is one of the few biological neural networks for which most of the connections and the synaptic parameters are known in detail. Tritonia swims by making alternating dorsal and ventral flexions. The dorsal and ventral motor neurons are innervated by the DSI and VSI cells, respectively. Shown in Figure 7a and 7b is a simplified schematic diagram for the network and the corresponding output. The DSI and VSI cells fire out of phase, which is consistent with the alternating nature of the animal's swimming motion. The basic circuit consists of reciprocal inhibition between DSI and VSI paralleled by delayed excitation via the C2 cell. The DSI and VSI cells fire out of phase, and the DSI and C2 cells fire in phase. Swimming is initiated by sensory stimuli which feed into DSI and cause it to begin to fire a burst of impulses. DSI inhibits VSI, and at the same time excites C2. C2 has excitatory synapses on VSIj however, the initial response of VSI neurons is delayed. VSI then fires, during which there is inhibition by VSI of C2 and DSI. During this period, VSI no longer receives excitatory input from C2, and hence the VSI firing rate declines; DSI is therefore released from inhibition, and is ready to fire again to initiate a new cycle. Figure 7c and 8 show the model circuit which is identical to the circuit Modeling Small Oscillating Biological Networks 391 shown in Figure 7a, and the output from this circuit. Note that although the model output closely resembles the biological data, there are small differences in the phase relationships between the cells which can be accounted for by taking into account other connections and delays in the circuit not currently incorporated in our model. (a) 1-0 OSI : I VSI B C2 - .' .~ .' .' ........... . - - --I \ \ .. \ ..... \ .. .. \ ..... \ ..... , ...... \ DSI t' ... , ...... ' , , , \ ,---------- (c) (b) C2 VSI 50 mV J 5 sec Figure 1. (a) Simplified schematic diagram of the Tritonia CPG (which actually has 14 cells) and (b) output from the three types of cells in the circuit.(c) The model circuit. 392 Ryckebusch, Bower and Mead VSI C2 DSI Figure 8. Output from the circuit shown in Figure 7c. CONCLUSIONS One may ask why it is interesting to model these systems in analog VLSI, or, for that matter, why it is interesting to model invertebrate networks altogether. Analog VLSI is a very nice medium for this type of modeling, because in addition to being compact, it runs in real time, eliminating the need to wait hours to get the results of a simulation. In addition, the electronic circuits rely on the same physical principles as neural processes {including gain, delays, and feedback}, allowing us to exploit the inherent properties of the medium in which we work rather than having to explicitly model them as in a digital simulation. Like all models, we hope that this work will help us learn something about the systems we are studying. But in addition, although invertebrate neural networks are relatively simple and have small numbers of cells, the behaviours of these networks and animals can be fairly complex. At the same time, their small size allows us to understand how they are engineered in detail. Accordingly, modeling these networks allows us to study a well engineered system at the component level-a level of modeling not yet possible for more complex mammalian systems, for which detailed structural information is scarce. Modeling Small Oscillating Biological Networks 393 Acknowledgments This work relies on information supplied by the hard work of many experimentalists. We would especially like to acknowledge the effort and dedication of Peter Getting who devoted 12 years to understanding the organization of the Tritonia network of 14 neurons. We also thank Hewlett-Packard for computing support, and DARPA and MOSIS for chip fabrication. This work was sponsored by the Office of Naval Research, the System Development Foundation, and the NSF (EET-8700064 to J.B.). Reference8 Eisen, Judith S. and Marder, Eve (1982). Mechanisms underlying pattern gener- ation in lobster stomatogastric ganglion as determined by selective inactivation of identified neurons. III. Synaptic connections of electrically coupled pyloric neurons. J. Neurophysiol. 48:1392-1415. Getting, Peter A. and Dekin, Michael S. (1985). Tritonia swimming: A model system for integration within rhythmic motor systems. In Allen I. Selverston (Ed.), Model Neural Networks and Behavior, New York, NY: Plenum Press. Mead, Carver A. (in press). Analog VLSI and Neural Systems. Reading, MA: Addison-Wesley. Miller, John P. and Selverston, Allen I. (1985). Neural Mechanisms for the produc- tion of the lobster pyloric motor pattern. In Allen I. Selverston (Ed.), Model Neural Networks and Behavior, New York, NY: Plenum Press. Miller, R. F. and Dacheux, R. F. (1976). Synaptic organization and ionic basis of on and off channels in mudpuppy retina. J. Gen. Physiol. 67:639-690. Robertson, R. M. and Pearson, K. G. (1985). Neural circuits in the ft.ight system of the locust. J. Neurophysiol. 53:110-128. Selverston, Allen I. and Moulins, Maurice (1985). Oscillatory neural networks. Ann. Rev. Physiol. 47:29-48. Wilson, M. and Bower, J. M. (in press). Simulation oflarge scale neuronal networks. In C. Koch and I. Segev (Eds.), Methods in Neuronal Modeling: From Synapses to Networks, Cambridge j MA: MIT Press.","[-0.09319544583559036, -0.08350309729576111, 0.052808403968811035, -0.02570999599993229, -0.07318122684955597, -0.016653336584568024, -0.05471774563193321, -0.054084908217191696, 0.08279450237751007, 0.012095468118786812, 0.015424118377268314, -0.06067100539803505, -0.02647978812456131, -0.02347915805876255, -0.02996194176375866, 0.03042936697602272, -0.04094310849905014, 0.0569804385304451, 0.0022174615878611803, -0.0019311296055093408, 0.0505937784910202, 0.06474584341049194, -0.030953777953982353, 0.04009244218468666, -0.07565241307020187, 0.01904347911477089, -0.05775170773267746, -0.0046011474914848804, -0.0006060022860765457, -0.055001989006996155, 0.07994525134563446, -0.028334250673651695, 0.019678547978401184, -0.027052145451307297, -0.06129688024520874, 0.07032780349254608, -0.021448826417326927, -0.046651389449834824, 0.02627885714173317, -0.006741910241544247, 0.09447916597127914, -0.013031098991632462, 0.07745040953159332, 0.0757269412279129, 0.004029387142509222, 0.004503399133682251, 0.01566949114203453, -0.08173868060112, -0.01730695739388466, -0.0724661722779274, -0.0009190106065943837, -0.03552905097603798, 0.030869152396917343, 0.008944874629378319, 0.03514580428600311, 0.0363311842083931, -0.04403725266456604, 0.048196595162153244, -0.005212073214352131, -0.06977193057537079, 0.05707576125860214, -0.008764535188674927, -0.02990676835179329, 0.007062227465212345, -0.03718418627977371, 0.016523249447345734, 0.007357065100222826, 0.03796592354774475, 0.02068670466542244, -0.07719429582357407, 0.01585359126329422, 0.009529613889753819, 0.009699394926428795, 0.0014422069070860744, 0.02514885552227497, 0.021087726578116417, -0.014966684393584728, 0.06556427478790283, 0.0443640761077404, -0.012500753626227379, -0.02895798534154892, -0.05302337184548378, 0.001144061447121203, -0.0007187679875642061, 0.06459403783082962, 0.03320392966270447, 0.061908822506666183, 0.017166493460536003, -0.014477063901722431, -0.02113397978246212, -0.06806115061044693, -0.06105049327015877, -0.015063616447150707, -0.09282846003770828, -0.019330088049173355, 0.0004200657131150365, 0.04311841353774071, -0.023097286000847816, 0.025754466652870178, 0.035990603268146515, 0.033548787236213684, 0.0753960981965065, 0.03261943906545639, 0.03315252065658569, 0.09350871294736862, 0.0019053819123655558, 0.06734777987003326, 0.08379404991865158, 0.031525518745183945, 0.017571905627846718, -0.0807371512055397, 0.07296780496835709, 0.001735346857458353, 0.12034112960100174, 0.07620033621788025, 0.032106395810842514, -0.046745914965867996, -7.148498116293922e-05, 0.1496001034975052, -0.002848708303645253, -0.008944321423768997, -0.020441656932234764, -0.11850153654813766, -0.048534639179706573, 0.04516546428203583, -0.0006452614325098693, -0.13926281034946442, 2.5740896315779756e-33, -0.03948146104812622, 0.0075616114772856236, 0.01768495887517929, -0.018731439486145973, 0.02938208542764187, -0.06664466857910156, -0.04763650521636009, -0.0297736544162035, 0.06977207213640213, 0.010525633580982685, -0.0708283931016922, 0.013444658368825912, -0.013414722867310047, 0.1322372555732727, 0.10869424790143967, -0.05416574701666832, -0.04121781513094902, -0.13061127066612244, 0.05628477782011032, -0.10470156371593475, 0.0237521193921566, 0.0004934972967021167, 0.01807958446443081, -0.0009516018908470869, -0.013532811775803566, -0.0009293858893215656, -0.021832408383488655, -0.01724180392920971, -0.0441867858171463, 0.015343908220529556, 0.010474972426891327, 0.04138071462512016, -0.020120903849601746, -0.032539576292037964, 0.01617717184126377, 0.017934545874595642, 0.09665621072053909, -0.05934511497616768, 0.05382617563009262, 0.036800432950258255, 0.02419978566467762, -0.03612484037876129, -0.041499871760606766, -0.0036692467983812094, -0.03130028024315834, -0.01231793686747551, 0.03738990053534508, 0.061692431569099426, 0.0008067425806075335, -0.019611738622188568, -0.0014289580285549164, -0.004724171012639999, 0.014574411325156689, -0.09023234248161316, 0.027565110474824905, 0.013891049660742283, -0.0006075422279536724, -0.01399021502584219, -0.05881354212760925, 0.12281926721334457, -0.005111175589263439, 0.01361200213432312, 0.05391744524240494, 0.03668779879808426, 0.056433938443660736, 0.026865409687161446, -0.08349353075027466, -0.07296127825975418, 0.039170973002910614, -0.016755053773522377, 0.02779250219464302, 0.09730933606624603, -0.04487568512558937, -0.11743707954883575, 0.05058824270963669, 0.015194794163107872, 0.008619043044745922, -0.022938983514904976, -0.12833838164806366, -0.016849922016263008, 0.02761690691113472, -0.039060816168785095, -0.11287563294172287, 0.05977660045027733, 0.030711738392710686, 0.019846530631184578, 0.12779000401496887, 0.008569585159420967, -0.06485482305288315, -0.030975524336099625, -0.022663656622171402, -0.07313993573188782, 0.05365786701440811, -0.03737851232290268, -0.0456555113196373, -4.298163135029031e-33, -0.06418641656637192, 0.021415170282125473, 0.048859674483537674, 0.023181667551398277, -0.017153145745396614, 0.06591952592134476, -0.010746441781520844, -0.028947627171874046, -0.1140410453081131, -0.03067578747868538, -0.009283943101763725, 0.0833669900894165, 0.07118482142686844, 0.03389444202184677, 0.0007771819364279509, -0.0069582462310791016, 0.013557634316384792, -0.027779314666986465, 0.11561994254589081, -0.06488475203514099, 0.019138043746352196, 0.11808592826128006, -0.10894932597875595, 0.012228036299347878, 0.021105928346514702, 0.015187699347734451, -0.08526879549026489, 0.0696108266711235, -0.018424997106194496, 0.02406403049826622, -0.07778622210025787, 0.013498466461896896, 0.07957318425178528, -0.04795420169830322, 0.007424558978527784, 0.08661004155874252, 0.015802443027496338, 0.013379326090216637, -0.008826528675854206, -0.09209288656711578, 0.07603716850280762, -0.025326702743768692, -0.007579697296023369, 0.03263046219944954, 0.02442779205739498, -0.0034201506059616804, -0.04911787807941437, 0.08427449315786362, -0.06047931686043739, 0.05104533210396767, 0.05149353668093681, -0.02579650841653347, -0.02526814304292202, -0.1338365226984024, -0.01965204067528248, 0.05298062786459923, -0.029727527871727943, -0.021605564281344414, 0.026365188881754875, -0.06379365175962448, -0.07333490997552872, -0.07094628363847733, 0.047120217233896255, -0.09963775426149368, -0.02969624474644661, 0.015396875329315662, 0.003828730434179306, 0.034997694194316864, 0.08417173475027084, -0.03213324770331383, -0.008050785399973392, 0.0844772532582283, 0.04708288237452507, 0.00868203118443489, 0.019395554438233376, -0.020193025469779968, -0.0670926421880722, -0.047562144696712494, -0.005950151477009058, -0.004947194363921881, -0.0446370430290699, 0.01939351111650467, -0.014853307977318764, -0.07413195818662643, 0.012256113812327385, 0.02250991202890873, 0.003367035649716854, 0.026930460706353188, 0.033137645572423935, -0.009547139517962933, 0.014547632075846195, 0.1306127905845642, 0.0031931204721331596, 0.06245000660419464, -0.03877519443631172, -4.804546449577174e-08, 0.02909986302256584, -0.0021618350874632597, 0.017169851809740067, 0.025693194940686226, 0.04524359479546547, 0.016363633796572685, 0.07002852112054825, -0.1260487139225006, -0.03515411540865898, -0.027783110737800598, 0.033389072865247726, 0.024576693773269653, 0.0012730532325804234, -0.0010721153812482953, 0.07465526461601257, 0.08363482356071472, 0.03452429920434952, -0.009487143717706203, -0.026635145768523216, -0.03738578036427498, -0.0007103571551851928, 0.002206826815381646, -0.04669015109539032, 0.06720801442861557, 0.09397836774587631, -0.07401593774557114, -0.06616166234016418, 0.015338979661464691, -0.0009011887013912201, 0.016648942604660988, 0.06340987980365753, 0.06532399356365204, -0.0009989981772378087, 0.0028966718818992376, -0.0122255003079772, -0.0091119185090065, -0.018743736669421196, -0.03154591843485832, 0.0392342209815979, -0.017125550657510757, -0.026363659650087357, -0.06828988343477249, -0.047271259129047394, 0.01792754977941513, -0.047574255615472794, -0.02876507304608822, 0.028198637068271637, -0.13793613016605377, -0.03188615292310715, -0.002871329430490732, -0.05152302235364914, 0.023422695696353912, -0.010964745655655861, 0.02542448788881302, -0.05394068732857704, 0.029008544981479645, -0.01407699566334486, -0.07844463735818863, -0.05625549703836441, -0.0006524003692902625, -0.02018219791352749, 0.043100755661726, -0.00903038028627634, -0.04448506236076355]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\NeuronalMapsforSensoryMotorControlintheBarnOwl.pdf,Deep Learning,"560 A MODEL OF NEURAL OSCILLATOR FOR A UNIFIED SUEt10DULE A.B.Kirillov, G.N.Borisyuk, R.M.Borisyuk, Ye.I.Kovalenko, V.I.Makarenko,V.A.Chulaevsky, V.I.Kryukov Research Computer Center USSR Academy of Sciences Pushchino, Moscow Region 142292 USSR AmTRACT A new model of a controlled neuron oscillatJOr, proposed earlier {Kryukov et aI, 1986} for the interpretation of the neural activity in various parts of the central nervous system, may have important applications in engineering and in the theory of brain functions. The oscillator has a good stability of the oscillation period, its frequency is regulated linearly in a wide range and it can exhibit arbitrarily long oscillation periods without changing the time constants of its elements. The latter is achieved by using the critical slowdown in the dynamics arising in a network of nonformal excitatory neurons {Kovalenko et aI, 1984, Kryukov, 1984}. By changing the parameters of the oscillator one can obtain various functional modes which are necessary to develop a model of higher brain function. mE CECILLATOR Our oscillator comprises several hundreds of modelled excitatory neurons (located at the 6i tes of a plane lattice) and one inhibitory neuron. The latter receives output stgnals from all the excitatory neurons and its own output is transmitted via feedback to every excitatory neuron (Fig. 1). Each excit~tory neuron is connected bilaterally with its four nearest neighbours. Each neuron has a threshold r(t) decaying exponentially to a A Model of Neural Oscillator for a Unified Submodule 561 e i value roo or roo (for an excitatory or inhibitory neuron). A Gaussian noise with zero mean and standard deviation a is added to a threshold. A membrane potential of a neuron is the sum of input impulses decaying exponentially when there are no inwt. If the membrane potential exceeds the threshold, the neuron fires and sends impulses to the neighbouring neurons. An imWlse from excitatory neuron to excitatory one increases the membrane potential of the latter by aee, from the excitatory to the inhibitory - by aei, and from the inhibitory to the excitatory - decreases the membrane potential by aie' We consider a discrete time model J the time step being equal to the absolute refractory period. We associate a variable xi(t) with each excitatory neuron. If the i-th neuron fires at step t, we take x.(t)=1; if it 1 does not, then Xi (t)=O. The mean E(t)=l/N ~i (t) will be referred to as the network acti vi ty, where N is the number of excitatory neurons. A B 'f.! ----- ---- Figure 1. A - neuron, B - scheme of interconnections Let us consider a situation when inhibitory feedback is cut off. Then such a model exhibits a critical slowdown of the dynamics {Kovalenko et al, 1984, Kryukov, 1984}. Namely, if the interconnections and parameters of neurons are chosen appropriately , initial pattern of activated neurons has an unusually long lifetime as compared with the time of membrane potential decay. In this mode R(t) is slowly increasing and 562 Kirillov, et al causes the inhibitory neuron to fire. Now J if we tum on the negative feedback, outPUt impulse from inhibitory neuron sharply decreases membrane potentials of excitatory neurons. As a a consequence, K( t) falls down and process starts from the beginning. We studied this oscillator by means of simulation model. There are 400 excitatory neurons (20*20 lattice) and one inhibitory neuron in our model. THE MAIN PKFml'IHS OF THE <EClILATOO a. When the thresholds of excitatory neurons are high enough, the inhibitory neuron does not fire and there are no A B I f I I I I I I I I I I I I I I I ""'-i I I I. I I II • I I • • • L ~ 'I1.z I I I I I I I I I I I II fl.l .. I n, • i- • II i • ill I n2 • II. •• III • fl.J rep. 126 + 16 t. ~&·,aLJJ~Ji.l I I III • __ II! •• • II •. I fl.z H11111 •• I n3 Figure 2. Oscillatory mode. A - network activity, B - neuron spike trains ~~!!l~~~~lues of r! the network activity R(t) changes periodically and excitatory neurons generate bursts of spikes (Fig. 2). The inhibitory neuron generates regular periodical spike trains. c. If the parameters are chosen appropriately, the mean oscillation period is much greater than the mean interspike interval of a network neuron. The frequency of oscillations is regulated by r! (Fig. 3A) or, which is the same, by the A Model of Neural Oscillator for a Unified Submodule 563 intensity of the inp.lt flow. The miniIwm period is determined by the decay rate of the inhibitory input, the maximum - by the lifetime of the metastable state. A B lIT k 60 .4 50 40 .3 30 ~ .... 20 10 .1 j 9. 10. II. 12. 13. 't!. ';C J: 1:.C I·T :.cc r F~ 3. A - oscillation frequency lIT vs. threshold r!, B - coefficient of variation of the period K vs. period d. The coefficient of variation of the period is of the order of several percent, rut it increases at low frequencies (F~. 3B). The stability of oscillations can be increased by introducing some inhomogeneity in the network, for example, when a part of excitatory neurons will receive no inhibitory signals. OOCILLA'lOO UNDER ntPULSE STIMOLATI~ In this section we consider first the neural network without the inhibitory neuron. But we imitate a periodic input to the network by slowly varying the thresholds ret) of the excitatory neurons. Namely, we add to r( t) a value Ar-A· sin(Wt) and fire a part of the network at some phase of the sine wave. Then we look at the time needed for the network to restore its background activity. There are specific values of a phase for which this time is rather b~ (Fig. 4A). Now consider the full ocsillator with an oscillation period T (in this section T=35±2. 5 time steps) . We stimulate the oscillator by periodical (with the period tat <35) sharp increase of membrane potential of each excitatory neuron by a value 8st. As the stimulation proceeds, the oscillation period gradually decreases from T--35 to some value Tat' remaining then equal to Tat. The 564 Kirillov, et al value of Tat depends on the stimulation intensity Sst: as Sat gets greater, Tat tends to the st1lw.lation period tst' A B • ,It """" • a o 10 5 o c 10 20 Figure 4, A - threshold modulation, B - duration of the network responce vs. phase of threshold modulation, C - critical st1lw.lation intensity vs, stimulation period For every stimulation period tst there is characteristic value &0 of the st1lw.lation intensity Sst' such that with Sst>&o the value of Tst is equal to the stimulation period tat' The dependence between &0 and tat is close to a linear one (Fig, 4B). The usual relaxation oscillator also exibitB a linear dependence between &0 and tat' At the same time, we did not find in our oscillator any resonance phenomena essential to a linear oscillator, '1'HK NE'l1«)BK WITH INTKBNAL R>ISE In a further development of the neural oscillator we tried to ooild a model that will be more adequate to the biological counterpart. To this end, we changed the structure of interconnections and tried to define more correctly the noise component of the i.np.lt signal coming to an excitatory neuron, In the model described above we A Model of Neural Oscillator for a Unified Submodule 565 imitated the sum of inputs from distant neurons by independent Gaussian noise. Here we used real noise produced by the network. In order to simulate this internal noise, we randomly choose 16 distant neighbours for every exi tatory neuron. Then we assume that the network elements are adjusted to work in a certain noise environment. This means that a ' mean' internal noise would provide conditions for the neuron to be the most sensitive for the information coming from its nearest nelghbors . So. for every neuron i we calculate the sum k. =&c . (t), where 1 J summation is over all distant nelghbors of this neuron, and compare it with the mean internal noise k=1/N Lk.. The 1 internal noise for the neuron i now is ni=C(ki-k), where C>O is a constant. We choose model parameters in such a way that the noise component is of the order of several percent of the membrane potential. Nevertheless, the network exhibits in this case a dramatic increase of the lifetime of initial pattern of activated neurons, as compared with the network with independent Gaussian noise. A range of parameters, for which this slowdown of the dynamics is observed, is also considerably irtCreased. Hence, longer perioos and better perioo stability could be obtained for our generator if we use internal noise. THE CHAIN OF THREE SUBMODULES: A MODEL OF COLUMN OSCILLATOR Now we consider a small System constituted of three oscillator submodules, A, B and C, connected consecutively so that submodule A can transmit excitation to submodule B, B to C, and C to A. The excitation can only be transmitted when the total activity of the submodule reaches its threshold level, i.e. when the corresponding inhibitory neuron fires. After the inhibitory neuron has fired, the activity of its submodule is set to be small enough for the submodule not to be active with large probability until the excitation from another submodule comes. Therefore, we expect A, B and C to work consecutively. In fact, in our simulation experiments we observed such behavior of the 566 Kirillov, et al SeT) A T 20 35 o '-------- 15 L..-______ _ 10 12 10 12 Figure 5. Chain of three sutmodules. Period of oscillations (A) and its standard deviation (B) vs. noise amplitude closed chain of 3 basic submodules. The activity of the whole system is nearly periodic. Figure 5A displays the period T vs. the noise amplitude a. The scale of a is chosen so that 0.5 corresponds approximately to the resting potential. An interesting feature of the chain is that the standard deviation SeT) of the period (Fig. 5B) is small enough, even for the oscillator of relatively small size. The upper lines in Fig. 5 correspond to square 10*10 network, middle - to 9*9, lower - to 8*8 one. One can see that the loss of 36 percent of elements only causes a reduction of the working range without the loss of stability. CXHUJSI~ Though we have not considered all the interesting modes of the oscillator, we believe that, owing to the phenomenon of metastability, the same oscillator exhibits different behaviour under slightly different threshold parameters and the same and/or different inPuts. Let us enumerate the most interesting functional possibilities of the oscillator, which can be easily obtained from our results. 1.Pacemaker with the frequency regulated in a wide range and with a high period stability, as compared with the neuron (Fig. 313). 2. Integrator (input=threshold, output=phase) with a wide A Model of Neural Oscillator for a Unified Submodule 567 range of linear regulation (see Fig. 3A). 3.Generator of damped oscillations (for discontinuous inPut). 4. Delay device controlled by an external signal. 5.Phase comparator (see Fig. 4A). We have already used these functions for the interPretation of electrical activity of several functionally different neural structures {Kryukov et aI, 1986}. The other functions will be used in a system model of attention {Kryukov, 1989} presented in this volume. All these considerations justify the name of our neural oscillator - a unified submodule for a ' resonance' neurocomputer. References E. I. Kovalenko, G. N. Borisyuk, R. M. Borisyuk, A. B. Kirillov, V . I . Kryukov. Short-tenn memory as a metastable state. II. S1IWlation model, Cybernetics and Systems Research3 2, R. Trappl (ed.), Elsevier, pp. 266-270 (1984) V. I. Kryukov. Short-tenn memory as a metastable state. I. Master equation approach, Cybernetics and Systems Research 3 2, R. Trappl (ed.), Elsevier, pp. 261-265 (1984) V. I. Kryukov. ""Neurolocator"" , a model of attention (1989) (in this volume). V. I. Kryukov, G. N. Borisyuk, R. M. Borisyuk, A. B. Kirillov, Ye. I. Kovalenko. The Metastable and Unstable States in the Brain (in Russian), Pushchino, Acad. Sci. USSR (1986) (to appear in Stochastic Cellular Syste.ms: Ergodici tY3 l1emory3 Morphogenesis, Manchester University Press, 1989).","[-0.06396576017141342, -0.08260013908147812, -0.03922831267118454, 0.017746832221746445, -0.05755418911576271, 0.07386276870965958, 0.0017344446387141943, 0.03585926070809364, 0.08829519152641296, -0.028770852833986282, 0.03723897412419319, 0.010229669511318207, 0.003332428866997361, 0.008156245574355125, -0.06883436441421509, -0.029364285990595818, -0.022280052304267883, 0.01590326800942421, -0.07450952380895615, 0.00789486151188612, 0.048068877309560776, -0.015276414342224598, -0.006309813354164362, -0.04215873405337334, -0.04053451493382454, -0.03556552901864052, 0.0004936010227538645, 0.014282817021012306, 0.03697206825017929, 0.0019231216283515096, 0.05358919873833656, -0.03699534386396408, -0.023996327072381973, 0.0383988581597805, -0.017102088779211044, 0.0032800319604575634, -0.06005522236227989, -0.05793597176671028, -0.016547249630093575, -0.005311649292707443, 0.0812654122710228, 0.08912073820829391, -0.011067814193665981, 0.01600068435072899, 0.10240087658166885, 0.06679533421993256, 0.03429926559329033, -0.08041585236787796, -0.12256920337677002, -0.031171804293990135, -0.028545692563056946, -0.04637106880545616, 0.00953352265059948, 0.044690102338790894, -0.009531652554869652, 0.019477112218737602, -0.04747038334608078, 0.022993797436356544, -0.03977120667695999, 0.012002152390778065, 0.0043751150369644165, 0.05370660871267319, 0.013270242139697075, -0.046402182430028915, 0.049932196736335754, 0.036335863173007965, -0.035327959805727005, 0.0266067273914814, -0.0325191468000412, -0.0015573999844491482, 0.06731628626585007, 0.002943973755463958, -0.01924089528620243, -0.07693195343017578, 0.014613873325288296, -0.042056746780872345, -0.0005102475988678634, 0.028335295617580414, 0.013473644852638245, -0.046659115701913834, 0.02382630854845047, -0.05721396207809448, -0.010828080587089062, -0.08849235624074936, -0.0272138062864542, 0.007354170083999634, 0.05229341611266136, 0.006209843792021275, -0.003614563960582018, 0.048632193356752396, -0.03136347234249115, 0.014677777886390686, -0.022587036713957787, -0.09674768894910812, 0.03191809356212616, 0.0002277700259583071, 0.06113792955875397, 0.04542433097958565, 0.03921492025256157, 0.05633581802248955, 0.01467414852231741, 0.0052920738235116005, 0.04189294949173927, 0.0010576896602287889, 0.013851901516318321, 0.07094260305166245, 0.12058288604021072, 0.027086393907666206, 0.0014047067379578948, 0.012354537844657898, -0.06266449391841888, 0.013357455842196941, -0.029872819781303406, 0.07816599309444427, 0.11736779659986496, -0.0036887091118842363, -0.06450413167476654, 0.04107685759663582, 0.10025224834680557, -0.027452193200588226, 0.045061785727739334, -0.07516523450613022, -0.10586434602737427, -0.031091084703803062, -0.01659458503127098, 0.02718850038945675, -0.1228182390332222, 7.277090265965318e-33, -0.03667531907558441, -0.040426403284072876, -0.010552036575973034, -0.0881849080324173, -0.011634285561740398, -0.030336830765008926, -0.02096295915544033, 0.014929568395018578, 0.02979879640042782, -0.007098894566297531, -0.11175570636987686, -0.02423342689871788, -0.03920798748731613, 0.03681546822190285, 0.006360028870403767, -0.06847747415304184, -0.02341204509139061, -0.043092165142297745, 0.059460774064064026, -0.0601842887699604, 0.16949349641799927, -0.04265207797288895, -0.009177380241453648, 0.029655979946255684, -0.021126672625541687, -0.014921053312718868, -0.046491727232933044, 0.03400467708706856, -0.05506192520260811, -0.003614811459556222, -0.005740325897932053, 0.0276163462549448, -0.1292923092842102, 0.0028905002400279045, 0.02682703360915184, -0.03351564332842827, 0.09780487418174744, 0.044537633657455444, 0.04608607292175293, -0.015743710100650787, -0.0013835188001394272, -0.04395369067788124, -0.012430991977453232, 0.03784635290503502, 0.05494152009487152, -0.039476390928030014, 0.06251294910907745, 0.07840712368488312, 0.05580228194594383, -0.029849126935005188, -0.0266764834523201, -0.016342932358384132, -0.013162008486688137, -0.056953106075525284, 0.031898610293865204, 0.046755459159612656, 0.06906016916036606, 0.08120317757129669, -0.024290315806865692, 0.10990805178880692, -0.007506860885769129, -0.0541318841278553, 0.07300771027803421, -0.0060587078332901, 0.08427640050649643, 0.11030616611242294, -0.06110566854476929, -0.05153936147689819, 0.04780016839504242, -0.027257295325398445, 0.03768352419137955, 0.10422908514738083, 0.04706211015582085, -0.08715789020061493, -2.700684308365453e-05, -0.034390684217214584, -0.026594387367367744, 0.0256190188229084, -0.15096917748451233, -0.006643014494329691, 0.0602654404938221, -0.022345662117004395, -0.05416513979434967, 0.07664617896080017, 0.011863318271934986, -0.014421013183891773, 0.09367972612380981, 0.010160472244024277, -0.04411057382822037, 0.0037089933175593615, 0.012385731562972069, -0.017016414552927017, 0.07332397252321243, -0.0007209308096207678, -0.00411501107737422, -6.470205229621976e-33, 0.011910277418792248, 0.00415659137070179, 0.017435036599636078, 0.06035050377249718, -0.017107931897044182, 0.08901338279247284, 0.00046672634198330343, 0.06885407865047455, -0.06502396613359451, -0.021029194816946983, 0.06165303289890289, -0.04547528550028801, 0.045198727399110794, -0.02859581634402275, 0.028674811124801636, -0.0633416399359703, 0.023693513125181198, 0.03399089351296425, 0.09238919615745544, -0.020661979913711548, -0.01616058312356472, 0.1177605390548706, -0.10922662913799286, -0.04617257043719292, 0.023151125758886337, 0.027469728142023087, -0.03785182908177376, 0.06747541576623917, -0.04310194030404091, 0.0628943219780922, -0.07701529562473297, 0.04703952744603157, -0.03747880086302757, -0.06261745095252991, -0.024228541180491447, 0.10590866953134537, 0.026382606476545334, -0.06692810356616974, -0.03309449926018715, -0.016485542058944702, 0.0897376760840416, 0.04086974635720253, 0.06825427711009979, 0.01066890824586153, 0.04359690472483635, 0.010302651673555374, -0.052617523819208145, 0.0066503253765404224, -0.019782669842243195, 0.02452862076461315, 0.09163700044155121, -0.07007703930139542, -0.05160408467054367, -0.0607527494430542, -0.015463797375559807, 0.08839856833219528, -0.00411535520106554, -0.024985801428556442, 0.014450769871473312, -0.0635361522436142, -0.07407516986131668, -0.1442001760005951, 0.06226355582475662, -0.05844566971063614, -0.0019309044582769275, -0.03215490281581879, -0.016074441373348236, 0.05552690103650093, 0.1338219940662384, -0.0071701775304973125, -0.0029424703679978848, 0.006442527752369642, 0.078965924680233, 0.02612515538930893, -0.0034302237909287214, -0.022233108058571815, -0.012613487429916859, -0.006927520036697388, -0.011904386803507805, -0.031989481300115585, -0.04391428083181381, 0.036053165793418884, -0.05102543160319328, -0.03889065235853195, -0.027613503858447075, -0.03707139566540718, -0.060054853558540344, 0.04374747350811958, 0.03442857414484024, -0.028872331604361534, 0.004468963947147131, 0.044563230127096176, 0.006100395228713751, 0.06369540095329285, -0.0023417670745402575, -5.9210126579500866e-08, 0.05352430418133736, -0.0196914654225111, -0.04198316112160683, 0.05336703732609749, 0.10376740247011185, -0.06797467917203903, 0.060353994369506836, -0.13690944015979767, -0.06799532473087311, 0.025588054209947586, 0.08834342658519745, -0.033101215958595276, -0.00890254694968462, -0.09577631205320358, -0.01472493913024664, -0.0033637050073593855, 0.022265566512942314, 0.06437008082866669, 0.028572509065270424, -0.053249821066856384, 0.04306145757436752, -0.022595779970288277, -0.02838575653731823, 0.004696083255112171, 0.009247618727385998, -0.012082242406904697, -0.06717164069414139, -0.0028991864528506994, -0.040858712047338486, 0.023076998069882393, -0.027668997645378113, 0.04930822178721428, 0.01035104040056467, 0.014943616464734077, -0.018075525760650635, 0.00983983464539051, -0.006549343466758728, 0.03755646198987961, -0.015643056482076645, 0.08224285393953323, -0.02032027393579483, -0.004688899498432875, -0.06422674655914307, 0.03407401219010353, -0.046635523438453674, -0.003043451812118292, 0.044087138026952744, -0.07667502015829086, 0.032757166773080826, 0.015443060547113419, -0.016007397323846817, 0.036759816110134125, -0.053266070783138275, -0.015043611638247967, -0.09125377982854843, 0.026864981278777122, 0.006323504261672497, -0.03351269289851189, -0.10174772143363953, 0.03074587509036064, -0.04119057208299637, 0.08024021983146667, -0.03865106403827667, -0.05912025272846222]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\OntheKWinnersTakeAllNetwork.pdf,Computer Vision,"A NETWORK FOR IMAGE SEGMENTATION USING COLOR Anya Hurlbert and Tomaso Poggio Center for Biological Information Processing at Whitaker College Department of Brain and Cognitive Science and the MIT AI Laboratory Cambridge, MA 02139 (hur lbert@wheaties.ai.mit.edu) ABSTRACT We propose a parallel network of simple processors to find color boundaries irrespective of spatial changes in illumi- nation, and to spread uniform colors within marked re- . glOns. INTRODUCTION To rely on color as a cue in recognizing objects, a visual system must have at least approximate color constancy. Otherwise it might ascribe different characteristics to the same object under different lights. But the first step in using color for recog- nition, segmenting the scene into regions of different colors, does not require color constancy. In this crucial step color serves simply as a means of distinguishing one object from another in a given scene. Color differences, which mark material boundaries, are essential, while absolute color values are not. The goal of segmen- tation algorithms is to achieve this first step toward object recognition by finding discontinuities in the image irradiance that mark material boundaries. The problems that segmentation algorithms must solve is how to choose color la- bels, how to distinguish material boundaries from other changes in the image that give rise to color edges, and how to fill in uniform regions with the appropriate color labels. (Ideally, the color labels should remain constant under changes in the illumination or scene composition and color edges should occur only at material boundaries.) Rubin and Richards (1984 ) show that algorithms can solve the sec- ond problem under some conditions by comparing the image irradiance signal in distinct spectral channels on either side of an edge. The goal of the segmentation algorithms we discuss here is to find boundaries be- tween regions of different surface spectral reflectances and to spread uniform colors within them, without explicitly requiring the colors to be constant under changes in illumination. The color labels we use are analogous to the CIE chromaticity coordinates x and y. Under the single source assumption, they change across space 297 298 Hurlbert and Poggio only when the surface spectral reflectance changes, except when strong speculari- ties are present. (The algorithms therefore require help at a later stage to identify between color label changes due to specularities, which we have not yet explicitly incorporated.) The color edges themselves are localised with the help of luminance edges, by analogy with psychophysics of segmentation and filling-in. The Koftka Ring illusion, for example, indicates that color is attributed to surfaces by an inter- action between an edge-finding operator and a filling-in operator.1 The interaction is justified by the fact that in the real world changes in surface spectral reflectance are almost always accompanied by changes in brightness. Color Labels We assume that surfaces reflect light according to the neutral-interface-reflection model. In this model (Lee, 1986 , Shaefer, 1984 [3]) the image irradiance I(X,y,A) is the sum of two components, the surface reflection and the body reflection: I(x, y, A) = L(r(x, y), A)[a(r, A)g(6(r)) + bh(6(r))], where A labels wavelength and r( x, y) is the point on the 3D surface to which the image coordinates (x, y) correspond. L(r(x, y), A) is the illumination on the surface. a(r, A) is the spectral reflectance factor of the body reflection component and g(6(r)) its magnitude, which depends on the viewing geometry parameters lumped together in 6(r). The spectral reflectance factor of the specular, or surface reflection, component b is assumed to be constant with respect to A, as is true for inhomogeneous materials such as paints and plastics. For most materials, the magnitude of the specular component h depends strongly on the viewing geometry. Using the single source assumption, we may factor the illumination L into separate spatial and spectral components (L(r, A) L(r)c(A)). Multiplying I by the spectral sensitivities of the color sensors i = 1,2,3 and integrating over wavelength yields the triplet of color values (R, G, B), where and so forth and where the ai and bi are the reflectance factors in the spectral channels defined by the sensor spectral sensitivities. We define the hues u and v as R u= --__ -- R+G+B and 1 Note that Land's original retinex algorithm, which thresholds and swns the differences in image irradiance between adjacent points along many paths, accounts for the contribution of edges to color, without introducing a separate luminance edge detector. A Network for Image Segmentation Using Color 299 G v=----- R+G+B at each pixel. In Lambertian reflection, the specular reflectance factor b is zero. In this case, u and v are piecewise constant: they change in the image only when the ai(x,y) change. Thus u or v mark discontinuities in the surface spectral reflectance function, e.g they mark material boundaries. Conversely, image regions of constant u correspond to regions of constant surface color. Synthetic images generated with standard computer graphics algorithms (using, for example, the Phong reflectance model) behave in this way: u is constant across the visible surface of a shaded sphere. Across specularities, u in general changes but often not much. Thus one approach to the segmentation problem is to find regions of ""constant"" u and their boundaries. The difficulty with this approach is that real u data are noisy and unreliable: u is the quotient of numbers that are not only noisy themselves but also, at least for biological photosensor spectral sensitivities, very close to one another. The goals of segmentation algorithms are therefore to enhance discontinuities in u and, within the regions marked by the discontinuities, to smoothe over the noise and fill in the data where they are unreliable. We have explored several methods of meeting these goals. Segmentation Algorithms One method is to regularize - to eliminate the noise and fill in the data, while preserving the discontinuities. Using an algorithm based on Markov Random Field techniques, we have obtained encouraging results on real images (see Poggio et al., 1988). The MRF technique exploits the constraint that u should be piecewise constant within the discontinuity contours and uses image brightness edges as guides in finding the contours. An alternative to the MRF approach is a cooperative network that fills in data and filters out noise while enforcing the constraint of piecewise constancy. The network, a type of Hopfield net, is similar to the cooperative stereo network of Marr and Poggio (1976). Another approach consists of a one-pass winner-take-all scheme. Both algorithms involve loading the initial hue values into discrete bins, an undesirable and biologically unlikely feature. Although they produce good results on noisy synthetic images and can be improved by modification (see Hurlbert, 1989), another class of algorithms which we now describe are simple and effective, especially on parallel computers such as the Connection Machine. Averaging Network One way to avoid small step changes in hue across a uniform surface resulting from initial loading into discrete bins is to relax the local requirement for piecewise 300 Hurlbert and Poggio b. 41 97 "" 74 Figure 1: (a) Image of a Mondrian-textured sphere - the red channel. (b) Vertical slice through the specularity in a 75 x 75 pixel region of the three-channel image (R + G + B) of the same sphere. constancy and instead require only that hue be smooth within regions delineated by the edge input. We will see that this local smoothness requirement actually yields an iterative algorithm that provides asymptotically piecewise constant hue regions. To implement the local smoothness criterion we use an averaging scheme that simply replaces the value of each pixel in the hue image with the average of its local surround, iterating many times over the whole image. The algorithm takes as input the hue image (either the u-image or the v-image) and one or two edge images, either luminance edges alone, or luminance edges plus u or v edges, or u edges plus v edges. The edge images are obtained by performing Canny edge detection or by using a thresholded directional first derivative. On each iteration, the value at each pixel in the hue image is replaced by the average of its value and those in its contributing neighborhood. A neighboring pixel is allowed to contribute if (i) it is one of the four pixels sharing a full border with the central pixel (ii) it shares the same edge label with the central pixel in all input edge images (iii) its value is non-zero and (iv) its value is within a fixed range of the central pixel value. The last requirement simply reinforces the edge label requirement when a hue image serves as an input edge image - the edge label requirement allows only those pixels that lie on the same side of an edge to be averaged, while the other insures that only those pixels with similar hues are averaged. More formally A Network for Image Segmentation Using Color 301 where Cn(hf,j) is the set of N(Cn) pixels among the next neighbors of i,j that differ from h~. less than a specified amount and are not crossed by an edge in the edge map(s) (on the assumption that the pixel (i,j) does not belong to an edge). The iteration of this operator is similar to nonlinear diffusion and to discontinuous regularization of the type discussed by Blake and Zisserman (1987), Geman and Geman (1984) and Marroquin (9]. The iterative scheme of the above equation can be derived from minimization via gradient descent of the energy function E = L:Ei,j with where V(x, y) = V(x - y) is a quadratic potential around 0 and constant for Ix - yl above a certain value. The local averaging smoothes noise in the hue values and spreads uniform hues across regions marked by the edge inputs. On images with shading but without strong specularities the algorithm performs a clean segmentation into regions of different hues. Conclusions The averaging scheme finds constant hue regions under the assumptions of a single source and no strong specularities. A strong highlight may originate an edge that could then ""break"" the averaging operation. In our limited experience most spec- ularities seem to average out and disappear from the smoothed hue map, largely because even strong specularities in the image are much reduced in the initial hue image. The iterative averaging scheme completely eliminates the remaining gradi- ents in hue. It is possible that more powerful discrimination of specularities will require specialized routines and higher-level knowledge (Hurlbert, 1989). Yet this simple network alone is sufficient to reproduce some psychophysical phe- nomena. In particular, the interaction between brightness and color edges enables the network to mimic such visual ""illusions"" as the Koftka Ring. We replicate the illusion in the following way. A black-and-white Koft'ka Ring (a uniform grey annu- lus against a rectangular bipartite background, one side black and the other white) (Hurlbert and Poggio, 1988b) is filtered through the lightness filter estimated in 302 Hurlbert and Poggio a. c. 9.1989 9 72 9.49679872 9.1122449 9 h. 299 Figure 2: (a) A 75x75 pixel region of the u image, including the specularity. (b) The image obtained after 500 iterations of the averaging network on (a), using as edge input the Canny edges of the luminance image. A threshold on differences in the v image allows only similar v values to be averaged. (c) Vertical slice through center of (a). (d) Vertical slice at same coordinates through (b) (note different scales of (c) and (d». A Network for Image Segmentation Using Color 303 the way described elsewhere (Hurlbert and Poggio, 1988a). (For black-and-white images this step replaces the operation of obtaining u and v: in both cases the goal is to eliminate spatial gradients of in the effective illumination.) The filtered Koffka Ring is then fed to the averaging network together with the brightness edges. When in the input image the boundary between the two parts of the background continues across the annulus, in the output image (after 2000 iterations of the averaging net- work) the annulus splits into two semi-annuli of different colors in the output image, dark grey against the white half, light grey against the black half (Hurlbert, 1989). When the boundary does not continue across the annulus, the annulus remains a uniform grey. These results agree with human perception. Acknowledgements This report describes research done within the Center for Biological Information Processing, in the Department of Brain and Cognitive Sciences, and at the Artifi- cial Intelligence Laboratory. This research is sponsored by a grant from the Office of Naval Research (ONR), Cognitive and Neural Sciences Division; by the Artificial Intelligence Center of Hughes Aircraft Corporation; by the Alfred P. Sloan Foun- dation; by the National Science Foundation; by the Artificial Intelligence Center of Hughes Aircraft Corporation (SI-801534-2); and by the NATO Scientific Affairs Di- vision (0403/87). Support for the A. I. Laboratory's artificial intelligence research is provided by the Advanced Research Projects Agency of the Department of Defense under Army contract DACA76-85-C-001O, and in part by ONR contract NOOOI4- 85-K-0124. Tomaso Foggio is supported by the Uncas and Helen Whitaker Chair at the Massachusetts Institute of Technology, Whitaker College. References John Rubin and Whitman Richards. Colour VISIon: representing material cate- gories. Artificial Intelligence Laboratory Memo 764, Massachusetts Institute of Technology, 1984. Hsien-Che Lee. Method for computing the scene-illuminant chromaticity from spec- ular highlights. Journal of the Optical Society of America, 3:1694-1699, 1986. Steven A. Shafer. Using color to separate reflection components. Color Research and Applications, 10(4):210-218, 1985. Tomaso Poggio, J. Little, E. Gamble, W. Gillett, D. Geiger, D. Weinshall, M. Vil- lalba, N. Larson, T. Cass, H. Biilthoff, M. Drumheller, P. Oppenheimer, W. Yang, and A. Hurlbert. The MIT Vision Machine. In Proceedings Image Understanding Workshop, Cambridge, MA, April 1988. Morgan Kaufmann, San Mateo, CA. David Marr and Tomaso Poggio. Cooperative computation of stereo disparity. Sci- ence, 194:283-287, 1976. Anya C. Hurlbert. The Computation of Color. PhD thesis, Massachusetts Institute of Technology, Cambridge, MA, 1989. Jose L. Marroquin. Probabilistic Solution of Inverse Problems. PhD thesis, Mas- sachusetts Institute of Technology, Cambridge, MA, 1985. 304 Hurlbert and Poggio Andrew Blake and Andrew Zisserman. Visual Reconstruction. MIT Press, Cam- bridge, Mass, 1987. Stuart Geman and Don Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern A nalysis and Machine Intelligence, PAMI-6:721-741, 1984. Anya C. Hurlbert and Tomaso A. Poggio. Learning a color algorithm from examples. In Dana Z. Anderson, editor, Neural Information Processing Systems. American Institute of Physics, 1988. A. C. Hurlbert and T. A. Poggio. Synthesizing a color algorithm from examples. Science, 239:482-485, 1988.","[0.028468240052461624, -0.013177086599171162, 0.057888101786375046, -0.046181004494428635, 0.07169095426797867, -0.07216846197843552, 0.07012870907783508, -0.046842772513628006, 0.03834204375743866, -0.07596070319414139, -0.06802932173013687, -0.08027859032154083, 0.04022059217095375, 0.07485613971948624, -0.03900262340903282, 0.00908189918845892, 0.008403417654335499, 0.1188545748591423, -0.10865148901939392, -0.031229179352521896, 0.02046581357717514, -0.05072671175003052, 0.013396993279457092, -0.03249449282884598, -0.04062278941273689, 0.03905804082751274, 0.04863879829645157, -0.05939518287777901, -0.007688390091061592, -0.010742990300059319, 0.04742792248725891, -0.05472821369767189, 0.028321970254182816, 0.06505949795246124, -0.029890691861510277, -0.00887308456003666, -0.0018137210281565785, -0.022561069577932358, -0.0317709818482399, 0.010504513047635555, -0.02751189097762108, 0.01045245211571455, -0.012094923295080662, -0.01638675108551979, 0.08767063915729523, 0.05158153176307678, 0.05542756989598274, 0.009627608582377434, -0.028540650382637978, -0.05007077008485794, -0.001270797336474061, -0.018101872876286507, -0.07539363950490952, 0.07926030457019806, -0.012209437787532806, 0.05974864587187767, 0.055893488228321075, -0.03389422222971916, 0.03407556936144829, -0.01333486195653677, 0.009541712701320648, -0.07146447151899338, 0.016984600573778152, -0.024502942338585854, -0.024910634383559227, 0.021068187430500984, -0.010837110690772533, -0.0458514504134655, 0.013126084581017494, -0.06590345501899719, 0.06906293332576752, 0.08077201247215271, 0.06637551635503769, 0.03245648741722107, -0.005093865096569061, 0.049981944262981415, 0.1261470913887024, 0.08855650573968887, -0.036201152950525284, -0.12952032685279846, 0.06425817310810089, 0.034557223320007324, 0.022371875122189522, 0.04289717227220535, 0.1476522833108902, -0.00828706007450819, -0.08383894711732864, 0.08970238268375397, -0.03776399418711662, 0.027477676048874855, -0.006012627389281988, 0.004327991511672735, 0.0029492012690752745, -0.04084456339478493, 0.03542347997426987, -0.07038499414920807, 0.01859115995466709, -0.05670235678553581, 0.03791284188628197, 0.04672398790717125, -0.020051805302500725, -0.05655478686094284, 0.03823895379900932, -0.055806469172239304, 0.010234001092612743, -0.004074486903846264, 0.05393586307764053, 0.004829710815101862, 0.09184399992227554, -0.05596001818776131, 0.058592747896909714, 0.004817838780581951, -0.022443091496825218, 0.02450253628194332, 0.009650284424424171, -0.010053430683910847, 0.031040441244840622, 0.03509846329689026, 0.07419849932193756, 0.0303523950278759, 0.020636126399040222, -0.04149334877729416, -0.06672936677932739, 0.0653008371591568, 0.09731283783912659, -0.036533091217279434, -0.033862195909023285, 4.40901519104745e-33, 0.012565182521939278, -0.05102887377142906, 0.00832583848387003, -0.026826871559023857, 0.06056647002696991, 0.06262540072202682, -0.053036753088235855, -0.0128566799685359, -0.03344490006566048, -0.01100880280137062, -0.0010544467950239778, 0.0056573860347270966, -0.02934960089623928, 0.05043816193938255, 0.07795340567827225, -0.04670826718211174, 0.04923687502741814, -0.028042022138834, -0.035620059818029404, -0.00906011089682579, -0.09459049254655838, -0.03846196457743645, 0.03951309993863106, 0.04412276670336723, -0.03187181428074837, 0.019945785403251648, -0.010714071802794933, -0.035363320261240005, 0.05924849212169647, -0.019146669656038284, 0.0007666242308914661, 0.028958026319742203, 0.021300729364156723, 0.10725859552621841, -0.0047712260857224464, 0.0013523087836802006, 0.03757577762007713, 0.025433961302042007, 0.03710095211863518, 0.011589963920414448, -0.025327306240797043, 0.11722775548696518, 0.023605238646268845, -0.029736701399087906, -0.015731647610664368, 0.005650699604302645, 0.034532152116298676, 0.07159022986888885, -0.007833946496248245, 0.08132488280534744, -0.02862462028861046, 0.001991088269278407, 0.03208719566464424, -0.142039954662323, -0.033529117703437805, 0.017595654353499413, 0.008283381350338459, 0.07513437420129776, 0.11092576384544373, -0.044938232749700546, 0.035845205187797546, 0.009386459365487099, 0.01952061429619789, 0.08759589493274689, 0.02481178566813469, 0.01480117253959179, -0.05930250883102417, 0.012763982638716698, 0.016049707308411598, 0.04053984954953194, -0.05110419541597366, 0.05136245861649513, 0.0009280912927351892, -0.03360973298549652, 0.03233593702316284, 0.006562422029674053, 0.0009262540261261165, -0.0467657670378685, -0.0020069899037480354, -0.043910957872867584, -0.10159867256879807, 0.03985501080751419, -0.09370310604572296, -0.09455665946006775, -0.08638457953929901, 0.05615248903632164, 0.04312704876065254, 0.026408758014440536, -0.05577603727579117, -0.06666935980319977, -0.01619429886341095, -0.020159104838967323, 0.010134163312613964, -0.030624067410826683, -0.00022448769595939666, -3.638681784200942e-33, 0.008444109000265598, -0.02346424199640751, -0.07671230286359787, 0.055222202092409134, -0.011464008130133152, -0.06734593957662582, -0.007313723210245371, -0.015583975240588188, -0.05770982801914215, -0.015105762518942356, 0.03563546761870384, 0.05432882532477379, -0.07686767727136612, 0.08876015990972519, -0.056614089757204056, -0.012800927273929119, -0.01934775523841381, 0.07921053469181061, 0.018606700003147125, 0.011162896640598774, -0.09164884686470032, 0.0982983186841011, -0.08995009958744049, -0.06052194535732269, -0.15836642682552338, 0.09386777132749557, -0.07218097895383835, 0.01598755642771721, -0.06505226343870163, -0.06965086609125137, -0.07426087558269501, 0.0074602775275707245, -0.04500637203454971, -0.0414847806096077, 0.021429171785712242, 0.021090181544423103, 0.04352737218141556, -0.07474169135093689, -0.04757989943027496, 0.05888112261891365, 0.06582760810852051, -0.05605504289269447, -0.04612730070948601, -0.003260140074416995, -0.049423910677433014, -0.04278784990310669, -0.03931859880685806, 0.09211736172437668, -0.10867727547883987, 0.06095448136329651, -0.02636115811765194, 0.019364871084690094, -0.07021670788526535, -0.04466154798865318, 0.04876070097088814, 0.042206671088933945, -0.06904381513595581, 0.038421910256147385, 0.013574535958468914, 0.03938673064112663, -0.09678268432617188, -0.0479111447930336, -0.016987435519695282, 0.06193682178854942, 0.03456674888730049, 0.00821787677705288, -0.03495370224118233, 0.03515622392296791, 0.05006352439522743, 0.019915543496608734, 0.029564451426267624, 0.0006172481807880104, 0.037881117314100266, -0.0011529880575835705, 0.03204387426376343, -0.044962890446186066, 0.02775583416223526, -0.02721339277923107, -0.008569687604904175, -0.002198323141783476, -0.045547645539045334, -0.11834240704774857, -0.029513875022530556, 0.06731104850769043, 0.05445648729801178, 0.03855963051319122, 0.00453766155987978, -0.05508854240179062, 0.0759509801864624, -0.04989675059914589, -0.0039579737931489944, 0.01325627975165844, 0.05924883484840393, -0.0033918628469109535, -0.09000685811042786, -5.2674373307581845e-08, -0.0050553325563669205, -0.03624819219112396, 0.022369742393493652, 0.007120031397789717, 0.08458653837442398, -0.039890967309474945, -0.013492046855390072, 0.03126715123653412, -0.03172900900244713, 0.03476850688457489, 0.045134685933589935, -0.08591658622026443, -0.019355082884430885, -0.030348045751452446, -0.026691116392612457, 0.07183124870061874, 0.008666981011629105, -0.05517930909991264, 0.008086657151579857, 0.10255816578865051, -0.075611412525177, -0.009684108197689056, -0.04162866622209549, 0.07025811076164246, 0.044020187109708786, -0.04044002294540405, -0.061068084090948105, 0.07153552770614624, 0.05739044398069382, -0.005263982340693474, 0.06183160841464996, 0.030166400596499443, 0.08104069530963898, 0.036786969751119614, 0.06164337322115898, -0.016417909413576126, -0.03859352320432663, 0.08774125576019287, -0.02678670547902584, -0.05350881814956665, -0.029517266899347305, -0.01904934085905552, 0.02660091035068035, -0.002802231814712286, 0.007882503792643547, -0.027886725962162018, 0.054308775812387466, -0.002805874915793538, -0.001667700707912445, 0.045504212379455566, -0.012332684360444546, 0.01177718210965395, -0.02633301168680191, 0.04933515936136246, -0.04445991665124893, -0.10632461309432983, 0.024388523772358894, -0.0106595354154706, 0.0736912339925766, 0.07142847031354904, 0.007123582065105438, 0.007824469357728958, -0.09473878890275955, -0.07829225808382034]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\OptimizationbyMeanFieldAnnealing.pdf,Deep Learning,"LEARNING WITH TEMPORAL DERIVATIVES IN PULSE-CODED NEURONAL SYSTEMS Mark Gluck David B. Parker Department of Psychology Stanford University Stanford, CA 94305 Abstract Eric S. Reifsnider A number of learning models have recently been proposed which involve calculations of temporal differences (or derivatives in continuous-time models). These models. like most adaptive network models. are formulated in tenns of frequency (or activation), a useful abstraction of neuronal firing rates. To more precisely evaluate the implications of a neuronal model. it may be preferable to develop a model which transmits discrete pulse-coded information. We point out that many functions and properties of neuronal processing and learning may depend. in subtle ways. on the pulse-coded nature of the informa- tion coding and transmission properties of neuron systems. When com- pared to formulations in terms of activation. computing with temporal derivatives (or differences) as proposed by Kosko (1986). Klopf (1988). and Sutton (1988). is both more stable and easier when refor- mulated for a more neuronally realistic pulse-coded system. In refor- mulating these models in terms of pulse-coding. our motivation has been to enable us to draw further parallels and connections between real-time behavioral models of learning and biological circuit models of the substrates underlying learning and memory. INTRODUCTION Learning algorithms are generally defined in terms of continuously-valued levels of input and output activity. This is true of most training methods for adaptive networks. (e.g .• Parker. 1987; Rumelhart. Hinton. & Williams, 1986; Werbos. 1974; Widrow & Hoff, 1960). and also for behavioral models of animal and hwnan learning. (e.g. Gluck & Bower. 1988a, 1988b; Rescorla & Wagner. 1972). as well as more biologically oriented models of neuronal function (e.g .• Bear & Cooper, in press; Hebb, 1949; Granger. Abros-Ingerson, Staubli, & Lynch, in press; Gluck & Thompson, 1987; Gluck. Reifsnider. & Thompson. in press; McNaughton & Nadel. in press; Gluck & Rumelhart. in press). In spite of the attractive simplicity and utility of the ""activation"" construct 195 196 Parker, Gluck and Reifsnider neurons use discrete trains of pulses for the transmission of information from cell to cell. Frequency (or activation) is a useful abstraction of pulse trains. especially for bridging the gap between whole-animal and single neuron behavior. To more precisely evaluate the implications of a neuronal model. it may be preferable to develop a model which transmits discrete pulse-coded information; it is possible that many functions and proper- ties of neuronal processing and learning may depend. in subtle ways. on the pulse-coded nature of the information coding and transmission properties of neuron systems. In the last few years, a number of learning models have been proposed which involve computations of temporal differences (or derivatives in continuous-time models). Klopf (1988) presented a formal real-time model of classical conditioning that predicts the magnitude of conditioned responses (CRs). given the temporal relationships between conditioned stimuli (eSs) and an unconditional stimulus (US). Klopf's model incor- porates a ""differential-Hebbian"" learning algorithm in which changes in presynaptic lev- els of activity are correlated with changes in postsynaptic levels of activity. Motivated by the constraints and motives of engineering. rather than animal learning. Kosko (1986) proposed the same basic rule and provided extensive analytic insights into its properties. Sutton (1988) introduced a class of incremental learning procedures. called ""temporal difference"" methods. which update associative (predictive) weights according to the difference between temporally successive predictions. In addition to the applied potential of this class of algorithms. Sutton & Barto (1987) show how their model. like Klopf's (1988) model. provides a good fit to a wide range of behavioral data on classical condi- tioning. These models. all of which depend on computations involving changes over time in activation levels. have been successful both for predicting a wide range of behavioral animal learning data (Klopf. 1988; Sutton & Barto. 1987) and for solving useful engineering problems in adaptive prediction (Kosko. 1986; Sutton. 1988). The possibility that these models might represent the computational properties of individual neurons. seems, at first glance. highly unlikely. However. we show by reformulating these models for pulse-coded communication (as in neuronal systems) rather than in terms of abstract activation levels. the computational soundness as well as the biological relevance of the models is improved. By avoiding the use of unstable differencing methods in computing the time-derivative of activation levels. and by increasing the error-tolerance of the com- putations, pulse coding will be shown to improve the accuracy and reliability of these models. The pulse coded models will also be shown to lend themselves to a closer com- parison to the function of real neurons than do models that operate with activation levels. As the ability of researchers to directly measure neuronal behavior grows. the value of such close comparisons will increase. As an example. we describe here a pulse-coded version of Klopf's differential-Hebbian model of classification learning. Further details are contained in Gluck. Parker. & Reifsnider. 1988. Learning with Temporal Derivatives 197 Pulse-Coding in Neuronal Systems We begin by outlining the general theory and engineering advantages of pulse-coding and then describe a pulse-coded refonnulation of differential-Hebbian learning. The key idea is quite simple and can be summarized as follows: Frequency can be seen, loosely speaking, as an integral of pulses; conversely, therefore, pulses can be thought of as car- rying infonnation about the derivatives of frequency. Thus, computing with the ""deriva- tives of frequency"" is analogous to computing with pulses. As described below, our basic conclusion is that differential-Hebbian learning (Klopf, 1988; Kosko, 1986) when refonnulated for a pulse-coded system is both more stable and easier to compute than is apparent when the rule is fonnulated in tenns of frequencies. These results have impor- tant implications for any learning model which is based on computing with time- derivatives, such as Sutton's Temporal Difference model (Sutton, 1988; Sutton & Barto, 1987) There are many ways to electrically transmit analog information from point to point. Perhaps the most obvious way is to transmit the infonnation as a signal level. In elec- tronic systems, for example, data that varies between 0 and 1 can be transmitted as a vol- tage level that varies between 0 volts and 1 volt This method can be unreliable, how- ever, because the receiver of the information can't tell if a constant DC voltage offset has been added to the information, or if crosstalk has occurred with a nearby signal path. To the exact degree that the signal is interfered with, the data as read by the receiver will be erroneously altered. The consequences of faults appearing in the signal are particularly serious for systems that are based on derivatives of the signal. In such systems, even a small, but sudden, unintended change in signal level can drastically alter its derivative, creating large errors. A more reliable way to transmit analog information is to encode it as the frequency of a series of pulses. A receiver can reliably detennine if it has received a pulse, even in the face of DC voltage offsets or moderate crosstalk. Most errors will not be large enough to constitute a pulse, and thus will have no effect on the transmitted infonnation. The receiver can count the number of pulses received in a given time window to detennine the frequency of the pulses. Further infonnation on encoding analog infonnation as the frequency of a series of pulses can be found in many electrical engineeri.ng textbooks (e.g., Horowitz & Hill, 1980). As noted by Parker (1987), another advantage of coding an analog signal as the fre- quency of a series of pulses is that the time derivative of the signal can be easily and stably calculated: If x (t) represents a series of pulses (x equals 1 if a pulse is occuring at time t; otherwise it equals 0) then we can estimate the frequency, f (t), of the series of pulses using an exponentially weighted time average: f (t) = Jllx ('t)e-Jl{t-'t) d't 198 Parker, Gluck and Reifsnider where Jl is the decay constant. The well known formula for the derivative of 1 (t) is AtJP- = Jl~(t)-/(t)) Thus. the time derivative of pulse-coded information can be calculated without using any unstable differencing methods. it is simply a function of presence or absence of a pulse relative to the current expectation (frequency) of pulses. As described earlier. calculation of time derivatives is a critical component of the learning algorithms proposed by Klopf (1988). Kosko (1986) and Sutton (Sutton. 1988; Sutton & Barto 1987). They are also an important aspect of 2nd order (pseudo-newtonian) extensions of the backpropogation learning rule for multi-layer adaptive ""connectionist"" networks (parker. 1987). Summary 01 Klopf s Model Klopf (1988) proposed a model of classical conditioning which incorporates the same learning rule proposed by Kosko (1986) and which extends some of the ideas presented in Sutton and Barto's (1981) real-time generalization of Rescorla and Wagner's (1972) model of classical conditioning. The mathematical specification of Klopf s model con- sists of two equations: one which calculates output signals based on a weighted sum of input signals (drives) and one which determines changes in synapse efficacy due to changes in signal levels. The specification of signal output level is defined as where: y (t ) is the measure of postsynaptic frequency of firing at time t; Wi (t) is the efficacy (positive or negative) of the i th synapse; Xi (t) is the frequency of action poten- tials at the i th synapse; 9 is the threshold of firing; and n is the number of synapses on the ""neuron"". This equation expresses the idea that the postsynaptic firing frequency depends on the summation of the weighted presynaptic firing frequencies. Wi (t )Xi (t ). relative to some threshold. 9. The learning mechanism is defined as where: ~Wi (t) is the change in efficacy of the i th synapse at time t; ~y (t) is the change in postsynaptic firing at time t; 't' is the longest interstimulus interval over which delayed conditioning is effective. The C j are empirically established learning rate constants -- each corresponding to a different inter-stimulus interval. In order to accurately simulate various behavioral phenomena observed in classical con- ditioning. Klopf adds three ancillary assumptions to his model. First. he places a lower bound of 0 on the activation of the node. Second. he proposes that changes in synaptic Learning with Temporal Derivatives 199 weight, ~w; (t), be calculated only when the change in presynaptic signal level is positive -- that is, when Ax; (t-j) > O. Third, he proposes separate excitatory and inhibitory weights in contrast to the single real-valued associative weights in other conditioning models (e.g., Rescorla & Wagner, 1972; Sutton & Barto, 1981). It is intriguing to note that all of these assumptions are not only sufficiently justified by constraints from behavioral data but are also motivated by neuronal constraints. For a further examination of the biological and behavioral factors supporting these assumptions see Gluck, Parker, and Reifsnider (1988). The strength of Klopf's model as a simple formal behavioral model of classical condi- tioning is evident. Although the model has not yielded any new behavioral predictions, it has demonstrated an impressive ability to reproduce a wide, though not necessarily com- plete, range of Pavlovian behavioral phenomena with a minimum of assumptions. Klopf (1988) specifies his learning algorithm in terms of activation or frequency levels. Because neuronal systems communicate through the transmission of discrete pulses, it is difficult to evaluate the biological plausibility of an algorithm when so formulated. For this reason, we present and evaluate a pulse-coded reformulation of Klopf's model. A Pulse-Coded Reformulation of Klopf s Model We illustrate here a pulse-coded reformulation of Klopf's (1988) model of classical con- ditioning. The equations that make up the model are fairly simple. A neuron is said to have fired an output pulse at time t if vet) > e, where e is a threshold value and vet) is defined as follows: vet) = (l-d)v(t-l) + !:Wi(t)Xi(t) (1) where v (t) an auxiliary variable, d is a small positive constant representing the leakage or decay rate, Wi (t) is the efficacy of synapse i at time t, and Xi (t) is the frequency of presynaptic pulses at time t at synapse i. The input to the decision of whether the neuron will fire consists of the weights and efficacies of the synapses as well as information about previous activation levels at the neuronal output Note that the leakage rate, d, causes older information about activation levels to have less impact on current values of v (t) than does recent information of the same type. The output of the neuron, p (t), is: v (t) > e then p (t) = 1 (pulse generated) v (t ) ~ e then p (t) = 0 (no pulse generated) It is important that once p (t) has been determined, v (t) will need to be adjusted if 200 Parker, Gluck and Reifsnider p (t) = 1. To reflect the fact that the neuron has fired, (i.e., p (t) = 1) then v (t) = v (t) - 1. This decrement occurs after p (t) has been determined for the current t. Frequencies of pulses at the output node and at the synapses are calculated using the following equa- tions: / (t) = / (t-l) + 11/(t) where 11/ (t) = m(p (t) - / (t-l)) where / (t) is the frequency of outgoing pulses at time t; p (t) is the ouput (1 or 0) of the neuron at time t ; and m is a small positive constant representing a leakage rate for the frequency calculation. Following Klopf (1988), changes in synapse efficacy occur according to (2) where I1Wi(t) = Wi (t+l) - Wi(t) and l1y (t) and ru:i (t) are calculated analogously to 11/ (t); 't is the longest interstimulus interval (lSI) over which delay conditioning is effective; and C j is an empirically esta- blished set of learning rates which govern the efficacy of conditioning at an lSI of j . Changes in Wi (t) are governed by the learning rule in Equation 2 which alters v (t) via Equation 1. Figure 1 shows the results of a computer simulation of a pulse-coded version of Klopf's conditioning model. The first graph shows the excitatory weight (dotted line) and inhibi- tory weight (dashed line) of the CS ""synapse"". Also on the same graph is the net synaptic weight (solid line), the sum of the excitatory and inhibitory weights. The subsequent graphs show CS input pulses, US input pulses, and the output (CR) pulses. The simula- tion consists of three acquisition trials followed by three extinction trials. Learning with Temporal Derivatives 201 0.6 en l! 0.4 ( .............................. .J ~ .................................. ( ................................. , ·V1···········U ............................. . .~ 02 ; . ~ 0.0 ·0.2 . ....... . ------------------------------------------~-----~----------- o 50 100 150 cycle 200 250 Figure 1. Simulation of pulse.coded version of Klopf's conditioning model. Top panel shows excitatory and inhibitory weights as dashed lines and the net synaptic weight of the CS as a solid line. Lower panels show the CS and US inputs and the CR output. As expected, excitatory weight increases in magnitude over the three ~quisition trials, while inhibitory weight is stable. During the first two extinction trials, the excitatory and the net synaptic weights decrease in magnitude, while the inhibitory weight increases. Thus, the CS produces a decreasing amount of output pulses (the CR). During the third extinction trial the net synaptic weight is so low that the CS cannot produce output pulses, and so the CR is extinct. However, as net weight and excitatory weight remain positive, there are residual effects of the acquisition which will accelerate reacquisition. Because a threshold must be reached before a neuronal output pulse can be emitted, and because output must occur for weight changes to occur, pulse coding adds to the accelerated reacquisition effect that is evident in the original Klopf model; extinction is halted before net weight is zero, when pulses can no longer be produced. 300 202 Parker, Gluck and Reifsnider Discussion To facilitate comparison between learning algorithms involving temporal derivative com- putations and actual neuronal capabilities. we formulated a pulse-coded variation of Klopfs classical conditioning model. Our basic conclusion is that computing with tem- poral derivatives (or differences) as proposed by Kosko (1986). Klopf (1988). and Sutton (1988). is more stable and easier when reformulated for a more neuronally realistic. pulse-coded system. than when the rules are fonnulated in terms of frequencies or activa- tion. It is our hope that further examination of the characteristics of pulse-coded systems may reveal facts that bear on the characteristics of neuronal function. In refonnulating these algorithms in terms of pulse-coding. our motivation has been to enable us to draw further parallels and connections between real-time behavioral models of learning and biological circuit models of the substrates underlying classical conditioning. (e.g .• Thompson. 1986; Gluck & Thompson. 1987; Donegan. Gluck. & Thompson. in press). More generally. noting the similarities and differences between algorithmic/behavioral theories and bio- logical capabilities is one way of laying the groundwork for developing more complete integrated theories of the biological bases of associative learning (Donegan. Gluck. & Thompson. in press). Acknowledgments Correspondence should be addressed to: Mark A. Gluck. Dept of Psychology. Jordan Hall; Bldg. 420. Stanford. CA 94305. For their commentary and critique on earlier drafts of this and related papers. we are indebted to Harry Klopf. Bart Kosko. Richard Sutton. and Richard Thompson. This research was supported by an Office of Naval Research Grant to R. F. Thompson and M. A. Gluck. References Bear. M. F., & Cooper, L. N. (in press). Molecular mechanisms for synaptic modification in the visual cortex: Interaction between theory and experiment. In M. A. Gluck, & D. E. Rumelhart (Eds.), Neuroscience and Connectionist Theory. Hillsdale, N.J.: Lawrence Erl- baum Associates .. Donegan, N. H., Gluck, M. A., & Thompson, R. F. (1989). Integrating behavioral and biological models of classical conditioning. In R. D. Hawkins, & G. H. Bower (Eds.), Computational models of learning in simple neural systems (Volume 22 of the Psychology of Learning and Motivation). New York: Academic Press. Gluck, M. A., & Bower. G. H. (1988a). Evaluating an adaptive network model of human learning. Journal of Memory and Language, 27, 166-195. Gluck, M. A., & Bower, G. H. (1988b). From conditioning to category learning: An adaptive net- work model. Journal of Experimental Psychology: General, 117(3), 225-244. Learning with Temporal Derivatives 203 Gluck, M. A., Parker, D. B., & Reifsnider, E. (1988). Some biological implications of a differential-Hebbian learning rule. Psychobiology, 16(3), 298-302. Gluck, M. A, Reifsnider, E. S., & Thompson, R. F. (in press). Adaptive signal processing and tem- poral coarse coding: Cerebellar models of classical conditioning and VOR Adaptation. In M. A. Gluck, & D. E. Rumelhart (Eds.), Neuroscience and Connectionist Theory. Hillsdale, N.1.: Lawrence Erlbaum Associates .. Gluck, M. A, & Rumelhart, D. E. (in press). Neuroscience and Connectionist Theory. Hillsdale, N.J.: Lawrence Erlbaum Associates .. Gluck, M. A., & Thompson, R. F. (1987). Modeling the neural substrates of associative learning and memory: A computational approach. Psychological Review, 94, 176-191. Granger, R., Ambros-Ingerson, 1., Staubli, U., & Lynch, G. (in press). Memorial operation of multi- pIe, interacting simulated brain structures. In M. A. Gluck, & D. E. Rumelhart (Eds.), Neu- roscience and Connectionist Theory. Hillsdale, N.J.: Lawrence Erlbaum Associates .. Hebb, D. (1949). Organization of Behavior. New York: Wiley & Sons. Horowitz, P., & Hill, W. (1980). The Art of Electronics. Cambridge, England: Cambridge Univer- sity Press. Klopf, A. H. (1988). A neuronal model of classical conditioning. Psychobiology, 16(2), 85-125. Kosko, B. (1986). Differential hebbian learning. In 1. S. Denker (Ed.), Neural Networksfor Com- puting, AlP Conference Proceedings 151 (pp. 265-270). New York: American Institute of Physics. McNaughton, B. L., & Nadel, L. (in press). Hebb-Marr networks and the neurobiological represen- tation of action in space. In M. A. Gluck, & D. E. Rumelhart (Eds.), Neuroscience and Con- nectionist Theory. Hillsdale, N.J.: Lawrence Erlbaum Associates .. Parker, D. B. (1987). Optimal Algorithms for Adaptive Networks: Second Order Back Propaga- tion, Second Order Direct Propagation, and Second Order Hebbian Learning. Proceedings of the IEEE First Annual Conference on Neural Networks. San Diego, California:, . Rescorla. R. A, & Wagner, A. R. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and non-reinforcement. In A. H. Black, & W. F. Prokasy (Eds.), Classical conditioning II: Current research and theory. New York: Appleton- Century-Crofts. RumeIhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations by error propogation. In D. Rumelhart, & 1. McClelland (Eds.), Parallel distributed process- ing: Explorations in the microstructure of cognition (Vol. 1: Foundations). Cambridge, M.A.: MIT Press. Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learn- ing, 3, 9-44. Sutton, R. S., & Barto, A. G. (1981). Toward a modem theory of adaptive networks: Expectation and prediction. Psychological Review, 88, 135-170. Sutton, R. S., & Barto, A. G. (1987). A temporal-difference model of classical conditioning. In Proceedings of the 9th Annual Conference of the Cognitive Science Society. Seattle, WA. Thompson, R. F. (1986). The neurobiology ofleaming and memory. Science, 233, 941-947. Werbos, P. (1974). Beyond regression: New tools for prediction and analysis in the behavioral sci- ences. Doctoral dissertation (Economics), Harvard University, Cambridge, Mass .. Widrow, B., & Hoff, M. E. (1960). Adaptive switching circuits. Institute of Radio Engineers, Western Electronic Show and Convention, Convention Record, 4,96-194. Part II Application","[-0.06287873536348343, -0.09002242982387543, 0.0405648835003376, 0.0036148426588624716, -0.047358930110931396, -0.00240135844796896, 0.007532183546572924, -0.040392108261585236, 0.11198010295629501, -0.012010028585791588, 0.04627276211977005, 0.039559803903102875, -0.009183291345834732, 0.05071820691227913, -0.01240544393658638, -0.048898275941610336, 0.0013108359416946769, 0.05057657137513161, -0.07211653143167496, -0.01618676818907261, 0.027809767052531242, -0.015223309397697449, -0.03454061225056648, 0.005475847981870174, 0.02443859353661537, -0.02785685285925865, -0.005060432478785515, -0.03170362859964371, 0.04354700446128845, 0.010838406160473824, 0.07494447380304337, -0.044778093695640564, -0.027123117819428444, -0.0026935068890452385, -0.09333031624555588, 0.0013710689963772893, -0.008019471541047096, 0.0060086483135819435, -0.010970835573971272, 0.021321212872862816, 0.05083785578608513, -0.030226020142436028, -0.03266070783138275, 0.04056660458445549, 0.0590214729309082, 0.004148309119045734, 0.039259470999240875, -0.0629732683300972, -0.12104689329862595, -0.0324685275554657, 0.013465030118823051, -0.01833542250096798, -0.026373034343123436, -0.018336275592446327, 0.09781555086374283, 0.03717673197388649, -0.03261953219771385, 0.025832241401076317, -0.0430879183113575, 0.003136433195322752, -0.10495512932538986, -0.029405832290649414, -0.04852985590696335, -0.04340602830052376, -0.0026215813122689724, 0.02021096833050251, -0.06391981244087219, 0.04078712314367294, 0.04423728212714195, 0.03450154885649681, 0.022490059956908226, 0.054993096739053726, -0.033207591623067856, -0.06394448131322861, 0.0252253245562315, -0.08550852537155151, 0.08998827636241913, 0.08687048405408859, 0.0012532123364508152, -0.08025018870830536, 0.05865910276770592, 0.008187845349311829, -0.004921033512800932, -0.07948300987482071, 0.06222550570964813, 0.01693848706781864, 0.010825637727975845, 0.09953979402780533, 0.004195279907435179, -0.048512984067201614, 0.0002939209225587547, -0.01693311147391796, 0.004164356738328934, -0.02399064414203167, -0.006122425198554993, -0.02748204581439495, 0.01234134379774332, 0.015328452922403812, 0.05576758459210396, 0.05833888053894043, -0.04808966815471649, -0.03254301846027374, -0.05118684098124504, -0.029007086530327797, 0.006757659371942282, 0.017023688182234764, 0.03398553654551506, 0.11736168712377548, 0.009412259794771671, 0.0006510648527182639, -0.006664687767624855, 0.007820287719368935, -0.02780238166451454, 0.04443929344415665, 0.0010395876597613096, -0.03805861622095108, -0.02484673634171486, -0.05202903226017952, 0.171522855758667, 0.016422534361481667, 0.0493331141769886, -0.04281720891594887, -0.06942714750766754, -0.04166225343942642, -0.032314665615558624, -0.019094591960310936, -0.01756611280143261, 1.9639296359874895e-33, -0.03647620975971222, 0.022555774077773094, -0.04910211265087128, -0.04433376342058182, 0.039924412965774536, -0.03897125646471977, 0.034022893756628036, -0.06377047300338745, 0.09753840416669846, 0.027288639917969704, -0.12721888720989227, 0.05545276030898094, -0.05969647318124771, 0.06811030954122543, 0.038208819925785065, 0.022608555853366852, -0.05368432775139809, 0.005983377806842327, 0.08661731332540512, -0.10716629773378372, 0.10534057766199112, -0.05991804227232933, -0.04876057431101799, 0.012476656585931778, -0.05926128104329109, 0.0161471925675869, -0.03325871378183365, 0.020112931728363037, -0.022266807034611702, 0.015149924904108047, -0.020406441763043404, 0.023326609283685684, -0.022526543587446213, -0.06427882611751556, 0.05605149641633034, 0.018594948574900627, 0.039242956787347794, -0.0006464137695729733, -0.019768251106142998, -0.049508459866046906, 0.020668113604187965, -0.0188184455037117, -0.009176014922559261, 0.013605771586298943, -0.03085145726799965, -0.023073233664035797, -0.020043734461069107, -0.0066692885011434555, -0.07768053561449051, -0.06418293714523315, -0.015574207529425621, 0.0031644885893911123, 0.013772238045930862, -0.12736445665359497, 0.00765711534768343, 0.07333395630121231, -0.03479323163628578, 0.03955228999257088, -0.010654604062438011, 0.11915279179811478, 0.0029994510114192963, 0.01413559541106224, 0.09234483540058136, 0.016010325402021408, 0.08846452832221985, 0.08853220194578171, -0.10473087430000305, -0.0018174549331888556, 0.05329417064785957, -0.02770894579589367, 0.0033157640136778355, 0.09012456238269806, -0.013661529868841171, -0.07662341743707657, 0.0713442787528038, -0.08499310165643692, 0.025443581864237785, -0.029420800507068634, -0.11752573400735855, 0.028430817648768425, 0.026048870757222176, -0.05353335291147232, -0.13715428113937378, 0.06719048321247101, 0.01937197893857956, 0.026647310703992844, 0.04821057617664337, 0.0184413380920887, -0.07010961323976517, 0.010741141624748707, -0.05882195010781288, -0.008503717370331287, 0.05512094125151634, -0.04232510179281235, 0.033637791872024536, -4.7616234560178114e-33, 0.01693086139857769, 0.0638180673122406, -0.03641049191355705, 0.005906067322939634, -0.01123865693807602, 0.021987853571772575, -0.024294018745422363, 0.02280685491859913, -0.022279435768723488, -0.057533442974090576, -0.0032501686364412308, -0.0322934165596962, -0.06639817357063293, 0.0027730276342481375, -0.033449139446020126, -0.022326134145259857, -0.07459517568349838, -0.03673097863793373, 0.03777744621038437, -0.020172318443655968, -0.01945512369275093, 0.043694064021110535, -0.14566417038440704, -0.0014147647889330983, -0.028024738654494286, -0.00631965696811676, -0.065607450902462, 0.16702371835708618, 0.007116326130926609, 0.05378073826432228, -0.06840517371892929, -0.047089703381061554, -0.05012800917029381, -0.029454626142978668, -0.03431371971964836, 0.09609749913215637, 0.057948481291532516, -0.02984391711652279, -0.01841278374195099, 0.030036844313144684, 0.09779515117406845, -0.0285330917686224, 0.024162203073501587, 0.02436881884932518, 0.08115015923976898, 0.013092693872749805, -0.04816683381795883, 0.03205171599984169, -0.06569096446037292, -0.010142398066818714, 0.040474019944667816, 0.006289341952651739, -0.04231800511479378, -0.056921541690826416, -0.04188342019915581, 0.04238316789269447, 0.04953118786215782, -0.06199462711811066, 0.021430140361189842, 0.003705112962052226, -0.07028085738420486, -0.1457587331533432, 0.01622409000992775, 0.024594413116574287, -0.04423123598098755, -0.003423939924687147, 0.02959287539124489, -0.011699693277478218, 0.0864725410938263, -0.044549476355314255, 0.03059334121644497, -0.010803138837218285, -0.023587632924318314, -0.029777146875858307, -0.013831266202032566, -0.07663623243570328, -0.03355838358402252, 0.015329452231526375, -0.053589221090078354, -0.019838375970721245, -0.037632476538419724, 0.007974288426339626, -0.02553919143974781, 0.023021545261144638, 0.01590447872877121, 0.08909836411476135, 0.04724627733230591, 0.0413665771484375, 0.051464322954416275, -0.1241675540804863, 0.005144353955984116, 0.059948693960905075, 0.011465027928352356, 0.0536842942237854, -0.056096382439136505, -4.938072351023948e-08, -0.01700889877974987, 0.021377334371209145, -0.024823853746056557, 0.0111730070784688, 0.09216760843992233, 0.0084072295576334, 0.01020726840943098, -0.07045789062976837, -0.023276695981621742, -0.039942026138305664, 0.097013458609581, 0.0016315184766426682, 0.05719936266541481, -0.057257868349552155, 0.01947091892361641, 0.12425734847784042, 0.05314784497022629, -0.01612350530922413, 0.026869595050811768, 0.028871262446045876, 0.03745881840586662, 0.049113959074020386, 0.006979251280426979, 0.04222176969051361, 0.013386666774749756, 0.0020114495418965816, 0.029060589149594307, 0.04840211570262909, 0.00047192175406962633, 0.06545732915401459, 0.07149223983287811, 0.07840047776699066, 0.03802303969860077, 0.06153210252523422, 0.02088187262415886, -0.019538765773177147, 0.05266162008047104, -0.04281485453248024, -0.048741355538368225, 0.04914752021431923, -0.03812231495976448, -0.027466021478176117, -0.03886721655726433, 0.01426831167191267, -0.029720796272158623, -0.043700430542230606, 0.015196427702903748, -0.10281460732221603, -0.01718832738697529, 0.05435311049222946, 0.003483135486021638, 0.09381389617919922, -0.046438734978437424, -0.02019483596086502, 0.0898924320936203, -0.06152338162064552, 0.007144968491047621, -0.10136835277080536, -0.023623492568731308, 0.061368271708488464, 0.04694093391299248, 0.0916258692741394, -0.055600982159376144, -0.026527326554059982]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\PerformanceofSyntheticNeuralNetworkClassificationofNoisyRadarSignals.pdf,Deep Learning,"2 CONSTRAINTS ON ADAPTIVE NETWORKS FOR MODELING HUMAN GENERALIZATION M. Pavel Mark A. Gluck Departm£1Il of Psychology Stanford University Stanford. CA 94305 ABSTRACT Van Henkle The potential of adaptive networks to learn categorization rules and to model human performance is studied by comparing how natural and artificial systems respond to new inputs, i.e., how they generalize. Like humans, networks can learn a detenninistic categorization task by a variety of alternative individual solutions. An analysis of the con- straints imposed by using networks with the minimal number of hidden units shows that this ""minimal configuration"" constraint is not sufficient to explain and predict human performance; only a few solu- tions were found to be shared by both humans and minimal adaptive networks. A further analysis of human and network generalizations indicates that initial conditions may provide important constraints on generalization. A new technique, which we call ""reversed learning"", is described for finding appropriate initial conditions. INTRODUCTION We are investigating the potential of adaptive networks to learn categorization tasks and to model human performance. In particular we have studied how both natural and artificial systems respond to new inputs, that is, how they generalize. In this paper we first describe a computational technique to analyze generalizations by adaptive networks. For a given network structure and a given classification problem, the technique enumerates all possible network solutions to the problem. We then report the results of an empirical study of human categorization learning. The generalizations of human sub- jects are compared to those of adaptive networks. A cluster analysis of both human and network generalizations indicates, significant differences between human perfonnance and possible network behaviors. Finally, we examine the role of the initial state of a net- work for biasing the solutions found by the network. Using data on the relations between human subjects' initial and final performance during training, we develop a new tech- nique, called ""reversed learning"", which shows some potential for modeling human learning processes using adaptive networks. The scope of our analyses is limited to gen- eralizations in deterministic pattern classification (categorization) tasks. Modeling Human Generalization 3 The basic difficulty in generalization is that there exist many different classification rules (""solutions"") that that correctly classify the training set but which categorize novel objects differently. The number and diversity of possible solutions depend on the language defining the pattern recognizer. However, additional constraints can be used in conjunction with many types of pattern categorizers to eliminate some, hopefully undesirable, solutions. One typical way of introducing additional constraints is to minimize the representation. For example minimizing the number of equations and parameters in a mathematical expression, or the number of rules in a rule-based system would assure that some identification maps would not be computable. In the case of adaptive networks, minimiz- ing the size of adaptive networks, which reduces the number of possible encoded func- tions, may result in improved generalization perfonnance (Rumelhart, 1988). The critical theoretical and applied questions in pattern recognition involve characteriza- tion and implementation of desirable constraints. In the first part of this paper we describe an analysis of adaptive networks that characterizes the solution space for any particular problem. ANALYSES OF ADAPTIVE NETWORKS Feed-forward adaptive networks considered in this paper will be defined as directed graphs with linear threshold units (LTV) as nodes and with edges labeled by real-valued weights. The output or activations of a unit is detennined by a monotonic nonlinear func- tion of a weighted sum of the activation of all units whose edges tenninate on that unit There are three types of units within a feed-forward layered architecture: (1) Input units whose activity is determined by external input; (2) output units whose activity is taken as the response; and (3) the remaining units, called hidden units. For the sake of simplicity our discussion will be limited to objects represented by binary valued vectors. A fully connected feed-forward network with an unlimited number of hidden units can compute any boolean function. Such a general network, therefore, provides no con- straints on the solutions. Therefore, additional constraints must be imposed for the net- work to prefer one generalization over another. One such constraint is minimizing the size of the network. In order to explore the effect of minimizing the number of hidden units we first identify the minimal network architecture and then examine its generaliza- tions. Most of the results in this area have been limited to finding bounds on the expected number of possible patterns that could be classified by a given network (e.g. Cover, 1965; Volper and Hampson, 1987; Valiant, 1984; Baum & Haussler, 1989). The bounds found by these researchers hold for all possible categorizations and are, therefore, too broad to be useful for the analysis of particular categorization problems. To determine the generalization behavior for a particular network architecture, a specific 4 Gluck, Pavel and Henkle categorization problem and a training set it is necessary to find find all possible solutions and the corresponding generalizations. To do this we used a computational (not a simu- lation) procedure developed by Pavel and Moore (1988) for finding minimal networks solving specific categorization problems. Pavel and Moore (1988) defined two network solutions to be different if at least one hidden unit categorized at least one object in the training set differently. Using this definition their algorithm finds all possible different solutions. Because finding network solutions is NP-complete (Judd, 1987), for larger problems Pavel and Moore used a probabilistic version of the algorithm to estimate the distribution of generalization responses. One way to characterize the constraints on generalization is in terms of the number of possible solutions. A larger number of possible solutions indicates that generalizations will be less predictable. The critical result of the analysis is that, even for minimal net- works. the number of different network solutions is often quite large. Moreover. the number of solutions increases rapidly with increases in the number of hidden units. The apparent lack of constraints can also be demonstrated by finding the probability that a network with a randomly selected hidden layer can solve a given categorization problem. That is, suppose that we se~t n different hidden units, each unit representing a linear discriminant fwction. The activations of these random hidden wits can be viewed as a ttansformation of the input patterns. We can ask what is the probability that an output unit can be found to perfonn the desired dichotomization. A typical example of a result of this analysis is shown in Figure 1 for the three-dimensional (3~) parity problem. In the minimal configuration involving three hidden units there were 62 different solutions to the 3D parity problem. The rapid increase in probability (high slope of the curve in Figure 1) indicates that adding a few more hidden units rapidly increases the probability that a random hidden layer will solve the 3D parity problem. 100 ...... -. ~. , , 10 "" , , • , z II g 80 , , !; , , -' , i 40 , , ~ , , ---- EXPERIMENT , 20 , -- 3D PARITY ~ , , , 0 0 2 4 6 • 10 12 HIOOENUNITS Figure 1 1be proportion of solutions to 3D parity problem (solid line) and the experimental task (dashed line) as a function of the number of hidden units. The results of a more detailed analysis of the generalization performance of the minimal networks will be discussed following a description of a categorization experiment with Modeling Human Generalization 5 human subjects. HUMAN CA TEGORIZA TION EXPERIMENT In this experiment human subjects learned to categorize objects which were defined by four dimensional binary vectors. Of the 24 possible objects, subjects were trained to clas- sify a subset of 8 objects into two categories of 4 objects each. The specific assignments of objects into categories was patterned after Medin et aI. (1982) and is shown in Figure 2. Eight of the patterns are designated as a training set and the remaining eight comprise the test seL The assignment of the patterns in the training set into two categories was such that there were many combinations of rules that could be used to correctly perfonn the categorization. For example, the first two dimensions could be used with one other dimension. The training patterns could also be categorized on the basis of an exclusive or (XOR) of the last two dimensions. The type of solution obtained by a human subject could only be determined by examining responses to the test set as well as the training seL TRAINING SET TEST SET X1 1 1 0 1 001 0 000 1 1 1 0 1 DIMENSIONS ~ 1 1 1 0 000 1 001 0 1 1 1 0 ~ 101 0 101 0 o 1 0 1 o 1 0 1 X. 101 0 o 1 0 1 o 1 0 1 o 1 0 1 CATEGORY AAAA BBBB ??? ? ???? FigllTe 2. PattemI to be clulmed. (Adapted from Medin et aI .• 1982). In the actual experiments, subjects were asked to perform a medical diagnosis for each pattern of four symptoms (dimensions). The experimental procedure will be described here only briefly because the details of this experiment have been described elsewhere in detail (pavel, Gluck, Henkle, 1988). Each of the patterns was presented serially in a ran- domized order. Subjects responded with one of the categories and then received feed- back. The training of each individual continued until he reached a criterion (responding correctly to 32 consecutive stimuli) or until each pattern had been presented 32 times. The data reported here is based on 78 subjects, half (39) who learned the task to criterion and half who did DOL Following the training phase, subjects were tested using all 16 possible patterns. The results of the test phase enabled us to determine the generalizations perfonned by the subjects. Subjects' generalizations were used to estimate the ""functions"" that they may have been using. For example, of the 39 criterion subjects, 15 used a solution that was consistent with the exclusive-or (XOR) of the dimensions x 3 and X4. We use ""response profiles"" to graph responses for an ensemble of functions, in this case for a group of subjects. A response profile represents the probability of assigning each 6 Gluck, Pavel and Henkle /I) z a:: loll ~ C ~ pattern to category ""A"". For example, the response profile for the XOR solution is shown in Figure 3A. For convenience we define the responses to the test set as the ""gen- eralization profile"". The response profile of all subjects who reached the criterion is shown in Figure 3D. The responses of our criterion subjects to the training set were basi- cally identical and correct The distribution of subjects' genezalization profiles reflected in the overall generalization profile are indicative of considerable individual differences 1001 0110 1101 1110 1011 0100 0011 0000 0101 1010 0001 0010 1000 0111 1100 1111 00 02 04 06 08 10 12 PROPORTION "" .. - /I) z a:: loll ~ C ~ 1001 0110 1101 1110 1011~ 0100 -=:===::-- 0011~ 0000 r--- 0101 1010 0001 0010 1000 0111 1100 1111 . 00 02 04 06 01 10 12 PROPORTION "" It.- Figwe 3. (A) Response profile of the XOR solution. and (B) a proportion of the response ""A"" to all patterns for human subjects (dark bars) and minimal networks (light bars). The lower 8 patterns are from the training set and the upper 8 patterns from the test set. MODEliNG THE RESPONSE PROFILE One of our goals is to model subjects' distribution of categorizations as represented by the response profile in Figure 3D. We considered three natural approaches to such modeling: (1) Statistical/proximity models, (2) Minimal disjunctive normal forms (DNF), and (3) Minimal two-layer networks. The statistical approach is based on the assumption that the response profile over subjects represents the probability of categorizations performed by each subject Our data are not consistent with that assumption because each subject appeared to behave deterministi- cally. The second approach, using the minimal DNF is also not a good candidate because there are only four such solutions and the response profile over those solutions differs considerably from that of the SUbjects. Turning to the adaptive network solutions, we found all the solutions using the linear programming technique described above (pavel & Moore, 1988). The minimal two-layer adaptive network that was capable of solving the training set problem consisted of two hidden units. The proportion of solutions as a Modeling Human Generalization 7 function of the number of hidden units is shown in Figure 1 by the dashed line. For the minimal network there were 18 different solutions. These 18 solutions had 8 dif- ferent individual generalization profiles. Assuming that each of the 18 network solution is equally likely. we computed the generalization profile for minimal network shown in Figure 3B. The response profile for the minimal network represents the probability that a randomly selected minimal network will assign a given pattern to category ""A"". Even without statistical testing we can conclude that the generalization profiles for humans and networks are quite different. It is possible. however. that humans and minimal networks obtain similar solutions and that the differences in the average responses are due to the particular statistical sampling assumption used for the minimal networks (i.e. each solu- tion is equally likely). In order to determine the overlap of solutions we examined the generalization profiles in more detail. CLUSTERING ANALYSIS OF GENERALIZATION PROFILES To analyze the similarity in solutions we defined a metric on generalization profiles. The Hamming distance between two profiles is equal to the number of patterns that are categorized differently. For example. the distance between generalization profile •• A A B A B B B B"" and ""A A B B B B A B"" is equal to two. because the two profiles differ on only the fourth and seventh pattern. Figure 4 shows the results of a cluster analysis using a hierarchical clustering procedure that maximizes the average distance between clusters. o c • • c c • • c c ~ • • • • • • • • • ; ~ • c ! c • • ~ c ~ • ~ • • = ~ • ~ ~ ~ I c • • • • • 3 c c c • • • • c • ~ • • • • • I • • • • • • • • • Figlll'll 4. Results of hierarchical clustering for human (left) and network (right) generalization profiles. • • c • • • c • • 3 c • c In this graph the average distance between any two clusters is shown by the value of the lowest common node in the tree. The clustering analysis indicates that humans and 8 Gluck, Pavel and Henkle networks obtained widely different generalization profiles. Only three generalization profiles were found to be common to human and networks. This number of common generalizations is to be expected by chance if the human and network solutions are independent Thus, even if there exists a learning algorithm that approximates the human probability distribution of responses, the minimal network would not be a good model of human perfonnance in this task. It is clear from the previously described network analysis that somewhat larger networks with different constraints could account for human solutions. In order to characterize the additional constraints, we examined subjects' individual strategies to find out why indivi- dual subjects obtained different solutions. ANALYSIS OF HUMAN LEARNING STRATEGIES Human learning strategies that lead to preferences for particular solutions may best be modeled in networks by imposing constraints and providing hints (Abu-Mostafa 1989). These include choosing the network architecture and a learning rule, constraining con- nectivity, and specifying initial conditions. We will focus on the specification of initial conditions. 30 20 10 o CI .. CONSISTENT • CONSISTENT lOR NON lOR SUBJECT TYPES NO CRrTERION FiglU'e 5. The number of consistent or non-stable responses (black) and the nwnber of stable incorrect responses (light) for XOR, Non-XOR criterion su~ jeers, and for those who never reached criterion. Our effort to examine initial conditions was motivated by large differences in learning curves (Pavel et al., 1988) between subjects who obtained the XOR solutions and those who did not The subjects who did not obtain the XOR solutions would perfonn much better on some patterns (e.g. 0001) then the XOR subjects, but worse on other patterns (e.g. 10(0). We concluded that these subjects during the first few trials discovered rules Modeling Human Generalization 9 that categorized most of the training patterns correctly but failed on one or two training patterns. We examined the sequences of subjects' responses to see how well they adhered to ""incorrect"" rules. We designated a response to a pattern as stable if the individual responded the same way to that pattern at least four times in a row. We designated a response as consistent if the response was stable and correct The results of the analysis are shown in Figure 5. These results indicate that the subjects who eventually achieved the XOR solution were less likely to generate stable incorrect solutions. Another impor- tant result is that those subjects who never learned the correct responses to the training set were not responding randomly. Rather, they were systematically using incorrect rules. On the basis of these results, we conclude that subjects' initial strategies may be important detenninants of their final solutions. REVERSED LEARNING For simplicity we identify subjects' initial conditions by their responses on the first few trials. An important theoretical question is whether or not it is possible to find a network structure, initial conditions and a learning rule such that the network can represent both the initial and final behavior of the subject In order to study this problem we developed a technique we call """"reversed leaming"". It is based on a perturbation analysis of feed- forward networks. We use the fact that the error surface in a small neighborhood of a minimum is well approximated by a quadratic surface. Hence, a well behaved gradient descent procedure with a starting point in the neighborhood of the minimum will find that 'minimum. The reversed learning procedure consists of three phases. (1) A network is trained to a final desired state of a particular individual, using both the training and the test patterns. (2) Using only the training patterns, the network is then trained to achieve the initial state of that individual subject closest to the desired final state (3) The network is trained with only the training patterns and the solution is compared to the subject's response profiles. Our preliminary results indicate that this procedure leads in many cases to initial condi- tions that favor the desired solutions. We are currently investigating conditions for finding the optimal initial states. CONCLUSION The main goal of this study was to examine constraints imposed by humans (experimen- tally) and networks (linear programming) on learning of simple binary categorization tasks. We characterize the constraints by analyzing responses to novel stimuli. We showed that. like the humans, networks learn the detenninistic categorization task and find many, very different. individual solutions. Thus adaptive networks are better models than statistical models and DNF rules. The constraints imposed by minimal networks, however, appear to differ from those imposed by human learners in that there are only a few solutions shared between human and adaptive networks. After a detailed analysis of 10 Gluck, Pavel and Henkle the human learning process we concluded that initial conditions may provide imPOl'Wlt constraints. In fact we consider the set of initial conditions as .powerful ""hints"" (Abu- Mostafa, 1989) which reduces the number of potential solutions. without reducing the complexity of the problem. We demonstrated the potential effectiveness of these con- straints using a perturbation technique. which we call reversed learning, for finding appropriate initial conditions. Acknowledgements This work was supported by research grants from the National Science Foundation (BNS-86-18049) to Gordon Bower and Mark Gluck. and (IST-8511589) to M. Pavel. and by a grant from NASA Ames (NCC 2-269) to Stanford University. We thank Steve Slo- man and Bob Rehder for useful discussions and their comments on this draft References Abu-Mostafa, Y. S. Learning by example with hints. NIPS. 1989. Baum, E. B .• & Haussler. D. What size net gives vaUd generalization? NIPS, 1989. Cover. T. (June 1965). Geometrical and statistical properties of systems of linear inequal- ities with applications in pattern recognition. IEEE Transactions on Electronic Computers. EC-14. 3. 326-334. Judd. J. S. Complexity of connectionist learning with various node functions. Presented at the First IEEE International Conference on Neural Networks. San Diego, June 1987. Medin. D. L .• Altom. M. W .• Edelson. S. M .• & Freko. D. (1982). Correlated symptoms and simulated medical classification. Journal of Experimental Psychology: Learn- ing. Memory. & Cognition, 8(1).37-50. Pavel. M .• Gluck, M. A .• & Henkle. V. Generalization by humans and multi-layer adap- tive networks. Submitted to Tenth Annual Conference of the Cognitive Science Society. August 17-19, 1988. Pavel. M .• & Moore, R. T. (1988). Computational analysis of solutions of two-layer adaptive networks. APL Technical Repon, Dept. of Psychology. Stanford Univer- sity. Valiant, L. G. (1984). A theory of the learnable. Comm. ACM. 27.11.1134-1142. Volper. D. J •• & Hampson. S. E. (1987). Learning and using specific instances. Biological Cybernetics, 56 •.","[-0.03414612635970116, -0.06819866597652435, 0.0331869050860405, 0.08309256285429001, 0.012745536863803864, -0.043142445385456085, 0.00886482559144497, -0.012677437625825405, 0.0016854081768542528, -0.04256734251976013, -0.06904799491167068, -0.017264561727643013, 0.03507070988416672, 0.013580516912043095, -0.04734817519783974, -0.08156578242778778, 0.026744849979877472, 0.08961863070726395, -0.06131603941321373, -0.004930609371513128, 0.006032045930624008, -0.015751156955957413, -0.03296767920255661, -0.019277144223451614, -0.034454043954610825, -0.01994416117668152, -0.017236875370144844, 0.010850581340491772, 0.009712914004921913, -0.02187696285545826, 0.07617036253213882, -0.011227590031921864, 0.07326619327068329, 0.019493108615279198, -0.11144083738327026, 0.026326164603233337, -0.08303048461675644, 0.0896642729640007, 0.008626166731119156, 0.010708882473409176, -0.036408133804798126, -0.012764057144522667, -0.05308971926569939, 0.010582965798676014, 0.05239015817642212, 0.07492569088935852, -0.060093916952610016, -0.06557633727788925, -0.037243183702230453, -0.01083449274301529, -0.08159022778272629, 0.040004536509513855, -0.07209806144237518, 0.04775754362344742, 0.05585024878382683, 0.016374578699469566, 0.013693307526409626, 0.021663250401616096, -0.051516272127628326, -0.04575560614466667, 0.015131911262869835, -0.1590055674314499, -0.04325640946626663, -0.00324295018799603, 0.08362486958503723, 0.035501573234796524, -0.030592964962124825, 0.07674947381019592, 0.03698284924030304, 0.024034105241298676, -0.00046773924259468913, 0.058787018060684204, -0.06443610787391663, 0.03754793107509613, 0.11505024135112762, 0.03212318569421768, 0.05551360547542572, 0.0751335397362709, -0.03552620857954025, -0.0241432785987854, 0.01440875418484211, 0.015367254614830017, 0.022877540439367294, 0.02937903441488743, 0.08465191721916199, -0.0416448675096035, -0.056183766573667526, 0.017368784174323082, 0.050105832517147064, 0.0029868544079363346, -0.05741384997963905, -0.02486470900475979, 0.01597999781370163, -0.04378429800271988, 0.05745638906955719, 0.03204351291060448, 0.004107095301151276, -0.037273108959198, -0.03641968220472336, 0.08557100594043732, -0.010973060503602028, 0.005831969901919365, 0.018956152722239494, 0.009509656578302383, 0.035909008234739304, 0.059196487069129944, 0.006703604944050312, 0.005731428507715464, 0.06722041219472885, -0.07026524096727371, 0.0025184969417750835, 0.05468830466270447, -0.03712374344468117, -0.05454814061522484, -0.02365838550031185, -0.08250214159488678, 0.04015810042619705, 0.014211973175406456, 0.018665127456188202, 0.053485602140426636, 0.005808677989989519, 0.011170119978487492, 0.014006653800606728, -0.025721175596117973, 0.07301948964595795, 0.010380014777183533, -0.10680355876684189, 2.6212919745772292e-33, -0.0020581306889653206, -0.025381870567798615, 0.06459949910640717, -0.0019873897545039654, 0.013801522552967072, -0.03452974185347557, -0.07743044197559357, -0.012309225276112556, 0.07016221433877945, 0.017949683591723442, -0.08125559985637665, 0.09190365672111511, -0.04022911190986633, 0.10825196653604507, 0.07205086946487427, 0.02752348594367504, -0.0215689055621624, 0.043338630348443985, 0.0008031729375943542, -0.11454607546329498, 0.0004437261959537864, -0.060387320816516876, 0.005781229585409164, -0.1003536581993103, -0.025099080055952072, -0.02591603435575962, 0.04313945025205612, -0.037406161427497864, -0.0403188057243824, -0.037796590477228165, -0.03362993150949478, 0.03814062103629112, 0.039431679993867874, 0.04290716350078583, 0.020559759810566902, 0.02092224359512329, 0.028245821595191956, -0.000679589225910604, 0.029510948807001114, 0.013446764089167118, -0.00736777950078249, 0.03231815993785858, 0.0635094866156578, -0.013655445538461208, -0.04529740288853645, -0.05321129411458969, -0.009763429872691631, 0.0006986955995671451, -0.12703830003738403, -0.017967885360121727, -0.019258365035057068, 0.003676757449284196, -0.017339279875159264, -0.08476429432630539, 0.007153554819524288, 0.04709600657224655, -0.004136531613767147, 0.03089211694896221, -0.057957135140895844, 0.025723421946167946, -0.011123687960207462, 0.03408180922269821, 0.021026140078902245, 0.0651053935289383, 0.030352596193552017, 0.021670600399374962, -0.022548124194145203, 0.02319510281085968, 0.048976119607686996, 0.0007116733468137681, -0.05170014500617981, 0.028235988691449165, -0.04399098455905914, 0.024052482098340988, 0.05706031620502472, 0.011868682689964771, 0.01721423864364624, -0.09347120672464371, -0.06230156868696213, 0.016257168725132942, -0.05524852126836777, -0.0025402973406016827, -0.057789627462625504, -0.011887541972100735, -0.09450545907020569, -0.017944037914276123, 0.11317156255245209, -0.06381276249885559, 0.006340128369629383, -0.012279343791306019, -0.061849262565374374, -0.013216293416917324, -0.03166466951370239, 0.09596623480319977, 0.047514256089925766, -4.530750855386545e-33, -0.1127033531665802, 0.009851151145994663, -0.08055119961500168, -0.0019386941567063332, 0.04211526736617088, 0.03337730094790459, -0.02887576073408127, -0.031227128580212593, -0.04944336414337158, -0.033280231058597565, -0.004748614504933357, -0.0516180656850338, 0.021964149549603462, 0.04135720804333687, -0.023406172171235085, 0.024959247559309006, -0.07548511773347855, 0.0237741656601429, 0.057462118566036224, 0.01875939406454563, 0.009192802011966705, 0.1346602886915207, -0.12638401985168457, 0.004123856779187918, -0.0015403738943859935, -0.026870472356677055, 0.006142093800008297, 0.10355813801288605, -0.025446433573961258, 0.013591441325843334, -0.059685710817575455, -0.024150323122739792, -0.0654846578836441, 0.038963936269283295, 0.03605462238192558, 0.10280192643404007, -0.04148641228675842, -0.02353915199637413, -0.0014807537663727999, 0.07919935882091522, 0.00887247733771801, -0.01520415861159563, -0.0850517675280571, -0.02792530506849289, -0.0008762375800870359, -0.06563936173915863, -0.035894617438316345, -0.031861066818237305, -0.05197187140583992, 0.02272205613553524, 0.03193960711359978, 0.020978014916181564, -0.08340602368116379, -0.05399872735142708, -0.06054774671792984, -0.008140342310070992, 0.07208321243524551, -0.031192289665341377, 0.12155748903751373, 0.0578114315867424, -0.07863541692495346, -0.061158277094364166, 0.035205092281103134, 0.016748810186982155, -0.08958190679550171, 0.010144148953258991, 0.0020083857234567404, 0.02433651313185692, 0.004947517067193985, 0.002966598141938448, 0.031048472970724106, 0.04795370250940323, -7.746303890598938e-05, 0.0017304087523370981, 0.015802031382918358, -0.061829909682273865, -0.061198119074106216, 0.013816401362419128, -0.035719841718673706, -0.08012160658836365, -0.015240021981298923, -0.011356853879988194, 0.03648657724261284, 0.024315910413861275, 0.008455488830804825, 0.07576178759336472, 0.08391716331243515, 0.17014285922050476, 0.04443010687828064, -0.0230142530053854, -0.005949426908046007, 0.0051450603641569614, -0.06068084016442299, 0.07842119038105011, -0.07181042432785034, -4.891395022355027e-08, -0.13426995277404785, 0.046398285776376724, -0.01041560247540474, 0.06168944761157036, 0.07160282135009766, 0.03266922011971474, -0.022956985980272293, 0.02417617104947567, -0.10192698985338211, -0.01430025976151228, 0.04061216115951538, 0.04472101479768753, 0.08090992271900177, 0.026751428842544556, 0.045107241719961166, 0.11096718162298203, 0.01954929158091545, 0.013264430686831474, -0.02609885297715664, -0.008776484988629818, 0.08553227037191391, 0.007911824621260166, -0.024447135627269745, 0.03632029891014099, 0.08529389649629593, -0.1049661934375763, -0.06273596733808517, 0.03780407831072807, -0.04581914097070694, 0.05953503027558327, 0.018643992021679878, 0.08374291658401489, -0.0316997654736042, -0.007559777703136206, 0.07561580091714859, 0.10270588099956512, 0.004425897262990475, -0.08317220956087112, -0.03123878687620163, -0.03022409789264202, -0.03984244540333748, 0.06798091530799866, -0.03906268626451492, 0.024263881146907806, 0.014705653302371502, -0.05103018507361412, 0.01377750001847744, -0.059806276112794876, -0.02886868268251419, -0.025633400306105614, 0.09208974987268448, -0.0016561226220801473, 0.017522050067782402, -0.03937918320298195, 0.11195223778486252, -0.018848076462745667, -0.007641032803803682, -0.006627662573009729, 0.00673535093665123, 0.08054416626691818, -0.021953511983156204, 0.03658997640013695, -0.054380666464567184, -0.05254426226019859]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ProgrammableAnalogPulseFiringNeuralNetworks.pdf,Deep Learning,"GENESIS: A SYSTEM FOR SIMULATING NEURAL NETWOfl.KS Matthew A. Wilson, Upinder S. Bhalla, John D. Uhley, James M. Bower. Division of Biology California Institute of Technology Pasadena, CA 91125 ABSTRACT We have developed a graphically oriented, general purpose simulation system to facilitate the modeling of neural networks. The simulator is implemented under UNIX and X-windows and is designed to support simulations at many levels of detail. Specifically, it is intended for use in both applied network modeling and in the simulation of detailed, realistic, biologically- based models. Examples of current models developed under this system include mammalian olfactory bulb and cortex, invertebrate central pattern generators, as well as more abstract connectionist simulations. INTRODUCTION Recently, there has been a dramatic increase in interest in exploring the computational properties of networks of parallel distributed processing elements (Rumelhart and McClelland, 1986) often referred to as Itneural networks"" (Anderson, 1988). Much of the current research involves numerical simulations of these types of networks (Anderson, 1988; Touretzky, 1989). Over the last several years, there has also been a significant increase in interest in using similar computer simulation techniques to study the structure and function of biological neural networks. This effort can be seen as an attempt to reverse-engineer the brain with the objective of understanding the functional organization of its very complicated networks (Bower, 1989). Simulations of these systems range from detailed reconstructions of single neurons, or even components of single neurons, to simulations of large networks of complex neurons (Koch and Segev, 1989). Modelers associated with each area of research are likely to benefit from exposure to a large range of neural network simulations. A simulation package capable of implementing these varied types of network models would facilitate this interaction. 485 486 Wilson, Bhalla, Uhley and Bower DESIGN FEATURES OF THE SIMULATOR We have built GENESIS (GEneral NEtwork SImulation System) and its graphical interface XODUS (X-based Output and Display Utility for Simulators) to provide a standardized and flexible means of constructing neural network simulations while making minimal assumptions about the actual structure of the neural components. The system is capable of growing according to the needs of users by incorporating user-defined code. We will now describe the specific features of this system. Device independence. The entire system has been designed to run under UNIX and X-windows (version 11) for maximum portability. The code was developed on Sun workstations and has been ported to Sun3's, Sun4's, Sun 386i's, and Masscomp computers. It should be portable to all installations supporting UNIX and X-II. In addition, we will be developing a parallel implementation of the simulation system (Nelson et al., 1989). Modular design. The design of the simulator and interface is based on a ""building-block"" approach. Simulations are constructed of modules which receive inputs, perform calculations on them, and generate outputs (figs. 2,3). This approach is central to the generality and flexibility of the system as it allows the user to easily add new features without modification to the base code. Interactive specification and control. Network specification and control is done at a high level using graphical tools and a network specification language (fig. 1). The graphics interface provides the highest and most user friendly level of interaction. It consists of a number of tools which the user can configure to suit a particular simulation. Through the graphical interface the user can display, control and adjust the parameters of simulations. The network specification language we have developed for network modeling represents a more basic level of interaction. This language consists of a set of simulator and interface functions that can be executed interactively from the keyboard or from text flies storing command sequences (scripts). The language also provides for arithmetic operations and program control functions such as looping, conditional statements, and subprograms or macros. Figures 3 and 4 demonstrate how some of these script functions are used. Simulator and interrace toolkits. Extendable toolkits which consist of module libraries, graphical tools and the simulator base code itself (fig. 2) provide the routines and modules used to construct specific simulations. The base code provides the common control and support routines for the entire system. GENESIS: A System for Simulating Neural Networks 487 Script Files Genesis command window and ke board Genesis 1% Gra hics Interface .. ~ .. ~ .DP~~Data ( Script Language Interpreter Figure 1. Levels Of Interaction With The Simulator CONSTRUCTING SIMULATIONS Files The first step in using GENESIS involves selecting and linking together those modules from the toolkits that will be necessary for a particular simulation (fig. 2,3). Additional commands in the scripting language establish the network and the graphical interface (fig. 4). Module Classes. Modules in GENESIS are divided into computational modules, communications modules and graphical modules. All instances of computational modules are called elements. These are the central components of simulations, performing all of the numerical calculations. Elements can communicate in two ways: via links and via connections. Links allow the passing of data between two elements with no time delay and with no computation being performed on the data. Thus. links serve to unify a large number of elements into a single computational unit (e.g. they are used to link elements together to form the neuron in fig. 3C). Connections. on the other hand. interconnect computational units via simulated communication channels which can incorporate time delays and perform transformations on data being transmitted (e.g. axons in fig. 3C). Graphical modules called widgets are used to construct the interface. These modules can issue script commands as well as respond to them, thus allowing interactive access to simulator structures and functions. 488 Wilson, Bhalla, Uhley and Bower Hierarchical organization. In order to keep track of the structure of a simulation, elements are organized into a tree hierarchy similar to the directory structure in UNIX (fig. 3B). The tree structure does not explicitly represent the pattern of links and connections between elements, it is simply a tool for organizing complex groups of elements in the simulation. Simulation example. As an example of the types of modules available and the process of structuring them into a network simulation and graphical interface, we will describe the construction of a simple biological neural simulation (fig. 3). The I11pdel consists of two neurons. Each neuron contains a passive dendritic compartment, an active cell body, an axonal output, and a synaptic input onto the dendrite. The axon of one neuron connects to a synaptic input of the other. Figure 3 shows the basic structure of the model as implemented under GENESIS. In the model, the synapse, channels, Simulator and interrace toolkit -----------------------------------------------------------------~ Graphics Modules Computational Modules ( A oDCO Earn Simulation Communications modules .... -----0001 CLinker • Simulator => __ ffi ~ ca .. .... ;.......... .::<::;:::;"";::::,:::-:.<., ..... . \.< .. :· j~ : CQdK .. ... Figure 2. Stages In Constructing A Simulation. GENESIS: A System for Simulating Neural Net~orks 489 A B network ~ neuron! neuron2 ~~ dendrite \ cell-body A axon Na K synapse C KEY Element Connection dendrite -Link D Figure 3. Implementation of a two neuron model in GENESIS. (A) Schematic dia- gram of compartmentally modeled neurons. Each cell in this simple model has a pas- sive dendritic compartment, an active cell-body, and an output axon. There is a synaptic input to the dendrite of one cell and two ionic channels on the cell body. (B) Hierarchical representation of the components of the simulation as maintained in GENESIS. The cell-body of neuron 1 is referred to as /network/neuronl/cell-body. (C) A representation of the functional links between the basic components of one neuron. (D) Sample interface control and display widgets created using the XODUS toolkit. 490 Wilson, Bhalla, Uhley and Bower dendritic compartments, cell body and axon are each treated as separate computational elements (fig. 3C). Links allow elements to share information (e.g. the Na channel needs to have access to the cell-body membrane voltage). Figure 4 shows a portion of the script used to construct this simulation. Create different types or elements and assign them names. create neuronl create active compartment cell-body create passive_compartment dendrite create synapse dendrite/synapse Establish functional ""links"" between the elements. link dendrite to cell-body link dendrite/synapse to dendrite Set parameters associated with the elements. set dendrit~ capacitance l.Oe-6 Make copies or entire element subtrees. copy neuronl to neuron2 Establish ""connections"" between two elements. connect neuronl/axon to neuron2/dendrite/ synapse Set up a graph to monitor an element variable graph neuronl/cell-body potential Make a control panel with several control ""widgets"". xform control xdialo g nstep xdialog dt Xloggle Euler set-nstep -default 200 set-dt -default 0.5 set-euler Figure 4. Sample script commands for constructing a simulation (see fig. 3) SIMULATOR SPECIFICATIONS Memory requirements or GENESIS. Currently. GENESIS consists of about 20,000 lines of simulator code and a similar amount of graphics code, all written in C. The executable binaries take up about 1.5 Megabytes. A rough estimate of the amount of additional memory necessary for a particular simulation can be calculated from the sizes and number of modules used in a simulation. Typically, elements use around 100 bytes, connections 16 and messages 20. Widgets use 5-20 Kbytes each. GENESIS: A System for Simulating Neural Networks 491 Performance The overall efficiency of the GENESIS system is highly simulation specific. To consider briefly a specific case, the most sophisticated biologically based simulation currently implemented under GENESIS, is a model of piriform (olfactory) cortex (Wilson et al., 1986; Wilson and Bower, 1988; Wilson and Bower, 1989). This simulation consists of neurons of four different types. Each neuron contains from one to five compartments. Each compartment can contain several channels. On a SUN 386i with 8 Mbytes of RAM. this simulation with 500 cells runs at I second per time step. Other models that have been implemented under GENESIS The list of projects currently completed under GENESIS includes approximately ten different simulations. These include models of the olfactory bulb (Bhalla et al., 1988), the inferior olive (Lee and Bower, 1988). and a motor circuit in the invertebrate sea slug Tritonia (Ryckebusch et aI., 1989)~ We have also built several tutorials to allow students to explore compartmental biological models (Hodgkin and Huxley, 1952), and Hopfield networks (Hopfield. 1982). Access/use of GENESIS GENESIS and XODUS will be made available at the cost of distribution to all interested users. As described above, new user-defined modules can be linked into the simulator to extend the system. Users are encouraged to support the continuing development of this system by sending modules they develop to Caltech. These will be reviewed and compiled into the overall system by GENESIS support staff. We would also hope that users would send completed published simulations to the GENESIS data base. This will provide others with an opportunity to observe the behavior of a simulation first hand. A current listing of modules and full simulations will be maintained and available through an electronic mail newsgroup. Babel. Enquiries about the system should be sent to GENESIS@caltech.edu or GENESIS@caltech.biblet. Acknowledgments We would like to thank Mark Nelson for his invaluable assistance in the development of this system and specifically for his suggestions on the content of this manuscript. We would also like to recognize Dave Bilitch. Wojtek Furmanski. Christof Koch, innumerable Caltech students and the students of the 1988 MBL summer course on Methods in Computational Neuroscience for their contributions to the creation and evolution of GENESIS (not mutually exclusive). This research was also supported by the NSF (EET-8700064). the NIH (BNS 22205). the ONR (Contract NOOOI4-88-K-0513). the Lockheed Corporation. the Caltech Presidents Fund, the JPL Directors Development Fund. and the Joseph Drown Foundation. 492 Wilson, Bhalla, Uhley and Bower References D. Anderson. (ed.) Neural information processing systems. American Institute of Physics, New York (1988). U.S. Bhalla, M.A. Wilson, & J.M. Bower. Integration of computer simulations and multi-unit recording in the rat olfactory system. Soc. Neurosci. Abstr. 14: 1188 (1988). I.M. Bower. Reverse engineering the nervous system: An anatomical, physiological, and computer based approach. In: An Introduction to Neural and Electronic Networks. Zornetzer, Davis, and Lau, editors. Academic Press (1989)(in press). A.L. Hodgkin and A.F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. I.Physiol, (Lond.) 117, 500- 544 (1952). 1.J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci. USA. 79,2554-2558 (1982). C. Koch and I. Segev. (eds.) Methods in Neuronal Modeling: From Synapses to Networks. MIT Press, Cambridge, MA (in press). M. Lee and I.M. Bower. A structural simulation of the inferior olivary nucleus. Soc. Neurosci. Abstr. 14: 184 (1988). M. Nelson, W. Furmanski and I.M. Bower. Simulating neurons and neuronal networks on parallel computers. In: Methods in Neuronal Modeling: From Synapses to Networks. C. Koch and I. Segev, editors. MIT Press, Cambridge, MA (1989)(in press). S. Ryckebusch, C. Mead and I.M. Bower. Modeling a central pattern generator in software and hardware: Tritonia in sea moss (CMOS). (l989)(this volume). D.E. Rumelhart, 1.L. McClelland and the PDP Research Group. Parallel Distributed Processing. MIT Press, Cambridge, MA (1986). D. Touretzky. (ed.) Advances in Neural Network Information Processing Systems. Morgan Kaufmann Publishers, San Mateo, California (1989). M.A. Wilson and I.M. Bower. The simulation of large-scale neuronal networks. In: Methods in Neuronal Modeling: From Synapses to Networks. C. Koch and I. Segev, editors. MIT Press, Cambridge, MA (1989)(in press). M.A. Wilson and I.M. Bower. A computer simulation of olfactory cortex with functional implications for storage and retrieval of olfactory information. In: Neural information processing systems. pp. 114-126 D. Anderson, editor. Published by AlP Press, New York, N.Y (1988). M.A. Wilson, I.M. Bower and L.B. Haberly. A computer simulation of piriform cortex. Soc. Neurosci. Abstr. 12.1358 (1986). Part IV Structured Networks","[-0.08066598325967789, -0.13244637846946716, 0.006140535697340965, -0.01553046889603138, -0.0480838380753994, -0.04583942890167236, -0.05085620656609535, 0.010206107050180435, 0.024457760155200958, 0.0035898475907742977, -0.027232401072978973, -0.017825298011302948, 0.03881838172674179, -0.009213321842253208, -0.0578521192073822, -0.025944391265511513, -0.04369640350341797, 0.0010643642162904143, -0.054806869477033615, -0.012629258446395397, 0.018652405589818954, 0.026316743344068527, -0.026686707511544228, -0.09417589008808136, -0.042697254568338394, 0.0034814979881048203, -0.04421035945415497, -0.046698905527591705, -0.03727584704756737, 0.009695438668131828, 0.1137402206659317, -0.016265355050563812, -0.024370908737182617, -0.014569765888154507, -0.06302794069051743, -0.005195241887122393, -0.04999622330069542, -0.03441218286752701, 0.01577921211719513, 0.04609370604157448, 0.0447276346385479, 0.012344860471785069, -0.03016965463757515, 0.036335233598947525, 0.05158436298370361, 0.0717059001326561, 0.005937138106673956, -0.04182753711938858, -0.028640907257795334, -0.044518209993839264, -0.06577126681804657, -0.045379359275102615, 0.03579992055892944, 0.04762992635369301, 0.09312734007835388, -0.008575176820158958, -0.03795260936021805, -0.05735950544476509, -0.09503644704818726, -0.008970370516180992, 0.09300033748149872, -0.029134707525372505, -0.017479944974184036, -0.02149595133960247, 0.03720946982502937, 0.06139334663748741, -0.033465079963207245, 0.08916638791561127, 0.05018177628517151, -0.12770140171051025, 0.0016752827214077115, 0.04918152466416359, -0.11495701968669891, 0.007075859699398279, 0.00916260201483965, 0.1051676943898201, 0.03860701993107796, 0.05060649290680885, 0.022054046392440796, -0.10182847082614899, 0.01477968692779541, 0.08001190423965454, 0.037715960294008255, -0.03718830645084381, -0.05670805647969246, 0.05739787966012955, -0.045191820710897446, 0.08138276636600494, -0.049819979816675186, 0.03247513249516487, 0.011013447307050228, 0.013183210045099258, -0.03126973286271095, -0.06095322221517563, -0.0002123524172930047, 0.02142866887152195, 0.07969573885202408, -0.003925911616533995, -0.005698428954929113, 0.04265192523598671, 0.0030394073110073805, -0.04113828018307686, 0.06421515345573425, -0.07012753188610077, 0.06942147016525269, 0.020320676267147064, 0.05312163755297661, -0.0126147186383605, 0.03237408027052879, -0.047061216086149216, -0.06322529166936874, 0.028774132952094078, -0.025619233027100563, 0.08091419190168381, 0.05165134742856026, -0.06776022166013718, 0.0282248817384243, -0.011902322992682457, 0.12013071775436401, 0.09032479673624039, -0.014762462116777897, -0.010818295180797577, -0.0036842769477516413, 0.03279479220509529, 0.0344308540225029, 5.39698958164081e-05, -0.11156855523586273, 4.967015012543097e-33, -0.06096290796995163, -0.01104077510535717, 0.020158661529421806, 0.01740691252052784, 0.11486750841140747, -0.03158993646502495, 0.005237168166786432, -0.027934066951274872, 0.009216399863362312, 0.0077601331286132336, -0.16449439525604248, 0.06559495627880096, -0.03233576565980911, 0.10729382187128067, 0.0057381754741072655, -0.04895155504345894, -0.00839655939489603, -0.00825290847569704, 0.0642981305718422, -0.05543293058872223, 0.022327320650219917, -0.011506909504532814, -0.003951573744416237, 0.03992190584540367, -0.0005055049550719559, 0.006020169239491224, -0.045351795852184296, -0.025149475783109665, -0.004478127229958773, 0.017407003790140152, -0.013065160252153873, 0.060529571026563644, -0.07826198637485504, 0.009480010718107224, 0.00444190576672554, 0.027392087504267693, 0.10698201507329941, -0.02540726773440838, -0.048719774931669235, -0.04423103481531143, -0.01214657910168171, -0.03289794921875, 0.033775102347135544, 0.04789012297987938, -0.07799237966537476, -0.01937282830476761, -0.04399435594677925, 0.01644955389201641, 0.0650179460644722, -0.09339068830013275, -0.016839856281876564, 0.010613932274281979, 0.05244312807917595, -0.09463725239038467, 0.03962073475122452, 0.08975797891616821, 0.008975952863693237, 0.029210632666945457, 0.026824848726391792, 0.12495216727256775, -0.00999506376683712, 0.039771758019924164, -0.021163102239370346, 0.05234518647193909, 0.0419897735118866, 0.009761513210833073, -0.10111790150403976, -0.07932912558317184, 0.07132408767938614, 0.031205855309963226, 0.01736651360988617, 0.006703645456582308, -0.07319789379835129, -0.06922739744186401, 0.018220769241452217, 0.029032854363322258, -0.016565296798944473, 0.008370942436158657, -0.1456141173839569, 0.03596474602818489, -0.021454066038131714, 0.01793760247528553, -0.10345231741666794, -0.005300993099808693, -0.008606747724115849, 0.012574934400618076, 0.08350524306297302, 0.030096646398305893, -0.06708697229623795, -0.06780745834112167, 0.025435850024223328, -0.012025011703372002, 0.11019667983055115, -0.04717034846544266, -0.009145235642790794, -5.587236688763645e-33, -0.10593211650848389, 0.07839195430278778, -0.06498409062623978, 0.026617154479026794, -0.012295282445847988, 0.05166184902191162, -0.007153522223234177, -0.02589433826506138, -0.06994912028312683, -0.030577749013900757, 0.00986467394977808, 0.020531343296170235, 0.050683606415987015, 0.025738226249814034, 0.07638048380613327, -0.058483533561229706, 0.025847086682915688, -0.04612158611416817, 0.0812026858329773, -0.05622732639312744, -0.037318259477615356, 0.05032450333237648, -0.08791831135749817, -0.0532546266913414, -0.05505992844700813, 0.010870854370296001, -0.034889694303274155, 0.03898534178733826, -0.014995494857430458, 0.022076411172747612, -0.05443616211414337, 0.01224649976938963, 0.0003450035583227873, 0.018321383744478226, 0.08068174868822098, 0.07694552093744278, 0.019400019198656082, -0.04428647458553314, 0.018086718395352364, -0.14355507493019104, 0.043108224868774414, -0.041647929698228836, -0.10655822604894638, 0.026796109974384308, 0.03812401369214058, 0.00779165793210268, -0.09589165449142456, 0.0022832483518868685, -0.007977500557899475, 0.0746561735868454, 0.05698650702834129, 0.041107453405857086, -0.02845856361091137, -0.06900111585855484, 0.013256706297397614, 0.00517857214435935, 0.06342822313308716, -0.010852928273379803, 0.05021638795733452, -0.030259720981121063, -0.10174079984426498, -0.1368686854839325, 0.05695775896310806, -0.03872000426054001, -0.020886367186903954, 0.010434027761220932, -0.03138703852891922, 0.006060873158276081, 0.04826536774635315, -0.01766175404191017, 0.04652874544262886, 0.07861171662807465, 0.038638174533843994, 0.03530946373939514, 0.0026475791819393635, -0.025258729234337807, -0.0355977937579155, 0.004492069594562054, -0.015909060835838318, 0.04087459295988083, -0.05273829773068428, 0.03608602657914162, 0.04153909906744957, -0.0015606796368956566, 0.0566280335187912, -0.023376118391752243, 0.0634242370724678, 0.06444960832595825, 0.026236429810523987, -0.0029108617454767227, 0.027237316593527794, 0.055037468671798706, -0.0031685165595263243, 0.09867184609174728, -0.03528083488345146, -5.767475030893365e-08, -0.05792327597737312, -0.01265718974173069, 0.06513725966215134, 0.0689580962061882, 0.05444866046309471, 0.015673957765102386, 0.07032439112663269, -0.028342315927147865, -0.030201254412531853, -0.011954804882407188, 0.016234496608376503, 0.014829708263278008, 0.039668332785367966, 0.020151881501078606, 0.08901717513799667, 0.05853080376982689, 0.06454581767320633, -0.02697121724486351, 0.016519205644726753, -0.024493327364325523, 0.03461579978466034, 0.0007673442596569657, -0.053714364767074585, 0.05329306051135063, 0.046690504997968674, -0.07230354845523834, -0.0712958425283432, 0.022581545636057854, -0.024270057678222656, 0.036620914936065674, 0.003630804130807519, 0.0002719716285355389, -0.01785919815301895, 0.058349523693323135, -0.029857927933335304, 0.036025520414114, -0.015049343928694725, -0.0013634541537612677, -0.015427444130182266, -0.048393335193395615, -0.06400395929813385, -0.050130296498537064, -0.0019064382649958134, -0.0009468678617849946, 0.08836328238248825, 0.000730350730009377, 0.01181126944720745, 0.0024654066655784845, 0.04653429985046387, 0.009553903713822365, -0.05788999795913696, 0.06838375329971313, -0.02441227063536644, 0.031836383044719696, -0.04447147995233536, -0.06121152266860008, -0.023007873445749283, -0.05761849880218506, 0.021987907588481903, 0.058466073125600815, -0.03925406187772751, 0.08272102475166321, -0.04728057235479355, -0.04059182479977608]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\RangeImageRestorationUsingMeanFieldAnnealing.pdf,Optimization,"CONVERGENCE AND PATTERN STABILIZATION IN THE BOLTZMANN MACHINE MosheKam Dept. of Electrical and Computer Eng. Drexel University, Philadelphia PA 19104 ABSTRACT Roger Cheng Dept. of Electrical Eng. Princeton University, NJ 08544 The Boltzmann Machine has been introduced as a means to perform global optimization for multimodal objective functions using the principles of simulated annealing. In this paper we consider its utility as a spurious-free content-addressable memory, and provide bounds on its performance in this context. We show how to exploit the machine's ability to escape local minima, in order to use it, at a constant temperature, for unambiguous associative pattern-retrieval in noisy environments. An association rule, which creates a sphere of influence around each stored pattern, is used along with the Machine's dynamics to match the machine's noisy input with one of the pre-stored patterns. Spurious fIxed points, whose regions of attraction are not recognized by the rule, are skipped, due to the Machine's fInite probability to escape from any state. The results apply to the Boltzmann machine and to the asynchronous net of binary threshold elements (Hopfield model'). They provide the network designer with worst-case and best-case bounds for the network's performance, and allow polynomial-time tradeoff studies of design parameters. I. INTRODUCTION The suggestion that artificial neural networks can be utilized for classification, pattern recognition and associative recall has been the main theme of numerous studies which appeared in the last decade (e.g. Rumelhart and McClelland (1986) and Grossberg (1988) - and their references.) Among the most popular families of neural networks are fully connected networks of binary threshold elements (e.g. Amari (1972), HopfIeld (1982).) These structures, and the related family of fully connected networks of sigmOidal threshold elements have been used as error-correcting decoders in many applications, among which were interesting applications in optimization (Hopfield and Tank, 1985; Tank and Hopfield, 1986; Kennedy and Chua, 1987.) A common drawback of the many studied schemes is the abundance of 'spurious' local minima, which 'trap' the decoder in undesirable, and often non-interpretable, states during the process of input I stored-pattern association. It is generally accepted now that while the number of arbitrary binary patterns that can be stored in a fully-connected network is of the order of magnitude of N (N = number of the neurons in the network,) the number of created local attractors in the 511 512 Kam and Cheng network's state space is exponential in N. It was proposed (Acldey et al., 1985; Hinton and Sejnowski, 1986) that fully-connected binary neural networks, which update their states on the basis of stochastic state-reassessment rules, could be used for global optimization when the objective function is multi-modal. The suggested architecture, the Boltzmann machine, is based on the principles of simulated annealing ( Kirkpatrick et al., 1983; Geman and Geman, 1984; Aarts et al., 1985; Szu, 1986,) and has been shown to perform interesting tasks of decision making and optimization. However, the learning algorithm that was proposed for the Machine, along with its ""cooling"" procedures, do not lend themselves to real-time operation. Most studies so far have concentrated on the properties of the Machine in global optimization and only few studies have mentioned possible utilization of the Machine (at constant 'temperature') as a content-addressable memory (e.g. for local optimization. ) In the present paper we propose to use the Boltzmann machine for associative retrieval, and develop bounds on its performance as a content-addressable memory. We introduce a learning algorithm for the Machine, which locally maximizes the stabilization probability of learned patterns. We then proceed to calculate (in polynomial time) upper and lower bounds on the probability that a tuple at a given initial Hamming distance from a stored pattern will get attracted to that pattern. A proposed association rule creates a sphere of influence around each stored pattern, and is indifferent to 'spurious' attractors. Due to the fact that the Machine has a nonzero probability of escape from any state, the 'spurious' attractors are ignored. The obtained bounds allow the assessment of retrieval probabilities, different learning algorithms and necessary learning periods, network 'temperatures' and coding schemes for items to be stored. II. THE MACHINE AS A CONTENT ADDRESSABLE MEMORY The Boltzmann Machine is a multi-connected network of N simple processors called probabilistic binary neurons. The ith neuron is characterized by N-1 real numbers representing the synaptic weights (Wij, j=1,2, ... ,i-1,i+1, ... ,N; Wii is assumed to be zero for all i), a real threshold ('tj) and a binary activity level (Ui E B ={ -1,1},) which we shall also refer to as the neuron's state. The binary N-tuple U = [Ul,U2, ..• ,UN] is called the network's state. We distinguish between two phases of the network's operation: a) The leamjn& phase - when the network parameters Wij and 'ti are determined. This determination could be achieved through autonomous learning of the binary pattern environment by the network (unsupervised learning); through learning of the environment with the help of a 'teacher' which supplies evaluative reinforcement signals (supervised learning); or by an external flxed assignment of parameter values. b) The production phase - when the network's state U is determined. This determination could be performed synchronously by all neurons at the same predetermined time instants, or asynchronously - each neuron reassesses its state independently of the other neurons at random times. The decisions of the neurons regarding their states during reassessment can be arrived at deterministically (the set of neuron inputs determines the neuron's state) or Convergence and Pattem-Stabilization 513 stochastically (the set of neuron inputs shapes a probability distribution for the state-selection law.) We shall describe fast the (asynchronous and stochastic) production rule which our network employs. At random times during the production phase, asynchronously and independently of all other neurons, the ith neuron decides upon its next state, using the probabilistic decision rule u·= J 1 1 with probabilty --~~ l+e-T II er II with probabilty --~~ where -1 l+e-T II N Lllii = L WijUj-ti j=l~ (1) is called the ith energy gap, and Te is a predetermined real number called temperature. The state-updating rule (1) is related to the network's energy level which is described by E=-.![~ u.( ~ w .. u.-t.)] 2 £..J I £..J lJ J I • i=l j=l~ (2) If the network is to fmd a local minimum of E in equation (2), then the ith neuron, when chosen (at random) for state updating, should choose deterministically ui=sgn[ ± Wii Ui - 1i]' (3) j=l~ We note that rule in equation (3) is obtained from rule in equation (1) as Te --+ O. This deterministic choice of Ui guarantees, under symmetry conditions on the weights (Wij=Wji), that the network's state will stabilize at afzxed point in the 2N-tuple state space of the network (Hoptield, 1982), where Definition I: A state UfE BN is called afued point in the state space of the N-neuron network if p .ro<~+1)= Uf I U<'Y = Uf] = 1. (4) A fixed point found through iterations of equation (3) (with i chosen at random at each iteration) may not be the global minimum of the energy in equation (2). A mechanism which seeks the global minimum should avoid local-minimum ""traps"" by allowing 'uphill' climbing with respect to the value of E. The decision scheme of equation (1) is devised for that purpose, allowing an increase in E with nonzero probability. This provision for progress in the locally 'wrong' direction in order to reap a 'global' advantage later, is in accordance with the principles of simulated annealing techniques, which are used in multimodal optimization. In our case, the probabilities of choosing the locally 'right' decision (equation (3» and the locally 'wrong' decision are determined by the ratio 514 Kam and Cheng of the energy gap ~i to the 'temperature' shaping constant T e . The Boltzmann Machine has been proposed for global minimization and a considerable amount of effort has been invested in devising a good cooling scheme, namely a means to control T e in order to guarantee the finding of a global minimum in a short time (Geman and Geman, 1984, Szu, 1987.) However, the network may also be used as a selective content addressable memory which does not suffer from inadvertently-installed spurious local minima. We consider the following application of the Boltzmann Machine as a scheme for pattern classification under noisy conditions: let an encoder emit a sequence of NXI binary code vectors from a set of Q codewords (or 'patterns',) each having a probability of occurrence of TIm (m = 1,2, ... ,Q). The emitted pattern encounters noise and distortion before it arrives at the decoder, resulting in some of its bits being reversed. The Boltzmann Machine, which is the decoder at the receiving end, accepts this distorted pattern as its initial state (U(O», and observes the consequent time evolution of the network's state U. At a certain time instant nO, the Machine will declare that the input pattern U(O) is to be associated with pattern Bm if U at that instant (u(no» is 'close enough' to Bm. For this purpose we defme Definition 2: The dmax-sphere of influence of pattern B m, a( Bm, dmax) is o(Bm,dmax)={UeBN : HD(U, Bm)~~}. (5) dmax is prespecified. Letl:(~)=uo(Bm' ~)andletno be the smallest integer such that dnJel:(~~ m Therule of atsociation is : associate dO) with Bm at time no, if dllo> which has evolved from u<0) satisfies: U<llo>e o(Bm' ~~ Due to the finite probability of escape from any minimum, the energy minima which correspond to spurious fIXed points are skipped by the network on its way to the energy valleys induced by the designed flXed points (Le. Bl, ... ,BQ.) Ill. ONE-STEP CONTRACTION PROBABILITIES Using the association rule, the utility of the Boltzmann machine for error correction involves the probabilities P r {HD [lfn) ,BnJ ~ d.nax I HD [If°),BnJ = d} m= 1 ,2, ... , Q (6) for predetermined n and dmax . In order to calculate (6) we shall frrst calculate the following one-step attraction probabilities: P(Bm,d,B)=Pr{HDrd~ + 1), BnJ=d+B I HDrd~ ), BnJ=d}whereB= -1,0, 1 (7) For B = -1 we obtain the probability of convergence ; For B = + 1 we obtain the probability of divergence; For B = 0 we obtain the probability of stalemate. An exact calculation of the attraction probabilities in equation (7) is time-exponential and we shall therefore seek lower and upper bounds which can be calculated in polynomial time. We shall reorder the weights of each neuron according to their contribution to the Convergence and Pattern-Stabilization 515 ~i for each pattern, using the notation w;n= {wit bm1, wi2bm2"" •• , wiNbmN} ~=max wr ~=max{wF-{~,~, ... ,'fs-l}} (8) i = 1,2, ... ,N, s =2,3, ... ,N, m= 1,2, ... ,Q d d LetL1E~i(d)=~ -2L!f and~~(d)=~ -2L'fN+l-f"" (9) 1'=1 1'=1 These values represent the maximum and minimum values that the ith energy gap could assume when the network is at HD of d from Bm. Using these extrema, we can fmd the worst case attraction probabilities : N pwc(B d -1) =.! ~ . [ U-1 (brni) m"" N ti' PI AEJ.d) 1+ e---:r- e AEJ.d) U_l(bmi)e-~ !IF:J.d) l+e----y- e and the best case attraction probabilities : + N - d L [ U 1 (b .) 1 - U (b .) AE.Jd)] pbc(B d -1)=- . - mJ -1 mJ -- ~, N ~ + + e T i=1 AEJ.d) AEuJd) e where for both cases l+e--r l+e--r e e 1-U-1(bmi) AEJ.d) l+e--r e ~) d -~) ~) P (Bm"" O)-l-P (Bm' d, -l)-P (Bm' d, 1). (lOa) (lOb) (lla) (lIb) (12) For the worst- (respectively, best-) case probabilities, we have used the extreme values of .1Emi(d) to underestimate (overestimate) convergence and overestimate (underestimate) divergence, given that there is a disagreement in d of the N positions between the network's state and Bm ; we assume that errors are equally likely at each one of the bits. IV. ESTIMATION OF RETRIEVAL PROBABILITIES To estimate the retrieval probabilities, we shall study the Hamming distance of the 516 Kam and Cheng network's state from a stored pattern. The evolution of the network from state to state, as affecting the distance from a stored pattern, can be interpreted in terms of a birth-and-death Markov process (e.g. Howard, 1971) with the probability transition matrix I-Pt,o Pbo 0 0 0 0 0 PII I-Pbl-PII Pbl 0 0 0 0 Pd2 I-Pb2-P& Pb2 0 0 0 0 0 'I' ,lPbbPdi)= 0 Pdt I-Pbk-Pdt Pbk 0 0 0 0 0 PdN-l I-PbN-CPdN-I PbN-I 0 0 0 PdN I-PdN (13) where the birth probability Pbi is the divergence probability of increasing the lID from i to i+ 1, and the death probability Pdi is the contraction probability of decreasing the HD from i to i-I. Given that an input pattern was at HD of do from Bm, the probability that after n steps the network will associate it with Bm is C\- P r {lin) --+BrrJ I lID [dO) ,BrrJ = do} = Il r [HD(U(n), Bm) = r I lID(U(O), Bm) = <\,] (14) r=O where we can use the one-step bounds found in section III in order to calculate the worst-case and best-case probabilities of association. Using equations (10) and (11) we define two matrices for e~h pattern Bm; a worst case matrix, V:' and a best case matrix,~: Worst case matrix Pbi=Pwc(Bm,i ,+1) Best case matrix Pbi=pbc(Bm,i ,+1) pdi=pwc(Bm, i ,-1) Pdi=Pbc(Bm, i ,-1). U sing these matrices, it is now possible to calculate lower and upper bounds for the association probabilities needed in equation (14): [1tdo ('I'~1n]r~ Pr[HD(U(n),Bm)=r I lID(U(O), Bm)= <\,] ~[1tdo ('I'~)n]r (ISa) where [x]i indicatestheithelementofthevectorx, and1tdois the unit 1 xn+l vector Convergence and Pattem-Stabilization 51 7 1 i=do (ISb) o The bounds of equation 14(a) can be used to bound the association probability in equation (13). The upper bound of the association probability is obtained by replacing the summed terms in (13) by their upper-bound values: '\- P r {U(n) --+Bm11 lID [(f0) ,BnJ= do} S L[1tdo ('P~)nlr (16a) r=O The lower bound cannot be treated similarly, since it is possible that at some instant of time prior to the present time-step (n), the network has already associated its state U with one of the other patterns. We shall therefore use as the lower bound on the convergence probability in equation (14): C\z. L[1Ii1o(~nlrSPr{lfn)--iJnJ IHD[lf°),Bm} (16b) r=O where the underlined matrix is the birth-and-death matrix (13) with (16c) 1 o for i = Ili' Jli+ I, ... , N am Jli = min HD(Bi, Bj)- dmax j = 1, ... ,Q,j~i (16d) Equation (16c) and (16d) assume that the network wanders into the dmax- sphere of influence of a pattern other than Bi, whenever its distance from Bi is Ili or more. This assumption is very conservative, since Ili represents the shortest distance to a competing dmar sphere of influence, and the network could actually wander to distances larger than Ili and still converge eventually into the dmax -sphere of influence of Bi. CONCLUSION We have presented how the Boltzmann Machine can be used as a content-addressable memory, exploiting the stochastic nature of its state-selection procedure in order to escape undesirable minima. An association rule in terms of patterns' spheres of influence is used, along with the Machine's dynamics, in order to match an input tuple with one of the predetermined stored patterns. The system is therefore indifferent to 'spurious' states, whose spheres of influence are not recognized in the retrieval process. We have detailed a technique to calculate the upper and lower bounds on retrieval probabilities of each stored 518 Kam and Cheng pattern. These bounds are functions of the network's parameters (i.e. assignment or learning rules, and the pattern sets); the initial Hamming distance from the stored pattern; the association rule; and the number of production steps. They allow a polynomial-time assessment of the network's capabilities as an associative memory for a given set of patterns; a comparison of different coding schemes for patterns to be stored and retrieved; an assessment of the length of the learning period necessary in order to guarantee a pre specified probability of retrieval; and a comparison of different learning/assignment rules for the network parameters. Examples and additional results are provided in a companion paper (Kam and Cheng, 1989). Acknowledgements This work was supported by NSF grant IRI 8810186. References [1] Aarts,E.H.L., Van Laarhoven,P.J.M. : ""Statistical Cooling: A General Approach to Combinatorial Optimization Problems,"" Phillips 1. Res., Vol. 40, 1985. [2] Ackley,D.H., Hinton,J.E., Sejnowski,T J. : "" A Learning Algorithm for Boltzmann Machines,"" Cognitive Science, Vol. 19, pp. 147-169, 1985. [3] Amari,S-I: ""Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements,"" IEEE Trans. Computers, Vol. C-21, No. 11, pp. 1197-1206, 1972. [4] Geman,S., Geman,D. : ""Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"" IEEE Trans. Patte Anal. Mach. Int., pp. 721-741,1984. [5] Grossberg,S.: ""Nonlinear Neural Networks: Principles, Mechanisms, and Architectures,"" Neural Networks, Vol. 1, 1988. [6] Hebb,D.O.: The Organization of Behavior, New York:Wiley, 1949. [7] Hinton,J.E., Sejnowski,T J. ""Learning and Relearning in the Boltzmann Machine,"" in [14] [8] Hopfield,J.J.: ""Neural Networks and Physical Systems with Emergent Collective Computational Abilities,"" Proc. Nat. Acad. Sci. USA, pp. 2554-2558, 1982. [9] Hopfield,J.J., Tank,D. :"" 'Neural' Computation of Decision in Optimization Problems,"" Biological Cybernetics, Vol. 52, pp. 1-12, 1985. [10] Howard,R.A.: Dynamic Probabilistic Systems, New York:Wiley, 1971. [11] Kam,M., Cheng,R.: "" Decision Making with the Boltzmann Machine,"" Proceedings of the 1989 American Control Conference, Vol. 1, Pittsburgh, PA, 1989. [12] Kennedy,M.P., Chua, L.O. :""Circuit Theoretic Solutions for Neural Networks,"" Proceedings of the 1st Int. Con!. on Neural Networks, San Diego, CA, 1987. [13] Kirkpatrick,S., Gellat,C.D.,Jr., Vecchi,M.P. : ""Optimization by Simulated Annealing,"" Science, 220, pp. 671-680, 1983. [14] Rumelhart,D.E., McClelland,J.L. (editors): Parallel Distributed Processing, Volume 1: Foundations, Cambridge:MIT press, 1986. [15] Szu,H.: ""Fast Simulated Annealing,"" in Denker,J.S.(editor) : Neural Networks for Computing, New York:American Inst. Phys., Vol. 51.,pp. 420-425, 1986. [16] Tank,D.W., Hopfield, J.J. : ""Simple 'Neural' Optimization Networks,"" IEEE Transactions on Circuits and Systems, Vol. CAS-33, No.5, pp. 533-541, 1986.","[0.0010981577215716243, -0.03676684573292732, -0.011883458122611046, 0.04599474370479584, 0.0291106179356575, -0.044143859297037125, 0.007561256643384695, -0.06729670614004135, 0.0221524927765131, -0.060128141194581985, -0.04179834574460983, -0.02090517058968544, 0.1142389327287674, -0.0924903005361557, -0.06292781233787537, 0.03775656968355179, 0.0571548230946064, 0.04445391520857811, -0.1054045632481575, -0.07899250090122223, -0.0006877901032567024, 0.015604755841195583, -0.012620548717677593, -0.0696047693490982, -0.07280820608139038, 0.027173219248652458, 0.052131060510873795, 0.03665970265865326, 0.02077478915452957, -0.02260977402329445, 0.046024881303310394, 0.014760185033082962, 0.029836995527148247, 0.085371233522892, 0.06597162038087845, 0.08724966645240784, -0.13158991932868958, -0.011786977760493755, -0.028632551431655884, 0.0379396453499794, 0.026874035596847534, 0.008500874973833561, -0.02177894301712513, -0.024763472378253937, 0.0816696286201477, 0.07103388756513596, 0.04738147556781769, -0.009540555998682976, -0.01386392954736948, -0.059794750064611435, -0.02579466439783573, 0.03359461575746536, -0.013236211612820625, 0.07569665461778641, -0.008185531944036484, -0.008804894983768463, 0.07327379286289215, -0.01130697038024664, -0.0340178906917572, 0.009918342344462872, 0.09448148310184479, -0.1074841246008873, -0.07166551053524017, -0.04466908052563667, 0.0445365346968174, -0.06298571825027466, 0.05183529108762741, 0.022774284705519676, 0.07204162329435349, 0.0432266891002655, 0.04551250860095024, 0.04748331382870674, -0.06751921027898788, 0.03562382981181145, 0.018107945099473, 0.06032072752714157, 0.04825170338153839, -0.09141363948583603, 0.005379658192396164, -0.0017399784410372376, -0.01724589802324772, -0.05532500147819519, 0.05257768556475639, -0.07737981528043747, 0.042188581079244614, -0.02157074585556984, 0.008953804150223732, -0.0004668180481530726, 0.14004510641098022, 0.019520942121744156, -0.05545371398329735, -0.02690785564482212, 0.019200587645173073, -0.04582563787698746, 0.08546799421310425, -0.019487159326672554, 0.13630545139312744, -0.042820267379283905, 0.02225319668650627, 0.07586631178855896, 0.007541488856077194, -0.016991643235087395, -0.0008335768943652511, 0.05072370171546936, 0.0852988213300705, -0.04575994238257408, 0.06101575121283531, 0.037139542400836945, 0.006471735890954733, -0.08429887145757675, 0.005431058816611767, 0.07756952196359634, 0.016654575243592262, 0.04882480204105377, -0.05633583292365074, -0.07463814318180084, 0.03367539495229721, -0.02554180845618248, 0.0007862604688853025, 0.08925372362136841, -0.022106029093265533, -0.012440395541489124, -0.04784791171550751, 0.03825856372714043, -0.019438238814473152, 0.026162434369325638, -0.08492115885019302, 6.327203494944063e-34, -0.034832265228033066, -0.03971397131681442, -0.03557617962360382, -0.046755384653806686, 0.11074251681566238, 0.006796502508223057, -0.007918061688542366, -0.01387812476605177, -0.011976445093750954, 0.01650373265147209, -0.015330950729548931, 0.05593196302652359, 0.01134069636464119, 0.07671238481998444, 0.05619855970144272, -0.04503852501511574, 0.029533451423048973, 0.0015437323600053787, -0.008288133889436722, 0.008958060294389725, 0.050229791551828384, -0.019347257912158966, -0.009884943254292011, 0.0018982322653755546, -0.058363012969493866, -0.008551957085728645, 0.018797433003783226, -0.05659584701061249, 0.015462668612599373, 0.03282096982002258, -0.08441435545682907, 0.008715050294995308, -0.0022687294986099005, -0.03611336275935173, 0.024768732488155365, -0.002727099461480975, 0.020367685705423355, -0.054314401000738144, 0.014717460609972477, -0.06573311239480972, 0.06056550517678261, 0.00691860169172287, 0.038050707429647446, -0.048572126775979996, -0.11483480036258698, -0.0204174742102623, 0.058268651366233826, 0.03129648417234421, -0.009257553145289421, -0.08631663769483566, 0.004340818151831627, -0.01427359227091074, 0.03580382838845253, -0.08414239436388016, 0.10063426196575165, -0.03877018392086029, 0.052203577011823654, 0.014610194601118565, 0.07560338824987411, 0.11603502184152603, -0.06680631637573242, 0.06843592971563339, 0.047341976314783096, 0.06848414987325668, 0.0804746150970459, 0.056007228791713715, -0.047243185341358185, 0.0038819042965769768, 0.06042563542723656, -0.023638363927602768, -0.004070961382240057, 0.06750748306512833, 0.010581626556813717, -0.06293340027332306, 0.024005863815546036, 0.03503140062093735, 0.03320857137441635, -0.050267934799194336, -0.04638083279132843, -0.06398314982652664, -0.028293145820498466, 0.025045929476618767, -0.03721447288990021, -0.02490508183836937, -0.08142472803592682, -0.049763403832912445, 0.026636548340320587, -0.007488816510885954, -0.14121416211128235, -0.049657270312309265, -0.0018585451180115342, -0.0326722078025341, 0.023467788472771645, 0.02824362739920616, -0.05505957081913948, -2.4658025409672495e-33, -0.014796704053878784, -0.02180267497897148, -0.00831932108849287, 0.0382193922996521, 0.0008178722346201539, 0.0005844183615408838, -0.07132870703935623, -0.0875561460852623, -0.06774337589740753, -0.052426185458898544, -0.04607117548584938, 0.038216907531023026, 0.04856004938483238, -0.008009355515241623, 0.039455439895391464, 0.015926549211144447, 0.015667201951146126, -0.036928702145814896, 0.05413927882909775, 0.0244625024497509, -0.03218945488333702, 0.08875338733196259, -0.08022837340831757, -0.04459378123283386, -0.06392247974872589, -0.009879411198198795, -0.03295176476240158, 0.04135945811867714, -0.0013199667446315289, 0.03048277460038662, -0.08258040994405746, 0.04149630293250084, -0.05965854227542877, 0.005804836750030518, 0.018863702192902565, 0.05381787195801735, -0.02052101492881775, 0.039064567536115646, 0.029667316004633904, 0.04515049234032631, 0.025432419031858444, -0.026821216568350792, -0.11833824962377548, 0.011178364977240562, 0.08615043014287949, -0.007883054204285145, -0.05568558722734451, 0.06137445569038391, 0.06838440895080566, 0.03443844988942146, 0.017557069659233093, -0.04781590774655342, 0.012143011204898357, 0.011825875379145145, -0.02238037809729576, 0.03877238184213638, -0.0659535676240921, 0.03443819656968117, 0.07257737219333649, -0.03244604170322418, -0.038757406175136566, -0.11668089032173157, 0.02458726428449154, -0.0165246594697237, -0.023922353982925415, -0.013401985168457031, -0.020667212083935738, 0.010680517181754112, 0.018678823485970497, -0.020485365763306618, 0.03519083559513092, 0.06541936099529266, 0.0351472906768322, 0.006116463337093592, -0.02493733912706375, 0.00891798920929432, 0.05854633077979088, 0.03275298699736595, 0.013245243579149246, 0.0013600480742752552, -0.11065708100795746, 0.1375260353088379, -0.05307116359472275, 0.019672466441988945, 0.03321686014533043, 0.02584785968065262, 0.039508018642663956, -0.005367327481508255, 0.024541977792978287, -0.08082140237092972, 0.031117431819438934, -0.009105809032917023, 0.04324089363217354, -0.032218288630247116, -0.10960444808006287, -5.789316048776527e-08, -0.045451775193214417, -0.06441090255975723, 0.09052594751119614, -0.028781704604625702, 0.11075142025947571, -0.012339971028268337, 0.01573074795305729, -0.008006728254258633, 0.05079341679811478, -0.04592534527182579, 0.11676707863807678, -0.006797280162572861, -7.767545321257785e-05, 0.00901781301945448, -0.02353072352707386, 0.045374538749456406, 0.002733746776357293, -0.061008717864751816, 0.0006431021029129624, 0.003495734417811036, -0.023289520293474197, 0.018144726753234863, 0.021673336625099182, 0.034552931785583496, 0.03676415979862213, -0.04110730439424515, 0.003281183307990432, -0.07208746671676636, -0.0212034210562706, -0.03116476908326149, -0.045020829886198044, -0.014461618848145008, 0.07342164218425751, 0.06172878295183182, -0.04759945347905159, 0.036358874291181564, 0.006723026279360056, 0.05702733248472214, -0.06396392732858658, -0.10886326432228088, 0.015562733635306358, -0.007738872896879911, -0.020612698048353195, -0.026296211406588554, 0.019929582253098488, -0.016239333897829056, -0.010820025578141212, -0.026226378977298737, 0.0811026319861412, 0.02746909111738205, -0.04464344307780266, 0.01081077940762043, -0.002739180810749531, 0.06800391525030136, 0.01574493758380413, -0.06459493935108185, -0.04656600207090378, -0.08647523075342178, 0.08407529443502426, 0.07954251021146774, -0.029334504157304764, 0.06380310654640198, -0.15620195865631104, -0.05244456231594086]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\ScalingandGeneralizationinNeuralNetworksACaseStudy.pdf,Optimization,"186 AN APPLICATION OF THE PRINCIPLE OF MAXIMUM INFORMATION PRESERVATION TO LINEAR SYSTEMS Ralph Linsker IBM T. J. Watson Research Center, Yorktown Heights, NY 10598 ABSTRACT This paper addresses the problem of determining the weights for a set of linear filters (model ""cells"") so as to maximize the ensemble-averaged information that the cells' output values jointly convey about their input values, given the statistical properties of the ensemble of input vectors. The quantity that is maximized is the Shannon information rate, or equivalently the average mutual information between input and output. Several models for the role of processing noise are analyzed, and the biological motivation for considering them is described. For simple models in which nearby input signal values (in space or time) are correlated, the cells resulting from this optimization process include center-surround cells and cells sensitive to temporal variations in input signal. INTRODUCTION I have previously proposed [Linsker, 1987, 1988] a principle of ""maximum information preservation,"" also called the ""infomax"" principle, that may account for certain aspects of the organization of a layered perceptual network. The principle applies to a layer L of cells (which may be the input layer or an intermediate layer of the network) that provides input to a next layer M. The mapping of the input signal vector L onto an output signal vector M, f:L ~ M, is characterized by a conditional probability density function (""pdf"") p(MI L). The set S of allowed mappings I is specified. The input pdf PL(L) is also given. (In the cases considered here, there is no feedback from M to L.) The infomax principle states that a mapping I should be chosen for which the Shannon information rate [Shannon, 1949] R(j) == f dL PL(L) f dM p(MI L) 10g[P(MI L)/PM(M)] (1) is a maximum (over allIin the set S). Here PM(M) == fdLPL(L)P(MIL) is the pdf of the output signal vector M. R is identical to the average mutual information between Land M. Maximum Infonnation Preservation to Linear Systems 187 To understand better how the info max principle may be applied to biological systems and complex synthetic networks, it is useful to solve the infomax optimization problem explicitly for simpler systems whose properties are nonetheless biologically motivated. This paper therefore deals with the practical computation of infomax solutions for cases in which the mappings! are constrained to be linear. INFOMAX SOLUTIONS FOR A SET OF LINEAR FILTERS We consider the case of linear model ""neurons"" with multivariate Gaussian input and additive Gaussian noise. There are N input (L) cells and N' output (M) cells. The input column vector L = (Lt,~, ... ,LNF is randomly selected from an N-dimensional Gaussian distribution having mean zero. That is, (2) where QL is the covariance matrix of the input activities, Q6 = J dL PL(L)LjLj • (Superscript T denotes the matrix transpose.) To specify the set S of allowed mappings !:L .... M, we define a processing model that includes a description of (i) how noise enters during processing, (ii) the independent variables over which we are to maximize R, and (iii) any constraints on their values. Figure 1 shows several such models. We shall analyze the simplest, then explain the motivation for the more complex models and analyze them in turn. Model A -- Additive noise of constant variance In Model A of Fig. 1 the output signal value of the nth M cell is: (3) The noise components ""11 are independently and identically distributed (fli.i.d. "") random variables drawn from a Gaussian distribution having a mean of zero and variance B. Each mapping !:L .... M is characterized by the values of the {Cnj } and the noise parameter B. The elements of the covariance matrix of the output activities are (using Eqn. 3) (4) where ~nm = 1 if n = m and 0 otherwise. Evaluating Eqn. 1 for this processing model gives the information rate: R(j) = (1/2) In Det W(j) (5) where ~m = Q:!'/ B. (R is the difference of two entropy terms. See [Shannon, 1949], p.57, for the entropy of a Gaussian distribution.) 188 Linsker If the components Cni of the C matrix are allowed to be arbitrarily large, then the information rate can be made arbitrarily large, and the effects of noise become arbitrarily small. One way to limit C is to impose a ""resource constraint"" on each M cell. An example of such a constraint is ~jqj = 1 for all n. One can then attempt directly, using numerical methods, to maximize Eqn. 5 over all allowed C for given B. However, when some additional conditions (below) are satisfied, further analytical progress can be made. Suppose the NL-cells are uniformly spaced along the line interval [0,1] with periodic boundary conditions, so that cell N is next to cell 1. [The analysis can be extended to a two- (or higher-) dimensional array in a straightforward manner.] Suppose also that (for given N) the covariance Q6 of the input values at cells i and j is a function QL(Sj) only of the displacement s'J from i to j. (We deal with the periodicity by defining Sab = b - a - Ya~ and choosing the integer Yab such that -N/2 S Sab < N/2.) Then QL is a Toeplitz matrix, and its eigenvalues {Ak} are the components of the discrete Fourier transform (""F.T."") of QL(S): Ak = ~sQL(s) exp( -2~ks/N), (-N/2) S k < N/2. (6) We now impose two more conditions: (1) N' = N. This simplifies the resulting expressions, but is otherwise inessential, as we shall discuss. (2) We constrain each M cell to have the same arrangement of C-values relative to the M cell's position. That is, Cnj is to be a function C(Sni) only of the displacement Sni from n to i. This constraint substantially reduces the computational demands. We would not expect Figure 1. L· I L· I (S,C) (D) Four processing models (A)-(D): Each diagram shows a single M cell (indexed by n) having output activity Mn. Inputs {LJ may be common to many M cells. All noise contributions (dotted lines) are uncorrelated with one another and with {LJ. GC = gain control (see text). Maximum Information Preservation to Linear Systems 189 it to hold in general in a biologically realistic model -- since different M cells should be allowed to develop different arrangements of weights -- although even then it could be used as an Ansatz to provide a lower bound on R. The section, ""Temporally-correlated input patterns,"" deals with a situation in which it is biologically plausible to impose this constraint. Under these conditions, (Q:!') is also a Toeplitz matrix. Its eigenvalues are the components of the F.T. of QM(snm). For N' = N these eigenvalues are (B + A~k) , where Zk = ICkl2 and Ck == ~sC(s) exp( -2'TT~ks/N) is the F.T. of C(s). [This expression for the eigenvalues is obtained by rewriting Eqn. 4 as: QM(snm) = B8n_m.o + ~j.jC(snJQL(Sj)C(sm) ,and taking the F.T. of both sides.] Therefore R = (1/2)~k In[l + AJcZk/ B]. (7) We want to maximize R subject to ~sC(S)2 = 1, which is equivalent to ~Zk = N . Using the Lagrange multiplier method, we maximize A == R + 11-(~Zk - N) over all nonnegative {Zk}' Solving for (JA/ (JZk = 0 and requiring Zk ~ 0 for all k gives the solution: Zk = max[( -1/211-) - (B/Ak)' 0], (8) where (given B) 11- is chosen such that ~Zk = N. Note that while the optimal {Zk} are uniquely determined, the phases of the {ck} are completely arbitrary [except that since the {C(s)} are real, we must have Ck * = c_ k for all k]. The {C(s)} values are therefore not uniquely determined. Fig. 2a shows two of the solutions for .an example in which QL(S) = exp[ - (s/ So)2] with So = 6, N=N'=64, and B.:..:.l. Both solutions have ZO.±1 ..... ±6=5.417, 5.409, 5.378, 5.306, 5.134,4.689,3.376, and all other Zk == O. Setting all Ck phases to zero yields the solid curve; a particular random choice of phases yields the dotted dHve. We shall later see that imposing locality conditions on the {C(s)} (e.g., penalizing nonzero C(s) for large I s I) can remove the phase ambiguity. Our solution (Eqn. 8) can be described in terms of a so-called ""water-filling"" analogy: If one plots B /Ak versus k, then Zk is the depth of ""water"" at k when one ""pours"" into the ""vessel"" defined by the B / Ak curve a total quantity of ""water"" that corresponds to ~Zk = N and brings the ""water level"" to ( -1/211-). Let us contrast this problem with two other problems to which the ""water-filling"" analogy has been applied in the information-theory literature. In our notation, they are: 1. Given a transfer function {C(s)} and the noise variance B, how should a given total input signal power ~Ak be apportioned among the various wavenumbers k so as to maximize the information rate R [Gallager, 1968]? Our problem is complementary to this: we fix the input signal properties and seek an optimal transfer function subject to constraints. 190 Linsker 2. Rate-distortion (R-D) calculation [Berger, 1971]: Given a distortion measure (that defines a ""distance"" between the actual input signal and an estimate of it that can be reconstructed from the channel's output), and the input power spectrum p.k}, what choice of {Zk} minimizes the average distortion for given information rate (or minimizes the required rate for given distortion)? In the R-D problem there is a process of reconstruction, and a given measure for assessing the ""goodness"" of reconstruction. In contrast, in our network there is no reconstruction of the input signal, and no criterion of the ""goodness"" of such a hypothetical reconstruction is provided. Note also that infomax optimization is not the same as computing which channel (that is, which mapping !:L .... M) selected from an allowed set has the maximum information-theoretic capacity. In that problem, one is free to encode the inputs before transmission so as to make optimal use of (Le., ""achieve the capacity of"") the channel. In our case, there is no such pre-encoding; the input ensemble is prescribed (by the environment or by the output of an earlier processing stage) and we need to maximize the channel rate for that ensemble. The simplifying condition that N = N' (above) is unnecessarily restrictive. Eqn. 7 can be easily generalized to the case in which N is a mUltiple of N' and the N' M cells are uniformly spaced on the unit interval. Moreover, in the limit that 1/ N' is much smaller than the correlation length scale of QL, it can be shown that R is unchanged when we simultaneously increase N' and B by the same factor. (For example, two adjacent M cells each having noise variance 2B jointly convey the same information Figure 2. c c \ ,: \,/ (0) l ..-.' s (b) -10 Example infomax solutions C(s) for locally-correlated inputs: (a) Model A; region of nonnegligible C(s) extends over all s; phase ambiguity in Ck yields non unique C(s) solutions, two of which are shown. See text for details. (b) Models C (solid curve) and D (dotted curve) with Gaussian g(S)-l favoring short connections; shows center-surround receptive fields, more pronounced in Model D. (c) ""Temporal receptive field"" using Model D for temporally correlated scalar input to a single M cell; C(s) is the weight applied to the input signal that occurred s time steps ago. Spacing between ordinate marks is 0.1; ~ C(S)2 = 1 in each case. c Maximum Information Preservation to Linear Systems 191 about L as one M cell having noise variance B.) For biological applications we are mainly interested in cases in which there are many L cells [so that C(s) can be treated as a function of a continuous variable] and many M cells (so that the effect of the noise process is described by the single parameter B/ N). The analysis so far shows two limitations of Model A. First, the constraint ~iqi = 1 is quite arbitrary. (It certainly does not appear to be a biologically natural constraint to impose!) Second, for biological applications we are interested in predicting the favored values of {C(s)}, but the phase ambiguity prevents this. In the next section we show that a modified noise model leads naturally, without arbitrary constraints on ~iqi' to the same results derived above. We then turn to a model that favors local connections over long-range ones, and that resolves the phase ambiguity issue. Model B -- Independent noise on each input line In Model B of Fig. 1 each input Li to the nth M cell is corrupted by Li.d. Gaussian noise Vl1i of mean zero and variance B. The output is (9) Since each Vni is independent of all other noise terms (and of the inputs {Li}), we find (10) We may rewrite the last term as B~l1m (~iqy!2 (~jc;)l/2. The information rate is then R = (1/2) In DetWwhere (11) Define c' ni == Cl1i(~kqk)-1/2 ; then J¥,.m = ~lIm + (~,.jc'lIiQbC' mj)/ B. Note that this is identical (except for the replacement C ~ C') to the expression following Eqn. (5), in which QM was given by Eqn. (4). By definition, the {C' nil satisfy ~iC';i = 1 for all n. Therefore, the problem of maximizing R for this model (with no constraints on ~jq;) is identical to the problem we solved in the previous section. Model C -- Favoring of local connections Since the arborizations of biological cells tend to be spatially localized in many cases, we are led to consider constraints or cost terms that favor localization. There are various ways to implement this. Here we present a way of modifying the noise process so that the infomax principle itself favors localized solutions, without requiring additional terms unrelated to information transmission. Model C of Fig. 1 is the same as Model B, except that now the longer connections are ""noisier"" than the shorter ones. That is, the variance of VIIi is <V;i> = B~(sn;) where g(s) increases with 1 s I. [Equivalently, one could attenuate the signal on the (i ~ n) line by g(sll;) 1/2 and have the same noise variance Bo on all lines.] 192 Linsker This change causes the last term of Eqn. 10 to be replaced by Bo8I1m~g(SIl)qi . Under the conditions discussed earlier (Toeplitz QL and QM, and N = N), we derive (12) Recall that the {ck } are related to {C(s)} by a Fourier transform (see just before Eqn. 7). To cotppute which choice of IC(s)} maximizes R for a given problem, we used a gradient ascent algorithm several times, each time using a different random set of initial I C(s)} values. For the problems whose solutions are exhibited in Figs. 2b and 2c, multiple starting points usually yielded the same solution to within the error tolerance specified for the algorithm [apart from an arbitrary factor by which all of the C(s)'s can be multiplied without affecting R], and that solution had the largest R of any obtained for the given problem. That is, a limitation sometimes associated with gradient ascent algorithms -- namely, that they may yield multiple ""solutions"" that are local, but far from global, maxima -- did not appear to be a difficulty in these cases. Fig. 2b (solid curve) shows the infomax solution for an example having QL(S) = exp[ - (S/sO)2] and g(s) = exp[(s/s.)2] with So = 4, s. = 6, N = N = 32, and Bo = 0.1. There is a central excitatory peak flanked by shallow inhibitory sidelobes (and weaker additional oscillations). (As noted, the negative of this solution, having a central inhibitory region and excitatory sidelobes, gives the same R.) As Bo is increased (a range from 0.001 to 20 was studied), the peak broadens, the sidelobes become shallower (relative to the peak), and the receptive fields of nearby M cells increasingly overlap. This behavior is an example of the ""redundancy-diversity"" tradeoff discussed in [Linsker, 1988]. Model D -- Bounded output variance Our previous models all produce output values Mn whose variance is not explicitly constrained. More biologically realistic cells have limited output variance. For example, a cell's firing rate must lie between zero and some maximum value. Thus, the output of a model nonlinear cell is often taken to be a sigmoid function of (~iCII;L)· Within the context of linear cell models, we can capture the effect of a bounded output variance by using Model D of Fig. 1. We pass the intermediate output ~iClIi(Li + VIIi) through a gain control QC that normalizes the output variance to unity, then we add a final (Li.d. Gaussian) noise term V'II of variance R.. That is, (13) Without the last term, this model wo~ld be identical to Model C, since mUltiplying both the signal and the VIIi noise by the same factor GC would not affect R. The last term in effect fixes the number of output values that can be discriminated (Le., not confounded with each other by the noise process V'II) to be of order Rl1!2. The information rate for this model is derived to be (cf. Eqn. 12): Maximum Information Preservation to Linear Systems 193 (14) where V( C) is the variance of the intermediate output before it is passed through GC: (15) Fig. 2b (dotted curve) shows the infomax solution (numerically obtained as above) for the same QL(S) and g(s) functions and parameter values as were used to generate the solid curve (for Model C), but with the new parameter Bl = 0.4. The effect of the new Bl noise process in this case is to deepen the inhibitory sidelobes (relative to the central peak). The more pronounced center-surround character of the resulting M cell dampens the response of the cell to differences (between different input patterns) in the spatially uniform component of the input pattern. This response property allows the L .... M mapping to be info max-optimal when the dynamic range of the cells' output response is constrained.· (A competing effect can complicate the analysis: If Bl is increased much further, for example to 50 in the case discussed, the sidelobes move to larger s and become shallower. This behavior resembles that discussed at the end of the previous section for the case of increasing Bo; in the present case it is the overall noise level that is being increased when Bl increases and Bo is kept constant.) TemporaUy-correlated input patterns Let us see how infomax can be used to extract regularities in input time series, as contrasted with the spatially-correlated input patterns discussed above. We consider a single M cell that, at each discrete time denoted by n, can process inputs {LJ from earlier times i ~ n (via delay !ines, for example). We use the same Model D as before. There are two differences: First, we want g(s) = 00 for all s > 0 (input lines from future times are ""infinitely noisy""). [A technical point: Our use of periodic boundary conditions, while computationally convenient, means that the input value that will occur s time steps from now is the same value that occurred (N - s) steps ago. We deal with this by choosing g(s) to equal 1 at s = 0, to increase as s .... -N/2 (going into the past), and to increase further as s decreases from +N/2 to 1, corresponding to increasingly remote past times. The periodicity causes no unphysical effects, provided that we make g(s) increase rapidly enough (or make N large enough) so that C(s) is negligible for time intervals comparable to N.] Second, the fact that C,,; is a function only of s'"" is now a consequence of the constancy of connection weights C(s) of a single M cell with time, rather than merely a convenient Ansatz to facilitate the infomax computation for a set of many M cells (as it was in previous sections). The infomax solution is shown in Fig. 2c for an example having QL(S) = exp[ - (S/So)2]; g(s) = exp[ -t(s}/s.J with t(s} = s for s ~ 0 and t(s} = s - N for s ~ 1; So = 4, Sl = 6, N = 32, Bo = 0.1, and Bl = 0.4. The result is that the ""temporal receptive field"" of the M cell is excitatory for recent times, and 194 Linsker inhibitory for somewhat more remote times (with additional weaker oscillations). The cell's output can be viewed approximately as a linear combination of a smoothed input and a smoothed first time derivative of the input, just as the output of the center-surround cell of Fig. 2b can be viewed as a linear combination of a smoothed input and a smoothed second spatial derivative of the input. As in Fig. 2b, setting BI = 0 (not shown) lessens the relative inhibitory contribution. SUMMARY To gain insight into the operation of the principle of maximum information preservation, we have applied the principle to the problem of the optimal design of an array of linear filters under various conditions. The filter models that have been used are motivated by certain features that appear to be characteristic of biological networks. These features include the favoring of short connections and the constrained range of output signal values. When nearby input signals (in space or time) are correlated, the infomax-optimal solutions for the cases studied include (1) center-surround cells and (2) cells sensitive to temporal variations in input. The results of the mathematical analysis presented here apply also to arbitrary input covariance functions of the form QL( I i - j I). We have also presented more general expressions for the information rate, which can be used even when QL is not of this form. The cases discussed illustrate the operation of the infomax principle in some relatively simple but instructive situations. The analysis and results suggest how the principle may be applied to more biologically realistic networks and input ensembles. References T. Berger, Rate Distortion Theory (Prentice-Hall, Englewood Cliffs, N.J., 1971), chap. 4. R. G. Gallager, Information Theory and Reliable Communication (John Wiley and Sons, N.Y., 1968), p. 388. R. Linsker, in: Neural Information Processing Systems (Denver, Nov. 1987), ed. D. Z. Anderson (Amer. Inst. of Physics, N.Y.), pp. 485-494. R. Linsker, Computer 21 (3) 105-117 (March 1988). C. E. Shannon and W. Weaver, The Mathematical Theory of Communication (Univ. of Illinois Press, Urbana, 1949).","[-0.03355740010738373, -0.05124766752123833, 0.034716393798589706, -0.03171086311340332, 0.02163890190422535, 0.04512663185596466, 0.03231312707066536, -0.06704042106866837, 0.09120155870914459, -0.02716299518942833, -0.09170413017272949, 0.0945662185549736, 0.10543069988489151, -0.023238353431224823, -0.11915892362594604, 0.0562683641910553, 0.12169025838375092, 0.0628357008099556, -0.1579439342021942, -0.033724889159202576, 0.05520201101899147, -0.031688183546066284, -0.07544863969087601, 0.013909471221268177, 0.047369059175252914, -0.004970800597220659, -0.06851078569889069, -0.0331706702709198, 0.008247684687376022, -0.04637829214334488, 0.015469773672521114, -0.0578894279897213, 0.0894094780087471, -0.010298199020326138, -0.08908311277627945, -0.007829579524695873, -0.04632972180843353, 0.014956275001168251, 0.05118439346551895, -0.004231430124491453, -0.057272445410490036, 0.046118903905153275, -0.06212220340967178, 0.03295239061117172, 0.013627538457512856, 0.1009613648056984, 0.03269394859671593, -0.08315479010343552, -0.0400702990591526, -0.0002551809011492878, -0.09413959085941315, 0.02328404039144516, -0.0010079079074785113, 0.029589645564556122, 0.030035119503736496, 0.0027416148222982883, -0.03814173489809036, -0.020227469503879547, -0.0390162393450737, 0.013760998845100403, -0.0037888502702116966, 0.007777479011565447, -0.019411079585552216, 0.02708893083035946, 0.05956675857305527, 0.0354270413517952, -0.009707758203148842, 0.043540168553590775, 0.006608135066926479, 0.04039614275097847, -0.06954158842563629, 0.08606766909360886, -0.08045921474695206, -0.013711107894778252, 0.0823344960808754, 0.003004597034305334, 0.00037758206599391997, 0.02315732091665268, 0.056574515998363495, 0.051682181656360626, 0.049542494118213654, 0.01701672188937664, -0.004901268519461155, -0.04455485939979553, 0.039764534682035446, -0.04052411764860153, -0.03944467753171921, 0.04854974150657654, 0.09100186824798584, -0.05732753127813339, -0.11212896555662155, -0.034973058849573135, -0.05214836448431015, 0.06732495874166489, 0.011455926112830639, -0.034545112401247025, -0.00436109583824873, 0.01150531880557537, 0.1396160125732422, 0.04675887152552605, 0.007045110687613487, -0.04318031296133995, 0.06265284866094589, 0.029469266533851624, 0.0892312154173851, -0.08857634663581848, 0.03984229639172554, 0.02713020332157612, 0.020638663321733475, -0.0397498793900013, -0.04822586104273796, 0.011788240633904934, -0.0453784316778183, 0.04154511168599129, 0.07068246603012085, -0.011097369715571404, 0.07169554382562637, 0.06332313269376755, 0.025604642927646637, 0.00964485015720129, 0.010883664712309837, -0.08143100142478943, -0.03241993486881256, -0.006819605827331543, 0.05393831804394722, 0.05658429116010666, -0.025013601407408714, 2.3150415333834525e-33, -0.028118237853050232, -0.04142811521887779, -0.01447868999093771, -0.008822057396173477, 0.013613034039735794, -0.023786034435033798, -0.0299144946038723, -0.08834971487522125, 0.08852997422218323, 0.05039514601230621, -0.10997757315635681, 0.022694095969200134, 0.0010607377626001835, 0.0727519541978836, 0.07016915082931519, 0.017362454906105995, -0.03467358648777008, 0.06954649090766907, 0.022689659148454666, -0.14293023943901062, 0.03958921879529953, -0.00826902687549591, -0.0006336618098430336, -0.0793476551771164, 0.01532130129635334, -0.04675767198204994, 0.034178148955106735, -0.05726410821080208, -0.09059800952672958, -0.004586436320096254, -0.08093827217817307, 0.03907712548971176, 0.004823981784284115, -0.05216478928923607, 0.015714092180132866, 0.005137785337865353, 0.03280196711421013, 0.021180273965001106, 0.02540408819913864, -0.007513667922466993, 0.06570563465356827, 0.05102702975273132, -0.003506499109789729, -0.011852914467453957, -0.011971421539783478, -0.0244916919618845, -0.046758513897657394, 0.07359995692968369, -0.0682097002863884, -0.07947002351284027, -0.015924353152513504, -0.03982251137495041, 0.00916988030076027, -0.03587249666452408, -0.042251795530319214, 0.06101393327116966, -0.013461834751069546, 5.729322765546385e-06, -0.011133953928947449, 0.08175534754991531, -0.06142965331673622, 0.03727664425969124, 0.07675281167030334, 0.025565601885318756, 0.12507760524749756, 0.04629570245742798, -0.044451598078012466, -0.017970819026231766, 0.10507576167583466, -0.05793945491313934, 0.05198613181710243, 3.0953826808399754e-06, -0.0019540037028491497, -0.06912901252508163, 0.07692918926477432, -0.06665592640638351, 0.012076884508132935, -0.008470428176224232, -0.06578072160482407, 0.005892491899430752, -0.017564766108989716, -0.06447804719209671, -0.06454504281282425, -0.05532209202647209, -0.05796821787953377, 0.0463716983795166, 0.044584523886442184, -0.0074213179759681225, -0.0950678214430809, 0.01795634627342224, -0.008959805592894554, 0.0038621793501079082, 0.007568299304693937, -0.01789993792772293, -0.02400810644030571, -2.988620000213778e-33, 0.024829167872667313, -0.01961401104927063, -0.002008696785196662, 0.004678697790950537, 0.007499766536056995, 0.01053478941321373, -0.07627051323652267, -0.00410703057423234, -0.0647323876619339, 0.01042709406465292, -0.031070547178387642, -0.014785902574658394, 0.05068019777536392, 0.030588245019316673, -0.020512986928224564, 0.005537576507776976, -0.004148225300014019, 0.011879438534379005, 0.03826279938220978, -0.040573809295892715, -0.044503409415483475, -0.007690630853176117, -0.04009249806404114, 0.013314735144376755, 0.04140320047736168, 0.02817823737859726, -0.06187032535672188, 0.12082535773515701, 0.016223009675741196, -0.03201146051287651, -0.10573957860469818, -0.0659797191619873, -0.0011746621457859874, -0.015030103735625744, 0.00020915376080665737, 0.013147734105587006, 0.022824814543128014, -0.01233961433172226, 0.01947813481092453, 0.048948902636766434, 0.011322719976305962, 0.05601489543914795, -0.05022728070616722, -0.046019524335861206, 0.09256026148796082, -0.08112367242574692, -0.06775574386119843, 0.004979044198989868, -0.016011936590075493, -0.007826041430234909, 0.033653803169727325, 0.02910839021205902, -0.024531960487365723, -0.009989502839744091, -0.024961799383163452, 0.06527433544397354, -0.008015075698494911, 0.023863693699240685, 0.1104154884815216, 0.02006302960216999, -0.04701687768101692, -0.060930680483579636, -0.12676100432872772, 0.0463726781308651, 0.02205839939415455, 0.07564212381839752, 0.04040496423840523, 0.0024159885942935944, 0.054663337767124176, 0.02418234758079052, 0.03661584481596947, 0.002044645370915532, 0.011085211299359798, 0.018933886662125587, 0.037795890122652054, 0.04217842221260071, 0.013906752690672874, -0.02254563383758068, -0.048640307039022446, 0.039223190397024155, -0.11221081018447876, 0.026099223643541336, 0.06309638917446136, 0.02738894335925579, 0.02847122959792614, 0.02242695353925228, 0.03146045282483101, -0.05308152362704277, 0.06279969215393066, -0.1363103687763214, 0.015409206040203571, -0.009055059403181076, -0.08546745777130127, -0.04420478641986847, -0.08248085528612137, -5.83847494795009e-08, -0.015309148468077183, 0.029171524569392204, -0.0631590187549591, 0.009480168111622334, 0.05234108865261078, -0.03262081369757652, 0.05731252580881119, 0.026247570291161537, 0.0008501387201249599, -0.009512282907962799, 0.12051624804735184, 0.04433049261569977, -0.02737848460674286, 0.00027468870393931866, 0.014096606522798538, -0.020383521914482117, -0.01360638439655304, -0.046856366097927094, 0.008343140594661236, -0.04778964817523956, 0.03984260931611061, -0.03151089698076248, -0.04935441166162491, 0.08889187127351761, 0.08844861388206482, 0.02253505401313305, 0.04314694553613663, 0.03232244774699211, 0.07355187833309174, 0.07688070088624954, -0.03067968226969242, 0.06376784294843674, 0.021674182265996933, 0.016690555959939957, 0.04109025001525879, 0.021160444244742393, 0.00016088578558992594, -0.04373522102832794, -0.060964543372392654, -0.005143987014889717, -0.043797507882118225, -0.003001944860443473, -0.05731567367911339, -0.020436318591237068, 0.13194023072719574, 0.022408941760659218, 0.038310740143060684, -0.09671809524297714, 0.03965428099036217, -0.0016839071176946163, 0.055910829454660416, 0.01915081776678562, 0.007615566719323397, 0.029450509697198868, 0.030615953728556633, -0.05298018455505371, -0.007604902144521475, -0.03941475227475166, 0.005943290423601866, -0.019366439431905746, 0.0037586914841085672, 0.08999071270227432, -0.0963410884141922, -0.06939121335744858]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\SelfOrganizingNeuralNetworksfortheIdentificationProblem.pdf,Deep Learning,"554 STABILITY RESULTS FOR NEURAL NETWORKS A. N. Michell, J. A. FarreUi , and W. Porod2 Department of Electrical and Computer Engineering University of Notre Dame Notre Dame, IN 46556 ABSTRACT In the present paper we survey and utilize results from the qualitative theory of large scale interconnected dynamical systems in order to develop a qualitative theory for the Hopfield model of neural networks. In our approach we view such networks as an inter- connection of many single neurons. Our results are phrased in terms of the qualitative properties of the individual neurons and in terms of the properties of the interconnecting structure of the neural networks. Aspects of neural networks which we address include asymptotic stability, exponential stability, and instability of an equilibrium; estimates of trajectory bounds; estimates of the domain of attraction of an asymptotically stable equilibrium; and stability of neural networks under structural perturbations. INTRODUCTION In recent years, neural networks have attracted considerable attention as candidates for novel computational systemsl- 3 . These types of large-scale dynamical systems, in analogy to biological structures, take advantage of distributed information processing and their inherent potential for parallel computation4,5. Clearly, the design of such neural-network-based computational systems entails a detailed understanding of the dynamics of large-scale dynamical systems. In particular, the stability and instability properties of the various equilibrium points in such networks are of interest, as well as the extent of associated domains of attraction (basins of attraction) and trajectory bounds. In the present paper, we apply and survey results from the qualitative theory oflarge scale interconnected dynamical systems6 - 9 in order to develop a qualitative theory for neural networks. We will concentrate here on the popular Hopfield model3 , however, this type of analysis may also be applied to other models. In particular, we will address the following problems: (i) determine the stability properties of a given equilibrium point; (ii) given that a specific equilibrium point of a neural network is asymptotically stable, establish an estimate for its domain of attraction; (iii) given a set of initial condi- tions and external inputs, establish estimates for corresponding trajectory bounds; (iv) give conditions for the instability of a given equilibrium point; (v) investigate stability properties under structural perturbations. The present paper contains local results. A more detailed treatment of local stability results can be found in Ref. 10, whereas global results are contained in Ref. 1l. In arriving at the results of the present paper, we make use of the method of anal- ysis advanced in Ref. 6. Specifically, we view high dimensional neural network as an IThe work of A. N. Michel and J. A. Farrell was supported by NSF under grant ECS84-19918. 2The work of W. Porod was supported by ONR under grant NOOOI4-86-K-0506. © American Institute of Physics 1988 555 interconnection of individual subsystems (neurons). This interconnected systems view- point makes our results distinct from others derived in the literature1,12. Our results are phrased in terms of the qualitative properties of the free subsystems (individual neurons, disconnected from the network) and in terms of the properties of the intercon- necting structure of the neural network. As such, these results may constitute useful design tools. This approach makes possible the systematic analysis of high dimensional complex systems and it frequently enables one to circumvent difficulties encountered in the analysis of such systems by conventional methods. The structure of this paper is as follows. We start out by defining the Hopfield model and we then introduce the interconnected systems viewpoint. We then present representative stability results, including estimates of trajectory bounds and of domains of attraction, results for instability, and conditions for stability under structural pertur- bations. Finally, we present concluding remarks. THE HOPFIELD MODEL FOR NEURAL NETWORKS In the present paper we consider neural networks of the Hopfield type3 • Such systems can be represented by equations of the form N Ui = ..... biUi + I:Aij Gj(Uj) + Ui(t), for i = 1, ... ,N, j=1 (1) where Aij = *""Ui(t) = l~g) and bi = *.. As usual, Ci > O,Tij i:;,RijfR = (-00,00),':. = ~ +E.f=IITiil, Ri > O,Ii: R+ = [0,00) ~ R,Ii is continuous, Ui = ~,Gi : R ~ (-1,1), Gi is continuously differentiable and strictly monotoni- cally increasing (Le., Gi( uD > Gi( u~') if and only if u~ > u~'), UiGi( Ui) > 0 for all Ui ::j; 0, and Gi(O) = O. In (1), Ci denotes capacitance, Rij denotes resistance (possibly includ- ing a sign inversion due to an inverter), Gi (·) denotes an amplifier nonlinearity, and Ii(') denotes an external input. In the literature it is frequently assumed that Tij = Tji for all i,j = 1, ... , N and that Tii = 0 for all i = 1, ... , N. We will make these assumptions only when explicitly stated. We are interested in the qualitative behavior of solutions of (1) near equilibrium points (rest positions where Ui == 0, for i = 1, ... , N). By setting the external inputs Ui(t), i = 1, ... , N, equal to zero, we define U* = [ui, ... , u""NV fRN to be an equilibrium for (1) provided that -biui' + E.f=l Aij Gj(uj) = 0, for i = 1, ... ,N. The locations of such equilibria in RN are determined by the interconnection pattern of the neural network (i.e., by the parameters Aij, i,j = 1,. "", N) as well as by the parameters bi and the nature of the nonlinearities Gi(')' i = 1, ... ,N. Throughout, we will assume that a given equilibrium u* being analyzed is an isolated equilibrium for (1), i.e., there exists an r > 0 such that in the neighborhood B( u*, r) = {( u - u*)fRN : lu - u*1 < r} no equilibrium for (1), other than u = u*, exists. When analyzing the stability properties of a given equilibrium point, we will be able to assume, without loss of generality, that this equilibrium is located at the origin u = 0 of RN. If this is not the case, a trivial transformation can be employed which shifts the equilibrium point to the origin and which leaves the structure of (1) the same. 556 INTERCONNECTED SYSTEMS VIEWPOINT We will find it convenient to view system (1) as an interconnection of N free sub- systems (or isolated sUbsystems) described by equations of the form Pi = -biPi + Aii Gi(Pi) + Ui(t). (2) Under this viewpoint, the interconnecting structure of the system (1) is given by N Gi(Xb"" . ,xn ) ~ L AijGj(Xj), i = 1, ... ,N. (3) j=1 ii:i Following the method of analysis advanced in6 , we will establish stability results which are phrased in terms of the qualitative properties of the free subsystems (2) and in terms of the properties of the interconnecting structure given in (3). This method of analysis makes it often possible to circumvent difficulties that arise in the analysis of complex high-dimensional systems. Furthermore, results obtained in this manner frequently yield insight into the dynamic behavior of systems in terms of system com- ponents and interconnections. . GENERAL STABILITY CONDITIONS We demonstrate below an example of a result for exponential stability of an equi- librium point. The principal Lyapunov stability results for such systems are presented, e.g., in Chapter 5 of Ref. 7. We will utilize the following hypotheses in our first result. (A-I) For system (1), the external inputs are all zero, i.e., Ui(t) == 0, i = 1, ... , N. (A-2) For system (1), the interconnections satisfy the estimate for all Ixil < ri, Ix;1 < rj, i,j = 1, ... , N, where the ail are real constants. (A-3) There exists an N-vector a> ° (i.e., aT = (al, ... ,aN) and ai > 0, for all ~ = 1, ... ,N) such that the test matrix S = [Sij] is negative definite, where the bi are defined in (1) and the aij are given in (A-2). 557 We are now in a position to state and prove the following result. Theorem 1 The equilibrium x = 0 of the neural network (1) is exponentially stable if hypotheses (A-l), (A-2) and (A-3) are satisfied. Proof. For (1) we choose the Lyanpunov function (4) where the ai are given in (A-3). This function is clearly positive definite. The time deri vati ve of v along the solutions of (1) is given by N 1 N DV(1)(X) = 2: 2ai(2xd[-biXi + 2: Aij Gj(Xj)] i=1 j=1 where (A-l) has been invoked. In view of (A-2) we have N N DV(1)( x) < 2: ai( -bix~ + Xi 2: aijX j) i=1 j=1 for all IxI2 < r where r = mini(ri), IxI2 = (Ef:1 X~) 1/2, and the matrix R = [rij] is given by r;j = { ai( -bi + aii), t = J ai aij, i ::J j. But it follows that xT Rx = xT ( R ~ RT) X = xT Sx ::; )w(S) Ixl1 (5) where S is the matrix given in (A-3) and AM(S) denotes the largest eigenvalue of the real symmetric matrix S. Since S is by assumption negative definite, we have AM(S) < O. It follows from (4) and (5) that in some neighborhood of the origin x = 0, we have c1lxl~ ~ v(x) ~ c2lxl~ and DV(1)(X) ~ -c3Ixl~, where C1 = ! mini ai > 0, C2 = ! maxi ai > 0, and C3 = -AM(S) > O. Hence, the equilibrium x = ° of the neural network (1) is exponentially stable (c.f. Theorem 9.10 in Ref. 7). Consistent with the philosophy of viewing the neural network (1) as an intercon- nection of N free subsystems (2), we think of the Lyapunov function (4) as consisting of a weighted sum of Lyapunov functions for each free subsystem (2) (with Ui(t) == 0). The weighting vector a > 0 provides flexibility to emphasize the relative importance of the qualitative properties of the various individual subsystems. Hypothesis (A - 2) provides a measure of interaction between the various subsystems (3). Furthermore, it is emphasized that Theorem 1 does not require that the parameters Aij in (1) form a symmetric matrix. 558 WEAK COUPLING CONDITIONS The test matrix S given in hypothesis (A - 3) has off-diagonal terms which may be positive or nonpositive. For the special case where the off-diagonal terms of the test matrix S = [Sij] are non-negative, equivalent stability results may be obtained which are much easier to apply than Theorem 1. Such results are called weak-coupling conditions in the literature6,9. The conditions 8ij ~ 0 for all i ::J j may reflect properties of the system (1) or they may be the consequence of a majorization process. In the proof of the subsequent result, we will make use of some of the properties of M- matrices (see, for example, Chapter 2 in Ref. 6). In addition we will use the following assumptions. (A-4) For system (1), the nonlinearity Gi(Xi) satisfies the sector condition (A-S) The successive principal minors of the N X N test matrix D = [dij] are all positive where, the bi and Aij are defined in (1) and Ui2 is defined in (A - 4). Theorem 2 The equilibrium x = 0 of the neural network (1) is asymptotically sta- ble if hypotheses (A-1), (A-4) and (A-5) are true. Proof. The proof proceeds10 along lines similar to the one for Theorem 1, this time with the following Lyapunov function N v(x) = L: Qilxd· (6) i=l The above Lyapunov function again reflects the interconnected nature of the whole system. Note that this Lyapunov function may be viewed as a generalized Hamming distance of the state vector from the origin. ESTIMATES OF TRAJECTORY BOUNDS In general, one is not only interested in questions concerning the stability of an equilibrium of the system (1), but also in performance. One way of assessing the qual- itative properties of the neural system (1) is by investigating solution bounds near an equilibrium of interest. We present here such a result by assuming that the hypotheses of Theorem 2 are satisfied. In the following, we will not require that the external inputs Ui(t), i = 1, ... , N be zero. However, we will need to make the additional assumptions enumerated below. (A-6) Assume that there exist .xi > 0, for i = 1, ... , N, and an ( > 0 such that N L: j=1 i:/;j (~~) IAjil > ( > 0, i = 1, ... ,N where bi and Aij are defined in (1) and (Ti2 is defined in (A-4). (A-7) Assume that for system (1), N L: .xiIUi(t)1 ~ k for all t ~ 0 i=l for some constant k > 0 where the .xi, i = 1, ... , N are defined in (A-6). 559 In the proof of our next theorem, we will make use of a comparison result. We consider a scalar comparison equation of the form iJ = G(y) where y(R,G : B(r) - R for some r > 0, and G is continuous on B(r) = {XfR: Ixl < r}. We can then prove the following auxiliary theorem: Let p(t) denote the maximal solution of the comparison equation with p(to) = Yo(B(r), t ~ to > O. If r(t), t ~ to ~ 0 is a continuous function such that r(to) $ Yo, and if r(t) satisfies the differential inequality Dr(t) = limk-+O+ t sup[r(t + k) - r(t)] $ G(r(t)) almost everywhere, then r(t) $ p(t) for t ~ to ~ 0, for as long as both r(t) and p(t) exist. For the proof of this result, as well as other comparison theorems, see e.g., Refs. 6 and 7. For the next theorem, we adopt the following notation. We let 6 = mini (Til where (Til is defined in (A - 4), we let c = (6 , where ( is given in (A-6), and we let ¢(t,to,xo) = [¢I(t,to,xo)'''',</>N(t,to,xo)]T denote the solution of (1) with ¢(to, to, xo) = Xo = (XlO,"""" xNol for some to ~ O. We are now in a position to prove the following result, which provides bounds for the solution of(1). Theorem 3 Assume that hypotheses (A-6) and (A-7) are satisfied. Then ~ ~ I k) c(t t) k 11¢(t, to, xo)11 = L..."" .xil¢i(t, to, xo) ::; (a - - e- - 0 + -, t ~ to ~ 0 i=l C C provided that a > k/c and IIxoll = E~l .xilxiOI ~ a, where the .xi, i = 1,. "", N are given in (A-6) and k is given in (A-7). Proof. For (1) we choose the Lyapunov function N v(x) = L .xilxil· (7) i=l 560 Along the solutions of (1), we obtain N DV(l)(X) ~ AT Dw + z: Ai!Ui(t)\ (8) i=l where wT = [G1J;d\Xl\,'''' G'Z~N)lxN\]' A = (A}, ... ,ANf, and D = [dij] is the test matrix given in (A-5). Note that when (A-6) is satisfied, as in the present theorem, then (A-5) is automatically satisfied. Note also that w ~ 0 (Le., Wi ~ 0, i = 1, ... , N) and w = 0 if and only if x = O. Using manipulations involving (A-6), (A-7) and (8), it is easy to show that DV(l)(X) ~ -cv(x) + k. This ineqUality yields now the comparison equation iJ = -cy + k, whose unique solution is given by pet, to, Po) = (Po - ~) e-c(t-to) +~, for all t ~ to. H we let r = v, then we obtain from the comparison result N pet) ~ ret) = v(4)(t,to,xo)) = 2: Ail4>i(t,to,xo)1 = 114>(t,to,xo)\I, i=l i.e., the desired estimate is true, provided that Ir(to)\ = Ef:l Ai/XiOI = IIxoll ~ a and a> kjc. ESTIMATES OF DOMAINS OF ATTRACTION Neural networks of the type considered herein have many equilibrium points. If a given equilibrium is asymptotically stable, or exponentially stable, then the extent of this stability is of interest. As usual, we assume that x = 0 is the equilibrium of interest. If 4>(t, to, xo) denotes a solution of the network (1) with 4>(to, to, xo) = xo, then we would like to know for which points Xo it is true that 4>( t, to, xo) tends to the origin as t ---+ 00. The set of all such points Xo makes up the domain of attraction (the basin of attraction) of the equilibrium x = O. In general, one cannot determine such a domain in its entirety. However, several techniques have been devised to estimate subsets of a domain of attraction. We apply one such method to neural networks, making use of Theorem 1. This technique is applicable to our other results as well, by making appropriate modifications. We assume that the hypotheses (A-I), (A-2) and (A-3) are satisfied and for the free subsystem (2) we choose the Lyapunov function 1 2 Vi(Pi) = 2 Pi' (9) Then DVi(2) (Pi) ~ (-bi + aii)p~, \Pi/ < ri for some ri > O. If (A-3) is satisfied, we must have (-bi + aii) < 0 and DVi(2)(Pi) is negative definite over B(ri). Let Gvo; = {PifR : Vi(Pi) = !p~ < trl ~ Voi}. Then GVo; is contained in the domain of attraction of the equilibrium Pi = 0 for the free subsystem (2). To obtain an estimate for the domain of attraction of x = 0 for the whole neural network (1), we use the Lyapunov function N 1 N v(x) - '""' -""'·x~ - '""' o·v·(x·) -LJ2 ..... •• -LJ •••. i=l i=l It is now an easy matter to show that the set N C>. = {uRN: v(x) = LOiVi(Xi) < oX} i=l 561 (10) will be a subset of the domain of attraction of x = 0 for the neural network (1), where oX = min (OiVOi) = min (~Oir~) . l$.i$.N 1$.i$.N 2 • In order to obtain the best estimate of the domain of attraction of x = 0 by the present method, we must choose the 0i in an optimal fashion. The reader is referred to the literature9,l3,l4 where several methods to accomplish this are discussed. INSTABILITY RESULTS Some of the equilibrium points in a neural network may be unstable. We present here a sample instability theorem which may be viewed as a counterpart to Theorem 2. Instability results, formulated as counterparts to other stability results of the type considered herein may be obtained by making appropriate modifications. (A-B) For system (1), the interconnections satisfy the estimates XiAiiGi(Xi) < OiAiiX;, IXiAjjGj(xj)1 $ IxdlAijlO""j2lxil, if; j where OJ = O""il when Aii < 0 and Oi = O""i2 when Aii > 0 for all IXil < ri, and for alllXjl < Tj,i,j = 1, ... ,N. (A-9) The successive principal minors of the N x N test matrix D = [dij] given by are positive, where O""i = ~ - Au when ifFIl (i.e., stable subsystems) and O""i -!:; + Aji when ifFu (i.e., unstable subsystems) with F = FII U Fu and F = {I, ., . , N} and Fu f; </>. We are now in a position to prove the following result. Theorem 4 The equilibrium x = 0 of the neural network (1) is unstable if hypotheses (A-l), (A-8) and (A-g) are satisfied. If in addition, FII = </> (</> denotes the empty set), then the equilibrium x = 0 is completely unstable. 562 Proof. We choose the Lyapunov function (11) ifF .. ifF. where ai > 0, i = 1, ... ,N. Along the solutions of (1) we have (following the proof of Theorem 2), DV(l)(X) $ -aTDw for all x€B(r), r = miniri where aT = (a}, ... ,aN), D is defined in (A-9), and wT = [G1l;d IXll, ... , GNx~N) IXNI]. We conclude that DV(l)(X) is negative definite over B(r). Since every neighborhood of the origin x = ° contains at least one point x' where v(x') < 0, it follows that the equilibrium x = 0 for (1) is unstable. Moreover, when F, = </>, then the function v(x) is negative definite and the equilibrium x = 0 of (1) is in fact completely unstable (c.f. Chapter 5 in Ref. 7). STABILITY UNDER STRUCTURAL PERTURBATIONS In specific applications involving adaptive schemes for learning algorithms in neural networks, the interconnection patterns (and external inputs) are changed to yield an evolution of different sets of desired asymptotica.l1y stable equilibrium points with ap- propriate domains of attraction. The present diagonal dominance conditions (see, e.g., hypothesis (A-6)) can be used as constraints to guarantee that the desired equilibria always have the desired stability properties. To be more specific, we assume that a given neural network has been designed with a set of interconnections whose strengths can be varied from zero to some specified values. We express this by writing in place of (1), N Xi = -biXi + L:8ij Aij Gj(Xj) + Ui(t), for i = 1, ... ,N, j=l (12) where 0 $ 8ij $ 1. We also assume that in the given neural network things have been arranged in such a manner that for some given desired value ~ > 0, it is true that ~ = mini (!:; - 8iiAii). From what has been said previously, it should now be clear that if Ui( t) == 0, i = 1, ... ,N and if the diagonal dominance conditions ~ - t (~~) 18ij Aiji > 0, for i = 1, ... ,N j = 1 i:f;j (13) are satisfied for some Ai > 0, i = 1, ... , N, then the equilibrium x = ° for (12) will be asymptotically stable. It is important to recognize that condition (13) constitutes a sin- gle stability condition for the neural network under structural perturbations. Thus, the strengths of interconnections of the neural network may be rearranged in any manner to achieve some desired set of equilibrium points. If (13) is satisfied, then these equi- libria will be asymptotically stable. (Stability under structural perturbations is nicely surveyed in Ref. 15.) 563 CONCLUDING REMARKS In the present paper we surveyed and applied results from the qualitative theory of large scale interconnected dynamical systems in order to develop a qualitative the- ory for neural networks of the Hopfield type. Our results are local and use as much information as possible in the analysis of a given eqUilibrium. In doing so, we estab- lished cri-teria for the exponential stability, asymptotic stability, and instability of an equilibrium in such networks. We also devised methods for estimating the domain of attraction of an asymptotically stable equilibrium and for estimating trajectory bounds for such networks. Furthermore, we showed that our stability results are applicable to systems under structural perturbations (e.g., as experienced in neural networks in adaptive learning schemes). In arriving at the above results, we viewed neural networks as an interconnection of many single neurons, and we phrased our results in terms of the qualitative proper- ties of the free single neurons and in terms of the network interconnecting structure. This viewpoint is particularly well suited for the study of hierarchical structures which naturally lend themselves to implementations16 in VLSI. Furthermore, this type of ap- proach makes it possible to circumvent difficulties which usually arise in the analysis and synthesis of complex high dimensional systems. REFERENCES [1] For a review, see, Neural Networks for Computing, J. S. Denker, Editor, American Institute of Physics Conference Proceedings 151, Snowbird, Utah, 1986. [2] J. J. Hopfield and D. W. Tank, Science 233, 625 (1986). [3] J. J. Hopfield, Proc. Natl. Acad. Sci. U.S.A. 79,2554 (1982), and ibid. 81,3088 (1984). [4] G. E. Hinton and J. A. Anderson, Editors, Parallel Models of Associative Memory, Erlbaum, 1981. [5] T. Kohonen, Self-Organization and Associative Memory, Springer-Verlag, 1984. [6] A. N. Michel and R. K. Miller, Qualitative Analysis of Large Scale Dynamical Systems, Academic Press, 1977. [7] R. K. Miller and A. N. Michel, Ordinary Differential Equations, Academic Press, 1982. [8] I. W. Sandberg, Bell System Tech. J. 48, 35 (1969). [9] A. N. Michel, IEEE Trans. on Automatic Control 28, 639 (1983). [10] A. N. Michel, J. A. Farrell, and W. Porod, submitted for publication. [11] J.-H. Li, A. N. Michel, and W. Porod, IEEE Trans. Cire. and Syst., in press. [12] G. A. Carpenter, M. A. Cohen, and S. Grossberg, Science 235, 1226 (1987). [13] M. A. Pai, Power System Stability, Amsterdam, North Holland, 1981. [14] A. N. Michel, N. R. Sarabudla, and R. K. Miller, Circuits, Systems and Signal Processing 1, 171 (1982). [15] Lj. T. Grujic, A. A. Martynyuk and M. Ribbens-Pavella, Stability of Large-Scale Systems Under Structural and Singular Perturbations, Nauka Dumka, Kiev, 1984. [16] D. K. Ferry and W. Porod, Superlattices and Microstructures 2, 41 (1986).","[-0.035062581300735474, -0.13334514200687408, -0.019062060862779617, -0.041799843311309814, -0.05084720626473427, 0.024102874100208282, 0.00333057576790452, 0.03300287574529648, 0.05531761795282364, -0.06023072823882103, 0.0020577467512339354, 0.03850078582763672, 0.02308131940662861, -0.009493863210082054, -0.14115482568740845, 0.08133213967084885, -0.04997868090867996, 0.08079075068235397, -0.1506681740283966, -0.008743051439523697, -0.007501807529479265, 0.0011744023067876697, -0.05724797770380974, 0.026927247643470764, -0.03357977047562599, 0.026689082384109497, -0.07171256095170975, 0.050965119153261185, -0.029543958604335785, -0.00416444381698966, 0.058303046971559525, -0.0747232586145401, -0.046200331300497055, 0.05168831720948219, -0.04645558446645737, 0.029758421704173088, -0.027771029621362686, -0.009385161101818085, 0.01561734825372696, 0.030064702033996582, 0.04119741916656494, -0.01020536944270134, 0.008940190076828003, -0.0036578825674951077, 0.10338152199983597, 0.045523788779973984, -0.012645469978451729, -0.005170559510588646, -0.0144170131534338, -0.04941098392009735, -0.09015397727489471, 0.020949235185980797, 0.018521195277571678, 0.04008876532316208, 0.01855265162885189, -0.018040483817458153, -0.009428445249795914, 0.01828640140593052, -0.11403729766607285, 0.004869882948696613, 0.06677180528640747, -0.02281060814857483, 0.04536512494087219, 0.013464978896081448, 0.06398352235555649, 0.07414565980434418, -0.009290427900850773, 0.02557135745882988, 0.061644017696380615, 0.0008880147361196578, 0.025999557226896286, -0.0035303065087646246, -0.0514548122882843, -0.015162784606218338, 0.014343268238008022, 0.007215442135930061, 0.08023519814014435, 0.07336203008890152, 0.007431593257933855, -0.030419787392020226, 0.04893414303660393, 0.002575250342488289, -0.04076977074146271, -0.03610631823539734, 0.003960244357585907, -0.025629296898841858, 0.00461592897772789, 0.028790660202503204, 0.05488565191626549, -0.006651498842984438, -0.052815623581409454, 0.0852588564157486, 0.020762016996741295, -0.05462539941072464, 0.10856794565916061, -0.0345880463719368, 0.05562940612435341, -0.03504946455359459, 0.01791016012430191, 0.05441778898239136, -0.005686098709702492, -0.01561466883867979, 0.06901049613952637, 0.01580803468823433, 0.07387863099575043, 0.02304595522582531, 0.07265768945217133, 0.04984881728887558, -0.0009923850884661078, -0.0628521591424942, -0.02362361177802086, -0.010245244950056076, -0.0063858344219625, 0.12326649576425552, 0.012366090901196003, -0.05016881972551346, 0.04640810564160347, -0.05114084482192993, 0.07947015762329102, 0.02354496531188488, -0.006153790280222893, -0.002248498611152172, -0.05158482491970062, -0.07025496661663055, -0.037588298320770264, -0.01834891550242901, -0.0559534877538681, 1.7541720363038604e-33, -0.06719779968261719, -0.018394239246845245, 0.049129992723464966, -0.00489052152261138, 0.02201980911195278, -0.00623322743922472, 0.018323754891753197, -0.004324865993112326, -0.02608690969645977, 0.019833695143461227, -0.10654319077730179, 0.01991334930062294, 0.0005063683493062854, 0.07340721040964127, 0.042006004601716995, -0.05443757399916649, 0.057395048439502716, -0.09958053380250931, 0.07703763246536255, -0.08454561233520508, 0.11155248433351517, -0.08596466481685638, -0.06305182725191116, 0.016426291316747665, 0.0036606695502996445, 0.016475625336170197, 0.00452015595510602, 0.044698648154735565, -0.06827986985445023, 0.015648221597075462, -0.005785039626061916, 0.045935049653053284, -0.06253719329833984, -0.053056828677654266, 0.044255610555410385, -0.02112758345901966, 0.013014398515224457, 0.03953584283590317, 0.03836240991950035, -0.010118573904037476, 0.0038190188352018595, -0.013284334912896156, 0.04498407617211342, -0.0030778225045651197, -0.02854030579328537, -0.07093808799982071, 0.0011455127969384193, 0.07067202031612396, -0.06119759753346443, -0.11625482141971588, -0.044457994401454926, 0.014226272702217102, 0.021879471838474274, -0.09437956660985947, 0.11237645149230957, -0.006922543048858643, -0.010623552836477757, 0.05162516236305237, -0.042708467692136765, 0.0871729850769043, -0.08457672595977783, 0.033044248819351196, 0.004892908968031406, -0.014266817830502987, 0.07486598193645477, 0.08034487813711166, -0.08487334102392197, 0.010488543659448624, 0.0854666531085968, -0.054872915148735046, 0.04401569440960884, 0.048416804522275925, -0.0012453763047233224, -0.06328900158405304, -0.024689462035894394, -0.0015160874463617802, -0.06025940924882889, -0.06483002007007599, -0.10709577798843384, 0.08934342116117477, -0.06927032768726349, -0.05490527302026749, -0.07864931225776672, 0.07386672496795654, -0.024527287110686302, -0.04422341287136078, 0.07312807440757751, -0.0012551973341032863, -0.034924253821372986, 0.004969866015017033, -0.10291998088359833, 0.010256348177790642, 0.06744096428155899, -0.037761032581329346, -0.007723388262093067, -3.7542942249982065e-33, -0.03003864921629429, -0.035558927804231644, -0.06525373458862305, 0.06072129309177399, -0.01440419815480709, 0.0709814727306366, -0.050806496292352676, 0.017864730209112167, -0.0552610382437706, 0.014621704816818237, 0.01911344937980175, 0.07335696369409561, 0.01015559770166874, -0.03443618491292, 0.08025544881820679, 0.0592581108212471, -0.019027888774871826, -0.014992826618254185, 0.12975414097309113, -0.030590683221817017, -0.0391787551343441, 0.07896605134010315, -0.07202062755823135, -0.005455663427710533, 0.029199931770563126, 7.909206760814413e-05, -0.06385066360235214, -0.020798608660697937, -0.014888841658830643, 0.07778307050466537, -0.023218359798192978, -0.06375015527009964, -0.0324542410671711, 0.08069588989019394, 0.0475015714764595, 0.0518299899995327, 0.018632985651493073, -0.07919743657112122, 0.014700882136821747, -0.02918674796819687, 0.02429415099322796, -0.043594710528850555, -0.056811753660440445, 0.0020405659452080727, 0.10418606549501419, -0.037019744515419006, -0.045022159814834595, 0.06361053138971329, -0.10079209506511688, 0.09020013362169266, 0.026348864659667015, -0.015638582408428192, -0.06811165064573288, -0.0392628088593483, 0.0098132798448205, 0.04449831694364548, 0.04893038049340248, -0.02603902481496334, 0.03995057940483093, -0.07106075435876846, -0.04366294667124748, -0.1716620773077011, 0.0005164880421943963, 0.016980934888124466, 0.01590091735124588, -0.03910140320658684, -0.050856687128543854, 0.019754696637392044, 0.07251188904047012, -0.02401735819876194, 0.002844808856025338, 0.0022824062034487724, 0.06859160214662552, 0.03714773431420326, -0.0225517600774765, -0.033696915954351425, -0.029528528451919556, 0.033620234578847885, 0.04487711191177368, 0.009126235730946064, -0.012904549017548561, -0.008009236305952072, 0.03315356373786926, -0.02020304463803768, 0.06129346787929535, -0.011064757592976093, 0.011193355545401573, 0.05987151712179184, 0.053509194403886795, -0.035063136368989944, 0.09236367046833038, 0.07200168073177338, -0.049240075051784515, -0.03143225982785225, -0.04277023300528526, -4.818813792439869e-08, 0.031434036791324615, 0.042333804070949554, 0.054381635040044785, 0.02994556911289692, 0.05875413119792938, 0.0006467254715971649, 0.10387320816516876, -0.04468199238181114, 0.006893215700984001, 0.04308066889643669, 0.05230065807700157, -0.01953762210905552, -0.0036368980072438717, -0.046186838299036026, -0.010703960433602333, 0.035449136048555374, -0.028607778251171112, -0.02711365930736065, 0.03219672664999962, 0.017877645790576935, 0.009035867638885975, -0.019639985635876656, -0.047120820730924606, 0.05492866411805153, 0.04501751437783241, -0.09379524737596512, -0.08835776895284653, -0.06368859112262726, -0.06543300300836563, 0.03770386055111885, 0.020162483677268028, 0.04823789745569229, 0.024426111951470375, 0.061789173632860184, 0.0071919159963727, 0.02252320945262909, -0.023545226082205772, 0.01780698634684086, -0.05656195059418678, -0.03351761773228645, -0.02557070180773735, 0.006062759086489677, -0.020197249948978424, -0.02234635315835476, 0.06653428077697754, -0.04622046276926994, 0.05101082846522331, -0.026799390092492104, 0.022163229063153267, 0.03681184723973274, -0.0643458440899849, 0.09684588015079498, 0.01922011747956276, 0.04890289902687073, 0.008811206556856632, -0.03831019625067711, 0.008938133716583252, -0.07573352009057999, 0.04681854322552681, 0.037487272173166275, -0.06854651868343353, 0.05054346099495888, -0.07500103116035461, -0.0760553777217865]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\SimulationandMeasurementoftheElectricFieldsGeneratedbyWeaklyElectricFish.pdf,NLP,"IMPLICATIONS OF RECURSIVE DISTRIBUTED REPRESENTATIONS Jordan B. Pollack Laboratory for A I Research Ohio State University Columbus, OH -'3210 ABSTRACT I will describe my recent results on the automatic development of fixed- width recursive distributed representations of variable-sized hierarchal data structures. One implication of this wolk is that certain types of AI-style data-structures can now be represented in fixed-width analog vectors. Simple inferences can be perfonned using the type of pattern associations that neural networks excel at Another implication arises from noting that these representations become self-similar in the limit Once this door to chaos is opened. many interesting new questions about the representational basis of intelligence emerge, and can (and will) be discussed. INTRODUCTION A major problem for any cognitive system is the capacity for, and the induction of the potentially infinite structures implicated in faculties such as human language and memory. Classical cognitive architectures handle this problem through finite but recursive sets of rules, such as fonnal grammars (Chomsky, 1957). Connectionist architectures, while yielding intriguing insights into fault-tolerance and machine leaming, have, thus far, not handled such productive systems in an adequate fashion. So, it is not surprising that one of the main attacks on connectionism, especially on its application to language processing models, has been on the adequacy of such systems to deal with apparently rule-based behaviors (Pinker & Prince, 1988) and systematicity (Fodor & Pylyshyn, 1988). I had earlier discussed precisely these challenges for connectionism, calling them the generative capacity problem for language, and the representational adequacy problem for data structures (Pollack, 1987b). These problems are actually intimately related, as the capacity to recognize or generate novel language relies on the ability to represent the underlying concept. Recently, I have developed an approach to the representation problem, at least for recur- sive structures like sequences and trees. Recursive auto-associative memory (RAAM) (Pollack, 1988a). automatically develops recursive distributed representations of finite training sets of such structures, using Back-Propagation (Rumelhart et al., 1986). These representations appear to occupy a novel position in the space of both classical and con- nectionist symbolic representations. A fixed-width representation of variable-sized symbolic trees leads immediately to the implication that simple fonns of neural-netwolk associative memories may be able to perfonn inferences of a type that are thought to require complex machinery such as vari- able binding and unification. But when we take seriously the infinite part of the representational adequacy problem, we are lead into a strange intellectual area, to which the second part of this paper is addressed. 527 528 Pollack BACKGROUND RECURSIVE AUTO-ASSOCIATIVE MEMORY A RAAM is composed of two mechanisms: a compressor, and a reconstructor, which are simultaneously trained. The job of the compressor is to encode a small set of fixed-width patterns into a single pattern of the same width. This compression can be recursively applied, from the bottom up, to a fixed-valence tree with distinguished labeled terminals (leaves), resulting in a fixed-width pattern representing the entire structure. The job of the reconstructor is to accurately decode this pattern into its parts, and then to further decode the parts as necessary, until the tenninal patterns are found, resulting in a recon- struction of the original tree. For binary trees with k-bit binary patterns as the leaves, the compressor could be a single-layer feedforward network with 2k inputs and k outputs, along with additional con- trol machinery. The reconstructor could be a single-layer feedforward-network with k inputs and 2k outputs, along with a mechanism for testing whether a pattern is a tenninal. We simultaneously train these two networks in an auto-associative framework as follows. Consider the tree, «0 (A N»(Y (P (0 N»), as one member of a training set of such trees, where the lexical categories are pre-encoded as k-bit vectors. If the 2k-k-2k network is successfully trained (defined below) with the following patterns (among other such pat- terns in the training environment), the resultant compressor and reconstructor can reliably fonn representations for these binary trees. input pattern hidden pattern output pattern A+N ~ RAN(t) ~ A,+N, O+RAN(t) ~ RDAN(t) ~ O~RAN(t)' D+N ~ RDN(t) ~ O,+N, P+RDN(t) ~ RpDN(t) ~ P,+RDN(t), Y+RpDN(t) ~ RVPDN(t) ~ Y,+RpDN(t), RDAN(t)+RvPDN(t) ~ RDANVPDN<t) ~ RDAN(t)I+RvPDN(t), The (initially random) values of the hidden units, Rj(t), are part of the training environ- ment, so it (and the representations) evolve along with the weights. I Because the training regime involves multiple compressions, but only single reconstruc- tions, we rely on an induction that the reconstructor works. If a reconstructed pattern, say RpDN', is sufficiently close to the original pattern, then its parts can be reconstructed as well. AN EXPERIMENT The tree considered above was one member of the first experiment done on RAAM's. I used a simple context-free parser to parse a set of lexical-category sequences into a set of bracketed binary trees: (0 (A (A (A N»» «0 N)(P (0 N») (Y (0 N» (P (0 (A N») «0 N) Y) 1 This ~moving target"" strategy is also used by (Elman, 1988) and (Dyer et aI., 1988). Implications of Recursive Distributed Representations 529 «D N) (V (D (A N»») «D (A N» (V (P (D N»» Each terminal pattern (D A N V & P) was represented as a l-bit-in-5 code padded with 5 zeros. A 20-10-20 RAAM devised the representations shown in figure I. pp p (P (0 N» (P (0 (A N») oOOa· ,··00 DODO' .•. - 0 0000' .•. ·0 o· a - .• . 00· 0·0· . • ·00 D · -Do ·0 ' 000 · ·oD·o·o·O · ·0· ·o·oao · -0' ·0 · 000 (A N) • (A (A N» .. 000· . D a . A A AN' DODO· • • 0 - «0 N) V) 00·00 D 000· «0 N)(V (0 (A N»» o· D • • • • 0 D • «0 (A N» (V (P (0 N»» . o· · . 0 . 00 . Figure I. Representations of all the binary trees in the training set. devised by a 20-10-20 RAAM. manually clustered by phrase-type. The squares represent values between 0 and 1 by area. I labeled each tree and its representation by the phrase type in the grammar, and sorted them by type. The RAAM, without baving any intrinsic concepts of phrase-type, has clearly developed a representation with similarity between members of the same type. For example, the third feature seems to be clearly distinguishing sentences from non- sentences, the fifth feature seems to be involved in separating adjective phrases from oth- ers, while the tenth feature appears to distinguish prepositional and noun phrases from others.2 At the same time, the representation must be keeping enough information about the sub- trees in order to allow the reconstructor to accurately recover the original structure. So, knowledge about structural regularity flows into the wt:ights while constraints about con- text similarity guide the development of the representations. RECURSIVE DISTRIBUTED REPRESENTATIONS These vectors are a very new kind of representation, a recursive, distributed represen- tation, hinted at by Hinton's (1988) notion of a reduced description. They combine aspects of several disparate representations. Like feature-vectors, they are fixed-width, similarity-based, and their content is easily accessible. Like symbols, they combine only in syntactically well-formed ways. Like symbol-structures, they have con- stituency and compositionality. And, like pointers. they refer to larger symbol structures 2 In fact, by these metrics, the test case «D N)(P (D N))) should really be classified as a sentence; since it was not used in any other construction, there was no reason for the RAAM to believe otherwise. 530 Pollack which can be efficiently retrieved. But. unlike feature-vectors. they compose. Unlike symbols. they can be compared. Unlike symbol structures. they are fixed in size. And. unlike pointers. they have content. Recursive distributed representations could. potentially. lead to a reintegration of syntax and semantics at a very low level3• Rather than having meaning-free symbols which syn- tactically combine. and meanings which are recursively ascribed. we could functionally compose symbols which bear their own meanings. IMPLICATIONS One of the reasons for the historical split between symbolic AI and fields such as pattern recognition or neural networks is that the structured representations AI requires do not easily commingle with the representations offered by n-dimensional vectors. Since recursive distributed representations form a bridge from structured representations to n-dimensional vectors. they will allow high-level AI tasks to be accomplished with neural networks. ASSOCIATIVE INFERENCE There are many kinds of inferences which seem to be very easy for humans to perform. In fact, we must perform incredibly long chains of inferences in the act of understanding natural language (Birnbaum. 1986). And yet, when we consider performing those inferences using standard techniques which involve variable binding and unification, the costs seem prohibitive. For humans. how- ever. these inferences seem to cost no more than simple associative priming (Meyer & Schvaneveldt. 1971). Since RAAMS can devise representations of trees as analog patterns which can actually be associated, they may lead to very fast neuro-Iogical inference engines. For example. in a larger experiment. which was reported in (Pollack. 1988b). a 48-16-48 RAAM developed representations for a set of ternary trees. such as (THOUGHT PAT (KNEW JOHN (LOVED MARY JOHN») which corresponded to a set of sentences with complex constituent structure. This RAAM was able to represent. as points within a 16-dimensional hypercube. all cases of (LOVED X Y) where X and Y were chosen from the set {JOHN, MARY. PAT. MAN}. A simple test of whether or not associative inference were possible. then, would be to build a ""symmetric love"" network, which would perform the simple inference: ""If (LOVED X Y) then (LOVED Y X)"". A netwoIk with 16 input and output units and 8 hidden units was successfully trained on 12 of the 16 possible associations. and worked perfectly on the remaining 4. (Note that it accomplished this task without any explicit machinery for matching and moving X and Y.) One might think. that in order to chain simple inferences like this one we will need many hidden layers. But there has recently been some coincidental work showing that feed- 3 The wrong distinction is the inverse of the undifferentiated concept problem in science, such as the fusing of the notions of heat and temperature in the 17th century (Wiser & Carey. 1983). For example. a company which manufactured workstations based on a hardware distinction between characters and graphics had deep trouble when trying to build a modem window system ... Implications of Recursive Distributed Representations 531 forward networks with two layers of hidden units can compute arbitrary mappings (Lapedes & Farber. 1988a; Lippman. 1987). Therefore, we can assume that the sequen- tial application of associative-style inferences can be speeded up, at least by retraining. to a simple 3-cycle process. OPENING THE DOOR TO CHAOS The Capacity of RAAM's As discussed in the introduction. the question of infinite generative capacity is central. 10 the domain of RAAM's the question becomes: Given a finite set of trees to represent. how can the system then represent an infinite number of related trees. For the syntactic-tree experiment reported above. the 20-10-20 RAAM was ooly able to represent 32 new trees. The 48-16-48 RAAM was able to represent many more than it was trained on. but not yet an infinite number in the linguistics sense. I do not yet have any closed analytical forms for the capacity of a recursive auto- associative memory. Given that is is not really a file-cabinet or content-addressable memory, but a memory for a gestalt of rules for recursive pattern compression and recon- struction. capacity results such as those of (Willshaw. 1981) and (Hopfield, 1982) do not directly apply. Binary patterns are not being stored. so one cannot simply count how many. I have considered. however. the capacity of such a memory in the limit, where the actual functions and analog representations are not bounded by single linear transformations and sigmoids or by 32-bit floating point resolution. Figure 2. A plot of the bit-interspersal function. The x and y axis represent the left and right subtrees. and the height represents the output of the function. Consider just a 2-1-2 recursive auto-associator. It is really a reconstructible mapping from points in the unit square to points on the unit line. 10 order to work. the function should define a parametric I-dimensional curve in 2-space. perhaps a set of connected splines.4 As more and more data points need to be encoded. this parametric curve will become more convoluted to cover them . In the limit, it will no longer be a I-dimensional curve. but a space-filling curve with a fractional dimension. 4 (Saund, 1987) originally made the connection between auto-association and dimensionality reduction. If such 532 Pollack One possible functional basis for this ultimate 2-1-2 recursive auto-associator is ""bit- interspersal,"" where the compression function would return a number, between 0 and 1, by interleaving the bits of the binary-fractional representations of the left and right sub- trees. Figure 2 depicts this function, not as a space-filling curve, but as a surface, where no two points project to the same height. The surface is a 3-dimensional variant of a recognizable instance of Cantor dust called the devil's staircase. Thus, it is my working hypothesis that alternative activation functions (i.e. other than the usual sigmoidal or threshold), based on fractal or chaotic mathematics, is the critical missing link between neural networlcs and infinite capacity systems. Between AI and Chaos The remainder of this paper is what is behind the door; the result of simultaneous con- sideration of the fields of AI, Neural Networks, Fractals, and Olaos.s It is, in essence, a proposal on where (I am planning) to look for fruitful interplay between these fields, and what some interdisciplinary problems are which could be solved in this context. There has already been some intrusion of interest in chaos in the physics-based study of neural networlcs as dynamical systems. For example both (Hubennan & Hogg, 1987) and (Kurten, 1987) show how phase-transitions occur in particular neural-like systems, and (Lapedes & Farber, 1988b) demonstrate how a network trained to predict a simple iterated function would follow that function's bifurcations into chaos. However, these efforts are either noticing chaos, or working with it as a domain. At the other end of the spectrum are those relying on chaos to explain such things as the emer- gence of consciousness, or free will (Braitenberg, 1984, p. 65). In between these extremes lies some very hard problems recognized by AI which, I believe, could benefit from a new viewpoint. Self-Similarity and the Symbol-Grounding Problem The bifurcation between structure and form which leads to the near universality of discrete symbolic structures with ascribed meanings has lead to a yawning gap between cognitive and perceptual subareas of AI. This gulf can be seen between such fields as speech recognition and language comprehension, early versus late vision, and robotics versus planning. The low-level tasks require numeric, sensory re~resentations, while the high-level ones require compo- sitional symbolic representations. The idea of infinitely regressing symbolic representations which bottom-out at perception has been an unimplementable folk idea (""Turtles all the way down"") in AI for quite some time. The reason for its lack of luster is that the amount of information in such a structure is considered combinatorially explosive. Unless, of course, one considers self-similarity to be an information-limiting construction. a complete 2-1-2 RAAM could be found. it would give a unique number to every binary tree such that the number of a tree would be a invertible function of the numbers of its two subtrees . .5 Talking about 4 disciples is both difficult, and dangerous. considering the current size of the chasm. and the mutual hostilities: AI thinks NN is just a spectre. NN thinks AI is dead, F thinks it subsumes C, and C thinks F is its just showbiz. 6 It is no surprise then. that neural networks arc much more successful at the former tasks. Implications of Recursive Distributed Representations 533 While working on a ~w activation function for RAAMS which would magically have this property, I have started building modular systems of RAAMs, following Ballard's (1987) work on non-recursive auto-associators. When viewing a RAAM as a constrained system, one can see that the terminal patterns are overconstrained and the highest-level non-terminal patterns are unconstrained. Only those non-terminals which are further compressed have a reasonable similarity con- straint. One could imagine a cascade of RAAMs, where the highest non-terminal patterns of a low-level RAAM (say, for encodings of letters) are the terminal patterns for a middle-level RAAM (say, for words), whose non-terminal patterns are the terminals for a higher-level RAAM (say, for sentences). If all the representations were the same width, then there must be natural similarities between the structures at different conceptual scales. Induction Inference and Strange Automata The problem of inductive inference1, of developing a machine which can learn to recog- nize or generate a language is a pretty hard problem, even for regular languages. In the process of extending my work on a recurrent high-order neural network called sequential cascaded nets (Pollack, 1987a), something strange occurred. It is always possible to completely map out any unknown finite-state machine by provid- ing each known state with every input token, and keeping track of the states. TIris is, in fact, what defines such a machine as finite. Since a recurrent network is a dynamical system, rather than an automaton, one must choose a fuzz-factor for comparing real numbers. For a particular network trained on a context-free grammar, I was unable to map it out. Each time I reduced the fuzz-factor, the machine doubled in size, much like Mandelbrot's coastline (Mandelbrot, 1982) TIris suggests a bidirectional analogy between finite state automata and dynamical sys- tems of the neural network sort8. An automaton has an initial state, a set of states, a lexi- con, and and a function which produces a new state given an old state and input token. A subset of states are distinguished as accepting states. A dynamical system has an initial state, and an equation which defines its evolution over time, perhaps in response to environment. Such dynamical systems have elements known as attractor states, to which the state of the system usually evolves. Two such varieties, limit points and limit cycles, correspond directly to similar elements in finite-state automata, states with loops back to themselves, and short boring cycles of states (such as the familiar ""Please Login. Enter Password. Bad Password. Please Login ..... ). But there is an element in non-linear dynamical systems which does not have a correlate in formal automata theory, which is the notion of a chaotic, or strange, attractor, fiISt noticed in work on weather prediction (Lorenz, 1963). A chaotic attractor does not repeat. The implications for inductive inference is that while, formally, push-down automata and Turing machines are necessary for recognizing harder classes of languages, such as context-free or context-sensitive, respectively, the idiosyncratic state-table and external memory of such devices make them impossible to induce. On the other hand, chaotic dynamical systems look much like automata, and should be about as hard to induce. The 7 For a good survey see (Angluin & Smith, 1983). J. Feldman recently posed this as a ""challenge"" problem for neural networks (c.f. Servan-Scrieber, Cleermans, & McClelland (this volume». 8 Wolfram (1984) has, of course, made the analogy between dynamical systems and cellular automata. 534 Pollack infinite memory is internal to the state vector, and the finite-state-control is built into a more regular, but non-linear, function. Fractal Energy Landscapes and Natural Kinds Hopfield (1982) described an associative memory in which each of a finite set of binary vectors to be stored would define a local minima in some energy landscape. The Boltzmann Machine (Ackley et al., 1985) uses a similar physical analogy along with simulated annealing to seek the global minimum in such landscapes as well. Pineda (1987) has a continuous version of such a memory, where the attract or states are analog vectors. One can think of these energy minimization process as a ball rolling down hills. Given a smooth landscape, that ball will roll into a local minima. On the other hand, if the landscape were constructed by recursive similarity, or by a midpoint displacement tech- nique, such as those used in figures of fractal mountains, there will be an infinite number of local minima, which will be detected based on the size of the ball. N aillon and Theeten's report (this volume), in which an exponential number of attractors are used, is along the proposed line. The idea of high-dimensional feature vectors has a long history in psychological studies of memory and representation, and is known to be inadequate from that perspective as well as from the representational requirements of AI. But AI has no good empirical can- didates for a theory of mental representation either. Such theories generally break down when dealing with novel instances of Natural Kinds, such as birds, chairs, and games. A robot with necessary and sufficient conditions, logi- cal rules, or circumscribed regions in feature space cannot deal with walking into a room, recognizing and sitting on a hand-shaped chair. If the chairs we know fonn the large-scale local minima of an associative memory, then perhaps the chairs we don't know can also be found as local minima in the same space, albeit on a smaller scale. Of course, all the chairs we know are only smaller-scale minima in our memory for furniture. Fractal Compression and the Capacity of Memory Consider something like the Mandelbrot set as the basis for a reconstructive memory. Rather than storing all pictures, one merely has to store the ""pointer"" to a picture,9 and, with the help of a simple function and large computer, the picture can be retrieved. Most everyone has seen glossy pictures of the colorful prototype shapes of yeasts and dragons that infinitely appear as the location and scale are changed along the chaotic boundary. The first step in this hypothetical construction is to develop a related set with the addi- tional property that it can be inverted in the following sense: Given a rough sketch of a picture likely to be in the set, return the best ""pointer"" to it 10 The second step, perhaps using nonnal neural-netwOIk technology, is to build an inverti- ble DOn-linear mapping from the prototypes in a application domain (like chess positions, human faces, sentences, schemata, etc .. ) to the largest-scale prototypes in the mathemati- cal memory space. 9 I.e. a point on the complex plane and the window size 10 Related sets might show up with great frequency using iterated systems, like Newton's method or back- propagation. And a more precise notion of inversion, involving both representational tolerance and scale. is required. Implications of Recursive Distributed Representations 535 Taken together, this hypothetical system turns out to be a look-up table for an infinite set of similar representations which incurs no memory cost for its contents. Only the pointers and the reconstruction function need to be stored. Such a basis for reconstructive storage would render meaningless the recent attempts at ""counting the bits"" of human memory (Hillis, 1988; Landauer, 1986). While these two steps together sound quite fantastic, it is closely related to the RAAM idea using a chaotIc activation function. The reconstructor produces contents from pointers, while the compressor retums pointers from contents. And the idea of a unifonn fractal basis for memory is not really too distant from the idea of a unifonn basis for visual images, such as iterated fractal surfaces based on the collage theorem (Barnsley et aI.,1985). A moral could be that impressive demonstrations of compression, such as the bidirec- tional mapping from ideas to language, must be easy when one can discover the underly- ing regularity. CONCLUSION Recursive auto-associatIve memory can develop fixed-width recursive distributed representations for variable-sized data-structures such as symbolic trees. Given such representations, one implication is that complex inferences, which seemed to require complex infonnation handling strategies, can be accomplished with associations. A second implication is that the representations must become self-similar and space- filling in the limit. This implication, of fractal and chaotic structures in mental represen- tations, may lead to a reconsideration of many fundamental decisions in computational cognitive science. Dissonance for cognitive scientists can be induced by comparing the infinite output of a fonnallanguage generator (with anybody's rules), to the boundary areas of the Mandel- brot set with its simple underlying function. Which is vaster? Which more natural? For when one considers the relative success of fractal versus euclidean geometry at com- pactly describing natural objects, such as trees and coastlines, one must wonder at the accuracy of the pervasive description of naturally-occurring mental objects as features or propositions which bottom-out at meaningless tenns. References Ackley, D. H., Hinton, G. E. & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann Machines. Cognitive Science. 9, 147-169. Angluin, D. & Smith, C. H. (1983). Inductive Inference: Theory and Methods. Computing Surveys. 15, 237- 269. Ballard, D. H. (1987). Modular Learning in Neural Networl<;~. In Proceedings of the Sixth Nationd Conference on Artificial Intelligence. Seattle, 279-284. Bamsley, M. F., Ervin, V., Hardin, D. & Lancaster, J. (1985). Solution of an inverse problem for fractals and other sets. Proceedings of the National Academy of Science. 83. Birnbaum, L. (1986). Integrated processing in planning and understanding. Research Report 489, New Haven: Computer Science Dept., Yale Univeristy. Braitenberg, V. (1984). Vehicles: Experiments in synthetic psychology. Cambridge: MIT press. Otomsky, N. (1957). Syntactic structures. The Hague: Mouton and Co .. Dyer, M. G., Rowers, M. & Wang, Y. A. (1988). Weight Matrix = Pattern of Activation: Encoding Semantic Networks as Distributed Representations in DUAL, a PDP architecture. UCLA-Artificial Intelligence-88-5, Los Angeles: Artificial Intelligence Laboratory, UCLA. Elman. J. L. (1988). Finding Structure in Time. Report 8801. San Diego: Center for Research in Language. UCSD. Fodor, J. & Pylyshyn, A. (1988). Connectionism and Cognitive Architecture: A Critical Analysis. Cognition. 28,3-71. Hillis. W. D. (1988). Intelligence as emergent behavior; or. the songs of eden. Daedelus.117, 175-190. Hinton, G. (1988). Representing Part-Whole hierarchies in connectionist networks. In Proceedings of the Tenth Annual Conference of the Cognitive Science SOciety. Montreal, 48-54. 536 Pollack Hopfield, J. J. (1982). Neural Networks and physical systems with emergent collective computational abilities. Procudings of the National Academy of Sciences USA. 79, 2554-2558. Hubennan, B. A. & Hogg, T. (1987). Phase Transitions in Artificial Intelligence Systems. Artificial Intelligence. 33, 155-172. Kurten, K. E. (1987). Phase transitions in quasirandom neural networks. In Institute of Electrical and Electronics Engineers First International Conference on Neural Networks. San Diego, n-197-20. Landauer, T. K. (1986). How much do people remember? Some estimates on the quantity of learned infonnation in long-term memory .. Cognitive Science. 10, 477-494. Lapedes, A. S. & Farber, R. M. (1988). How Neural Nets Work. LAUR-88-418: Los Alamos. Lapedes, A. S. & Farber, R. M. (1988). Nonlinear Signal Processing using Neural Networks: Prediction and system modeling. Biological Cybernetics. To appear. Lippman, R. P. (1987). An introduction to computing with neural networks. Institute of Electrical and Electronics Engineers ASSP Magazine. April, 4-22. Lorenz, E. N. (1963). Detenninistic Nonperiodic flow. Journal of Atmospheric Sciences. 20, 130-141. Mandelbrot, B. (1982). The Fractal Geometry of Nature. San Francisco: Freeman. Meyer, D. E. & Schvaneveldt, R. W. (1971). Facilitation in recognizing pairs of words: Evidence of a dependence between retrieval operations. Journal of Experimental Psychology. 90, 227-234. Pineda, F. J. (1987). Generalization of Back-Propagation to Recurrent Neural Networks. Physical Review Letters. 59, 2229-2232. Pinker, S. & Prince, A. (1988). On Language and Connectionism: Analysis of a parallel distributed processing model of language inquisition .. Cognition. 28, 73-193. Pollack, J. B. (1987). Cascaded Back Propagation on Dynamic Connectionist Networks. In Proceedings oftM Ninth Conference of the Cognitive Science Society. Seattle, 391-404. Pollack, J. B. (1987). On Connectionist Models of Natural Language Processing. Ph.D. Thesis, Urbana: Computer Science Department, University of Illinois. (Available as MCCS-87-IOO, Computing Research Laboratory, Las Cruces, NM) Pollack, J. B. (1988). Recursive Auto-Associative Memory: Devising Compositional Distributed Representations. In Proceedings of the Tenth Annual Conference of the Cognitive Science Society. Montreal, 33-39. Pollack, J. B. (1988). Recursive Auto-Associative Memory: Devising Compositional Distributed Representations. MCCS-88-124, Las Cruces: Computing Research Laboratory, New Mexico State University. Rumelhart, D. E., Hinton, G. & Williams, R. ( 1986). Learning Internal Representations through Error Propagation. In D. E. Rumelhart, J. L. McClelland & the PDP research Group, (Eds.), Parallel Distributed Processing: Experiments in the Microstructure of Cognition, Vol. l. Cambridge: MIT Press. Saund, E. (1987). Dimensionality Reduction and Constraint in Later Vision. In Proceedings of tM Ninth Annual Conference of the Cognitive Science Society. Seattle, 908-915. Willshaw, D. J. (1981). Holography, Associative Memory, and Inductive Generalization. In G. E. Hinton & J. A. Anderson, (Eds.), Parallel models of associative memory. Hillsdale: Lawrence Erlbaum Associates. Wiser, M. & Carey, S. (1983). When heat and temperature were one. In D. Gentner & A. Stevens, (Eds.), Mental Models. Hillsdale: Erlbaum. Wolfram, S. (1984). Universality and Complexity in Cellular Automata. Physica.1OD, 1-35.","[-0.07571983337402344, -0.11302114278078079, -0.01778237894177437, -0.001207894179970026, -0.06125211715698242, 0.05179854482412338, 0.0033435323275625706, 0.0006568844546563923, 0.0407409705221653, -0.0001566677528899163, -0.03126053512096405, 0.01456442754715681, 0.044010911136865616, 0.03741059452295303, -0.04804278910160065, 0.040073566138744354, 0.01460727583616972, 0.0386076457798481, -0.05919179692864418, -0.010545642115175724, 0.00201660068705678, 0.01908051036298275, -0.07779064029455185, 0.0027944922912865877, -0.05812591314315796, 0.06410960853099823, -0.027144189924001694, -0.06577014178037643, 0.004887665156275034, -0.046646974980831146, 0.09341470897197723, -0.004771147854626179, -0.00827028788626194, 0.05171165242791176, 0.02239408902823925, 0.06553331017494202, 0.024778736755251884, 8.672477270010859e-05, -0.01595868356525898, -0.011230303905904293, 0.013577823527157307, 0.0432572178542614, -0.04876674711704254, 0.05808328464627266, 0.06927360594272614, -0.004501338116824627, 0.0324363075196743, -0.023506706580519676, -0.10312658548355103, -0.0355365164577961, -0.10201866924762726, 0.09465533494949341, 0.03878797963261604, 0.09206070750951767, -0.0010003603529185057, -0.011439366266131401, -0.00019058528414461762, 0.05776050686836243, -0.11994866281747818, 0.01719013787806034, -0.02133563905954361, -0.09083748608827591, -0.033939529210329056, -0.02908184565603733, -0.02418307214975357, 0.035160306841135025, 0.011184955947101116, 0.03728076070547104, -0.009855163283646107, -0.03456192836165428, 0.007877995260059834, 0.05017147958278656, -0.05955392122268677, -0.005713611841201782, 0.056695606559515, 0.054501354694366455, 0.058274827897548676, -0.028232606127858162, 0.1169215738773346, -0.05685415863990784, 0.0005029072053730488, 0.05632130801677704, 0.01817762665450573, -0.008322998881340027, 0.07478291541337967, -0.04463876411318779, -0.026874061673879623, 0.05219171196222305, 0.0323750264942646, 0.003168364055454731, 0.032085392624139786, -0.0949644222855568, 0.06653663516044617, 0.018508311361074448, 0.061095383018255234, 0.03652437403798103, 0.040778547525405884, -0.03568515554070473, 0.03488099202513695, 0.07936625182628632, 0.006321863271296024, 0.01991933025419712, 0.0528424046933651, -0.04789859056472778, -0.02798294834792614, -0.045314088463783264, 0.019288910552859306, 0.059526700526475906, -0.008136977441608906, -0.10813605040311813, -0.03399144485592842, -0.02759585715830326, 0.04384181275963783, 0.08735021203756332, 0.008415712043642998, -0.09552208334207535, -0.04674496501684189, 0.008043100126087666, 0.07496199756860733, -0.002198826754465699, 0.04922275245189667, -0.005259666126221418, -0.0528392530977726, 0.0791269913315773, 0.06996341049671173, -0.02168705314397812, -0.11659847944974899, 5.535697506293827e-33, -0.03394075483083725, 0.010384296998381615, 0.036312270909547806, 0.05339536443352699, 0.10957974195480347, -0.042631104588508606, -0.010679955594241619, 0.016110630705952644, -0.0009448929340578616, 0.024520039558410645, -0.02875334396958351, 0.026156924664974213, -0.019123774021863937, 0.10731185227632523, 0.024179207161068916, 0.03670535981655121, 0.0035081347450613976, -0.010406465269625187, -0.06429234892129898, -0.13648849725723267, 0.025673700496554375, 0.01893608830869198, 0.015026689507067204, -0.06885853409767151, 0.021033475175499916, -0.062202636152505875, -0.0029786117374897003, -0.042529959231615067, -0.03001016564667225, 0.022372949868440628, -0.11989177018404007, 0.017205532640218735, -0.01369418203830719, 0.0655633732676506, -0.04894222691655159, -0.023355595767498016, -0.030162563547492027, -0.07564260810613632, -0.00958521943539381, -0.0786682665348053, 0.0667719766497612, 0.02482907474040985, -0.042023420333862305, 0.04415534809231758, -0.059162687510252, -0.058864109218120575, -0.004679250530898571, 0.05407097563147545, -0.05189209431409836, -0.07595895975828171, -0.006546245887875557, 0.04930833727121353, -0.029032768681645393, -0.054860521107912064, 0.08027487993240356, 0.011880481615662575, -0.07880671322345734, 0.06332328170537949, 0.0647769644856453, 0.07772733271121979, -0.004704291000962257, 0.04288133606314659, -0.032452668994665146, 0.04789752885699272, 0.0388808473944664, 0.004937577992677689, -0.04461868107318878, 0.027189990505576134, 0.08754004538059235, 0.02165360003709793, 0.02838502824306488, 0.07515676319599152, -0.10118996351957321, -0.015492754057049751, -0.02350725419819355, -0.04091712459921837, 0.046025700867176056, -0.05847278609871864, -0.09060477465391159, 0.03029153123497963, -0.03298785537481308, -0.02832474559545517, -0.044685330241918564, 0.07679977267980576, -0.061671603471040726, -0.054526686668395996, 0.038750000298023224, -0.0165699552744627, -0.04441319778561592, -0.037633176892995834, 0.0003624902165029198, -0.07223641127347946, 0.09749015420675278, -0.019305339083075523, -0.06903916597366333, -6.72378213629739e-33, -0.12078247219324112, -0.01582256518304348, -0.0967417061328888, -0.018842021003365517, -0.03564176335930824, -0.0167252030223608, -0.027260221540927887, 0.017713826149702072, -0.05132992938160896, -0.08333316445350647, -0.08134965598583221, -0.018516333773732185, 0.017141833901405334, 0.023730691522359848, 0.051998235285282135, -0.029564591124653816, 0.0687158852815628, -0.021840525791049004, 0.06585382670164108, -0.025536172091960907, 0.03801940754055977, 0.07811204344034195, -0.07462792843580246, -0.03186311945319176, 0.06575033068656921, 0.02930072322487831, -0.07753723114728928, 0.06906022131443024, 0.014568250626325607, 0.07496711611747742, -0.02957005426287651, -0.07224840670824051, 0.013869932852685452, 0.029347345232963562, -0.01596948318183422, 0.00043224997352808714, -0.009358642622828484, -0.019215552136301994, -0.05551793426275253, 0.0538523904979229, 0.050587840378284454, -0.02167508378624916, -0.011599699035286903, 0.030262254178524017, 0.04826449602842331, -0.050974518060684204, -0.044431593269109726, 0.04427680745720863, 0.012577449902892113, 0.018186714500188828, 0.04611840844154358, 0.033152516931295395, -0.04468604177236557, -0.03615078702569008, -0.02876203879714012, 0.07430199533700943, 0.01625138521194458, -0.04297523945569992, 0.039133138954639435, 0.006110142916440964, -0.08454606682062149, -0.12146113812923431, -0.007458112668246031, -0.060394324362277985, 0.05909343063831329, -0.05517437309026718, -0.04149734601378441, -0.0024638248141855, 0.04903976246714592, -0.020031411200761795, 0.028276430442929268, 0.00883911456912756, -0.02088567242026329, 0.03643065690994263, 0.024639764800667763, -0.009032860398292542, -0.08306475728750229, -0.026253901422023773, -0.03317992761731148, -0.012733086943626404, -0.09077837318181992, -0.024909384548664093, 0.044512223452329636, 0.020692186430096626, 0.10706393420696259, -0.0298206377774477, 0.06464780122041702, -0.014562301337718964, 0.050276271998882294, 0.004065989516675472, 0.06813275068998337, 0.03038802556693554, -0.02916019596159458, 0.09409278631210327, -0.06262172013521194, -5.847972772698995e-08, -0.009028030559420586, 0.013016861863434315, -0.02000053972005844, 0.07326104491949081, 0.13011229038238525, -0.052085887640714645, 0.06910509616136551, -0.010862225666642189, -0.0633477196097374, 0.030007608234882355, 0.017825458198785782, -0.001645243144594133, 0.02685597538948059, 0.0026788765098899603, 0.041443049907684326, 0.05717421695590019, -0.011156661435961723, -0.034411992877721786, -0.03829327970743179, 0.045535504817962646, 0.1160799041390419, 0.00862760841846466, -0.08924251049757004, 0.05754644051194191, -0.03673408180475235, -0.08802375197410583, -0.04984019696712494, 0.023033201694488525, -0.03047841414809227, 0.03322082757949829, -0.017716173082590103, 0.10011845082044601, 0.030536767095327377, -0.018104035407304764, -0.014488180167973042, 0.014624306932091713, -0.004781730473041534, -0.0002786694385576993, -0.057863999158144, -0.08867226541042328, 0.019469769671559334, 0.01868484914302826, -0.009631304070353508, 0.04936669021844864, 0.11397822946310043, -0.06736447662115097, -0.04300997778773308, -0.03852330148220062, 0.0041924393735826015, -0.004500163719058037, 0.019054559990763664, 0.07943183928728104, 0.014943636953830719, 0.06698288023471832, 0.06668808311223984, -0.0197830181568861, 0.03743264824151993, -0.06777572631835938, -0.05716593936085701, 0.047209952026605606, 0.025538070127367973, 0.08125247806310654, 0.005729287397116423, -0.06693261116743088]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\SkeletonizationATechniqueforTrimmingtheFatfromaNetworkviaRelevanceAssessment.pdf,Deep Learning,"206 APPLICATIONS OF ~RROR BACK-PROPAGATION TO PHONETIC CLASSIFICATION Hong C. Leung & Victor W. Zue Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology Cambridge, MA 02139 ABSTRACT This paper is concerced with the use of error back-propagation in phonetic classification. Our objective is to investigate the ba- sic characteristics of back-propagation, and study how the frame- work of multi-layer perceptrons can be exploited in phonetic recog- nition. We explore issues such as integration of heterogeneous sources of information, conditioll~ that can affect performance of phonetic classification, internal representations, comparisons with traditional pattern classification techniques, comparisons of differ- ent error metrics, and initialization of the network. Our investiga- tion is performed within a set of experiments that attempts to rec- ognize the 16 vowels in American English independent of speaker. Our results are comparable to human performance. Early approaches in phonetic recognition fall into two major extremes: heuristic and algorithmic. Both approaches have their own merits and shortcomings. The heuristic approach has the intuitive appeal that it focuses on the linguistic informa- tion in the speech signal and exploits acoustic-phonetic knowledge. HO'fever, the weak control strategy used for utilizing our knowledge has been grossly inadequate. At the other extreme, the algorithmic approach relies primarily on the powerful con- trol strategy offered by well-formulated pattern recognition techniques. However, relatively little is known about how our speech knowledge accumulated over the past few decades can be incorporated into the well-formulated algorithms. We feel that artificial neural networks (ANN) have some characteristics that can potentially enable them to bridge the gap between these two extremes. On the one hand, our speech knowledge can provide guidance to the structure and design of the network. On the other hand, the self-organizing mechanism of ANN can provide a control strategy for utilizing our knowledge. In this paper, we extend our earlier work on the use of artificial neural networks for phonetic recognition [2]. Specifically, we focus our investigation on the following sets of issues. First, we describe the use of the network to integrate heterogeneous sources of information. We will see how classification performance improves as more Error Back-Propagation to Phonetic Classification 207 information is available. Second, we discuss several important factors that can sub- stantially affect the performance of phonetic classification. Third, we examine the internal representation of the network. Fourth, we compare the network with two traditional classification techniques: K-nearest neighbor and Gaussian classifica- tion. Finally, we discuss our specific implementations of back-propagation that yield improved performance and more efficient learning time. EXPERIMENTS Our investigation is performed within the context of a set of experiments that attempts to recognize the 16 vowels in American English independent of speaker. The vowels are excised from continuous speech and they can be preceded and fol- lowed by any phonemes, thus providing a rich environment to study contextual influence. We assume that the locations of the vowels have been detected. Given a time region, the network determines which one of the 16 vowels was spoken. CORPUS As Table 1 shows, our training set consists of 20,000 vowel tokens, excised from 2,500 continuous sentences spoken by 500 male and female speakers. The test set consists of about 2,000 vowel tokens, excised from 250 sentences spoken by 50 dif- ferent speakers. All the data are extracted from the TIMIT database, which has a wide range of American dialectical variations [1]. The speech signal is represented by spectral vectors obtained from an auditory model [4]. Speaker and energy nor- malization are also performed [5]. Tokens Sentences Speakers (M/F) Training 20,000 2500 500 (350/150) Testing 2,000 250 50 (33/17) Table 1: Corpus extracted from the TIMIT database. NETWORK STRUCTURE The structure of the network we have examined most extensively has 1 hidden layer as shown in Figure 1. It has 16 output units, with one unit for each of the 16 vowels. In order to capture dynamic information, the vowel region is divided into three equal subregions. An average spectrum is then computed in each subregion. These 3 average spectra are then applied to the first 3 sets of input units. Additional sources of information, such as duration and local phonetic contexts, can also be made available to the network. While spectral and durational inputs are continuous and numerical, the contextual inputs are discrete and symbolic. 208 Leung and Zue output from auditory model (synchrony spectrogram) -.......... -... -... ~.~j:.I._._ .... ---.......... -... -.. ~ Figure 1: Basic structure of the network. HETEROGENEOUS INFORMATION INTEGRATION In our earlier study, we have examined the integration of the Synchrony En- velopes and the phonetic contexts [2]. The Synchrony Envelopes, an output of the auditory model, have been shown to enhance the formant information. In this study, we add additional sources of information. Figure 2 shows the performance as heterogeneous sources of information are made available to the network. The performance is about 60% when only the Synchrony Envelopes are available. The performance improves to 64% when the Mean Rate Response, a different output of the auditory model which has been shown to enhance the temporal aspects of the speech signal, is also available. We can also see that the performance improves con- sistently to 77% as durational and contextual inputs are provided to the network. This experiment suggests that the network is able to make use of heterogeneous sources of information, which can be numerical and/or symbolic. Error Back-Propagation to Phonetic Classification 209 One may ask how well human listeners can recognize the vowels. Experiments have been performed to study how well human listeners agree with each other when they can only listen to sequences of 3 phonemes, i.e. the phoneme before the vowel, the vowel itself, and the phoneme after the vowel [3]. Results indicate that the average agreement among the listeners on the identities of the vowels is between 65% and 70%. 80· 70· 60- 5lr Synchrony Add Add Envelopes Mean Rate Duration Response Sources of Information : Add Phonetic Context Figure 2: Integration of heterogeneous sources of information. PERFORMANCE RESULTS We have seen that one of the important factors for the network performance is the amount of information available to the network. To gain additional insights about how the network performs under different conditions, several experiments were conducted using different databases. In these and the subsequent experiments we describe in this paper, only the Synchrony Envelopes are available to the network. Table 2 shows the performance results for several recognition tasks. In each of these tasks, the network is trained and tested with independent sets of speech data. The first task recognizes vowels spoken by one speaker and excised from the fbf-vowel-ftf environment, spoken in isolation. This recognition task is relatively straightforward, resulting in perfect performance. In the second experiment, vowel tokens are extracted from the same phonetic context, but spoken by 17 male and female speakers. Due to inter-speaker variability, the accuracy degrades to 86%. The third task recognizes vowels spoken by one speaker and excised from an un- restricted context, spoken continuously. We can see that the accuracy decreases further to 70%. Finally, data from the TIM IT database are used, spoken by multi- ple speakers. The accuracy drops to 60%. These results indicate that a substantial difference in performance can be expected under different conditions, depending on whether the task is speaker-independent, what is the restriction on the phonetic 210 Leung and Zue Speakers(M/F) Context Training Percent Remark Tokens Correct 1(1/0) b - t 64 100 isolated 17(8/9) b - t 256 86 isolated 1(1/0) * * 3,000 70 continuous 500(350/150) * * 20,000 60 continuous - Table 2: Performance for different tasks, using only the synchrony spectral infor- mation. ""*,, stands for any phonetic contexts. contexts, whether the speech material is spoken continuously, and how much data are used to train the network. INTERNAL REPRESENTATION To understand how the network makes use of the input information, we exam- ined the connection weights of the network. A vector is formed by extracting the connections from all the hidden units to one output unit as shown in Figure 3a. The same process is repeated for all output units to obtain a total of 16 vectors. The correlations among these vectors are then examined by measuring the inner prod- ucts or the angles between them. Figure 3b shows the distribution of the angles after the network is trained, as a function of the number of hidden units. The circles represent the mean of the distribution and the vertical bars stand for one standard deviation away from the mean. As the number of hidden units increases, the distri- bution becomes more and more concentrated and the vectors become increasingly orthogonal to each other. The correlations of the connection weights before training were also examined, as shown in Figure 3c. Comparing parts (b) and (c) of Figure 3, we can see that the distributions before and after training overlap more and more as the number of hidden units increases. With 128 hidden units, the two distributions are actually quite similar. This leads us to suspect that perhaps the connection weights between the hidden and the output layer need not be trained if we have a sufficient number of hidden units. Figure 4a shows the performance of recognizing the 16 vowels using three differ- ent techniques: (i) train all the connections in the network, (ii) fix the connections between the hidden and output layers after random initialization and train only the connections between the input and hidden layers, and (iii) fix the connections between the input and hidden layers and train only the connections between the hidden and output layers. We can see that with enough hidden units, training only the connections between the input and the hidden layers achieves almost the same performance as training all the connections in the network. We can also see that Error Back-Propagation to Phonetic Classification 211 for the same number of hidden units, training only the connections between the input and the hidden layer can achieve higher performance than training only the connections between the hidden and the output layer. Figure 4b compares the three training techniques for 8 vowels, resulting in 8 output units only. We can see similar characteristics in both parts (a) and (b) of Figure 4. 150 - 130 l110 ! 90 I w Ja 70 ~ < 50 30 1 I f Iff I 10 100 Output Layer Hidden Layer Cl"" Y ""j •• £r""Y""J' Input '~',~, "">-<.""-----';,.. ......... :., Layer (a) 150 -;;- 130 l 110 ! 90 w u 70 Cib c < 50 toOO 30 1 10 100 1000 Number of Hidden Units Number of Hidden Units (b) (c) Figure 3: (a) Correlations of the vectors from the hidden to output layers are examined. (b) Distribution of the angles between these vectors after training. (c) Distribution of the angles between these vectors before training. COMPARISONS WITH TRADITIONAL TECHNIQUES One of the appealing characteristics of back-propagation is that it does not as- sume any probability distributions or distance metrics. To gain further insights, we compare with two traditional pattern classification techniques: K-nearest neighbor (KNN) and multi-dimensional Gaussian classifiers. 212 Leung and Zue 70 90 (i) (i) __ ..0 .•••• 0-._. . •.. 60 y .. o ..•. o-.. • ·~D p.... "". •.. 0.···.0 ~ 70 I ~ ~ I ~.c//\ ... 50 / ..p/ ... ... ... 8 40 I /--C) 8 50 , • (iii) 1: i III 1: ~. ~ o """" CD ,....-0/ / u 30 ... CD / \("") CD o 'Qi) Q. Q. 30 20 cI II 10 10 1 10 100 1000 1 10 100 1000 Number of Hidden Units Number of Hidden Units (a) (b) Figure 4: Performance of recognizing (a) 16 vowels, (b) 8 vowels when (i) all the connections in the network are trained, (ii) only the connections between the input and hidden layers are trained, and (iii) only the connections between the hidden and output layers are trained. Figure 5a compares the performance results of the network with those of KNN, for different amounts of training tokens. Again, only the Synchrony Envelopes are made available to the network, resulting in input vectors of 100 dimensions. Each cluster of crosses corresponds to performance results of ten networks, each one randomly initialized differently. Due to different initialization, a fluctuation of 2% to 3% is observed even for the same training size. For comparison, we perform KNN using the Euclidean distance metric. For each training size, we run KNN 6 times, each one with a different K, which is chosen to be proportional to the square root of the number of training tokens, N. For simplicity, Figure 5a shows results for only 3 different values of K: (i) K = Vii, (ii) K = 10Vii, and (iii) K = 1. In this experiment, we have found that the performance is the best when K = ..fFi and is the worst when K = 1. We have also found that up to 20,000 training tokens, the network consistently compares favorably to KNN. It is possible that the network is able to find its own distance metric to achieve better performance. Since the true underlying probability distribution is unknown, we assume multi- dimensional Gaussian distribution in the second experiment. (i) We use the full covariance matrix, which has 100zl00 elements. To avoid problems with singularity, we obtain results only for large number of training tokens. (ii) We use the diagonal covariance matrix which has non-zero elements only along the diagonal. We can see from Figure 5b that the network compares favorably to the Gaussian classifiers. Our results also suggest that the Gaussian assumption is invalid. Error Back-Propagation to Phonetic Classification 213 60 .t. ! . ..! (i) 60 • • ~ ~--....-, (ii) ] 50 I ~~:~) 50 ....... ... / ........... D···· (iii) )..-/ iii j.. ..... ..-:. •. _ ..... 'e! . .... ~ J"""""".' ........ ~ I 40 /""Jr/ 40 (.- ~ Q. 30~----------------------------------~ ~~------------------------------------~ 100 1000 10000 100000 100 1000 10000 100000 Number of Training Tokens Number of Training Tokens (a) (b) Figure 5: (a) Comparison with KNN for different values of K (See text). (b) Comparison with Gaussian classification when using the (i) full covariance matrix, and (ii) diagonal covariance matrix. Each cluster of 10 crosses corresponds to the results of 10 different networks, each one randomly initialized. ERROR METRIC AND INITIALIZATION In order to take into account the classification performance of the network more explicitly, we have introduced a weighted mean square error metric [2]. By modu- lating the mean square error with weighting factors that depend on the classifica- tion performance, we have shown that the rank order statistics can be improved. Like simulated annealing, gradient descent takes relatively big steps when the per- formance is poor, and takes smaller and smaller steps as the performance of the network improves. Results also indicate that it is more likely for a unit output to be initially in the saturation regions of the sigmoid function if the network is randomly initialized. This is not desirable since learning is slow when a unit output is in a saturation region. Let the sigmoid function goes from -1 to 1. If the connection weights between the input and the hidden layers are initialized with zero weights, then all the hidden unit outputs in the network will initially be zero, which in turn results in zero output values for all the output units. In other words, all the units will initially operate at the center of the transition region of the sigmoid function, where learning is the fastest. We call this method center initialization (CI). Parts (a) and (b) of Figure 6 compare the learning speed and performance, respectively, of the 3 different techniques: (i) mean square error (MSE), (ii) weighted mean square error (WMSE), and (iii) center initialization (CI) with WMSE. We can see that both WMSE and CI seem to be effective in improving the learning time and the performance of the network. 214 Leung and Zue (iii) K'"" (ii) (i) 30~--~----------------~ o 10 20 30 40 50 Number of Training Iterations (a) 30~4-------------~----~ 100 1000 10000 100000 Number of Training Tokens (b) Figure 6: Comparisons of the (a) learning characteristics and, (b) performance results, for the 3 different techniques: (i) MSE, (ii) WMSE, and (iii) CI with WMSE. Each point corresponds to the average of 10 different networks, each one initialized randomly. SUMMARY In summary, we have described a set of experiments that were designed to help us get a better understanding of the use of back-propagation in phonetic classifica- tion. Our results are encouraging and we are hopeful that artificial neural networks may provide an effective framework for utilizing our acoustic-phonetic knowledge in speech recognition. References [1] Fisher, W.E., Doddington, G.R., and Goudie-Marshall, K.M., ""The DARPA Speech Recognition Research Database: Specifications and Status,"" Proceed- ings of the DARPA Speech Recognition Workshop Report No. SAIC-86/1546, February, 1986. [2] Leung, H.C., ""Some phonetic recognition experiments using artificial neural nets/' ICASSP-88, 1988. [3] Phillips, M.S., ""Speaker independent classification of vowels and diphthongs in continuous speech,"" Proc. of the 11th International Congress of Phonetic Sciences, Estonia, USSR, 1987. [4] Seneff S., ""A computational model for the peripheral auditory system: appli- cation to speech recognition research,"" Proc. ICASSP, Tokyo, 1986. [5] Seneff S., ""Vowel recognition based on 'line-formants' derived from an auditory- based spect(al representation,"" Proc. of the 11th International Congress of Phonetic Sciences, Estonia, USSR, 1987.","[-0.09434889256954193, -0.1091209203004837, -0.051934417337179184, -0.11712009459733963, -0.08962462097406387, 0.04435911774635315, 0.02153727039694786, -0.041887495666742325, -0.024986786767840385, -0.05826275050640106, -0.03984707221388817, -0.02991156466305256, 0.04093017801642418, 0.004045473877340555, -0.07234205305576324, -0.06662281602621078, 0.004036969970911741, 0.11054840683937073, -0.05316079407930374, -0.10259818285703659, -0.0233312975615263, 0.10448040068149567, -0.026395684108138084, 0.011923878453671932, 0.09062284976243973, 0.008446802385151386, -0.057532452046871185, -0.02141197770833969, 0.005494982935488224, 0.001815813360735774, 0.04916173592209816, 0.06103968620300293, 0.10513720661401749, 0.007853264920413494, -0.09159273654222488, 0.009580072946846485, -0.040701594203710556, 0.020234111696481705, -0.054665707051754, 0.021831553429365158, -0.04284997656941414, 0.04660942405462265, -0.024843130260705948, 0.0717124193906784, 0.015955448150634766, -0.04478501155972481, -0.004770240746438503, 0.01709989830851555, -0.02432294934988022, -0.03814612701535225, -0.020857147872447968, 0.030253378674387932, -0.028201255947351456, 0.06270550191402435, -0.09727917611598969, -0.014228428713977337, 0.039035942405462265, 0.12399868667125702, -0.0454157292842865, 0.018519632518291473, -0.03285151720046997, -0.06584480404853821, -0.004420562647283077, -0.038802824914455414, 0.05518333241343498, 0.062043823301792145, -0.025034306570887566, -0.035780660808086395, 0.051212411373853683, -0.0238044373691082, -0.03225062042474747, 0.037690259516239166, -0.045219313353300095, 0.07529160380363464, 0.003881062613800168, 0.08591735363006592, 0.07841895520687103, 0.03198356553912163, 0.010125931352376938, -0.050778571516275406, 0.08951243758201599, 0.035841066390275955, 0.0028156309854239225, -0.010582645423710346, 0.14097079634666443, -0.01107985619455576, -0.053541868925094604, -0.006360294762998819, -0.030923664569854736, -0.015477861277759075, 0.0049864607863128185, -0.09092476963996887, 0.014909309335052967, -0.04179459065198898, 0.0606246143579483, 0.043101850897073746, -0.043966371566057205, -0.011219867505133152, 0.01239759474992752, 0.05781916528940201, -0.022143447771668434, -0.004999623168259859, 0.010624675080180168, -0.04275310039520264, -0.036675337702035904, -0.0035637507680803537, 0.024494921788573265, -0.006029186770319939, 0.0809277817606926, -0.08550981432199478, -0.001760336454026401, 0.03076395019888878, 0.0038646606262773275, 0.0007044489611871541, 0.054430339485406876, -0.02953127585351467, -0.06412705034017563, 0.018042193725705147, 0.04975404217839241, 0.01883566379547119, -0.09686670452356339, 0.014351597055792809, -0.007849067449569702, -0.0041146655566990376, 0.04563956707715988, -0.02568318136036396, -0.055023107677698135, 1.7653127840137786e-33, -0.02499055489897728, 0.1175113245844841, -0.03214935213327408, 0.019788429141044617, -0.017512889578938484, -0.027122080326080322, -0.062208037823438644, -0.03377246484160423, 0.08892373740673065, -0.04566523805260658, -0.000519212509971112, 0.0733606219291687, -0.02234092727303505, -0.008078041486442089, 0.07167305052280426, 0.026187671348452568, -0.01329811755567789, 0.028787003830075264, -0.035149116069078445, -0.08733604848384857, 0.04604968801140785, 0.01857297122478485, 0.07158636301755905, -0.08464422821998596, 0.010263570584356785, 0.018300490453839302, 0.058341387659311295, -0.1366991400718689, -0.01933670975267887, 0.027352411299943924, -0.027586689218878746, -0.005189834162592888, 0.06020970642566681, -0.005702114664018154, 0.04938894882798195, -0.05784034729003906, 0.05985124036669731, 0.0070162583142519, 0.037099454551935196, -0.028602633625268936, -0.07299982756376266, 0.05115150660276413, 0.005891116801649332, 0.08841807395219803, -0.06617485731840134, -0.06274325400590897, -0.027109503746032715, 0.09020841866731644, 0.05734703317284584, -0.040541406720876694, -0.005698941182345152, -0.00442057428881526, -0.019644754007458687, -0.011503923684358597, 0.061857495456933975, -0.04559483006596565, 0.03784999996423721, 0.027892135083675385, -0.0016800557496026158, 0.0787271112203598, 0.040374867618083954, 0.07731623202562332, 0.009371450170874596, 0.042631641030311584, -1.9638633602880873e-05, -0.05376264825463295, -0.14709872007369995, 0.005213530268520117, 0.049993082880973816, 0.014189440757036209, 0.009803603403270245, 0.021464502438902855, 0.035983506590127945, -0.04055742546916008, 0.005763677414506674, 0.030955694615840912, 0.038156285881996155, -0.03338408097624779, -0.0281989648938179, -0.0236827302724123, -0.09075049310922623, 0.02359965816140175, -0.06042405962944031, -0.05085538700222969, -0.00913573894649744, -0.010704406537115574, 0.03033817559480667, -0.061918873339891434, -0.012481879442930222, 0.021546315401792526, -0.04223722219467163, 0.0004901086795143783, 0.004536386113613844, 0.03097863309085369, -0.019733594730496407, -2.522483592866954e-33, -0.09188304096460342, 0.07479776442050934, -0.01642102375626564, 0.019938308745622635, -0.052331507205963135, -0.03499802201986313, 0.05415482819080353, 0.0445249117910862, -0.017761899158358574, -0.006338523700833321, -0.04856635257601738, -0.012263775803148746, 0.10316749662160873, -0.015798859298229218, 0.04303695634007454, -0.0060203662142157555, 0.007822906598448753, 0.09469636529684067, 0.055576302111148834, 0.08170284330844879, 0.04348939657211304, 0.018088361248373985, -0.14922386407852173, 0.01641802117228508, -0.07147172093391418, 0.02167714387178421, -0.03681566193699837, 0.01528530940413475, 0.018970085307955742, -0.03654749318957329, -0.02018963359296322, -0.025115959346294403, -0.06833438575267792, 0.0026142331771552563, -0.04510169103741646, 0.07903244346380234, 0.021203147247433662, -0.0017244260525330901, -0.030803274363279343, 0.014745648019015789, 0.021703938022255898, 0.05766091123223305, -0.04866597801446915, -0.08716882020235062, 0.033539433032274246, -0.06128313019871712, -0.10981958359479904, 0.07115525752305984, 0.015820391476154327, 0.014313965104520321, 0.13732029497623444, -0.020321199670433998, 0.06160018593072891, -0.006887246388942003, -0.019046301022171974, 0.0018985988572239876, 0.008692878298461437, -0.009646282531321049, -0.023614490404725075, 0.04428903013467789, -0.06064333766698837, -0.08294105529785156, 0.04819194972515106, -0.07537376135587692, -0.0562177412211895, 0.001963643357157707, 0.050593651831150055, -0.004323665518313646, 0.09656840562820435, -0.07201685756444931, 0.0007134577026590705, -0.0035533723421394825, -0.016818055883049965, 0.02444436028599739, -0.02593250945210457, 0.01926627941429615, -0.06180606409907341, -0.11163952201604843, -0.03234592825174332, -0.027640467509627342, -0.11723732948303223, 0.0308669526129961, 0.013859924860298634, -0.02241385541856289, 0.09265907108783722, 0.10174527764320374, 0.05316084995865822, 0.011275850236415863, 0.08717796206474304, -0.01121350098401308, 0.02516580931842327, 0.09464520215988159, 0.06260931491851807, 0.04440298676490784, 0.005719780456274748, -4.8160515575546015e-08, -0.03393259271979332, -0.025829603895545006, 0.04769786819815636, -0.004070194438099861, 0.06140556558966637, -0.117047980427742, -0.015392320230603218, 0.013518810272216797, -0.013531500473618507, -0.06826739013195038, 0.06911585479974747, -0.0448467843234539, -0.03449951112270355, -0.011586942709982395, 0.0437660850584507, 0.10395552217960358, 0.022929450497031212, 0.029862558469176292, -0.036420758813619614, -0.057624902576208115, 0.024564366787672043, 0.050845492631196976, 0.012087698094546795, 0.07054360955953598, -0.025680556893348694, -0.059216003865003586, -0.019157225266098976, 0.02424183487892151, -0.025190550833940506, 0.0148751987144351, 0.0056150746531784534, 0.041563522070646286, 0.0025799174327403307, -0.06680498272180557, 0.007775207981467247, 0.05315345898270607, 0.04128387197852135, 0.006011521443724632, -0.02859962172806263, 0.04889418184757233, 0.005767164286226034, 0.010213448666036129, -0.03853599727153778, 0.08185591548681259, 0.058662451803684235, -0.05962458997964859, -0.038151420652866364, -0.07580628246068954, -0.016649121418595314, -0.03495524451136589, 0.05715057998895645, 0.020257310941815376, -0.0169115848839283, -0.01714290678501129, 0.08074606955051422, -0.018363263458013535, -0.03245151787996292, -0.04456150159239769, 0.015902722254395485, 0.05792907997965813, -0.022705521434545517, 0.10162646323442459, -0.03678180277347565, -0.011590397916734219]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\SongLearninginBirds.pdf,Deep Learning,"LEARNING BY CHOICE OF INTERNAL REPRESENTATIONS Tal Grossman, Ronny Meir and Eytan Domany Department of Electronics, Weizmann Institute of Science Rehovot 76100 Israel ABSTRACT We introduce a learning algorithm for multilayer neural net- works composed of binary linear threshold elements. Whereas ex- isting algorithms reduce the learning process to minimizing a cost function over the weights, our method treats the internal repre- sentations as the fundamental entities to be determined. Once a correct set of internal representations is arrived at, the weights are found by the local aild biologically plausible Perceptron Learning Rule (PLR). We tested our learning algorithm on four problems: adjacency, symmetry, parity and combined symmetry-parity. I. INTRODUCTION Consider a network of binary linear threshold elements i, whose state Si = ±1 is determined according to the rule Si = sign(L WijSj + Oi) . (1) j Here Wij is the (unidirectional) weight assigned to the connection from unit j to i; 0i is a local bias. We focus our attention on feed-forward networks in which N units of the input layer determine the states of H units of a hidden layer; these, in turn, feed one or more output elements. For a typical A vs B classification task such a network needs a single output, with sout = + 1 (or -1) when the input layer is set in a state that belongs to catego~y A (or B) of input space. The basic problem of learning is to find an algorithm, that produces weights which enable the network to perform this task. In the absence of hidden units learning can be accomplished by the PLR [Rosenblatt 1962], which we now briefly Jcscribe. Consider j = 1, ... , N source units and a single target unit i. When the source units are set in anyone of p. = 1, .. M patterns, i.e. Sj = er, we require that the target unit (determined using (1» takes preassigned values er. Learning takes place in the course of a training session. Starting from any arbitrary initial guess for the weights, an input 1/ is presented, resulting in the output taking some value Sr. Now modify every weight according to the rule W·· -+ W·· + ""(1 - SI!CI!)CI!Cl( 11 IJ"" 1 ~I ~I ~J ' (2) 73 74 Grossman, Meir and Domany where TJ > 0 is a parameter (er = 1 is used to modify the bias 0). Another input pattern is presented, and so on, until all inputs draw the correct output. The Perceptron convergence theorem states [Rosenblatt 1962, Minsky and Papert 1969] that the PLR will find a solution (if one exists), in a finite number of steps. However, of the 22N possible partitions of input space only a small subset (less than 2N2 / N!) is linearly separable [Lewis and Coates 1967], and hence soluble by single- layer perceptrolls. To get around this, hidden units are added. Once a single hidden layer (with a large enough number of units) is inserted beween input and output, every classification problem has a solution. But for such architectures the PLR cannot be implemented; when the network errs, it is not clear which connection is to blame for the error, and what corrective action is to be taken. Back-propagation [Rumelhart et al 1986] circumvents this ""credit-assignment"" problem by dealing only with networks of continuous valued units, whose response function is also continuous (sigmoid). ""Learning"" consists of a gradient-descent type minimization of a cost function that measure the deviation of actual outputs from the required ones, in the space of weights Wij, 0i. A new version of BP, ""back propagation of desired states"", which bears some similarity to our algorithm, has recently been introduced [Plaut 1987]. See also Ie Cun [1985] and Widrow and Winter [1988] for related methods. Our algorithm views the internal representations associated with various inputs as the basic independent variables of the learning process. This is a conceptually plausible assumption; in the course of learning a biological or artificial system should form maps and representations of the external world. Once such representations are formed, the weights can be found by simple and local Hebbian learning rules such as the PLR. Hence the problem of learning becomes one of searching for proper internal representations, rather than one of minimization. Failure of the PLR to converge to a solution is used as an indication that the current guess of internal representations needs to be modified. II. THE ALGORITHM If we know the internal representations (e.g. the states taken by the hidden layer when patterns from the training set are presented), the weights can be found by the PLR. This way the problem of learning becomes one of choosing proper internal representations, rather than of minimizing a cost function by varying the values of weights. To demonstrate our approache, consider the classification prob- lem with output values, sout,~ = eout,~, required in response to J1. = I, ... , M input patterns. If a solution is found, it first maps each input onto an internal represen- tation generated on the hidden layer, which, in turn, produces the correct output. Now imagine that we are not supplied with the weights that solve the problem; however the correct internal representations are revealed. That is, we are given a table with M rows, one for each input. Every row has H bits e;'~, for i = I, .. H, specifying the state of the hidden layer obtained in response to input pattern JJ. One can now view each hidden-layer cell i as the target cell of the PLR, with the N inputs viewed as source. Given sufficient time, the PLR will converge to a set Learning by Choice of Internal Representations 75 of weights Wi,j, connecting input unit j to hidden unit i, so that indeed the input- output association that appears in column i of our table will be realized. In a similar fashion, the PLR will yield a set of weights Wi, in a learning process that uses the hidden layer as source and the output unit as target. Thus, in order to solve the problem of learning, all one needs is a search procedure in the space of possible internal representations, for a table that can be used to generate a solution. Updating of weights can be done in parallel for the two layers, using the current table of internal representations. In the present algorithm, however, the process is broken up into four distinct stages: 1. SETINREP: Generate a table of internal representations {e?'II} by presenting each input pattern from the training set and calculating the state on the hidden layer,using Eq.(la), with the existing couplings Wij and ej. 2. LEARN23: The hidden layer cells are used as source, and the output as the target unit of the PLR. The current table of internal representations is used as the training set; the PLR tries to find appropriate weights Wi and e to obtain the desired outputs. If a solution is found, the problem has been solved. Otherwise stop after 123 learning sweeps, and keep the current weights, to use in IN REP. 3. INREP: Generate a new table of internal representations, which, when used in (lb), yields the correct outputs. This is done by presenting the table sequentially, row by row, to the 11idden layer. If for row v the wrong output is obtained, the internal representation eh ,1I is changed. Having the wrong output means that the ""field"" produced by the hidden layer on the output unit, hout,lI = Ej Wje~'11 is either too large or too small. We then randomly pick a site j of the hidden layer, and try to flip the sign of e;'II; if hout,lI changes in the right direction, we replace the entry of our table, i.e. &~,II -. _&~,II '3 'J' We keep picking sites and changing the internal representation of pattern v until the correct output is generated. We always generate the correct output this way, provided Ej IWjl > leoutl (as is the case for our learning process in LEARN23). This procedure ends with a modified table which is our next guess of internal representations. 4. LEARN12: Apply the PLR with the first layer serving as source, treating every hidden layer site separately as target. Actually, when an input from the training set is presented to the first layer, we first check whether the correct result is produced on the output unit of the network. If we get wrong overall output, we use the PLR for every hidden unit i, modifying weights incident on i according to (2), using column i of the table as the desired states of this unit. If input v does yield the correct output, we insert the current state of the hidden layer as the internal representation associated with pattern v, and no learning steps are taken. We sweep in this manner the training set, modifying weights Wij, (between input and hidden layer), hidden-layer thresholds ei, and, as explained above, internal 76 Grossman, Meir and Domany representations. If the network has achieved error-free performance for the entire training set, learning is completed. If no solution has been found after 112 sweeps of the training set, we abort the PLR stage, keep the present values of Wij, OJ, and start SETINREP again. This is a fairly complete account of our procedure (see also Grossman et al [1988]). There are a few details ·that need to be added. a) The ""impatience"" parameters: 112 and 123, which are rather arbitrary, are introduced to guarantee that the PLR stage is aborted if no solution is found. This is necessary since it is not clear that a solution exists for the weights, given the current table of internal representations. Thus, if the PLR stage does not converge within the time limit specified, a new table of internal representations is formed. The parameters have to be large enough to allow the PLR to find a solution (if one exists) with sufficiently high probability. On the other hand, too large values are wasteful, since they force the algorithm to execute a long search even when no solution exists. Therefore the best values of the impatience parameters can be determined by optimizing the performance of the network; our experience indicates, however, that once a ""reasonable"" range of values is found, performance is fairly insensitive to the precise choice. b) Integer weights: In the PLR correction step, as given by Eq.2, the size of D.. W is constant. Therefore, when using binary units, it can be scaled to unity (by setting T] = 0.5) and one can use integer Wi,j'S without any loss of generality. c) Optimization: The algorithm described uses several parameters, which should be optimized to get the best performance. These parameters are: 112 and 123 - see section (a) above; Imax - time limit, i.e. an upper bound to the total number of training sweeps; and the PLR training parameters - i.e the increment of the weights and thresholds during the PLR stage. In the PLR we used values of 1] ~ 0.1 [see Eq. (2) ] for the weights, and 1] ~ 0.05 for thresholds, whereas the initial (random) values of all weights were taken from the interval (-0.5,0.5), and thresholds from (-0.05,0.05). In the integer weights program, described above, these parameters are not used. d) Treating Multiple Outputs: In the version of inrep described above, we keep flipping the internal representations 'until we find one that yields the correct output, i.e. zero error for the given pattern. This is not always possible when using more than one output unit. Instead, we can allow only for a pre-specified number of attempted flips, lin' and go on to the next pattern even if vanishing error was not achieved. In this modified version we also introduce a slightly different, and less ""restrictive"" criterion for accepting or rejecting a flip. Having chosen (at random) a hidden unit i, we check the effect of flipping the sign of ~;,II on the total output error, i.e. the number of wrong bits (and not on the output field, as described above). If the output error is not increased, the flip is accepted and the table of internal representations is changed accordingly. This modified algorithm is applicable for multiple-output networks. Results of preliminary experiments with this version are presented in the next section. Learning by Choice of Internal Representations 77 III. PERFORMANCE OF THE ALGORITHM The ""time"" parameter that we use for measuring performance is the number of sweeps through the training set of M patterns needed in order to find the solution. Namely, how many times each pattern was presented to the network. In each cycle of the algorithm there are 112 + 123 such sweeps. For each problem, and each parameter choice, an ensemble of many independent runs, each starting with a different random choice of initial weights, is created. In general, when applying a learning algorithm to a given problem, there are cases in which the algorithm fails to find a solution within the specified time limit (e.g. when BP get stuck in a local minimum), and it is impossible to calculate the ensemble average of learning times. Therefore we calculate, as a performance measure, either the median number of sweeps, t m , or the ""inverse average rate"", T, as defined in Tesauro and Janssen [1988]. The first problem we studied is contiguity: the system has to determine whether the number of clumps (i.e. contiguous blocks) of +1 's in the input is, say, equal to 2 or 3. This is called [Denker et al 1987] the ""2 versus 3"" clumps predicate. We used, as our training set, all inputs that have 2 or 3 clumps, with learning cycles parametrized by 112 = 20 and 123 = 5. Keeping N = 6 fixed, we varied H; 500 cases were used for each data point of Fig.l. 400 - x BP 300 <> CHIR 200 100 3 4 5 6 7 8 H Figure 1. Median number of sweeps tm , needed to train a network of N = 6 input units, over an exhaustive training set, to solve the"" 2 vs 3"" clumps predicate, plotted against the number of hidden units H. Results for back-propagation [Denker et al 1987] (x) and this work (¢) are shown. 78 Grossman, Meir and Domany In the next problem, symmetry, one requires sout = 1 for reflection-symmetric inputs and -1 otherwise. This can be solved with H ~ 2 hidden units. Fig. 2 presents, for H = 2, the median number of exhaustive training sweeps needed to solve the problem, vs input size N. At each point 500 cases were run, with 112 = 10 and 123 = 5. We always found a solution in'less than 200 cycles. 6 N 8 10 Figure 2. Median number of sweeps t m , needed to train networks on symmetry (with H = 2). In the Parity problem one requires sout = 1 for an even number of +1 bits in the input, and -1 otherwise. In order to compare performance of our algorithm to that of BP, we studied the Parity problem, using networks with an architecture of N : 2N : 1, as chosen by Tesauro and Janssen [1988]. We used the integer version of our algorithm, briefly described above. In this version of the algorithm the weights and thresholds are integers, and the increment size, for both thresholds and weights, is unity. As an initial condition, we chose them to be +1 or -1 randomly. In the simulation of this version, all possible input patterns were presented sequentially in a fixed order (within the perceptron learning sweeps). The results are presented in Table 1. For all choices of the parameters ( It2, 123 ), that are mentioned in the table, our success rate was 100%. Namely, the algorithm didn't fail even once to find a solution in less than the maximal number of training cycles Imax specified in the table. Results for BP, r(BP) (from Tesauro and Janssen 1988) are also given in the table. Note that BP does get caught in local minima, but the percentage of such occurrences was not reported. Learning by Choice of Internal Representations 79 For testing the multiple output version of the algorithm we use8 the combined parity and symmetry problem; the network has two output units, both connected to all hidden units. The first output unit performs the parity predicate on the input, and the second performs the symmetry predicate. The network architecture was N:2N:2 and the results for N=4 .. 7 are given in Table 2. The choice of parameters is also given in that table. N (I12,/23) Ima.x tm T(CH IR) T(BP) 3 (8,4) 10 3 3 3g 4 (9,3)(6,6) 20 4 4 75 5 (12,4)(9,6) 40 8 6 130 6 (12,4)(10,5) 120 19 9 310 I 7 (12,4)(15,5) 240 290 30 80Q 8 (20,10) 900 2900 150 20db 9 (20,10) 900 2400 1300 Table 1. Parity with N:2N:1 architecture. N 112 h3 lin Ima.x tm T 4 12 8 7 40 50 33 5 14 7 7 400 900 350 6 18 9 7 900 5250 925 7 40 20 7 900 6000 2640 Table 2. Parity and Symmetry with N :2N:2 architecture. IV. DISCUSSION We have presented a learning algorithm for two-Iayerperceptrons, that searches for internal representations of the training set, and determines the weights by the local, Hebbian perceptron learning rule. Learning by choice of internal represen- tation may turn out to be most useful in situations where the ""teacher"" has some information about the desired internal representations. We demonstrated that our algorithm works well on four typical problems, and studied the manner in which training time varies with network size. Comparisons with back-propagation were also made. it should be noted that a training sweep involves much less computations than that of back-propagation. We also presented a generalization of the algorithm to networks with multiple outputs, and found that it functions well on various problems of the same kind as discussed above. It appears that the modification needed to deal with multiple outputs also enables us to solve the learning problem for network architectures with more than one hidden layer. 80 Grossman, Meir and Domany At this point we can offer only very limited discussion of the interesting ques- tion - why does our algorithm work at all? That is, how come it finds correct internal representations (e.g. ""tables"") while these constitute only a small fraction of the total possible number (2H2N)? The main reason is that our procedure ac- tually does not search this entire space of tables. This large space contains a small subspace, T, of ""target tables"", i.e. those that can be obtained, for all possible choices of w{j and OJ, by rule (1), in response to presentation of the input patterns. Another small subspace S, is that of the tables that can potentially produce the required output. Solutions of the learning problem constitute the space T n S. Our algorithm iterates between T and S, executing also a ""walk"" (induced by the modification of the weights due to the PLR) within each. An appealing feature of our algorithm is that it can be implemented in a manner that uses only integer-valued weights and thresholds. This discreteness makes the analysis of the behavior of the network much easier, since we know the exact number of bits used by the system in constructing its solution, and do not have to worry about round-off errors. From a technological point of view, for hardware implementation it may also be more feasible to work with integer weights. We are extending this work in various directions. The present method needs, in the learning stage, M H bits of memory: internal representations of all M training patterns are stored. This feature is biologically implausible and may be techno- logically limiting; we are developing a method that does not require such memory. Other directions of current study include extensions to networks with continuous variables, and to networks with feed-back. References Denker J., Schwartz D., Wittner B., SolI a S., Hopfield J.J., Howard R. and Jackel L. 1987, Complex Systems 1, 877-922 Grossman T., Meir R. and Domany E. 1988, Complex Systems in press. I1ebb D.O. 1949, The organization of Behavior, J. Wiley, N.Y Le Cun Y. 1985, Proc. Cognitiva 85, 593 Lewis P.M. and Coates C.L. 1967, Threshold Logic. (Wiley, New York) Minsky M. and Papert S. 1988, Perceptrons. (MIT, Cambridge). Plaut D.C., Nowlan S.J. and Hinton G.E. 1987, Tech. Report CMU-CS-86-126 Rosenblatt F. Principles of neurodynamics. (Spartan, New York, 1962) Rumelhart D.E., Hinton G.E. and Williams R.J. 1986, Nature 323,533-536 Tesauro G. and Janssen H. 1988, Complex Systems 2, 39 Widrow B. and Winter R. 1988, Computer 21, No.3, 25","[-0.0780409425497055, -0.08529409766197205, 0.051441289484500885, -0.02060771733522415, 0.03774223104119301, -0.039361122995615005, 0.0255181472748518, -0.03263797238469124, 0.01756509765982628, -0.010233014822006226, -0.012275248765945435, -0.059079550206661224, 0.015874378383159637, 0.04051268473267555, 0.007196926511824131, 0.025923574343323708, 0.024596964940428734, 0.05088963732123375, -0.06279604136943817, -0.016779225319623947, 0.021771514788269997, -0.07177282869815826, -0.053742002695798874, -0.03247781842947006, 0.013135932385921478, 0.002498184097930789, 0.04610579460859299, 0.027125637978315353, -0.003931477665901184, -0.055314913392066956, 0.07440896332263947, -0.056939203292131424, 0.04889015853404999, -0.04443182423710823, -0.04140596091747284, 0.06023017317056656, -0.052458252757787704, -0.017336493358016014, -0.025562003254890442, 0.003783147782087326, 0.0073671829886734486, 0.041606128215789795, -0.07639789581298828, 0.04080192372202873, 0.08488348126411438, 0.08033604919910431, 0.0493088997900486, 0.02902405336499214, -0.053936511278152466, -0.03586282581090927, 0.06553386151790619, 0.06726047396659851, -0.04994555562734604, 0.07704076170921326, -0.0073215169832110405, -0.04244489595293999, 0.01813192293047905, 0.01763281226158142, -0.07670239359140396, -0.008277307264506817, 0.028780346736311913, -0.10895691812038422, -0.04281600937247276, -0.09100860357284546, 0.04164884239435196, 0.010264982469379902, 0.0647297203540802, 0.007852692157030106, -0.020657317712903023, -0.013229975476861, 0.07658112049102783, 0.059664249420166016, -0.11861792206764221, 0.04642358049750328, 0.0961296483874321, 0.03726450726389885, 0.08288422971963882, 0.0009910407243296504, -0.031298283487558365, -0.017529575154185295, -0.04186758026480675, 0.04078550264239311, 0.033482443541288376, -0.03310742601752281, 0.09225314855575562, -0.03053504042327404, -0.03824108466506004, 0.006197257433086634, 0.038126274943351746, 0.03550765663385391, -0.02356967143714428, -0.025248989462852478, -0.10193473845720291, -0.03319277614355087, 0.07444901019334793, 0.013186442665755749, -0.00019832223188132048, -0.01746349409222603, 0.010924171656370163, 0.10476598143577576, -0.02628151699900627, -0.0477675162255764, 0.025616202503442764, -0.00025444780476391315, 0.07463978230953217, 0.06569058448076248, 0.02823304384946823, 0.03489929810166359, 0.07063368707895279, -0.14352154731750488, -0.029912294819951057, 0.022856662049889565, -0.021245867013931274, 0.03236912935972214, 0.004778503440320492, -0.044036634266376495, 0.08690787851810455, 0.00625187624245882, 0.09336164593696594, 0.05535188689827919, -0.11148929595947266, 0.005287355277687311, -0.09503500908613205, 0.05095976963639259, 0.018496301025152206, -0.04667766019701958, -0.10733433812856674, 6.390305409228646e-33, -0.042238421738147736, 0.041768088936805725, 0.01993323303759098, -0.14473135769367218, 0.041343335062265396, 0.004135573748499155, 0.013912439346313477, 0.023682184517383575, -0.0010970467701554298, 0.005232993979007006, -0.0717625766992569, -0.043509725481271744, 0.055693283677101135, 0.009707762859761715, 0.05221477895975113, -0.009256210178136826, -0.009027509950101376, -0.03288063406944275, 0.049702394753694534, -0.12425393611192703, 0.01036890596151352, -0.03278522565960884, -0.03197111189365387, -0.026769686490297318, -0.0649585947394371, -0.004418950527906418, 0.04377943277359009, -0.02662699855864048, -0.04368213191628456, 0.003943540621548891, -0.02268115244805813, 0.027957197278738022, 0.01733921840786934, -0.014194141142070293, 0.0314200259745121, -0.029264245182275772, 0.03505789116024971, -0.0199019405990839, 0.08205596357584, -0.0834541916847229, 0.018217215314507484, 0.056035976856946945, 0.03599443659186363, -0.016750453040003777, -0.0575534887611866, 0.0034877238795161247, 0.04248460754752159, 0.04200047627091408, -0.06962121278047562, -0.08171924948692322, -0.033407729119062424, -0.042880699038505554, -0.018790004774928093, -0.11195739358663559, 0.01190640963613987, 0.04099347069859505, 0.021677397191524506, 0.08721854537725449, 0.09041792899370193, 0.06252288818359375, 0.053742289543151855, 0.05451468378305435, 0.04513593390583992, 0.06704914569854736, -0.013673757202923298, 0.046690721064805984, -0.08015335351228714, 0.0017700818134471774, 0.10822625458240509, -0.05304088816046715, -0.05463312566280365, 0.12828399240970612, -0.03649105504155159, -0.08721765875816345, 0.09310567378997803, -0.033555250614881516, 0.020790722221136093, -0.082081638276577, 0.0069748046807944775, 0.017484793439507484, -0.06917336583137512, 0.06709718704223633, -0.05276303365826607, -0.04677076265215874, -0.10959302634000778, 0.01335145253688097, 0.026631243526935577, -0.0885317325592041, -0.03157986328005791, 0.028056243434548378, 0.015439581125974655, -0.03849715366959572, -0.015522398985922337, 0.005351996514946222, -0.04778766259551048, -6.12144376991468e-33, -0.07667453587055206, 0.061396755278110504, -0.018765810877084732, -0.0024634599685668945, -0.008131749927997589, -0.009460058063268661, -0.06520950794219971, 0.009525707922875881, -0.0331355445086956, 0.06592393666505814, 0.01833314821124077, -3.05954999930691e-05, -0.06267829239368439, -0.009283289313316345, 0.006260143127292395, 0.07337092608213425, -0.037917204201221466, 0.009404758922755718, 0.04371633753180504, -0.030276425182819366, -0.034902337938547134, 0.09871811419725418, -0.0750490054488182, 0.07525820285081863, -0.01634432002902031, -0.014016744680702686, -0.025002097710967064, 0.13571588695049286, 0.02037343941628933, 0.04517821595072746, -0.09568614512681961, -0.019815441220998764, -0.030138742178678513, 0.02134663797914982, 0.02426091581583023, -0.012870309874415398, -0.01696910709142685, 0.023317012935876846, 0.009710042737424374, -0.007092948537319899, -0.013221433386206627, -0.0073228562250733376, -0.09318238496780396, 0.07104175537824631, 0.001832837937399745, -0.061239708214998245, 0.0022286726161837578, 0.06911258399486542, -0.07057999819517136, 0.04561770334839821, -0.03094586916267872, -0.030584260821342468, -0.002334579825401306, -0.024693870916962624, -0.02882736548781395, 0.05146269127726555, 0.031244922429323196, 0.018843049183487892, 0.1131272241473198, -0.014067308977246284, -0.08634801208972931, -0.08246095478534698, 0.052224110811948776, 0.02612336166203022, -0.010769934393465519, -0.024814222007989883, 0.05203239247202873, 0.044264525175094604, 0.07078663259744644, 0.0260752085596323, -0.0066881924867630005, 0.04939946532249451, 0.0307296272367239, -0.0638650581240654, 0.01695146970450878, -0.010577408596873283, 0.019412746652960777, -0.008066879585385323, -0.06116478517651558, -0.017681650817394257, -0.053083181381225586, -0.0011245302157476544, -0.013255043886601925, -0.019171951338648796, 0.08041217923164368, 0.0040472401306033134, 0.09669492393732071, 0.023988477885723114, -0.0023625954054296017, -0.01977093145251274, -0.0017465431010350585, 0.08463853597640991, 0.02137049101293087, -0.0024665286764502525, -0.02985958382487297, -5.520266199710022e-08, -0.05916537344455719, -0.06604611873626709, 0.03459217771887779, 0.02063964493572712, 0.08119440078735352, -0.022934233769774437, 0.07469738274812698, 0.04030000418424606, -0.07441949099302292, 0.011628512293100357, 0.08320637792348862, 0.01251336932182312, -0.052482519298791885, -0.04794413223862648, 0.014801283366978168, 0.07755262404680252, 0.06260444968938828, 0.004881229717284441, -0.01394635159522295, -0.018795831128954887, -0.008043336682021618, -0.009452289901673794, 0.0255722738802433, 0.0318739116191864, 0.08236413449048996, -0.05417865142226219, -0.07138305902481079, 0.013548502698540688, 0.06037817522883415, 0.0891033411026001, -0.0035641149152070284, 0.00850294902920723, 0.05780462548136711, -0.0009097008151002228, 0.055351611226797104, 0.1302543431520462, 0.014560259878635406, 0.004765980411320925, -0.04719853401184082, -0.04169975221157074, -0.017405200749635696, 0.0015131777618080378, 0.006255005486309528, -0.008780395612120628, 0.04344531148672104, -0.025354737415909767, -0.03626212105154991, -0.04391321912407875, 0.03128928691148758, -0.05469455569982529, 0.05960864573717117, -0.03182654082775116, -0.013435895554721355, 0.046401139348745346, 0.010170375928282738, -0.0037997188046574593, -0.05512272194027901, -0.11584322899580002, 0.012818999588489532, 0.07811081409454346, 0.04145808145403862, 0.0472620353102684, -0.08578645437955856, 0.00519941933453083]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\SpreadingActivationoverDistributedMicrofeatures.pdf,Deep Learning,"442 Abstract: How Neural Nets Work Alan Lapedes Robert Farber Theoretical Division Los Alamos National Laboratory Los Alamos, NM 87545 There is presently great interest in the abilities of neural networks to mimic ""qualitative reasoning"" by manipulating neural incodings of symbols. Less work has been performed on using neural networks to process floating point numbers and it is sometimes stated that neural networks are somehow inherently inaccu- rate and therefore best suited for ""fuzzy"" qualitative reasoning. Nevertheless, the potential speed of massively parallel operations make neural net ""number crunching"" an interesting topic to explore. In this paper we discuss some of our work in which we demonstrate that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional tech- niques. In particular, prediction of future values of a chaotic time series can be performed with exceptionally high accuracy. We analyze how a neural net is able to do this , and in the process show that a large class of functions from Rn. ~ Rffl may be accurately approximated by a backpropagation neural net with just two ""hidden"" layers. The network uses this functional approximation to perform either interpolation (signal processing applications) or extrapolation (symbol processing applicationsJ. Neural nets therefore use quite familiar meth- ods to perform. their tasks. The geometrical viewpoint advocated here seems to be a useful approach to analyzing neural network operation and relates neural networks to well studied topics in functional approximation. 1. Introduction Although a great deal of interest has been displayed in neural network's capabilities to perform a kind of qualitative reasoning, relatively little work has been done on the ability of neural networks to process floating point numbers in a massively parallel fashion. Clearly, this is an important ability. In this paper we discuss some of our work in this area and show the relation between numerical, and symbolic processing. We will concentrate on the the subject of accurate prediction in a time series. Accurate prediction has applications in many areas of signal processing. It is also a useful, and fascinating ability, when dealing with natural, physical systems. Given some .data from the past history of a system, can one accurately predict what it will do in the future? Many conventional signal processing tests, such as correlation function anal- ysis, cannot distinguish deterministic chaotic behavior from from stochastic noise. Particularly difficult systems to predict are those that are nonlinear and chaotic. Chaos has a technical definition based on nonlinear, dynamical systems theory, but intuitivly means that the system is deterministic but ""random,"" in a rather similar manner to deterministic, pseudo random number generators used on conventional computers. Examples of chaotic systems in nature include turbulence in fluids (D. Ruelle, 1971; H. Swinney, 1978), chemical reactions (K. Tomita, 1979), lasers (H. Haken, 1975), plasma physics (D. Russel, 1980) to name but a few. Typically, chaotic systems also display the full range of non- linear behavior (fixed points, limit cycles etc.) when parameters are varied, and therefore provide a good testbed in which to investigate techniques of nonlinear signal processing. Clearly, if one can uncover the underlying, deterministic al- gorithm from a chaotic time series, then one may be able to predict the future time series quite accurately, © American Institute of Physics 1988 443 In this paper we review and extend our work (Lapedes and Farber ,1987) on predicting the behavior of a particular dynamical system, the Glass-Mackey equation. We feel that the method will be fairly general, and use the Glass- Mackey equation solely for illustrative purposes. The Glass-Mackey equation has a strange attractor with fractal dimension controlled by a constant param- eter appearing in the differential equation. We present results on a neural net- work's ability to predict this system at two values of this parameter, one value corresponding to the onset of chaos, and the other value deeply in the chaotic regime. We also present the results of more conventional predictive methods and show that a neural net is able to achieve significantly better numerical accuracy. This particular system was chosen because of D. Farmer's and J. Sidorowich's (D. Farmer, J. Sidorowich, 1987) use of it in developing a new, non-neural net method for predicting chaos. The accuracy of this non-neural net method, and the neural net method, are roughly equivalent, with various advantages or dis- advantages accruing to one method or the other depending on one's point of view. We are happy to acknowledge many valuable discussions with Farmer and Sidorowich that has led to further improvements in each method. We also show that a neural net never needs more than two hidden layers to solve most problems. This statement arises from a more general argument that a neural net can approximate functions from Rn. -+ Rm with only two hidden layers, and that the accuracy of the approximation is controlled by the number of neurons in each layer. The argument assumes that the global minimum to the backpropagation minimization problem may be found, or that a local minima very close in value to the global minimum may be found. This seems to be the case in the examples we considered, and in many examples considered by other researchers, but is never guaranteed. The conclusion of an upper bound of two hidden layers is related to a similar conclusion of R. Lipman (R. Lipman, 1987) who has previously analyzed the number of hidden layers needed to form arbitrary decision regions for symbolic processing problems. Related issues are discussed by J. Denker (J. Denker et.al. 1987) It is easy to extend the argument to draw similar conclusions about an upper bound of two hidden layers for symbol processing and to place signal processing, and symbol processing in a common theoretical framework. 2. Backpropagation Backpropagation is a learning algorithm for neural networks that seeks to find weights, T ij, such that given an input pattern from a training set of pairs of Input/Output patterns, the network will produce the Output of the training set given the Input. Having learned this mapping between I and 0 for the training set, one then applies a new, previously unseen Input, and takes the Output as the ""conclusion"" drawn by the neural net based on having learned fundamental relationships between Input and Output from the training set. A popular configuration for backpropagation is a totally feedforward net (Figure 1) where Input feeds up through ""hidden layers"" to an Output layer. 444 OUTPUT Figure 1. A feedforward neural net. Arrows schemat- ically indicate full feedforward connect- ivity Each neuron forms a weighted sum of the inputs from previous layers to which it is connected, adds a threshold value, and produces a nonlinear function of this sum as its output value. This output value serves as input to the future layers to which the neuron is connected, and the process is repeated. Ultimately a value is produced for the outputs of the neurons in the Output layer. Thus, each neuron performs: (1) where Tii are continuous valued, positive or negative weights, 9. is a constant, and g(x) is a nonlinear function that is often chosen to be of a sigmoidal form. For example, one may choose 1 g(z) = 2"" (1 + tanhz) (2) where tanh is the hyperbolic tangent, although the exact formula of the sigmoid is irrelevant to the results. If t!"") are the target output values for the pth Input pattern then ones trains the network by minimizing E = L L (t~P) - o!P)) 2 (3) p i where t~p) is the target output values (taken from the training set) and O~pl is the output of the network when the pth Input pattern of the training set is presented on the Input layer. i indexes the number of neurons in the Output layer. An iterative procedure is used to minimize S. For example, the commonly used steepest descents procedure is implemented by changing Tii and S, by AT'i and AS, where aE ~T. .. = --'E '1 aT. .. '1 445 (4a) ( 4b) This implies that ~E < 0 and hence E will decrease to a local minimum. Use o~ the chain .rule and definition of some intermediate quantities allows the followmg expressIons for ~Tij to be obtained (Rumelhart, 1987): ~Tij = L E6lp)o~.p) p where if i is labeling a neuron in the Output layer; and 6Jp) = O!p) (1 - o~p») LTij 6;p) j (Sa) (Sb) (6) (7) if i labels a neuron in the hidden layers. Therefore one computes 6Jp) for the Output layer first, then uses Eqn. (7) to computer 6ip ) for the hidden layers, and finally uses Eqn. (S) to make an adjustment to the weights. We remark that the steepest descents procedure in common use is extremely slow in simulation, and that a better minimization procedure, such as the classic conjugate gradient procedure (W. Press, 1986), can offer quite significant speedups. Many appli- cations use bit representations (0,1) for symbols, and attempt to have a neural net learn fundamental relationships between the symbols. This procedure has been successfully used in converting text to speech (T. Sejnowski, 1986) and in determining whether a given fragment of DNA codes for a protein or not (A. Lapedes, R. Farber, 1987). There is no fundamental reason, however, to use integer's as values for Input and Output. If the Inputs and Outputs are instead a collection of floating point numbers, then the network, after training, yields a specific continuous function in n variables (for n inputs) involving g(x) (Le. hyperbolic tanh's) that provides a type of nonlinear, least mean square interpolant formula for the discrete set of data points in the training set. Use of this formula a = 1(11, 1"", ... 1'1) when given a new input not in the training set, is then either interpolation or extrapolation. Since the Output values, when assumed to be floating point numbers may have a dynamic range great than 10,1\, one may modify the g(x) on the Output layer to be a linear function, instead of sigmoidal, so as to encompass the larger dynamic range. Dynamic range of the Input values is not so critical, however we have found that numerical problems may be avoided by scaling the Inputs (and 446 also the Outputs) to [0,1], training the network, and then rescaling the Ti;, (J, to encompass the original dynamic range. The point is that scale changes in I and 0 may, for feedforward networks, always be absorbed in the T ijJ (J, and vice versa. We use this procedure (backpropagation, conjugate gradient, linear outputs and scaling) in the following section to predict points in a chaotic time series. 3. Prediction Let us consider situations in Nature where a system is described by nonlin- ear differential equations. This is faily generic. We choose a particular nonlinear equation that has an infinite dimensional phase space, so that it is similar to other infinite dimensional systems such as partial differential equations. A differ- ential equation with an infinite dimensional phase space (i.e. an infinite number of values are necessary to describe the initial condition) is a delay, differential equation. We choose to consider the time series generated by the Glass-Mackey equation: X= az(t - 1') b t 1 + Z 10 (t _ 1') - Z ( ) (8) This is a nonlinear differential, delay equation with an initial condition specified by an initial function defined over a strip of width l' (hence the infinite di- mensional phase space i.e. initial functions, not initial constants are required). Choosing this function to be a constant function, and a = .2, b = .1, and l' = 17 yields a time series, x(t), (obtained by integrating Eqn. (8)), that is chaotic with a fractal attractor of dimension 2.1. Increasing l' to 30 yields more complicated evolution and a fractal dimension of 3.5. The time series for 500 time steps for 1'=30 (time in units of 1') is plotted in Figure 2. The nonlinear evolution of the system collapses the infinite dimensional phase space down to a low (approxi- mately 2 or 3 dimensional) fractal, attracting set. Similar chaotic systems are not uncommon in Nature. Figure 2. Example time series at tau ~ 30. 447 The goal is to take a set of values of xO at discrete times in some time window containing times less than t, and use the values to accurately predict x(t + P), where P is some prediction time step into the future. One may fix P, collect statistics on accuracy for many prediction times t (by sliding the window along the time series), and then increase P and again collect statistics on accuracy. This one may observe how an average index of accuracy changes as P is increased. In terms of Figure 2 we will select various prediction time steps, P, that correspond to attempting to predict within a ""bump,"" to predicting a couple of ""bumps"" ahead. The fundamental nature of chaos dictates that prediction accuracy will decrease as P is increased. This is due to inescapable inaccuracies of finite precision in specifying the x( t) at discrete times in the past that are used for predicting the future. Thus, all predictive methods will degrade as P is increased - the question is ""How rapidly does the error increase with P?"" We will demonstrate that the neural net method can be orders of magnitude more accurate than conventional methods at large prediction time steps, P. Our goal is to use backpropagation, and a neural net, to construct a function O(t + P) = f (11(t), 12(t - A) ... lm(t - mA)) (9) where O(t + P) is the output of a single neuron in the Output layer, and 11 ~ 1m are input neurons that take on values z(t), z(t - A) ... z(t - rnA), where A is a time delay. O(t + P) takes on the value x(t + P). We chose the network configuation of Figure 1. We construct a training set by selecting a set of input values: (10) 1m = x(tp - rnA) with associated output values 0 = x(tp + P), for a collection of discrete times that are labelled by tp. Typically we used 500 I/O pairs in the training set so that p ranged from 1~ 500. Thus we have a collection of 500 sets of {lip), l~p), ... , 1::); O(p)} to use in training the neural net. This procedure of using delayed sampled values of x{t) can be implemented by using tapped de- lay lines, just as is normally done in linear signal processing applications, (B. Widrow, 1985). Our prediction procedure is a straightforward nonlinear exten- sion of the linear Widrow Hoff algorithm. After training is completed, prediction is performed on a new set of times, tp, not in the training set i.e. for p = 500. We have not yet specified what m or A should be, nor given any indication why a formula like Eqn. (9) should work at all. An important theorem of Takens (Takens, 1981) states that for flows evolving to compact attracting manifolds of dimension d.A"" that a functional relation like Eqn. (9) does exist, and that m lies in the range d.A, < m + 1 < 2d.A, + 1. We therefore choose m = 4, for T = 30. Takens provides no information on A and we chose A = 6 for both cases. We found that a few different choices of m and A can affect accuracy by a factor of 2 - a somewhat significant but not overwhelming sensitivity, in view of the fact that neural nets tend to be orders of magnitude more accurate than other methods. Takens theorem gives no information on the form of fO in Eqn. (9). It therefore 448 is necessary to show that neural nets provide a robust approximating procedure for continuous fO, which we do in the following section. It is interesting to note that attempts to predict future values of a time series using past values of x(t) from a tapped delay line is a common procedUre in signal processing, and yet there is little, if any, reference to results of nonlinear dynamical systems theory showing why any such attempt is reasonable. After trainin, the neural net as described above, we used it to predict 500 new values of x(tJ in the future and computed the average accuracy for these points. The accuracy is defined to be the average root mean square error, divided by a constant scale factor, which we took to be the standard deviation of the data. It is necessary to remove the scale dependence of the data and dividing by the standard deviation of the data provides a scale to use. Thus the resulting ""index of accuracy"" is insensitive to the dynamic range of x( t). As just described, if one wanted to use a neural net to continuously predict x(t) values at, say, 6 time steps past the last observed value (i.e. wanted to construct a net predicting x( t + 6)) then one would train one network, at P = 6, to do this. If one wanted to always predict 12 time steps past the last observed x( t) then a separate, P = 12, net would have to be trained. We, in fact, trained separate networks for P ranging between 6 and 100 in steps of 6. The index of accuracy for these networks (as obtained by computing the index of accuracy in the prediction phase) is plotted as curve D in Figure 3. There is however an alternate way to predict. If one wished to predict, say, x(t + 12) using a P = 6 net, then one can iterate the P = 6 net. That is, one uses the P = 6 net to predict the x(t +6) values, and then feeds x(t +6) back into the input line to predict x(t + 12) using the predicted x(t + 6) value instead of the observed x(t + 6) value. in fact, one can't use the observed x(t +6) value, because it hasn't been observed yet - the rule of the game is to use only data occurring at time t and before, to predict x( t + 12). This procedure corresponds to iterating the map given by Eqn. (9) to perform prediction at multiples of P. Of course, the delays, ~, must be chosen commensurate with P. This iterative method of prediction has potential dangers. Because (in our example of iterating the P = 6 map) the predicted x(t + 6) is always made with some error, then this error is compounded in iteration, because predicted, and not observed values, are used on the input lines. However, one may pre- dict more accurately for smaller P, so it may be the case that choosing a very accurate small P prediction, and iterating, can ultimately achieve higher accu- racy at the larger P's of interest. This tUrns out to be true, and the iterated net method is plotted as curve E in Figure 3. It is the best procedure to use. Curves A,B,C are alternative methods (iterated polynomial, Widrow-Hoff, and non-iterated polynomial respectively. More information on these conventional methods is in (Lapedes and Farber, 1987) ). A C 1 1 I, /' ~ , : .8 !I: "" :J I / \f I / : .' . , I / I I I .. ,,: I .6 I I ~ I I ~ I ~ I = I - I I .4 , I , I , .2 I o o 4. Why It Works B D P1-~~ictlon ~~. P (T.U3~ 30) Figure 3. 449 E 400 Consider writing out explicitly Eqn. (9) for a two hidden layer network where the output is assumed to be a linear neuron. We consider Input connects to Hidden Layer 1, Hidden Layer 1 to Hidden Layer 2, and Hidden Layer 2 to Output, Therefore: Recall that the output neurons a linear computing element so that only two gOs occur in formula (11), due to the two nonlinear hidden layers. For ease in later analysis, let us rewrite this formula as where Ot = L TtJcg (SU Mle + Ole) + Ot IetH 2 (12a) (12b) 450 The T's and (Ps are specific numbers specified by the training algorithm, so that after training is finished one has a relatively complicated formula (12a, 12b) that expresses the Output value as a specific, known, function of the Input values: Ot == 1(117 12,"" .lm). A functional relation of this form, when there is only one output, may be viewed as surface in m + 1 dimensional space, in exactly the same manner one interprets the formula z == f(x,y) as a two dimensional surface in three ' dimensional space. The general structure of fO as determined by Eqn. (12a, 12b) is in fact quite simple. From Eqn. (12b) we see that one first forms a sum of gO functions (where gO is s sigmoidal function) and then from Eqn. (12a) one (orms yet another sum involving gO functions. It may at first be thought that this special, simple form of fO restricts the type of surface that may be represented by Ot = f(Ii)' This initial tl.ought is wrong - the special form of Eqn. (12) is actually a general representation for quite arbitrary surfaces. To prove that Eqn. (12) is a reasonable representation for surfaces we first point out that surfaces may be approximated by adding up a series of ""bumps"" that are appropriately placed. An example of this occurs in familiar Fourier analysis, where wave trains of suitable frequency and amplitude are added together to approximate curves (or surfaces). Each half period of each wave of fixed wavelength is a ""bump,"" and one adds all the bumps together to form the approximant. Let us noW see how Eqn. (12) may be interpreted as adding together bumps of specified heights and positions. First consider SUMk which is a sum of g( ) functions. In Figure (4) we plot an example of such a gO function for the case of two inputs. Figure 4. A sigmoidal surface. 451 The orientation of this sigmoidal surface is determined by T sit the position by 8;'1 and height by T""'i. Now consider another gO function that occurs in SUM"",. The 8;, of the second gO function is chosen to displace it from the first, the Tii is chosen so that it has the same orientation as the first, and T ""'i is chosen to have opposite sign to the first. These two g( ) functions occur in SUM"""" and so to determine their contribution to SUM"", we sum them together and plot the result in Fi ure 5. The result is a ridged surface. Figure 5. A ridge. Since our goal is to obtain localized bumps we select another pair of gO functions in SUMk, add them together to get a ridged surface perpendicular to the first ridged surface, and then add the two perpendicular ridged surfaces together to see the contribution to SUMk. The result is plotted in Figure (6). Figure 6. A pseudo-bump . 452 We see that this almost worked, in so much as one obtains a local maxima by this procedure. However there are also saddle-like configurations at the corners which corrupt the bump we were trying to obtain. Note that one way to fix this is to take g(SUMk + Ok) which will, if Ole is chosen appropriately, depress the local minima and saddles to zero while simultaneously sending the central maximum towards 1. The result is plotted in Figure (7) and is the sought after b~~ ____________________________________________ ___ Figure 7. A bump. Furthermore, note that the necessary gO function is supplied by Eqn. (12). Therefore Eqn. (12) is a procedure to obtain localized bumps of arbitrary height and position. For two inputs, the kth bump is obtained by using four gO func- tions from SUMk (two gO functions for each ridged surface and two ridged surfaces per bump) and then taking gO of the result in Eqn. (12a). The height of the kth bump is determined by T tJe in Eqn. (12a) and the k bumps are added together by that equation as well. The general network architecture which cor- responds to the above procedure of adding two gO functions together to form a ridge, two perpendicular ridges together to form a pseudo-bump, and the final gO to form the final bump is represented in Figure (8). To obtain any number ot bumps one adds more neurons to the hidden layers by repeatedly using the connectivity of Figure (8) as a template (Le. four neurons per bump in Hidden Layer 1, and one neuron per bump in HiClden Layer 2). 453 Figure 8. Connectivity needed to obtain one bump. Add four more neurons to Hidden layer 1, and one more neuron to Hidden Layer 2, for each additional bump. One never needs more than two layers, or any other type of connectivity than that already schematically specified by Figure (8). The accuracy of the approximation depends on the number of bumps, whIch in turn is specified, by the number of neurons per layer. This result is easily generalized to higher dimensions (more than two Inputs) where one needs 2m hiddens in the first hidden layer, and one hidden neuron in the second layer for each bump. The argument given above also extends to the situation where one is pro-- cessing symbolic information with a neural net. In this situation, the Input information is coded into bits (say Os and Is) and similarly for the Output. Or, the Inputs may still be real valued numbers, in which case the binary output is attempting to group the real valued Inputs into separate classes. To make the Output values tend toward 0 and lone takes a third and final gO on the output layer, i.e. each output neuron is represented by g(Ot) where Ot is given in Eqn. (11) . Recall that up until now we have used hnear neurons on the output layer. In typical backpropagation examples, one never actually achieves a hard 0 or 1 on the output layers but achieves instead some value between 0.0 and 1.0. Then typically any value over 0.5 is called 1, and values under 0.5 are called O. This ""postprocessing"" step is not really outside the framework of the network formalism, because it may be performed by merely increasing the slope of the sigmoidal function on the Output layer. Therefore the only effect of the third and final gO function used on the Output layer in symbolic information processing is to pass a hyperplane through the surface we have just been dis- cussing. This plane cuts the surface, forming ""decision regions,"" in which high values are called 1 and low values are called O. Thus we see that the heart of the problem is to be able to form surfaces in a general manner, which is then cut by a hyperplane into general decision regions. We are therefore able to conclude that the network architecture consisting of just two hidden layers is sufficient for learning any symbol processing training set. For Boolean symbol mappings one need not use the second hidden layer to remove the saddles on the bump (c.f. Fig. 6). The saddles are lower than the central maximum so one may choose a threshold on the output layer to cut the bump at a point over the saddles to yield the correct decision region. Whether this representation is a reasonable one for subsequently achieving good prediction on a prediction set, as opposed to ""memorizing"" a training set, is an issue that we address below. 454 We also note that use of Sigma IIi units (Rummelhart, 1986) or high order correlation nets (Y.-C. Lee, 1987) is an attempt to construct a surface by a general polynomial expansion, which is then cut by a hyperplane into decision regions, as in the above. Therefore the essential element of all these neural net learning algorithms are identical (Le. surface construction), only the particular method of parameterizing the surface varies from one algorithm to another. This geometrical viewpoint, which provides a unifying framework for many neural net algorithms, may provide a useful framework in which to attempt construction of new algorithms. Adding together bumps to approximate surfaces is a reasonable procedure to use when dealing with real valued inputs. It ties in to general approximation theory (c.f. Fourier series, or better yet, B splines), and can be quite successful as we have seen. Clearly some economy is gained by giving the neural net bumps to start with, instead of having the neural net form its own bumps from sigmoids. One way to do this would be to use multidimensional Gaussian functions with adjustable parameters. The situation is somewhat different when processing symbolic (binary val- ued) data. When input symbols are encoded into N bit bit-strings then one has well defined input values in an N dimensional input space. As shown above, one can learn the training set of input patterns by appropriately forming and placing bump surfaces over this space. This is an effective method for memorizing the training set, but a very poor method for obtaining correct predictions on new input data. The point is that, in contrast to real valued inputs that come from, say, a chaotic time series, the input points in symbolic processing problems are widely separated and the bumps do not add together to form smooth surfaces. Furthermore, each input bit string is a corner of an 2N vertex hypercube, and there is no sense in which one corner of a hypercube is surrounded by the other corners. Thus the commonly used input representation for symbolic processing problems requires that the neural net extrapolate the surface to make a new prediction for a new input pattern (i.e. new corner of the hypercube) and not interpolate, as is commonly the case for real valued inputs. Extrapolation is a farmore dangerous procedure than interpolation, and in view of the separated bumps of the training set one might expect on the basis of this argument that neural nets would fail dismally at symbol processing. This is not the case. The solution to this apparent conundrum, of course, is that although it is sufficient for a neural net to learn a symbol processing training set by forming bumps it is not necessary for it to operate in this manner. The simplest exam- ple of this occurs in the XOR problem. One can implement the input/output mapping for this problem by duplicating the hidden layer architecture of Figure (8) appropiately for two bumps ( i.e. 8 hid dens in layer 1, 2 hid dens in layer 2). As discussed above, for Boolean mappings, one can even eliminate the second hidden layer. However the architecture of Figure (9) will also suffice. OUTPUT Figure 9. Connectivity for XOR HIDDEN INPUT 455 Plotting the output of this network, Figure(9), as a function of the two inputs yields a ridge orientated to run between (0,1) and (1,0) Figure(lO). Thus a neural net may learn a symbolic training set without using bumps, and a high dimensional version of this process takes place in more complex symbol pro- cessing tasks.Ridge/ravine representations of the training data are considerably more efficient than bumps (less hidden neurons and weights) and the extended nature of the surface allows reasonable predictions i.e. extrapolations. 5. Conclusion. Figure 10 XOR surface (1, 1) Neural nets, in contrast to popular misconception, are capable of quite accurate number crunching, with an accuracy for the prediction problem we considered that exceeds conventional methods by orders of magnitude. Neural nets work by constructing surfaces in a high dimensional space, and their oper- ation when performing signal processing tasks on real valued inputs, is closely related to standard methods of functional ,,-pproximation. One does not need more than two hidden layers for processing real valued input data, and the ac- curacy of the approximation is controlled by the number of neurons per layer, and not the number of layers. We emphasize that although two layers of hidden neurons are sufficient they may not be efficient. Multilayer architectures may provide very efficient networks (in the sense of number of neurons and number of weights) that can perform accurately and with minimal cost. Effective prediction for symbolic input data is achieved by a slightly differ- ent method than that used for real value inputs. Instead of forming localized bumps (which would accurately represent the training data but would not pre- dict well on new inputs) the network can use ridge/ravine like surfaces (and generalizations thereof) to efficiently represent the scattered input data. While neural nets generally perform prediction by interpolation for real valued data, they must perform extrapolation for symbolic data if the usual bit representa- tions are used. An outstanding problem is why do tanh representations seem to extrapolate well in symbol processing problema? How do other functional bases do? How does the representation for symbolic inputs affect the ability to extra~ olate? This geometrical viewpoint provides a unifyimt framework for examimr: 456 many neural net algorithms, for suggesting questions about neural net operation, and for relating current neural net approaches to conventional methods. Acknowledgment. We thank Y. C. Lee, J. D. Farmer, and J. Sidorovich for a number of valuable discussions. References C. Barnes, C. Burks, R. Farber, A. Lapedes, K. Sirotkin, ""Pattern Recognition by Neural Nets in Genetic Databases"", manuscript in preparation J. Denker et. al.,"" Automatic Learning, Rule Extraction,and Generalization"", ATT, Bell Laboratories preprint, 1987 D. Farmer, J.Sidorowich, Phys.Rev. Lett., 59(8), p. 845,1987 H. Haken, Phys. Lett. A53, p77 (1975) A. Lapedes, R. Farber ""Nonlinear Signal Processing Using Neural Networks: Prediction and System Modelling"", LA-UR87-2662,1987 Y.C. Lee, Physica 22D,(1986) R. Lippman, IEEE ASAP magazine,p.4, 1987 D. Ruelle, F. Takens, Comm. Math. Phys. 20, p167 (1971) D. Rummelhart, J. McClelland in ""Parallel Distributed Processing"" Vol. 1, M.I.T. Press Cambridge, MA (1986) D. Russel et al., Phys. Rev. Lett. 45, pU75 (1980) T. Sejnowski et al., ""Net Talk: A Parallel Network that Learns to Read Aloud,"" Johns Hopkins Univ. preprint (1986) H. Swinney et al., Physics Today 31 (8), p41 (1978) F. Takens, ""Detecting Strange Attractor in Turbulence,"" Lecture Notes in Math- ematics, D. Rand, L. Young (editors), Springer Berlin, p366 (1981) K. Tomita et aI., J. Stat. Phys. 21, p65 (1979)","[-0.12435459345579147, -0.10179489105939865, 0.0742512121796608, 0.004045819863677025, -0.078097864985466, 0.0053307837806642056, 0.0004281917936168611, -0.02611212432384491, 0.0414557084441185, -0.09906022250652313, -0.05624318867921829, -0.033355411142110825, -0.022211844101548195, 0.016739021986722946, -0.11382760107517242, -0.004958766046911478, -0.011409923434257507, 0.01425799261778593, -0.06074581295251846, -0.04564579203724861, 0.02408304437994957, -0.05220290273427963, -0.02958301454782486, -0.0019657339435070753, 0.06177027150988579, 0.004667904693633318, -0.07698208838701248, 0.034782525151968, 0.02846677228808403, 0.06504588574171066, 0.07374067604541779, -0.00432072626426816, -0.06077729910612106, 0.02388477884232998, -0.06826776266098022, 0.029281483963131905, -0.06016998365521431, 0.008350751362740993, 0.02502014860510826, -0.0017073382623493671, -0.012502862140536308, -0.06953348964452744, 0.03876872733235359, 0.037506282329559326, 0.12488251179456711, 0.002788722049444914, 0.02167437970638275, -0.05182764679193497, -0.02011597342789173, 0.03529178723692894, -0.07562471181154251, 0.03376580774784088, -0.01660788245499134, 0.030123185366392136, 0.06398032605648041, 0.029810402542352676, -0.0019087521359324455, 0.0415593683719635, -0.031190983951091766, 0.0031196053605526686, 0.0037363090086728334, -0.017776764929294586, 0.04621659591794014, -0.02300783060491085, -0.005568098742514849, 0.009152806363999844, -0.0012559007154777646, -0.030151816084980965, 0.032555777579545975, 0.03237762674689293, 0.0528533011674881, 0.10866028070449829, -0.05276964232325554, 0.029427403584122658, -0.0029237132985144854, 0.06178926303982735, 0.08340542763471603, 0.015344846993684769, 0.017406990751624107, -0.045703329145908356, 0.02968480996787548, -0.035640496760606766, 0.00312992837280035, -0.019572051241993904, 0.03809758275747299, -0.019068904221057892, -0.0271777231246233, 0.049370598047971725, 0.004884749185293913, -0.013695744797587395, 0.019363660365343094, -0.0042449492029845715, 0.05071352794766426, -0.01879497431218624, 0.09197301417589188, 0.08545713871717453, -0.001558599527925253, 0.027561558410525322, -0.03694929555058479, 0.10255075991153717, -0.005761708598583937, 0.019673841074109077, 0.019111666828393936, -0.07788761705160141, 0.07720256596803665, 0.02691623382270336, 0.1148962676525116, -0.025321148335933685, 0.04652996361255646, -0.1472684144973755, -0.012949295341968536, -0.010574246756732464, -0.010834806598722935, 0.05120216682553291, 0.0102538438513875, -0.028368476778268814, 0.05084070563316345, -0.03815196827054024, 0.02090444229543209, 0.07557418942451477, -0.04621574655175209, 0.03306327015161514, -0.036047857254743576, 0.055854037404060364, 0.05686791613698006, 0.01800781860947609, -0.104115791618824, 5.769684799612788e-33, -0.04729616269469261, 0.005372785497456789, 0.0450877845287323, -0.050100117921829224, 0.06709746271371841, -0.027669314295053482, -0.02212432399392128, -0.01480578538030386, 0.0702977105975151, 0.06746721267700195, -0.09368852525949478, 0.03402025252580643, -0.06395161896944046, 0.0051125516183674335, 0.04416592791676521, -0.06608276069164276, 0.018723944202065468, -0.04249391332268715, 0.009632821194827557, -0.11406436562538147, 0.015160790644586086, -0.09001350402832031, -0.007326231803745031, 0.0013293593656271696, 0.026007989421486855, 0.08133558928966522, 0.043011900037527084, -0.009451306425035, -0.0018357692752033472, 0.011302568949759007, -0.039988335222005844, 0.006939796730875969, -0.06548620760440826, -0.03351138159632683, 0.09768133610486984, -0.06172972545027733, 0.04043839871883392, -0.02085801400244236, 0.0283005740493536, 0.04674423858523369, -0.05233123525977135, -0.006626561749726534, -0.07201112806797028, 0.05339474231004715, -0.03264167159795761, -0.014273005537688732, 0.04085053503513336, 0.07843523472547531, 0.021264858543872833, -0.07036557793617249, -0.03179408982396126, 0.018592044711112976, -0.07512561231851578, -0.11538489907979965, 0.07410912215709686, -0.0568988062441349, 0.07863172143697739, 0.04723794758319855, 0.03940005227923393, 0.11896741390228271, -0.06573047488927841, -0.006548229139298201, -0.040726229548454285, -0.011431855149567127, -0.0414331778883934, 0.040469009429216385, -0.013676543720066547, 0.0581110455095768, 0.11210595816373825, 0.014202609658241272, -0.010412367060780525, 0.04893915727734566, 0.004314181860536337, -0.07652140408754349, 0.025241142138838768, 0.029236631467938423, 0.022439641878008842, -0.07417561113834381, -0.04600246623158455, 0.019193192943930626, 0.008170245215296745, 0.027543330565094948, -0.021820101886987686, -0.021414024755358696, -0.012701231054961681, -0.009764299727976322, 0.031376637518405914, -0.0022373029496520758, 0.01921096257865429, -0.041608452796936035, -0.0803237035870552, -0.05084500461816788, 0.05514612793922424, -0.09697864204645157, -0.11426711827516556, -5.7853365068939866e-33, -0.1222367137670517, 0.03598778322339058, -0.1266404241323471, 0.08992761373519897, -0.0317801833152771, -0.04647727310657501, -0.05843644589185715, -0.015262466855347157, 0.008097630925476551, 0.01656804420053959, -0.026247762143611908, 0.019702531397342682, 0.0564095601439476, -0.029360946267843246, -0.015648245811462402, -0.055708326399326324, -0.045253001153469086, -0.008498189970850945, 0.07210835069417953, 0.03293346241116524, -0.012853446416556835, 0.04758452996611595, -0.10183578729629517, 0.022763798013329506, -0.02200266160070896, 0.06414121389389038, -0.03767498582601547, 0.049234841018915176, 0.005954447668045759, 0.04296909272670746, -0.06123274937272072, -0.11266487091779709, 0.056754887104034424, -0.05610353499650955, 0.02323915995657444, 0.020411107689142227, 0.11214286834001541, -0.02048962004482746, -0.01572481542825699, -0.030150031670928, 0.06377903372049332, -0.05513039231300354, 0.017580671235919, -0.05409526824951172, -0.06579992920160294, 0.05193657800555229, -0.08062266558408737, 0.031093979254364967, -0.05169878900051117, 0.06574402004480362, 0.07291890680789948, -0.034366872161626816, -0.05794458091259003, 0.02282317355275154, 0.016997838392853737, 0.04509546607732773, -0.0427817665040493, 0.009801391512155533, 0.05386507138609886, 0.024226615205407143, -0.06603533774614334, -0.05167306959629059, 0.05791907012462616, -0.03707395866513252, 0.019589321687817574, 0.04892673343420029, -0.07952474802732468, 0.028076084330677986, 0.012045222334563732, -0.04326609522104263, 0.041687365621328354, -0.03851616010069847, 0.021175077185034752, 0.0681689977645874, -0.05964023247361183, -0.035562556236982346, 0.045853354036808014, 0.021526655182242393, 0.037433166056871414, 0.03159492090344429, 0.03557601198554039, -0.01742362417280674, 0.04581059142947197, 0.0165119469165802, -0.028346488252282143, 0.00577544467523694, 0.08281651884317398, 0.0368138886988163, 0.04382972791790962, -0.03512656316161156, 0.02081337571144104, 0.12057328224182129, -0.052150219678878784, 0.0299743190407753, -0.05574578791856766, -4.938311448654531e-08, -0.00319144525565207, 0.06109650805592537, 0.029494211077690125, 0.030323022976517677, 0.09338056296110153, -0.09046139568090439, 0.034938376396894455, -0.026935221627354622, -0.031232181936502457, -0.02243456058204174, 0.09362268447875977, -0.026173260062932968, -0.025144560262560844, -0.06721992045640945, 0.0456463024020195, 0.012149570509791374, 0.029653331264853477, -0.0626402497291565, 0.006654008757323027, -0.025202147662639618, -0.0011122393188998103, 0.05227584391832352, -0.02648666314780712, 0.02474992536008358, 0.05704471841454506, -0.050494130700826645, -0.12060213088989258, 0.1022673174738884, 0.03489692881703377, 0.025461100041866302, -0.029971446841955185, 0.005932609084993601, 0.05128004774451256, 0.002742285607382655, 0.03528035059571266, 0.015319505706429482, 0.0012884364696219563, 0.045136891305446625, -0.021028567105531693, -0.04306681454181671, -0.06086376681923866, 0.01073623076081276, -0.024172203615307808, -0.020008323714137077, 0.017069201916456223, -0.08635863661766052, 0.015827449038624763, -0.10887039452791214, 0.01201219018548727, 0.04433063790202141, 0.026465987786650658, -0.0018375872168689966, 8.059530955506489e-05, 0.04938782751560211, -0.010435523465275764, -0.051351334899663925, -0.05831953138113022, -0.08644787967205048, -0.10626498609781265, 0.07771316915750504, -0.04011110961437225, 0.0397135354578495, -0.013716175220906734, -0.014941421337425709]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\TemporalRepresentationsinaConnectionistSpeechSystem.pdf,Deep Learning,"A SELF-LEARNING NEURAL NETWORK A. Hartstein and R. H. Koch IBM - Thomas J. Watson Research Center Yorktown Heights, New York ABSTRACf We propose a new neural network structure that is compatible with silicon technology and has built-in learning capability. The thrust of this network work is a new synapse function. The synapses have the feature that the learning parameter is em- bodied in the thresholds of MOSFET devices and is local in char- acter. The network is shown to be capable of learning by example as well as exhibiting the desirable features of the Hopfield type networks. The thrust of what we want to discuss is a new synapse function for an artificial neuron to be used in a neural network. We choose the synapse function to be readily implementable in VLSI technology, rather than choosing a function which is either our best guess for the function used by real synapses or mathematically the most tractable. In order to demonstrate that this type of synapse function provides interesting behavior in a neural network, we imbed this type of function in a Hopfield {Hopfield, 1982} type network and provide the synapses with a Hebbian {Hebb, 1949} learning capability. We then show that this type of net- work functions in much the same way as a Hopfield network and also learns by example. Some of this work has been discussed previously {Hartstein, 1988}. Most neural networks, which have been described, use a multiplicative function for the synapses. The inputs to the neuron are multiplied by weighting factors and then the results are summed in the neuron. The result of the sum is then put into a hard threshold device or a device with a sigmoid output. This is not the easiest function for a MOSFET to perform although it can be done. Over a large range of parameters, a MOSFET is a linear device with the output current being a linear function of the input voltage relative to a threshold voltage. If one could directly utilize these characteristics, one would be able to design a neural network more compactly. 769 770 Hartstein and Koch We propose that we directly use MOSFETs as the input devices for the neurons in the network, utilizing their natural characteristics. We assume the following form for the input of each neuron in our network: V; = 0 ( 2: IIj - T;j I ) 1 (1) where V, is the output, ~ are the inputs and T,j are the learned threshold voltages. In this network we use a representation in which both the V's and the T's range from 0 to + 1. The result of the summation is fed into a non-linear sigmoid func- tion (0). All of the neurons in the network are interconnected, the outputs of each neuron feeding the inputs of every other neuron. The functional form of Eq. 1 might, for instance, represent several n-channel and p-channel MOSFETs in parallel. The memories in this network are contained in the threshold voltages, 1',}"" We implement learning in this network using a simple linear Hebbian {Hebb, 1949} learning rule. We use a rule which locally reinforces the state of each input node in a neuron relative to the output of that neuron. The equation governing this learning algorithm is: (2) where 1';j are the initial threshold voltages and T'j are the new threshold voltages after a time,.6.t. Here TJ is a small learning parameter related to this time period, and the offset factor O.S is needed for symmetry. Additional saturation con- straints are imposed to ensure that 1';j remain in the interval 0 to + 1. This learning rule is one which is linear in the difference between each input and output of a neuron. This is an enhancing/inhibiting rule. The thresholds are ad- justed in such a way that the output of the neuron is either pushed in the same direction as the input (enhancing), or pushed in the opposite direction (inhibit- ing). For our simple simulations we started the network with all thresholds at O.S and let learning proceed until some saturation occurred. The somewhat more so- phisticated method of including a relaxation term in Eq. 2 to slowly push the val- ues toward O.S over time was also explored. The results are essentially the same as for our simple simulations. The interesting question is if we form a network using this type of neuron, what will the overall network response be like? Will the network learn multiple states or will it learn a simple average over all of the states it sees? In order to probe the functioning of this network, we have performed simulations of this network on a digital computer. Each simulation was divided into two phases. The first was a learning phase in which a fixed number of random patterns were presented to the network sequentially for some period of time. During this phase the threshold A Self-Learning Neural Network 771 voltages were allowed to change using the rule in Eq. 2. The second was a testing phase in which learning was turned off and the memories established in the net- work were probed to determine the essential features of these learned memories. In this way we could test how well the network was able to learn the initial test patterns, how well the network could reconstruct the learned patterns when pre- sented with test patterns containing errors, and how the network responded to random input patterns. We have simulated this network using N fully interconnected neurons, with N in the range of 10 to 200. M random patterns were chosen and sequentially pre- sented to the network for learning. M typically ranged up to N/3. After the learning phase, the nature of the stable states in the network was tested. In gen- eral we found that the network is capable of learning all of the input patterns as long as M is not too large. The network also learns the inverse patterns (l's and O's interchanged) due to the inherent symmetry of the network. Additional ex- traneous patterns are learned which have no obvious connection to the intended learned states. These may be analogous to either the spin glass states or the mixed pattern states discussed for the multiplicative network {Amit, 1985}. Fig. 1 shows the capacity of a 100 neuron network. We attempted to teach the network M states and then probed the network to see how many of the states were successfully learned. This process was repeated many times until we achieved good statistics. We have defined successful learning as 1000;6 accuracy. A more relaxed definition would yield a qualitatively similar curve with larger capacity. The functional form of the learning is peaked at a fixed value of the number of input patterns. For a small number of input patterns, the network essentially learns all of the patterns. Deviations from perfect learning here generally mean 1 bit of information was learned incorrectly. Near the peak the results become more noisy for different learning attempts. Most errors are still only 1 or 2 bits! but the learning in this region becomes marginal as the capacity of the network is approached. For larger values of the number of input patterns the network be- comes overloaded and it becomes incapable of learning most of the input states. Some small number of patterns are still learned, but the network is clearly not functioning well. Many of the errors in this region are large, showing little corre- lation with the intended learned states. This functional form for the learning in the network is the same for all of the net- work sizes tested. We define the capacity of the network as the average value of the peak number of patterns which can be successfully learned. The inset to Fig. 1 shows the memory capacity of a number of tested networks as a function of the size of the network. The network capacity is seen to be a linear function of the network size. The capacity is proportional to the number of T./s specified. In this 772 Hartstein and Koch example the network capacity was f ouod to be about 8010 of the maximum possi- ble for binary information. This rather low figure results from a trade-off of ca- pacity for the partic\Jlar types of functions that a neural network can perform. It is possible to construct simple memories with 1000.,.6 capacity. N 0 100 200 25------------------------~--------------~ 20 ] ~ 20 ~ 15 E o ~ '0 10 ~ ~ E ~ z 5 , , , '. 10 , , , 0 • o~--~----~--~----~--~ o 10 20 30 40 50 .?;- '0 C 0.. 0 U Figure 1. The number of successfully learned patterns as a func- tion of the number of input patterns for a 100 neuron network. The dashed curve is for perfect learning. The inset shows the memory capacity of a threshold neural network as a function of the size of the network. Some important measures of learning in the network are the distribution of stable states in the network after learning has taken place. and the basin of attraction r or each stable point. One can gain a handle on these parameters by probing the network with random test patterns after the network has learned M states. Fig. 2 shows the averaged results of such tests for a 100 neuron network and varying numbers of learned states. The figure shows the probability of finding particular states. both learned and extraneous. The states are ordered first by decreasing A Self-Learning Neural Network 773 probability for the learned states, followed by decreasing probability for the ex- traneous states. It is clear from the figure that both types of stable states are present in the network. It is also clear that the probabilities of finding different patterns are not equal. Some learned states are more robust than others, that is they have larger basins of attraction. This network model does not partition the available memory space equally among the input patterns. It also provides a large amount of memory space for the extraneous states. Clearly, this is not the opti- mum situation. 0.8 ~ (a) 0.6 Learned Q) 0.4 -.s (/) 0') 0.2 L Extraneous c: ~ 0.0 --- c: G: ..... 0.8 0 .b (b) ~ 0.6 :0 Extraneous e ~ 0.4 0.2 Learned 0.0 0 5 10 15 20 25 30 State Figure 2. The probability of the network finding a specific pat- tern. Both learned states and extraneous states are found. The figure was obtained for a 100 neuron network. Fig. 2a is for 5 learned patterns and 2b is for 10 learned patterns. Some of the learned states appear to have 0 probability of being found in this simulation. Some of these states are not stable states of the network and will never be found. This is particularly true-when the number of learned states is close to or exceeds the capacity of the network. Others of these states simply have an extremely small probability of being found in a random search because they have small basins of attraction. However, as discussed below, these are still viable states. When the network learns fewer states than its capacity (Fig. 2a), 774 Hartstein and Koch most of the stable states are the learned states. As the capacity is approached or exceeded, most of the stable states are extraneous states. The results shown in Fig. 2 address the question of the networks tolerance to er- rors. A pattern, which has a large basin of attraction, will be relatively tolerant to errors when being retrieved, whereas, a pattern, which has a small basin of at- traction, will be less tolerant of errors. The immunity of the learned patterns to errors in being retrieved can also be tested in a more direct way. One can probe the network with test patterns which start out as the learned patterns, but have a certain number of bits changed randomly. One then monitors the final pattern which the networks finds and compares to the known learned pattern . .$ .s (I) ""i 0.8 E o !3 t7' c: ;:; c: 0.6 ~ 04 '0 · ~ ~ 0.2 e 0.. • • 0.0 '------.1--.-.-.--___ .... ..-.4 ... o 10 20 30 40 Hamming Distance Figure 3. Probability of the network finding a specific learned state when the input pattern has a certain Hamming distance. This figure was obtained for a 100 neuron network which was taught 10 random patterns. Fig. 3 shows typical results of such a calculation. The probability of successfully retrieving a pattern is shown as a function of the Hamming distance. the number of bits which were randomly changed in the test pattern. For this simulation a tOO neuron network was used and it was taught 10 patterns. For small Hamming distances the patterns are successfully found 100°,.6 of the time. As the Hamming distance gets larger the network is no longer capable of finding the desired pat- tern. but rather finds one of the other fixed points. This result is a statistical av- A Self-Learning Neural Network 775 erase over all of the states and therefore tends to emphasize patterns with small basins of attraction. This is just the opposite of the types of states emphasized in the analysis shown in Fig. 2. We can define the maximum Hamming distance as the Hamming distance at which the probability of finding the learned state has dropped to SO%. Fig. 4 shows the maximum Hamming distance as a function of the number of learned states in our 100 neuron network. As one expects the maximum Hamming dis- tance gets smaller as the number of learned states increases. Perhaps surprisingly, the relationship is linear. These results are important since one requires a rea- sonable maximum Hamming distance for any real system to function. These considerations also shed some light on the nature of the functioning of the net- work and its ability to learn. 60 CI) (.) c ~ i5 40 0 c ·e E 0 :c § 20 . E .- ~ ::E 0 0 5 10 15 20 M Figure 4. The maximum Hamming distance for a given number of learned states. Results are for a 100 neuron network. This simulation gives us a picture of the way in which the network utilizes its phase space to store information. When only a few patterns are stored in the network, the network divides up the available space among these memories. The learning process is almost always successful. When a larger number of learn~ patterns are attempted, the available space is now divided among more memories. The maximum Hamming distance decreases and more space is taken up byex- traneous states. When the memory capacity is exceeded, the phase space allo- 776 Hartstein and Koch cated to any successful memory is very small and most of the space is taken up by extraneous states. The types of behavior we have described are similar to those found in the Hopfield type memory utilizing multiplicative synapses. In fact our central point is that by using a completely different type of synapse function, we can obtain the same behavior. At the same time we argue since this network was proposed using a synapse function which mirrors the operating characteristics of MOSFETs, it will be much easier to realize in hardware. Therefore, we should be able to con- struct a smaller more tolerant network with the same operating characteristics. We do not mean to imply that the type of synapse function we have explored can only be used in a Hopfield type network. In fact we feel that this type of neuron is quite general and can successfully be utilized in any type of network. This is at present just a conjecture which needs to be explored more fully. Perhaps the most important message from our work is the realization that one need not be constrained to the multiplicative type of synapse, and that other forms of synapses can perform similar functions in neural networks. This may open up many new avenues of investigation. REFERENCES D.l. Amit, H. Gutfreund and H. Sompolinsky, Phys. Rev. A32, 1001 (1985). A. Hartstein and R.H. Koch, IEEE Int. Conf. on Neural Networks, (SOS Printing, San Diego, 1988), Vol. I, 425. D. Hebb, The Organization of Behaviour, (Wiley, New York, 1949). 1.1. Hopfield, Proc. Natl. Acad. Sci. USA 79, 2554 (1982).","[-0.09283441305160522, -0.09559958428144455, 0.0246629249304533, 0.011693142354488373, -0.045523326843976974, -0.0024062860757112503, 0.08086935430765152, -0.007419901434332132, 0.05293487757444382, -0.06257101148366928, 0.07161080092191696, 0.012772560119628906, -0.0755508691072464, 0.02717154286801815, -0.0274299755692482, 0.07022171467542648, -0.008830619975924492, 0.008463302627205849, -0.023698365315794945, -0.04553147405385971, 0.0381929948925972, -0.04190046340227127, -0.0292144063860178, 0.018639622256159782, -0.0766647532582283, 0.004512475803494453, 0.011396910063922405, -0.01434246450662613, -0.05312152951955795, -0.05503327026963234, 0.04702911525964737, -0.011200490407645702, -0.025838444009423256, 0.021748434752225876, -0.09471208602190018, 0.05006866157054901, -0.03353403881192207, -0.014043361879885197, 0.045826736837625504, -0.03621659427881241, -0.005046111065894365, -0.03474431484937668, 0.0343463309109211, 0.035662032663822174, 0.06679247319698334, 0.06276656687259674, 0.027498040348291397, -0.02982972376048565, 0.024186398833990097, -0.0069974083453416824, -0.015327469445765018, 0.04857385531067848, 0.00613813754171133, 0.019016750156879425, 0.06594053655862808, 0.0938546434044838, -0.053307924419641495, 0.027769330888986588, -0.0493207722902298, 0.030272874981164932, 0.010108024813234806, -0.039054982364177704, 0.04563033953309059, -0.012555992230772972, 0.040142662823200226, 0.01250107679516077, 0.016226330772042274, 0.011280681006610394, 0.0621938593685627, -0.018671825528144836, 0.013962830416858196, 0.058256860822439194, -0.046690426766872406, 0.02293028123676777, 0.05529865249991417, 0.03511232137680054, 0.04168019816279411, 0.07284722477197647, 0.08744671940803528, -0.055543746799230576, -0.007167411502450705, 0.0151063846424222, -0.03193658962845802, 0.01959466189146042, 0.07126116752624512, 0.03513506427407265, -0.08234931528568268, 0.003694182727485895, 0.024006491526961327, -0.032792672514915466, -0.031702376902103424, -0.04751250892877579, -0.09125488996505737, -0.1243484690785408, 0.030569225549697876, -0.10322520136833191, 0.04610052332282066, -0.052214860916137695, -0.04786054044961929, 0.04329197108745575, -0.029617957770824432, 0.04361940920352936, 0.037259604781866074, -0.032673180103302, 0.00915216188877821, 0.023530123755335808, 0.03854838013648987, 0.055999163538217545, 0.009821786545217037, -0.06114070862531662, -0.02386827953159809, -0.0034371481742709875, -0.049989424645900726, -0.05507122352719307, 0.01847742311656475, 0.03690634295344353, 0.009091290645301342, 0.03356008976697922, 0.04280269145965576, 0.034050386399030685, -0.09035871922969818, 0.03371043875813484, -0.10441160947084427, -0.01388587523251772, -0.10149459540843964, -0.054221950471401215, -0.14510959386825562, 3.708900673931281e-33, -0.033589400351047516, 0.06293357163667679, -0.014968827366828918, -0.09085690975189209, -0.00028926751110702753, -0.008968868292868137, 0.008503121323883533, -0.005337374750524759, 0.04823005944490433, -0.03279053792357445, -0.05983737111091614, -0.06015172600746155, 0.005856080912053585, 0.041353728622198105, 0.09335869550704956, -0.06494041532278061, -0.06134820729494095, -0.12609460949897766, 0.02515001967549324, -0.12767980992794037, 0.0614263154566288, -0.006382191553711891, 0.03331489488482475, -0.05800449475646019, -0.018715642392635345, -0.05165349692106247, 0.03954169154167175, 0.0032464182004332542, -0.06361691653728485, 0.01581478677690029, -0.025822030380368233, 0.0490788035094738, -0.0624849908053875, -0.06037802994251251, 0.05880708247423172, -0.004537865519523621, -0.08878260850906372, -0.007760006934404373, 0.10466336458921432, -0.02696521393954754, -0.005789350252598524, 0.006557390093803406, 0.02097245305776596, -0.03901834413409233, -0.07805941253900528, -0.029693854972720146, 0.05388865992426872, 0.0653567835688591, 0.009195258840918541, -0.131230428814888, -0.03816964104771614, -0.03060152567923069, -0.031403303146362305, -0.09681235998868942, 0.11393477022647858, 0.020968927070498466, 0.07224035263061523, 0.10168485343456268, 0.06400317698717117, 0.15027780830860138, 0.017412282526493073, 0.006265808828175068, -0.004235656466335058, 0.06258178502321243, 0.031080925837159157, 0.08006425201892853, -0.0015492613893002272, -0.009398731403052807, 0.05810387060046196, -0.04843660816550255, 0.05696369707584381, 0.07221804559230804, -0.057926032692193985, -0.052742209285497665, 0.008138920180499554, -0.012389245443046093, -0.00585614237934351, -0.09481813758611679, 0.02021719142794609, -0.005353535991162062, -0.009930195286870003, -0.055467721074819565, -0.02610377036035061, 0.04192010313272476, -0.029633602127432823, 0.000451182306278497, 0.06691128760576248, -0.051607657223939896, -0.08605001866817474, 0.008554540574550629, -0.059918373823165894, -0.03312579169869423, 0.05692755803465843, -0.024580959230661392, 0.005102888680994511, -5.562455798480373e-33, -0.047205809503793716, 0.039842814207077026, -0.03151743486523628, -0.007475178688764572, -0.041322678327560425, 0.049412183463573456, 0.018758656457066536, 0.03880847990512848, -0.0413670614361763, 0.022887594997882843, -0.031800977885723114, 0.10073542594909668, 0.04386506974697113, -0.004421671386808157, 0.007915737107396126, 0.0876750499010086, -0.12637270987033844, -0.017001064494252205, 0.07774188369512558, -0.03421828895807266, -0.015490110963582993, 0.13262580335140228, -0.03361504152417183, 0.07627587765455246, -0.04971292242407799, -0.011969177052378654, -0.06968356668949127, 0.11654848605394363, 0.009228995069861412, 0.048971112817525864, -0.024749401956796646, -0.011887045577168465, -0.0274521391838789, 0.008559576235711575, 0.05229092389345169, 0.042150408029556274, 0.0816117525100708, -0.03361283242702484, 0.04654422402381897, -0.03226863965392113, 0.06549647450447083, -0.048579901456832886, -0.002245806623250246, -0.01637139357626438, 0.017775079235434532, -0.02935938909649849, -0.03618438541889191, 0.07049188762903214, -0.0645199865102768, 0.008668073453009129, -0.04114629328250885, -0.06498150527477264, -0.05646657198667526, -0.06037362664937973, -0.10136812180280685, 0.06030850484967232, 0.02741931937634945, -0.014340773224830627, 0.07617439329624176, -0.028555383905768394, -0.06917461007833481, -0.09654387831687927, 0.1067575141787529, 0.017051441594958305, -0.03319956362247467, -0.09132847189903259, -0.022159503772854805, 0.057248909026384354, 0.0349404402077198, -0.027240891009569168, 0.049219198524951935, 0.026142623275518417, 0.08307190984487534, -0.02985241450369358, -0.04196131229400635, -0.10843934118747711, 0.012189583852887154, 0.013153815641999245, 0.021748756989836693, -0.009228180162608624, -0.00016750223585404456, -0.00445563392713666, -0.0006310984026640654, 0.004036972299218178, 0.10867294669151306, 0.029288437217473984, 0.002452222863212228, 0.014180640690028667, 0.022549862042069435, -0.021312525495886803, 0.02835756726562977, 0.055351290851831436, -0.06257238984107971, 0.05298975482583046, -0.018988730385899544, -5.05352950597171e-08, 0.005364709999412298, 0.010716401971876621, 0.05055990442633629, 0.011264147236943245, 0.06847359985113144, -0.03132334724068642, 0.06735813617706299, -0.012509373016655445, -0.055281005799770355, -0.018460333347320557, 0.04438537359237671, 0.009811097756028175, -0.02593119628727436, -0.027691420167684555, -0.006218251306563616, 0.09660864621400833, 0.029993891716003418, 0.027724608778953552, 0.009253471158444881, -0.022178854793310165, 0.08359881490468979, -0.054693836718797684, 0.03667915239930153, 0.026899928227066994, 0.014206835068762302, -0.08456168323755264, -0.08215057104825974, -0.03010435588657856, -0.06062588840723038, 0.04312795400619507, 0.016731569543480873, 0.02317921817302704, 0.045071106404066086, -0.023219149559736252, 0.015964264050126076, 0.1055242270231247, -0.0077768657356500626, -0.018315598368644714, -0.03644050285220146, -0.01379708107560873, -0.05830271169543266, 0.048256684094667435, -0.01431019976735115, 0.015096563845872879, -0.013929758220911026, -0.07495823502540588, -0.011721321381628513, -0.021064195781946182, 0.012953267432749271, 0.038603756576776505, 0.029794078320264816, 0.028938524425029755, 0.04202650487422943, -0.014650996774435043, 0.0682692602276802, -0.022479038685560226, -0.016934731975197792, -0.02824704349040985, -0.03723566234111786, 0.09795709699392319, 0.030019644647836685, 0.06542574614286423, 0.018552038818597794, 0.01352864783257246]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\TheBoltzmannPerceptronNetworkAMultiLayeredFeedForwardNetworkEquivalenttotheBoltzmannMachine.pdf,Computer Vision,"384 MODELING SMALL OSCILLATING BIOLOGICAL NETWORKS IN ANALOG VLSI Sylvie Ryckebusch, James M. Bower, and Carver Mead California Instit ute of Technology Pasadena, CA 91125 ABSTRACT We have used analog VLSI technology to model a class of small os- cillating biological neural circuits known as central pattern gener- ators (CPG). These circuits generate rhythmic patterns of activity which drive locomotor behaviour in the animal. We have designed, fabricated, and tested a model neuron circuit which relies on many of the same mechanisms as a biological central pattern generator neuron, such as delays and internal feedback. We show that this neuron can be used to build several small circuits based on known biological CPG circuits, and that these circuits produce patterns of output which are very similar to the observed biological patterns. To date, researchers in applied neural networks have tended to focus on mam- malian systems as the primary source of potentially useful biological information. However, invertebrate systems may represent a source of ideas in many ways more appropriate, given current levels of engineering sophistication in building neural-like systems, and given the state of biological understanding of mammalian circuits. In- vertebrate systems are based on orders of magnitude smaller numbers of neurons than are mammalian systems. The networks we will consider here, for example, are composed of about a dozen neurons, which is well within the demonstrated capabilities of current hardware fabrication techniques. Furthermore, since much more detailed structural information is available about these systems than for most systems in higher animals, insights can be guided by real information rather than by guesswork. Finally, even though they are constructed of small numbers of neurons, these networks have numerous interesting and potentially even useful properties. CENTRAL PATTERN GENERATORS Of all the invertebrate neural networks currently being investigated by neurobi- ologists, the class of networks known as central pattern generators (CPGs) may be especially worthy of attention. A CPG is responsible for generating oscillatory neural activity that governs specific patterns of motor output, and can generate its pattern of activity when isolated from its normal neuronal inputs. This prop- Modeling Small Oscillating Biological Networks 385 erty, which greatly facilitates experiments, has enabled biologists to describe several CPGs in detail at the cellular and synaptic level. These networks have been found in all animals, but have been extensively studied in invertebrates [Selverston, 1985]. We chose to model several small CPG networks using analog VLSI technology. Our model differs from most computer simulation models of biological networks [Wilson and Bower, in press] in that we did not attempt to model the details of the individual ionic currents, nor did we attempt to model each known connection in the networks. Rather, our aim was to determine the basic functionality of a set of CPG networks by modeling them as the minimum set of connections required to reproduce output qualitatively similar to that produced by the real network under certain conditions. MODELING CPG NEURONS The basic building block for our model is a general purpose CPG neuron circuit. This circuit, shown in Figure 1, is our model for a typical neuron found in central pattern generators, and contains some of the essential elements of real biological neurons. Like real neurons, this model integrates current and uses positive feedback to output a train of pulses, or action potentials, whose frequency depends on the magnitude of the current input. The part of the circuit which generates these pulses is shown in Figure 2a [Mead, 19891. The second element in the CPG neuron circuit is the synapse. In Figure 1, each pair of transistors functions as a synapse. The p-well transistors are. excitatory synapses, whereas the n-well transistors are inhibitory synapses. One of the transistors in the pair sets the strength of the synapse, while the other transistor is the input of the synapse. Each CPG neuron has four different synapses. The third element of our model CPG neuron involves temporal delays. Delays are an essential element in the function of CPGs, and biology has evolved many different mechanisms to introduce delays into neural networks. The membrane capacitance of the cell body, different rates of chemical reactions, and axonal transmission are just a few of the mechanisms which have time constants associated with them. In our model we have included synaptic delay as the principle source of delay in the network. This is modeled as an RC delay, implemented by the follower-integrator circuit shown in Figure 2b [Mead, 19891. The time constant of the delay is a function of the conductance of the amplifier, set by the bias G. A multiple time constant delay line is formed by cascading several of these elements. Our neuron circuit uses a delay line with three time constants. The synapses which are before the delay element are slow synapses, whereas the undelayed synapses are fa.st synapses. We fabricated the circuit shown in Figure 1 using CMOS, VLSI technology. Several of these circuits were put on each chip, with all of the inputs and controls going out to pads, so that these cells could be externally connected to form the network of interest. 386 Ryckebusch, Bower, and Mead slow excitation -t G Figure 1. The CPG neuron circuit. r I- Pulse Length ~ (a) Yout. pulse length Yout. -111 0 -QJ- (b) Figure 2. (a). The neuron spike-generating circuit. (b). The follower-integrater circuit. Each delay box 0 contains a delay line formed by three follower-integrater circuits. The Endogenous Bursting Neuron One type of cell which has been found to play an important role in many oscilla- tory circuits is the endogenous bursting neuron. This type of cell has an intrinsic oscillatory membrane potential, enabling it to produce bursts of action potentials at rhythmic intervals. These cells have been shown to act both as external ""pace- makers"" which set the rhythm for the CPG, or as an integral part of a central pattern generator. Figure 3a shows the output from a biological endogenous burst- ing neuron. Figure 3b demonstrates how we can configure our CPG neuron to be an endogenous bursting neuron. The delay element in the cell must have three time constants in order for this circuit to oscillate stably. Note that in the circuit, the Modeling Small Oscillating Biological Networks 387 cell has internal negative feedback. Since real neurons don't actually make synaptic connections onto themselves, this connection should be thought of as representing an internal molecular or ionic mechanism which results in feedback within the cell. 4 mV AS 1 sec (a) (b) (c) Figure 3. (a). The output from the AB cell in the lobster stomatogastric ganglion CPG [Eisen and Marder, 1982]. This cell is known to burst endogenously. (b). The CPG neuron circuit configured as an endogenous bursting neuron and (c) the output from this circuit. Postinhibitory Rebound A neuron configured to be an endogenous burster also exhibits another property common to many neurons, including many CPG neurons. This property, illustrated in Figures 4a and 4b, is known as postinhibitory rebound (PIR). Neurons with this property display increased excitation for a certain period of time following the release of an inhibitory influence. This property is a useful one for central pattern generator neurons to have, because it enables patterns of oscillations to be reset following the release of inhibition. 388 Ryckebusch, Bower and Mead -(a) (b) I N' "" ... .. I .. • ' LA 4 t •••• I ........ (c) Figure 4. (a) The output of a ganglion cell of the mudpuppy retina exhibiting postinhibitory rebound [Miller and Dacheux, 19761. The bar under the trace indi- cates the duration of the inhibition. (b) To exhibit PIR in the CPG neuron circuit, we inhibit ,the cell with the square pulse shown in (c). When the inhibition is released, the circuit outputs a brief burst of pulses. MODELING CENTRAL PATTERN GENERATORS The Lobster Stomatogastric Ganglion The stomatogastric ganglion is a CPG which controls the movement of the teeth in the lobster's stomach. This network is relatively complex, and we have only modeled the relationships between two of the neurons in the CPG (the PD and LP cells) which have a kind of interaction found in many CPGs known as reciprocal inhibition (Figure Sa). In this case, each cell inhibits the other, which produces a pattern of output in which the cells fire alternatively (Figure Sb). Note that in the absence of external input, a mechanism such as postinhibitory rebound must exist in order for a cell to begin firing again once it has been released from inhibition. (b) Modeling Small Oscillating Biological Networks 389 (a) (d) (c) _ 120 ~rrN Figure 5. (a) Output from the PD and LP cells in the lobster stomatogastric ganglion [Miller and Selverston, 1985]. (c) and (d) demonstrate reciprocal inhibition with two CPG neuron circuits. The Locust Flight CPG A CPG has been shown to play an important role in producing the motor pattern for flight in the locust [Robertson and Pearson, 19851. Two of the cells in the CPG, the 301 and 501 cells, fire bursts of action potentials as shown in Figure 6a. The 301 cell is active when the wings of the locust are elevated, whereas the 501 cell is active when the wings are depressed. The phase relationship between the two cells is very similar to the reciprocal inhibition pattern just discussed, but the circuit that produces this pattern is quite different. The connections between these two cells are shown in Figure 6b. The 301 cell makes a delayed excitatory connection onto the 501 cell, and the 501 cell makes fast inhibitory contact with the 301 cell. Therefore, the 301 cell begins to fire, and after some delay, the 501 cell is activated. When the 501 cell begins to fire, it immediately shuts off the 301 cell. Since the 501 cell is no longer receiving excitatory input, it will eventually stop firing, releasing the 301 cell from inhibition. The cycle then repeats. This same circuit has been reproduced with our model in Figures 6c and 6d. 390 Ryckebusch, Bower and Mead • ---.J50mv ~ 100ms \5'~'1 I--~ 501 301~~~~=-~~~~~~ __ ~ __ ~~ Dl (a) (b) 301 301 CELL SOl CELL (e) (d) Figure 6. (a) The 301 and 501 cells in the locust flight CPG [Robertson and Pearson, 19851. (b) Simultaneous intracellular recordings of 301 and 501 during flight. (c) The model circuit and (d) its output. The Tritonia Swim CPG One of the best studied central pattern generators is the CPG which controls the swimming in the small marine mollusc Tritonia. This CPG was studied in great detail by Peter Getting and his colleagues at the University of Iowa, and it is one of the few biological neural networks for which most of the connections and the synaptic parameters are known in detail. Tritonia swims by making alternating dorsal and ventral flexions. The dorsal and ventral motor neurons are innervated by the DSI and VSI cells, respectively. Shown in Figure 7a and 7b is a simplified schematic diagram for the network and the corresponding output. The DSI and VSI cells fire out of phase, which is consistent with the alternating nature of the animal's swimming motion. The basic circuit consists of reciprocal inhibition between DSI and VSI paralleled by delayed excitation via the C2 cell. The DSI and VSI cells fire out of phase, and the DSI and C2 cells fire in phase. Swimming is initiated by sensory stimuli which feed into DSI and cause it to begin to fire a burst of impulses. DSI inhibits VSI, and at the same time excites C2. C2 has excitatory synapses on VSIj however, the initial response of VSI neurons is delayed. VSI then fires, during which there is inhibition by VSI of C2 and DSI. During this period, VSI no longer receives excitatory input from C2, and hence the VSI firing rate declines; DSI is therefore released from inhibition, and is ready to fire again to initiate a new cycle. Figure 7c and 8 show the model circuit which is identical to the circuit Modeling Small Oscillating Biological Networks 391 shown in Figure 7a, and the output from this circuit. Note that although the model output closely resembles the biological data, there are small differences in the phase relationships between the cells which can be accounted for by taking into account other connections and delays in the circuit not currently incorporated in our model. (a) 1-0 OSI : I VSI B C2 - .' .~ .' .' ........... . - - --I \ \ .. \ ..... \ .. .. \ ..... \ ..... , ...... \ DSI t' ... , ...... ' , , , \ ,---------- (c) (b) C2 VSI 50 mV J 5 sec Figure 1. (a) Simplified schematic diagram of the Tritonia CPG (which actually has 14 cells) and (b) output from the three types of cells in the circuit.(c) The model circuit. 392 Ryckebusch, Bower and Mead VSI C2 DSI Figure 8. Output from the circuit shown in Figure 7c. CONCLUSIONS One may ask why it is interesting to model these systems in analog VLSI, or, for that matter, why it is interesting to model invertebrate networks altogether. Analog VLSI is a very nice medium for this type of modeling, because in addition to being compact, it runs in real time, eliminating the need to wait hours to get the results of a simulation. In addition, the electronic circuits rely on the same physical principles as neural processes {including gain, delays, and feedback}, allowing us to exploit the inherent properties of the medium in which we work rather than having to explicitly model them as in a digital simulation. Like all models, we hope that this work will help us learn something about the systems we are studying. But in addition, although invertebrate neural networks are relatively simple and have small numbers of cells, the behaviours of these networks and animals can be fairly complex. At the same time, their small size allows us to understand how they are engineered in detail. Accordingly, modeling these networks allows us to study a well engineered system at the component level-a level of modeling not yet possible for more complex mammalian systems, for which detailed structural information is scarce. Modeling Small Oscillating Biological Networks 393 Acknowledgments This work relies on information supplied by the hard work of many experimentalists. We would especially like to acknowledge the effort and dedication of Peter Getting who devoted 12 years to understanding the organization of the Tritonia network of 14 neurons. We also thank Hewlett-Packard for computing support, and DARPA and MOSIS for chip fabrication. This work was sponsored by the Office of Naval Research, the System Development Foundation, and the NSF (EET-8700064 to J.B.). Reference8 Eisen, Judith S. and Marder, Eve (1982). Mechanisms underlying pattern gener- ation in lobster stomatogastric ganglion as determined by selective inactivation of identified neurons. III. Synaptic connections of electrically coupled pyloric neurons. J. Neurophysiol. 48:1392-1415. Getting, Peter A. and Dekin, Michael S. (1985). Tritonia swimming: A model system for integration within rhythmic motor systems. In Allen I. Selverston (Ed.), Model Neural Networks and Behavior, New York, NY: Plenum Press. Mead, Carver A. (in press). Analog VLSI and Neural Systems. Reading, MA: Addison-Wesley. Miller, John P. and Selverston, Allen I. (1985). Neural Mechanisms for the produc- tion of the lobster pyloric motor pattern. In Allen I. Selverston (Ed.), Model Neural Networks and Behavior, New York, NY: Plenum Press. Miller, R. F. and Dacheux, R. F. (1976). Synaptic organization and ionic basis of on and off channels in mudpuppy retina. J. Gen. Physiol. 67:639-690. Robertson, R. M. and Pearson, K. G. (1985). Neural circuits in the ft.ight system of the locust. J. Neurophysiol. 53:110-128. Selverston, Allen I. and Moulins, Maurice (1985). Oscillatory neural networks. Ann. Rev. Physiol. 47:29-48. Wilson, M. and Bower, J. M. (in press). Simulation oflarge scale neuronal networks. In C. Koch and I. Segev (Eds.), Methods in Neuronal Modeling: From Synapses to Networks, Cambridge j MA: MIT Press.","[-0.09319544583559036, -0.08350309729576111, 0.052808403968811035, -0.02570999599993229, -0.07318122684955597, -0.016653336584568024, -0.05471774563193321, -0.054084908217191696, 0.08279450237751007, 0.012095468118786812, 0.015424118377268314, -0.06067100539803505, -0.02647978812456131, -0.02347915805876255, -0.02996194176375866, 0.03042936697602272, -0.04094310849905014, 0.0569804385304451, 0.0022174615878611803, -0.0019311296055093408, 0.0505937784910202, 0.06474584341049194, -0.030953777953982353, 0.04009244218468666, -0.07565241307020187, 0.01904347911477089, -0.05775170773267746, -0.0046011474914848804, -0.0006060022860765457, -0.055001989006996155, 0.07994525134563446, -0.028334250673651695, 0.019678547978401184, -0.027052145451307297, -0.06129688024520874, 0.07032780349254608, -0.021448826417326927, -0.046651389449834824, 0.02627885714173317, -0.006741910241544247, 0.09447916597127914, -0.013031098991632462, 0.07745040953159332, 0.0757269412279129, 0.004029387142509222, 0.004503399133682251, 0.01566949114203453, -0.08173868060112, -0.01730695739388466, -0.0724661722779274, -0.0009190106065943837, -0.03552905097603798, 0.030869152396917343, 0.008944874629378319, 0.03514580428600311, 0.0363311842083931, -0.04403725266456604, 0.048196595162153244, -0.005212073214352131, -0.06977193057537079, 0.05707576125860214, -0.008764535188674927, -0.02990676835179329, 0.007062227465212345, -0.03718418627977371, 0.016523249447345734, 0.007357065100222826, 0.03796592354774475, 0.02068670466542244, -0.07719429582357407, 0.01585359126329422, 0.009529613889753819, 0.009699394926428795, 0.0014422069070860744, 0.02514885552227497, 0.021087726578116417, -0.014966684393584728, 0.06556427478790283, 0.0443640761077404, -0.012500753626227379, -0.02895798534154892, -0.05302337184548378, 0.001144061447121203, -0.0007187679875642061, 0.06459403783082962, 0.03320392966270447, 0.061908822506666183, 0.017166493460536003, -0.014477063901722431, -0.02113397978246212, -0.06806115061044693, -0.06105049327015877, -0.015063616447150707, -0.09282846003770828, -0.019330088049173355, 0.0004200657131150365, 0.04311841353774071, -0.023097286000847816, 0.025754466652870178, 0.035990603268146515, 0.033548787236213684, 0.0753960981965065, 0.03261943906545639, 0.03315252065658569, 0.09350871294736862, 0.0019053819123655558, 0.06734777987003326, 0.08379404991865158, 0.031525518745183945, 0.017571905627846718, -0.0807371512055397, 0.07296780496835709, 0.001735346857458353, 0.12034112960100174, 0.07620033621788025, 0.032106395810842514, -0.046745914965867996, -7.148498116293922e-05, 0.1496001034975052, -0.002848708303645253, -0.008944321423768997, -0.020441656932234764, -0.11850153654813766, -0.048534639179706573, 0.04516546428203583, -0.0006452614325098693, -0.13926281034946442, 2.5740896315779756e-33, -0.03948146104812622, 0.0075616114772856236, 0.01768495887517929, -0.018731439486145973, 0.02938208542764187, -0.06664466857910156, -0.04763650521636009, -0.0297736544162035, 0.06977207213640213, 0.010525633580982685, -0.0708283931016922, 0.013444658368825912, -0.013414722867310047, 0.1322372555732727, 0.10869424790143967, -0.05416574701666832, -0.04121781513094902, -0.13061127066612244, 0.05628477782011032, -0.10470156371593475, 0.0237521193921566, 0.0004934972967021167, 0.01807958446443081, -0.0009516018908470869, -0.013532811775803566, -0.0009293858893215656, -0.021832408383488655, -0.01724180392920971, -0.0441867858171463, 0.015343908220529556, 0.010474972426891327, 0.04138071462512016, -0.020120903849601746, -0.032539576292037964, 0.01617717184126377, 0.017934545874595642, 0.09665621072053909, -0.05934511497616768, 0.05382617563009262, 0.036800432950258255, 0.02419978566467762, -0.03612484037876129, -0.041499871760606766, -0.0036692467983812094, -0.03130028024315834, -0.01231793686747551, 0.03738990053534508, 0.061692431569099426, 0.0008067425806075335, -0.019611738622188568, -0.0014289580285549164, -0.004724171012639999, 0.014574411325156689, -0.09023234248161316, 0.027565110474824905, 0.013891049660742283, -0.0006075422279536724, -0.01399021502584219, -0.05881354212760925, 0.12281926721334457, -0.005111175589263439, 0.01361200213432312, 0.05391744524240494, 0.03668779879808426, 0.056433938443660736, 0.026865409687161446, -0.08349353075027466, -0.07296127825975418, 0.039170973002910614, -0.016755053773522377, 0.02779250219464302, 0.09730933606624603, -0.04487568512558937, -0.11743707954883575, 0.05058824270963669, 0.015194794163107872, 0.008619043044745922, -0.022938983514904976, -0.12833838164806366, -0.016849922016263008, 0.02761690691113472, -0.039060816168785095, -0.11287563294172287, 0.05977660045027733, 0.030711738392710686, 0.019846530631184578, 0.12779000401496887, 0.008569585159420967, -0.06485482305288315, -0.030975524336099625, -0.022663656622171402, -0.07313993573188782, 0.05365786701440811, -0.03737851232290268, -0.0456555113196373, -4.298163135029031e-33, -0.06418641656637192, 0.021415170282125473, 0.048859674483537674, 0.023181667551398277, -0.017153145745396614, 0.06591952592134476, -0.010746441781520844, -0.028947627171874046, -0.1140410453081131, -0.03067578747868538, -0.009283943101763725, 0.0833669900894165, 0.07118482142686844, 0.03389444202184677, 0.0007771819364279509, -0.0069582462310791016, 0.013557634316384792, -0.027779314666986465, 0.11561994254589081, -0.06488475203514099, 0.019138043746352196, 0.11808592826128006, -0.10894932597875595, 0.012228036299347878, 0.021105928346514702, 0.015187699347734451, -0.08526879549026489, 0.0696108266711235, -0.018424997106194496, 0.02406403049826622, -0.07778622210025787, 0.013498466461896896, 0.07957318425178528, -0.04795420169830322, 0.007424558978527784, 0.08661004155874252, 0.015802443027496338, 0.013379326090216637, -0.008826528675854206, -0.09209288656711578, 0.07603716850280762, -0.025326702743768692, -0.007579697296023369, 0.03263046219944954, 0.02442779205739498, -0.0034201506059616804, -0.04911787807941437, 0.08427449315786362, -0.06047931686043739, 0.05104533210396767, 0.05149353668093681, -0.02579650841653347, -0.02526814304292202, -0.1338365226984024, -0.01965204067528248, 0.05298062786459923, -0.029727527871727943, -0.021605564281344414, 0.026365188881754875, -0.06379365175962448, -0.07333490997552872, -0.07094628363847733, 0.047120217233896255, -0.09963775426149368, -0.02969624474644661, 0.015396875329315662, 0.003828730434179306, 0.034997694194316864, 0.08417173475027084, -0.03213324770331383, -0.008050785399973392, 0.0844772532582283, 0.04708288237452507, 0.00868203118443489, 0.019395554438233376, -0.020193025469779968, -0.0670926421880722, -0.047562144696712494, -0.005950151477009058, -0.004947194363921881, -0.0446370430290699, 0.01939351111650467, -0.014853307977318764, -0.07413195818662643, 0.012256113812327385, 0.02250991202890873, 0.003367035649716854, 0.026930460706353188, 0.033137645572423935, -0.009547139517962933, 0.014547632075846195, 0.1306127905845642, 0.0031931204721331596, 0.06245000660419464, -0.03877519443631172, -4.804546449577174e-08, 0.02909986302256584, -0.0021618350874632597, 0.017169851809740067, 0.025693194940686226, 0.04524359479546547, 0.016363633796572685, 0.07002852112054825, -0.1260487139225006, -0.03515411540865898, -0.027783110737800598, 0.033389072865247726, 0.024576693773269653, 0.0012730532325804234, -0.0010721153812482953, 0.07465526461601257, 0.08363482356071472, 0.03452429920434952, -0.009487143717706203, -0.026635145768523216, -0.03738578036427498, -0.0007103571551851928, 0.002206826815381646, -0.04669015109539032, 0.06720801442861557, 0.09397836774587631, -0.07401593774557114, -0.06616166234016418, 0.015338979661464691, -0.0009011887013912201, 0.016648942604660988, 0.06340987980365753, 0.06532399356365204, -0.0009989981772378087, 0.0028966718818992376, -0.0122255003079772, -0.0091119185090065, -0.018743736669421196, -0.03154591843485832, 0.0392342209815979, -0.017125550657510757, -0.026363659650087357, -0.06828988343477249, -0.047271259129047394, 0.01792754977941513, -0.047574255615472794, -0.02876507304608822, 0.028198637068271637, -0.13793613016605377, -0.03188615292310715, -0.002871329430490732, -0.05152302235364914, 0.023422695696353912, -0.010964745655655861, 0.02542448788881302, -0.05394068732857704, 0.029008544981479645, -0.01407699566334486, -0.07844463735818863, -0.05625549703836441, -0.0006524003692902625, -0.02018219791352749, 0.043100755661726, -0.00903038028627634, -0.04448506236076355]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\TrainingaLimitedInterconnectSyntheticNeuralIC.pdf,Deep Learning,"A SELF-LEARNING NEURAL NETWORK A. Hartstein and R. H. Koch IBM - Thomas J. Watson Research Center Yorktown Heights, New York ABSTRACf We propose a new neural network structure that is compatible with silicon technology and has built-in learning capability. The thrust of this network work is a new synapse function. The synapses have the feature that the learning parameter is em- bodied in the thresholds of MOSFET devices and is local in char- acter. The network is shown to be capable of learning by example as well as exhibiting the desirable features of the Hopfield type networks. The thrust of what we want to discuss is a new synapse function for an artificial neuron to be used in a neural network. We choose the synapse function to be readily implementable in VLSI technology, rather than choosing a function which is either our best guess for the function used by real synapses or mathematically the most tractable. In order to demonstrate that this type of synapse function provides interesting behavior in a neural network, we imbed this type of function in a Hopfield {Hopfield, 1982} type network and provide the synapses with a Hebbian {Hebb, 1949} learning capability. We then show that this type of net- work functions in much the same way as a Hopfield network and also learns by example. Some of this work has been discussed previously {Hartstein, 1988}. Most neural networks, which have been described, use a multiplicative function for the synapses. The inputs to the neuron are multiplied by weighting factors and then the results are summed in the neuron. The result of the sum is then put into a hard threshold device or a device with a sigmoid output. This is not the easiest function for a MOSFET to perform although it can be done. Over a large range of parameters, a MOSFET is a linear device with the output current being a linear function of the input voltage relative to a threshold voltage. If one could directly utilize these characteristics, one would be able to design a neural network more compactly. 769 770 Hartstein and Koch We propose that we directly use MOSFETs as the input devices for the neurons in the network, utilizing their natural characteristics. We assume the following form for the input of each neuron in our network: V; = 0 ( 2: IIj - T;j I ) 1 (1) where V, is the output, ~ are the inputs and T,j are the learned threshold voltages. In this network we use a representation in which both the V's and the T's range from 0 to + 1. The result of the summation is fed into a non-linear sigmoid func- tion (0). All of the neurons in the network are interconnected, the outputs of each neuron feeding the inputs of every other neuron. The functional form of Eq. 1 might, for instance, represent several n-channel and p-channel MOSFETs in parallel. The memories in this network are contained in the threshold voltages, 1',}"" We implement learning in this network using a simple linear Hebbian {Hebb, 1949} learning rule. We use a rule which locally reinforces the state of each input node in a neuron relative to the output of that neuron. The equation governing this learning algorithm is: (2) where 1';j are the initial threshold voltages and T'j are the new threshold voltages after a time,.6.t. Here TJ is a small learning parameter related to this time period, and the offset factor O.S is needed for symmetry. Additional saturation con- straints are imposed to ensure that 1';j remain in the interval 0 to + 1. This learning rule is one which is linear in the difference between each input and output of a neuron. This is an enhancing/inhibiting rule. The thresholds are ad- justed in such a way that the output of the neuron is either pushed in the same direction as the input (enhancing), or pushed in the opposite direction (inhibit- ing). For our simple simulations we started the network with all thresholds at O.S and let learning proceed until some saturation occurred. The somewhat more so- phisticated method of including a relaxation term in Eq. 2 to slowly push the val- ues toward O.S over time was also explored. The results are essentially the same as for our simple simulations. The interesting question is if we form a network using this type of neuron, what will the overall network response be like? Will the network learn multiple states or will it learn a simple average over all of the states it sees? In order to probe the functioning of this network, we have performed simulations of this network on a digital computer. Each simulation was divided into two phases. The first was a learning phase in which a fixed number of random patterns were presented to the network sequentially for some period of time. During this phase the threshold A Self-Learning Neural Network 771 voltages were allowed to change using the rule in Eq. 2. The second was a testing phase in which learning was turned off and the memories established in the net- work were probed to determine the essential features of these learned memories. In this way we could test how well the network was able to learn the initial test patterns, how well the network could reconstruct the learned patterns when pre- sented with test patterns containing errors, and how the network responded to random input patterns. We have simulated this network using N fully interconnected neurons, with N in the range of 10 to 200. M random patterns were chosen and sequentially pre- sented to the network for learning. M typically ranged up to N/3. After the learning phase, the nature of the stable states in the network was tested. In gen- eral we found that the network is capable of learning all of the input patterns as long as M is not too large. The network also learns the inverse patterns (l's and O's interchanged) due to the inherent symmetry of the network. Additional ex- traneous patterns are learned which have no obvious connection to the intended learned states. These may be analogous to either the spin glass states or the mixed pattern states discussed for the multiplicative network {Amit, 1985}. Fig. 1 shows the capacity of a 100 neuron network. We attempted to teach the network M states and then probed the network to see how many of the states were successfully learned. This process was repeated many times until we achieved good statistics. We have defined successful learning as 1000;6 accuracy. A more relaxed definition would yield a qualitatively similar curve with larger capacity. The functional form of the learning is peaked at a fixed value of the number of input patterns. For a small number of input patterns, the network essentially learns all of the patterns. Deviations from perfect learning here generally mean 1 bit of information was learned incorrectly. Near the peak the results become more noisy for different learning attempts. Most errors are still only 1 or 2 bits! but the learning in this region becomes marginal as the capacity of the network is approached. For larger values of the number of input patterns the network be- comes overloaded and it becomes incapable of learning most of the input states. Some small number of patterns are still learned, but the network is clearly not functioning well. Many of the errors in this region are large, showing little corre- lation with the intended learned states. This functional form for the learning in the network is the same for all of the net- work sizes tested. We define the capacity of the network as the average value of the peak number of patterns which can be successfully learned. The inset to Fig. 1 shows the memory capacity of a number of tested networks as a function of the size of the network. The network capacity is seen to be a linear function of the network size. The capacity is proportional to the number of T./s specified. In this 772 Hartstein and Koch example the network capacity was f ouod to be about 8010 of the maximum possi- ble for binary information. This rather low figure results from a trade-off of ca- pacity for the partic\Jlar types of functions that a neural network can perform. It is possible to construct simple memories with 1000.,.6 capacity. N 0 100 200 25------------------------~--------------~ 20 ] ~ 20 ~ 15 E o ~ '0 10 ~ ~ E ~ z 5 , , , '. 10 , , , 0 • o~--~----~--~----~--~ o 10 20 30 40 50 .?;- '0 C 0.. 0 U Figure 1. The number of successfully learned patterns as a func- tion of the number of input patterns for a 100 neuron network. The dashed curve is for perfect learning. The inset shows the memory capacity of a threshold neural network as a function of the size of the network. Some important measures of learning in the network are the distribution of stable states in the network after learning has taken place. and the basin of attraction r or each stable point. One can gain a handle on these parameters by probing the network with random test patterns after the network has learned M states. Fig. 2 shows the averaged results of such tests for a 100 neuron network and varying numbers of learned states. The figure shows the probability of finding particular states. both learned and extraneous. The states are ordered first by decreasing A Self-Learning Neural Network 773 probability for the learned states, followed by decreasing probability for the ex- traneous states. It is clear from the figure that both types of stable states are present in the network. It is also clear that the probabilities of finding different patterns are not equal. Some learned states are more robust than others, that is they have larger basins of attraction. This network model does not partition the available memory space equally among the input patterns. It also provides a large amount of memory space for the extraneous states. Clearly, this is not the opti- mum situation. 0.8 ~ (a) 0.6 Learned Q) 0.4 -.s (/) 0') 0.2 L Extraneous c: ~ 0.0 --- c: G: ..... 0.8 0 .b (b) ~ 0.6 :0 Extraneous e ~ 0.4 0.2 Learned 0.0 0 5 10 15 20 25 30 State Figure 2. The probability of the network finding a specific pat- tern. Both learned states and extraneous states are found. The figure was obtained for a 100 neuron network. Fig. 2a is for 5 learned patterns and 2b is for 10 learned patterns. Some of the learned states appear to have 0 probability of being found in this simulation. Some of these states are not stable states of the network and will never be found. This is particularly true-when the number of learned states is close to or exceeds the capacity of the network. Others of these states simply have an extremely small probability of being found in a random search because they have small basins of attraction. However, as discussed below, these are still viable states. When the network learns fewer states than its capacity (Fig. 2a), 774 Hartstein and Koch most of the stable states are the learned states. As the capacity is approached or exceeded, most of the stable states are extraneous states. The results shown in Fig. 2 address the question of the networks tolerance to er- rors. A pattern, which has a large basin of attraction, will be relatively tolerant to errors when being retrieved, whereas, a pattern, which has a small basin of at- traction, will be less tolerant of errors. The immunity of the learned patterns to errors in being retrieved can also be tested in a more direct way. One can probe the network with test patterns which start out as the learned patterns, but have a certain number of bits changed randomly. One then monitors the final pattern which the networks finds and compares to the known learned pattern . .$ .s (I) ""i 0.8 E o !3 t7' c: ;:; c: 0.6 ~ 04 '0 · ~ ~ 0.2 e 0.. • • 0.0 '------.1--.-.-.--___ .... ..-.4 ... o 10 20 30 40 Hamming Distance Figure 3. Probability of the network finding a specific learned state when the input pattern has a certain Hamming distance. This figure was obtained for a 100 neuron network which was taught 10 random patterns. Fig. 3 shows typical results of such a calculation. The probability of successfully retrieving a pattern is shown as a function of the Hamming distance. the number of bits which were randomly changed in the test pattern. For this simulation a tOO neuron network was used and it was taught 10 patterns. For small Hamming distances the patterns are successfully found 100°,.6 of the time. As the Hamming distance gets larger the network is no longer capable of finding the desired pat- tern. but rather finds one of the other fixed points. This result is a statistical av- A Self-Learning Neural Network 775 erase over all of the states and therefore tends to emphasize patterns with small basins of attraction. This is just the opposite of the types of states emphasized in the analysis shown in Fig. 2. We can define the maximum Hamming distance as the Hamming distance at which the probability of finding the learned state has dropped to SO%. Fig. 4 shows the maximum Hamming distance as a function of the number of learned states in our 100 neuron network. As one expects the maximum Hamming dis- tance gets smaller as the number of learned states increases. Perhaps surprisingly, the relationship is linear. These results are important since one requires a rea- sonable maximum Hamming distance for any real system to function. These considerations also shed some light on the nature of the functioning of the net- work and its ability to learn. 60 CI) (.) c ~ i5 40 0 c ·e E 0 :c § 20 . E .- ~ ::E 0 0 5 10 15 20 M Figure 4. The maximum Hamming distance for a given number of learned states. Results are for a 100 neuron network. This simulation gives us a picture of the way in which the network utilizes its phase space to store information. When only a few patterns are stored in the network, the network divides up the available space among these memories. The learning process is almost always successful. When a larger number of learn~ patterns are attempted, the available space is now divided among more memories. The maximum Hamming distance decreases and more space is taken up byex- traneous states. When the memory capacity is exceeded, the phase space allo- 776 Hartstein and Koch cated to any successful memory is very small and most of the space is taken up by extraneous states. The types of behavior we have described are similar to those found in the Hopfield type memory utilizing multiplicative synapses. In fact our central point is that by using a completely different type of synapse function, we can obtain the same behavior. At the same time we argue since this network was proposed using a synapse function which mirrors the operating characteristics of MOSFETs, it will be much easier to realize in hardware. Therefore, we should be able to con- struct a smaller more tolerant network with the same operating characteristics. We do not mean to imply that the type of synapse function we have explored can only be used in a Hopfield type network. In fact we feel that this type of neuron is quite general and can successfully be utilized in any type of network. This is at present just a conjecture which needs to be explored more fully. Perhaps the most important message from our work is the realization that one need not be constrained to the multiplicative type of synapse, and that other forms of synapses can perform similar functions in neural networks. This may open up many new avenues of investigation. REFERENCES D.l. Amit, H. Gutfreund and H. Sompolinsky, Phys. Rev. A32, 1001 (1985). A. Hartstein and R.H. Koch, IEEE Int. Conf. on Neural Networks, (SOS Printing, San Diego, 1988), Vol. I, 425. D. Hebb, The Organization of Behaviour, (Wiley, New York, 1949). 1.1. Hopfield, Proc. Natl. Acad. Sci. USA 79, 2554 (1982).","[-0.09283441305160522, -0.09559958428144455, 0.0246629249304533, 0.011693142354488373, -0.045523326843976974, -0.0024062860757112503, 0.08086935430765152, -0.007419901434332132, 0.05293487757444382, -0.06257101148366928, 0.07161080092191696, 0.012772560119628906, -0.0755508691072464, 0.02717154286801815, -0.0274299755692482, 0.07022171467542648, -0.008830619975924492, 0.008463302627205849, -0.023698365315794945, -0.04553147405385971, 0.0381929948925972, -0.04190046340227127, -0.0292144063860178, 0.018639622256159782, -0.0766647532582283, 0.004512475803494453, 0.011396910063922405, -0.01434246450662613, -0.05312152951955795, -0.05503327026963234, 0.04702911525964737, -0.011200490407645702, -0.025838444009423256, 0.021748434752225876, -0.09471208602190018, 0.05006866157054901, -0.03353403881192207, -0.014043361879885197, 0.045826736837625504, -0.03621659427881241, -0.005046111065894365, -0.03474431484937668, 0.0343463309109211, 0.035662032663822174, 0.06679247319698334, 0.06276656687259674, 0.027498040348291397, -0.02982972376048565, 0.024186398833990097, -0.0069974083453416824, -0.015327469445765018, 0.04857385531067848, 0.00613813754171133, 0.019016750156879425, 0.06594053655862808, 0.0938546434044838, -0.053307924419641495, 0.027769330888986588, -0.0493207722902298, 0.030272874981164932, 0.010108024813234806, -0.039054982364177704, 0.04563033953309059, -0.012555992230772972, 0.040142662823200226, 0.01250107679516077, 0.016226330772042274, 0.011280681006610394, 0.0621938593685627, -0.018671825528144836, 0.013962830416858196, 0.058256860822439194, -0.046690426766872406, 0.02293028123676777, 0.05529865249991417, 0.03511232137680054, 0.04168019816279411, 0.07284722477197647, 0.08744671940803528, -0.055543746799230576, -0.007167411502450705, 0.0151063846424222, -0.03193658962845802, 0.01959466189146042, 0.07126116752624512, 0.03513506427407265, -0.08234931528568268, 0.003694182727485895, 0.024006491526961327, -0.032792672514915466, -0.031702376902103424, -0.04751250892877579, -0.09125488996505737, -0.1243484690785408, 0.030569225549697876, -0.10322520136833191, 0.04610052332282066, -0.052214860916137695, -0.04786054044961929, 0.04329197108745575, -0.029617957770824432, 0.04361940920352936, 0.037259604781866074, -0.032673180103302, 0.00915216188877821, 0.023530123755335808, 0.03854838013648987, 0.055999163538217545, 0.009821786545217037, -0.06114070862531662, -0.02386827953159809, -0.0034371481742709875, -0.049989424645900726, -0.05507122352719307, 0.01847742311656475, 0.03690634295344353, 0.009091290645301342, 0.03356008976697922, 0.04280269145965576, 0.034050386399030685, -0.09035871922969818, 0.03371043875813484, -0.10441160947084427, -0.01388587523251772, -0.10149459540843964, -0.054221950471401215, -0.14510959386825562, 3.708900673931281e-33, -0.033589400351047516, 0.06293357163667679, -0.014968827366828918, -0.09085690975189209, -0.00028926751110702753, -0.008968868292868137, 0.008503121323883533, -0.005337374750524759, 0.04823005944490433, -0.03279053792357445, -0.05983737111091614, -0.06015172600746155, 0.005856080912053585, 0.041353728622198105, 0.09335869550704956, -0.06494041532278061, -0.06134820729494095, -0.12609460949897766, 0.02515001967549324, -0.12767980992794037, 0.0614263154566288, -0.006382191553711891, 0.03331489488482475, -0.05800449475646019, -0.018715642392635345, -0.05165349692106247, 0.03954169154167175, 0.0032464182004332542, -0.06361691653728485, 0.01581478677690029, -0.025822030380368233, 0.0490788035094738, -0.0624849908053875, -0.06037802994251251, 0.05880708247423172, -0.004537865519523621, -0.08878260850906372, -0.007760006934404373, 0.10466336458921432, -0.02696521393954754, -0.005789350252598524, 0.006557390093803406, 0.02097245305776596, -0.03901834413409233, -0.07805941253900528, -0.029693854972720146, 0.05388865992426872, 0.0653567835688591, 0.009195258840918541, -0.131230428814888, -0.03816964104771614, -0.03060152567923069, -0.031403303146362305, -0.09681235998868942, 0.11393477022647858, 0.020968927070498466, 0.07224035263061523, 0.10168485343456268, 0.06400317698717117, 0.15027780830860138, 0.017412282526493073, 0.006265808828175068, -0.004235656466335058, 0.06258178502321243, 0.031080925837159157, 0.08006425201892853, -0.0015492613893002272, -0.009398731403052807, 0.05810387060046196, -0.04843660816550255, 0.05696369707584381, 0.07221804559230804, -0.057926032692193985, -0.052742209285497665, 0.008138920180499554, -0.012389245443046093, -0.00585614237934351, -0.09481813758611679, 0.02021719142794609, -0.005353535991162062, -0.009930195286870003, -0.055467721074819565, -0.02610377036035061, 0.04192010313272476, -0.029633602127432823, 0.000451182306278497, 0.06691128760576248, -0.051607657223939896, -0.08605001866817474, 0.008554540574550629, -0.059918373823165894, -0.03312579169869423, 0.05692755803465843, -0.024580959230661392, 0.005102888680994511, -5.562455798480373e-33, -0.047205809503793716, 0.039842814207077026, -0.03151743486523628, -0.007475178688764572, -0.041322678327560425, 0.049412183463573456, 0.018758656457066536, 0.03880847990512848, -0.0413670614361763, 0.022887594997882843, -0.031800977885723114, 0.10073542594909668, 0.04386506974697113, -0.004421671386808157, 0.007915737107396126, 0.0876750499010086, -0.12637270987033844, -0.017001064494252205, 0.07774188369512558, -0.03421828895807266, -0.015490110963582993, 0.13262580335140228, -0.03361504152417183, 0.07627587765455246, -0.04971292242407799, -0.011969177052378654, -0.06968356668949127, 0.11654848605394363, 0.009228995069861412, 0.048971112817525864, -0.024749401956796646, -0.011887045577168465, -0.0274521391838789, 0.008559576235711575, 0.05229092389345169, 0.042150408029556274, 0.0816117525100708, -0.03361283242702484, 0.04654422402381897, -0.03226863965392113, 0.06549647450447083, -0.048579901456832886, -0.002245806623250246, -0.01637139357626438, 0.017775079235434532, -0.02935938909649849, -0.03618438541889191, 0.07049188762903214, -0.0645199865102768, 0.008668073453009129, -0.04114629328250885, -0.06498150527477264, -0.05646657198667526, -0.06037362664937973, -0.10136812180280685, 0.06030850484967232, 0.02741931937634945, -0.014340773224830627, 0.07617439329624176, -0.028555383905768394, -0.06917461007833481, -0.09654387831687927, 0.1067575141787529, 0.017051441594958305, -0.03319956362247467, -0.09132847189903259, -0.022159503772854805, 0.057248909026384354, 0.0349404402077198, -0.027240891009569168, 0.049219198524951935, 0.026142623275518417, 0.08307190984487534, -0.02985241450369358, -0.04196131229400635, -0.10843934118747711, 0.012189583852887154, 0.013153815641999245, 0.021748756989836693, -0.009228180162608624, -0.00016750223585404456, -0.00445563392713666, -0.0006310984026640654, 0.004036972299218178, 0.10867294669151306, 0.029288437217473984, 0.002452222863212228, 0.014180640690028667, 0.022549862042069435, -0.021312525495886803, 0.02835756726562977, 0.055351290851831436, -0.06257238984107971, 0.05298975482583046, -0.018988730385899544, -5.05352950597171e-08, 0.005364709999412298, 0.010716401971876621, 0.05055990442633629, 0.011264147236943245, 0.06847359985113144, -0.03132334724068642, 0.06735813617706299, -0.012509373016655445, -0.055281005799770355, -0.018460333347320557, 0.04438537359237671, 0.009811097756028175, -0.02593119628727436, -0.027691420167684555, -0.006218251306563616, 0.09660864621400833, 0.029993891716003418, 0.027724608778953552, 0.009253471158444881, -0.022178854793310165, 0.08359881490468979, -0.054693836718797684, 0.03667915239930153, 0.026899928227066994, 0.014206835068762302, -0.08456168323755264, -0.08215057104825974, -0.03010435588657856, -0.06062588840723038, 0.04312795400619507, 0.016731569543480873, 0.02317921817302704, 0.045071106404066086, -0.023219149559736252, 0.015964264050126076, 0.1055242270231247, -0.0077768657356500626, -0.018315598368644714, -0.03644050285220146, -0.01379708107560873, -0.05830271169543266, 0.048256684094667435, -0.01431019976735115, 0.015096563845872879, -0.013929758220911026, -0.07495823502540588, -0.011721321381628513, -0.021064195781946182, 0.012953267432749271, 0.038603756576776505, 0.029794078320264816, 0.028938524425029755, 0.04202650487422943, -0.014650996774435043, 0.0682692602276802, -0.022479038685560226, -0.016934731975197792, -0.02824704349040985, -0.03723566234111786, 0.09795709699392319, 0.030019644647836685, 0.06542574614286423, 0.018552038818597794, 0.01352864783257246]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\TrainingMultilayerPerceptronswiththeExtendedKalmanAlgorithm.pdf,Deep Learning,"AN ADAPTIVE NETWORK THAT LEARNS SEQUENCES OF TRANSITIONS C. L. Winter Science Applications International Corporation 5151 East Broadway, Suite 900 Tucson, Auizona 85711 ABSTRACT We describe an adaptive network, TIN2, that learns the transition function of a sequential system from observations of its behavior. It integrates two subnets, TIN-I (Winter, Ryan and Turner, 1987) and TIN-2. TIN-2 constructs state representations from examples of system behavior, and its dynamics are the main topics of the paper. TIN-I abstracts transition functions from noisy state representations and environmental data during training, while in operation it produces sequences of transitions in response to variations in input. Dynamics of both nets are based on the Adaptive Resonance Theory of Carpenter and Grossberg (1987). We give results from an experiment in which TIN2 learned the behavior of a system that recognizes strings with an even number of l's . INTRODUCTION Sequential systems respond to variations in their input environment with sequences of activities. They can be described in two ways. A black box description characterizes a system as an input-output function, m = B(u), mapping a string of input symbols, ll, into a single output symbol, m. A sequential automaton description characterizes a system as a sextuple (U, M, S, SO, f, g) where U and M are alphabets of input and output symbols, S is a set of states, sO is an initial state and f and g are transition and output functions respectively. The transition function specifies the current state, St, as a function of the last state and the current input, Ut, (1) In this paper we do not discuss output functions because they are relatively simple. To further simplify discussion, we restrict ourselves to binary input alphabets, although the neural net we describe here can easily be extended to accomodate more complex alphabets. 653 654 Winter A common engineering problem is to identify and then simulate the functionality of a system from observations of its behavior. Simulation is straightforward when we can actually observe the internal states of a system, since then the function f can be specified by learning simple associations among internal states and external inputs. In robotic systems, for instance, internal states can often be characterized by such parameters as stepper motor settings, strain gauge values, etc., and so are directly accessible. Artificial neural systems have peen found useful in such simulations because they can associate large, possibly noisy state space and input variables with state and output variables (Tolat and Widrow, 1988; Winter, Ryan and Turner, 1987). Unfortunately, in many interesting cases we must base simulations on a limited set of examples of a system's black box behavior because its internal workings are unobservable. The black box description is not, by itself, much use as a simulation tool since usually it cannot be specified without resorting to infinitely large input-output tables. As an alternative we can try to develop a sequential automaton description of the system by observing regularities in its black box behavior. Artificial neural systems can contribute to the development of physical machines dedicated to system identification because i) frequently state representations must be derived from many noisy input variables, ii) data must usually be processed in continuous time and iii) the explicit dynamics of artificial neural systems can be used as a framework for hardware implementations. In this paper we give a brief overview of a neural net, TIN2, which learns and processes state transitions from observations of correct black box behavior when the set of observations is large enough to characterize the black box as an automaton. The TIN2 net is based on two component networks. Each uses a modified adaptive resonance circuit (Carpenter and Grossberg, 1987) to associate heterogeneous input patterns. TIN-1 (Winter, Ryan and Turner, 1987) learns and executes transitions when given state representations. It has been used by itself to simulate systems for which explicit state representations are available (Winter, 1988a). TIN-2 is a highly parallel, continuous time implementation of an approach to state representation first outlined by Nerode (1958). Nerode's approach to system simulation relies upon the fact that every string, l!. moves a machine into a particular state, s(y). once it has been processed. The s(y) state can be characterized by putting the system initially into s(u) (by processing y) and then presenting a set of experimental strings. (~1 .... , ~n)' for further processing. Experiments consist of observing the output mi = BUt·~i) where • indicates concatenation. A state can then be represented by the entries in a row of a state characterization table, C (Table 1). The rows of C are indexed by strings, lI, its columns are indexed by experiments. Wi. and its entries are mi. In Table 1 annotations in parentheses indicate nodes (artificial neurons) and subnetworks of TIN-2 equivalent to the corresponding C table entry. During experimentation C expands as states are Adaptive Network That Learns Sequences of Transitions 655 distinguished from one another. The orchestration of experiments, their selection, the TABLE 1. C Table Constructed by TIN-2 A. o (Assembly 1) 1 (Assembly 2) A. 1 (Node 7) o (Node 2) o (Node 5) 1 o (Node 6) o (Node 9) 1 (Node 1) 0 o (Node 1) 1 (Node 6) o (Node 4) 10 o (Node 3) o (Node 2) o (Node 0) role of teachers and of the environment have been investigated by Arbib and Zeiger (1969), Arbib and Manes (1974), Gold (1972 and 1978) and Angluin (1987) to name a few. TIN-2 provides an architecture in which C can be embedded and expanded as necessary. Collections of nodes within TIN-21earn to associate triples (mi, 11, ~i) so that inputting II later results in the output of the representation (m 1, ... , mn)n of the state associated with 11. TIN-2 TIN-2 is composed of separate assemblies of nodes whose dynamics are such that each assembly comes to correspond to a column in the state characterization table C. Thus we call them column-assemblies. Competition among column-assemblies guarantees that nodes of only one assembly, say the ith, learn to respond to experimental pattern ~i' Hence column-assemblies can be labelled ~ 1 ' ~2 and so on, but since labelings are not assigned ahead of time, arbitrarily large sets of experiments can be learned. The theory of adaptive resonance is implemented in TIN-2 column-assemblies through partitioned adaptive resonance circuits (cf. Ryan, Winter and Turner, 1987). Adaptive resonance circuits (Grossberg and Carpenter, 1987; Ryan and Winter, 1987) are composed of four collections of nodes: Input, Comparison (FI), Recognition (F2) and Reset. In TIN-2 Input, Comparison and Reset are split into disjoint m,.u and ~ partitions. The net runs in either training or operational mode, and can move from one to the other as required. The training dynamics of the circuit are such that an F2 node is stimulated by the overall triple (m. n,~, but can be inhibited by a mismatch with any component. During operation input of.u recalls the state representation s(u) = (m 1 .... , mn)n' Node activity for the kth FI partition, FI k' k = m, u, W, is governed by , (2) Here t < 1 scales time, Ii,k is the value of the ith input node of partition k, xi,k is 656 Winter activity in the corresponding node of FI and f is a sigmoid function with range [0. 1]. The elements of I are either 1. -lor O. The dynamics of the TIN-2 circuit are such that 0 indicates the absence of a symbol, while 1 and -1 represent elements of a binary alphabet. The adaptive feedback filter. T. is a matrix (Tji) whose elements. after training. are also 1.-1 orO. Activity, yj. in the jth F2 node is driven by + L meFl,m Bmj h(xm)] - 4[ ~*j f(YTl) + Ruj + Rw] . (3) The feedforward fllter B is composed of matrices (Buj)' (Bmj) and (Bw) whose elements are normalized to the size of the patterns memorized. Note that (Bw) is the same for every node in a given column-assembly. i.e. the rows of (Bw) are all the same. Hence all nodes within a column-assembly learn to respond to the same experimental pattern. w. and it is in this sense that an assembly evolves to become equivalent to a column in table C. During training the sum ~*j f(YTl) in (3) runs through the recognition nodes of all TIN-2 column-assemblies. Thus. during training only one F2 node. say the Jth. can be active at a time across all assemblies. In operation. on the other hand. we remove inhibition due to nodes in other assemblies so that at any time one node in each column-assembly can be active. and an entire state representation can be recalled. The Reset terms Ru,j and Rw in (3) actively inhibit nodes of F2 when mismatches between memory and input occur. Ruj is specific to the jth F2 node. dRujldt = -Ruj + f(Yj) f(v 1I1u II - II £I.u II) . (4) Rw affects all F2 nodes in a column-assembly and is driven by dRw/dt = -Rw + [LjeF2 f(Yj)] f(v IIlw II-II fI.w II). (5) v < 1 is a vigilance parameter (Carpenter and Grossberg. 1987): for either (4) or (5) R > 0 at equilibrium just when the intersection between memory and input. PI = T n I. is relatively small, i.e. R > 0 when v 11111 > II PI II. When the system is in operation. we fix Rw = 0 and input the pattern Iw = O. To recall the row in table C indexed by 11, we input 11 to all column-assemblies. and at equilibrium xi.m = Lje F2 Tjif(Yj). Thus xi,m represents the memory of the element in C corresponding to 11 and the column in C with the same label as the column-assembly. Winter (1988b) discusses recall dynamics in more detail. Adaptive Network That Learns Sequences of Transitions 657 At equilibrium in either training or operational mode only the winning F2 node has YJ *- O. so LjTjif(Yj) = TJi in (2). Hence xi.k = 0 if TJi = -li.k. i.e. if memory and input mismatch; IXi.kl = 2 if TJi = Ii,k. i.e. when memory and input match; and IXi.kl = 1 if TJ.i = O. Ii.k *- 0 or ifTJ.i *- O. Ii.k = O. The F1 output function h in (3) is defined so that hex) = 1 if x> 1. hex) = -1 if x < -1 and hex) = 0 if -1 S x S 1. The output pattern ~1 = (h(x1) ..... h(xnl» reflects IJ ('\ Ik. as h(xi) *- 0 only if TJi = Ii.k. The adaptive filters (Buj) and (Bmj) store normalized versions of those patterns on FI.u and F1.m which have stimulated the jth F2 node. The evolution of Bij for u E FI.u or F1 m is driven by • (6) On the other hand (Bw) stores a normalized version of the experiment w which labels the entire column-assembly. Thus all nodes in a column-assembly share a common memory of~. (7) where w E F1 w . • The feedback mters (T uj). (T mj) and (T w) store exact memories of patterns on partitions ofFI: (8) for i E FI.u • F1.m • and (9) for i E FI.w' In operation long-term memory modification is suspended. EXPERIMENT Here we report partial results from an experiment in which TlN-2 learns a state characterization table for an automaton that recognizes strings containing even numbers of . . 658 Winter both I's and O's. More details can be found in Winter (1988b). For notational convenience in this section we will discuss patterns as if they were composed of l's and O's, but be aware that inside TIN-2 every 0 symbol is really a -1. Data is provided in the form of triples eM, ll, YD by a teacher; the data set for this example is given in Table 1. Data were presented to the net in the order shown. The net consisted of three column-assemblies. Each F2 collection contained ten nodes. Although the strings that can be processed by an automaton of this type are in principle arbitrarily long, in practice some limitation on the length of training strings is necessary if for no other reason than that the memory capacity of a computer is finite. For this simple example Input and F I partitions contain eight nodes, but in order to have a special symbol to represent A.. strings are limited to at most six elements. With this restriction the A. symbol can be distinguished from actual input strings through vigilance criteria. Other solutions to the problem of representing A. are being investigated, but for now the special eight bit symbol, 00000011, is used to represent A. in the strings A.-yt. The net was trained using fast-learning (Carpenter and Grossberg, 1987): a triple in Table 1 was presented to the net. and all nodes were allowed to come to their equilibrium values where they were held for about three long-term time units before the next triple was presented. Consider the processing that follows presentation of (0, 1,0) the first datum in Table 1. The net can obtain equivalents to two C table entries from (0, 1,0): the entry in row 1l = 10, column Yi. = A. and the entry in row II =1, column w = O. The string 10 and the membership value 0 were displayed on the A. assembly's input slabs, and in this case the 3rd F2 node learned the association among the two patterns. When the pattern (0, 1, 0) was input to other column-assemblies, one F2 node (in this case the 9th in column-assembly 1) learned to associate elements of the triple. Of course a side effect of this was that column-assembly 1 was labelled by Yi.. = 0 thereafter. When (1. 1, 1) was input next, node 9 in column-assembly 1 tried to respond to the new triple, all nodes in column-assembly 1 were then inhibited by a mismatch on Yi.., and finally node 1 on column-assembly 2 learned (1, 1, 1). From that point on column-assembly 2 was labelled by 1. LEARNING TRANSITIONS The TIN-I net (Winter. Ryan and Turner, 1987) is composed of i) a partitioned adaptive resonance circuit with dynamics similar to (2) - (9) for learning state transitions and ii) a Control Circuit which forces transitions once they have been learned. Transitions are unique in the sense that a previous state and current input completely determine the current state. The partitioned adaptive resonance circuit has three input fields: one for the previous state, one for the current input and one for the next state. TIN-l's F2 nodes learn transitions by associating patterns in the three input fields. Once trained. TIN-l processes strings sequentially. bit-by-bit. Adaptive Network That Learns Sequences of Transitions 659 1L~r--T-'N---2 -~:t ~ TIN-l TIN-2 I~ u.eu Figure 1. Training TIN2. The architecture of TIN2, the net that integrates TIN-2 and TIN-I. is shown in Figure 1. The system resorts to the TIN-2 nets only to learn transitions. If TIN-2 has learned a C table in which examples of all transitions appear, TIN-I can easily learn the automaton's state transitions. A C table contains an example of a transition from state si to state Sj forced by current input u, if it contains i) a row labelled by a string lli which leaves the automaton in si after processing and ii) a row labelled by the string lltu which leaves the automaton in Sj. To teach TIN-l the transition we simply present lli to the lower TIN-2 in Figure I, llieu to the upper TIN-2 net and u to TIN-I. CONCLUSIONS We have described a network, TIN-2, which learns the equivalent of state characterization tables (Gold, 1972). The principle reasons for developing a neural net implementation are i) neural nets are intrinsically massively parallel and so provide a nice model for systems that must process large data sets, ii) although in the interests of brevity we have not stressed the point, neural nets are robust against noisy data, iii) neural nets like the partitioned adaptive resonance circuit have continuous time activity dynamics and so can be synchronized with other elements of a larger real-time system through simple scaling parameters, and iv) the continuous time dynamics and precise architectural specifications of neural nets provide a blueprint for hardware implementations. We have also sketched a neural net, TIN2, that learns state transitions by integrating TIN-2 nets with the TIN-I net (Winter, Ryan and Turner, 1987). When a complete state characterization table is available from TIN-2, TIN2 can be taught transitions from examples of system behavior. However, the ultimate goal of a net like this lies in developing a system that ""or,rates acceptably"" with a partial state characterization table. To operate acceptably TIN must perform transitions correctly when it can, recognize when it cannot, signal for new data when it is required and expand the state charcterization taole when it must. Happily TIN2 already provides the first two capabilities, and combinations of TIN2 with rule-based controllers and with auxiliary control networks are currently being explored as approachws to satisfy the latter (Winter, 1988b). Nets like TIN2 may eventually prove useful as control elements in physical machines because sequential automata can respond to unpredictable environments with a wide range of behavior. Even very simple automata can repeat activities and make decisions based upon environmental variations. Currently, most physical machines that make decisions are dedicated to a single task; applying one to a new task requires re-programming by a 660 Winter skilled technician. A programmer must, furthermore, determine a priori precisely which machine state - environment associations are significant enough to warrant insertion in the control structure of a given machine. TIN2, on the other hand, is trained, not programmed, and can abstract significant associations from noisy input. It is a ""blank slate"" that learns the structure of a particular sequential machine from examples. References D. Angluin, ""Learning Regular Sets from Queries and Counterexamples"", Information and Computation, 75 (2), 1987. M. A. Arbib and E. G. Manes, ""Machines in a Category: an Expository Introduction"", SIAM Review, 16 (2), 1974. M. A. Arbib and H. P. Zeiger, ""On the Relevance of Abstract Algebra to Control Theory"", Automatica, 5, 1969. G. Carpenter and S. Grossberg, ""A Massively Parallel Architecture for a Self-Organizing Neural Pattern Recognition Machine"", Comput. Vision Graphics Image Process. 37 (54), 1987. E. M. Gold, ""System Identification Via State Characterization"", Automatica, 8, 1972. E. M. Gold, ""Complexity of Automaton Identification from Given Data"", Info. and Control, 37, 1978. A. Neroda, ""Linear Automaton Transformations"", Proc. Am. Math. Soc., 9, 1958. T. W. Ryan and C. L. Winter, ""Variations on Adaptive Resonance"", in Proc. 1st IntI. Conf. on Neural Networks, IEEE, 1987. T. W. Ryan, C. L. Winter and C. J. Turner, ""Dynamic Control of an Artificial Neural System: the Property Inheritance Network"", Appl. Optics, 261 (23) 1987. V. V. Tolat and B. Widrow, ""An Adaptive Neural Net Controller with Visual Inputs"", Neural Networks, I, S upp I, 1988. C. L. Winter, T. W. Ryan and C. J. Turner, ""TIN: A Trainable Inference Network"", in Proc. 1st Inti. Conf. on Neural Networks, 1987. C. L. Winter, ""An Adaptive Network that Flees Pursuit"", Neural Networks, I, Supp.l, 1988a. C. L. Winter, ""TIN2: An Adaptive Controller"", SAIC Tech. Rpt., SAIC, 5151 E. Broadway, Tucson, AZ, 85711, 1988b. Part V Implementation","[-0.06330797076225281, -0.08612273633480072, 0.027971748262643814, 0.015693413093686104, -0.04568346217274666, 0.033148229122161865, -0.01876954361796379, -0.04874955490231514, 0.03443455696105957, -0.083685964345932, -0.07828107476234436, -0.002955792238935828, 0.01843630149960518, -0.0193727258592844, -0.036394014954566956, 0.04084881767630577, 0.048231758177280426, -0.03935190290212631, -0.007487040478736162, -0.06983423978090286, -0.02573276497423649, 0.032908931374549866, 0.010711790062487125, 0.00018132300465367734, -0.032702453434467316, 0.06155167892575264, -0.05998656153678894, 0.034447163343429565, 0.05017897114157677, -0.02934768795967102, 0.05633934587240219, 0.04376892372965813, -0.013370424509048462, -0.04491617530584335, -0.04767253249883652, -0.030840305611491203, -0.07330193370580673, 0.021891186013817787, 0.008153059519827366, 0.002336210571229458, 0.026959342882037163, 0.017755968496203423, -0.013819643296301365, -0.010598069056868553, -0.033065371215343475, -0.001515295822173357, 0.022413983941078186, -0.10763676464557648, -0.05374827980995178, -0.02780192159116268, -0.03304910659790039, 0.003256024792790413, -0.03663400188088417, 0.05499172583222389, 0.07780034095048904, 0.08742715418338776, 0.05150704085826874, 0.06080017238855362, -0.05285027250647545, -0.05608728155493736, -0.014345509931445122, -0.0878591239452362, -0.002110377186909318, -0.031843606382608414, 0.056468021124601364, 0.06994280219078064, -0.005225067958235741, 0.11746004223823547, 0.029641887173056602, -0.058791518211364746, -0.006249265745282173, 0.04243205487728119, -0.07517941296100616, 0.020659558475017548, 0.07591075450181961, 0.011139006353914738, 0.01252929586917162, 0.07670612633228302, 0.0034474055282771587, -0.028416678309440613, -0.03921075537800789, -0.03092009574174881, -0.006386932916939259, 0.0009781117551028728, 0.027471236884593964, -0.01466465275734663, -0.09134738892316818, 0.030716547742486, -0.0013872601557523012, 0.032240692526102066, -0.04387426748871803, -0.015536061488091946, -0.034019749611616135, 0.009444192983210087, -0.011708436533808708, 0.07027322053909302, 0.0337274968624115, -0.03103451244533062, 0.08300209045410156, 0.11816804111003876, 0.00028929676045663655, 0.03663809597492218, 0.02895422838628292, -0.022782735526561737, 0.0016327410703524947, 0.02867252752184868, 0.009644788689911366, 0.001434221980161965, 0.05857916548848152, -0.058267347514629364, 0.013174810446798801, 0.02529333531856537, -0.0022473775316029787, 0.04638177901506424, -0.06941952556371689, -0.08036776632070541, 0.020020322874188423, -0.04450668767094612, 0.010052482597529888, 0.11392281949520111, 0.05838403478264809, -0.032736580818891525, -0.057038936764001846, 0.03502432629466057, 0.018627099692821503, -0.06200157478451729, -0.07555418461561203, -8.394188628832647e-34, 0.01678103767335415, -0.0063730026595294476, -0.0179231408983469, -0.004842269234359264, 0.06833498924970627, -0.04030885547399521, -0.022085823118686676, -0.06719854474067688, 0.022005200386047363, 0.010153187438845634, -0.07668542861938477, 0.12220609188079834, -0.006144351791590452, 0.008507758378982544, 0.08177157491445541, -0.06339084357023239, 0.028437577188014984, -0.025278957560658455, 0.02517363615334034, -0.11132945120334625, 0.021114209666848183, -0.005537960212677717, -0.010744948871433735, -0.06536535173654556, -0.015734244138002396, 0.04591110348701477, 0.054114919155836105, -0.0058863162994384766, 0.007477191276848316, -0.01421631220728159, 0.0024769525043666363, 0.04328729212284088, -0.0078775305300951, 0.08187815546989441, 0.08939813077449799, -0.0933065414428711, 0.02718699723482132, -0.017502542585134506, -0.0008209265070036054, -0.0835408940911293, 0.027628816664218903, -0.004735514521598816, 0.02530956268310547, 0.0654817670583725, -0.05985013395547867, -0.06808515638113022, 0.02092631720006466, -0.02659495361149311, -0.0658760815858841, -0.04504655301570892, 0.029445089399814606, -0.05707378685474396, -0.024257777258753777, -0.03485729172825813, 0.04176769405603409, 0.007925430312752724, -0.015839114785194397, 0.024502240121364594, -0.03586842864751816, 0.06175799295306206, 0.01893072947859764, 0.04072200506925583, 0.024922585114836693, 0.06582742184400558, 0.07762544602155685, 0.047387346625328064, -0.09973316639661789, -0.016830258071422577, 0.07648460566997528, 0.000502849230542779, 0.026388049125671387, 0.03761518746614456, -0.03134678304195404, 0.024481110274791718, 0.0706440731883049, -0.01304502971470356, -0.042984336614608765, -0.16296708583831787, -0.08780916035175323, 0.026024039834737778, -0.01690513640642166, -0.022983809933066368, -0.03581672161817551, 0.011822271160781384, -0.1029789000749588, -0.04617907106876373, 0.05075796693563461, -0.10334228724241257, 0.022007042542099953, -0.017193594947457314, -0.054059628397226334, -0.06382526457309723, 0.01687978021800518, 0.06426484882831573, 0.031286418437957764, -2.660479282813798e-33, 0.015455422922968864, 0.03791423141956329, -0.043344881385564804, 0.00478179519996047, 0.014319201931357384, 0.04847293347120285, 0.011457658372819424, 0.06242925301194191, -0.040730297565460205, -0.013075475580990314, -0.0036025189328938723, 0.026189008727669716, 0.053098492324352264, 0.018072502687573433, 0.019521337002515793, -0.03010520152747631, -0.01788729801774025, 0.009917831048369408, 0.0910356342792511, -0.014173857867717743, -0.02925209142267704, 0.05881115049123764, -0.1377173513174057, -0.040105678141117096, -0.05538344755768776, 0.007218572776764631, 0.031046655029058456, 0.10138744860887527, 0.033873770385980606, 0.050853922963142395, -0.045714445412158966, -0.0580136813223362, 0.004564277362078428, 0.07884052395820618, -0.009825145825743675, 0.07967483997344971, 0.12431701272726059, -0.04736088961362839, -0.042804453521966934, 0.004577485378831625, 0.0972297340631485, -0.06373561918735504, 0.0010021403431892395, 0.06086783483624458, -0.0032823195215314627, -0.049744557589292526, -0.10253577679395676, 0.07776359468698502, -0.10541027039289474, -0.04192807525396347, 0.07309328019618988, 0.0274064801633358, -0.03213241323828697, -0.07443149387836456, -0.02405594289302826, 0.043300945311784744, 0.038539621978998184, -0.0064515890553593636, 0.03589458391070366, 0.035397883504629135, -0.02792942151427269, -0.04066541790962219, -0.034986842423677444, 0.007440866436809301, -0.016082897782325745, 0.03846995159983635, -0.07190711796283722, -0.06593823432922363, 0.089264877140522, 0.003972653299570084, 0.06334861367940903, 0.017499063163995743, -0.04233163222670555, -0.029097745195031166, 0.06568015366792679, -0.10440168529748917, -0.09665457904338837, -0.07403433322906494, -0.06446678191423416, -0.034727513790130615, -0.08757641911506653, 0.05131477490067482, 0.02536395564675331, 0.031061049550771713, 0.0335543155670166, 0.01934303529560566, 0.0029977187514305115, 0.022882355377078056, 0.004738756455481052, -0.01800953783094883, 0.023405861109495163, 0.011125854216516018, -0.03406872600317001, 0.03281187638640404, -0.04662531614303589, -4.884761395373971e-08, -0.061494212597608566, 0.03132089972496033, 0.005462097004055977, 0.0549592599272728, 0.10678043216466904, 4.2193507397314534e-05, 0.03232622891664505, -0.02930000238120556, 0.005510805640369654, 0.010393040254712105, 0.03250487521290779, -0.033323902636766434, 0.03040659986436367, -0.0816735103726387, 0.015305709093809128, 0.13669738173484802, 0.09758438169956207, -0.026939569041132927, -0.017891893163323402, 0.011280370876193047, 0.05786965414881706, 0.06690434366464615, -0.026868389919400215, 0.023508649319410324, -0.018091153353452682, -0.09873956441879272, -0.053212862461805344, 0.0675799623131752, -0.006804839242249727, 0.04204690456390381, 0.022332631051540375, 0.07914488762617111, 0.054905105382204056, -0.023250192403793335, -0.060983363538980484, 0.03970765694975853, 0.027811825275421143, -0.10960407555103302, 0.006530864629894495, -0.055471524596214294, -0.04902143403887749, 0.03749170899391174, -0.14486263692378998, 0.0036926153115928173, -0.02011040784418583, -0.0649702399969101, -0.0029412147123366594, -0.10941159725189209, 0.00921990443021059, -0.008870365098118782, 0.07376222312450409, 0.03079346753656864, -0.021684298291802406, -0.020796600729227066, 0.06898701936006546, 0.018153376877307892, 0.00951513834297657, -0.04208580031991005, -0.055414777249097824, 0.09539607167243958, -0.016853133216500282, 0.05369926616549492, -0.03143438324332237, -0.07290484756231308]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\UseofMultiLayeredNetworksforCodingSpeechwithPhoneticFeatures.pdf,Deep Learning,"272 NEURAL NET RECEIVERS IN MULTIPLE-ACCESS COMMUNICATIONS Bernd-Peter Paris, Geoffrey Orsak, Mahesh Varanasi, Behnaam Aazhang Department of Electrical and Computer Engineering Rice University Houston, TX 77251-1892 ABSTRACT The application of neural networks to the demodulation of spread-spectrum signals in a multiple-access environment is considered. This study is motivated in large part by the fact that, in a multiuser system, the conventional (matched fil- ter) receiver suffers severe performance degradation as the relative powers of the interfering signals become large (the ""near-far"" problem). Furthermore, the optimum receiver, which alleviates the near-far problem, is too complex to be of practical use. Receivers based on multi-layer perceptrons are considered as a simple and robust alternative to the opti- mum solution. The optimum receiver is used to benchmark the performance of the neural net receiver; in particular, it is proven to be instrumental in identifying the decision regions of the neural networks. The back-propagation algorithm and a modified version of it are used to train the neural net. An importance sampling technique is introduced to reduce the number of simulations necessary to evaluate the performance of neural nets. In all examples considered the proposed neu- ral ~et receiver significantly outperforms the conventional recelver. INTRODUCTION In this paper we consider the problem of demodulating signals in a code-division multiple-access (CDMA) Gaussian channel. Multiple accessing in code domain is achieved by spreading the spectrum of the transmitted signals using preassigned code waveforms. The conventional method of demodulating a spread-spectrum sig- nal in a multiuser environment employs one filter matched to the desired signal. Since the conventional receiver ignores the presence of interfering signals it is reli- able only when there are few simultaneous transmissions. Furthermore, when the relative received power of the interfering signals become large (the ""near-far"" prob- lem), severe performance degradation of the system is observed even in situations with relatively low bandwidth efficiencies (defined as the ratio of the number of channel subscribers to the spread of the bandwidth) [Aazhang 87]. For this reason there has been an interest in designing optimum receivers for multi-user communica- tion systems [Verdu 86, Lupas 89, Poor 88]. The resulting optimum demodulators, Neural Net Receivers in Multiple-Access Communications 273 however, have a variable decoding delay with computational and storage complexity that depend exponentially on the number of active users. Unfortunately, this com- putational intensity is unacceptable in many applications. There is hence a need for near optimum receivers that are robust to near-far effects with a reasonable computational complexity to ensure their practical implementation. In this study, we introduce a class of neural net receivers that are based on mul- tilayer perceptrons trained via the back-propagation algorithm. Neural net receivers are very attractive alternatives to the optimum and conventional receivers due to their highly parallel structures. As we will observe, the performance of the neural net receivers closely track that of the optimum receiver in all examples considered. SYSTEM DESCRIPTION In the multiple-access network of interest, transmitters are assumed to share a radio band in a combination of the time and code domain. One way of multiple accessing in the code domain is spread spectrum, which is a signaling scheme that uses a much wider bandwidth than necessary for a given data rate. Let us assume that in a given time interval there are K active transmitters in the network. In a simple setting, the kth active user, in a symbol interval, transmits a signal from a binary signal set derived from the set of code waveforms assigned to the corresponding user. The signal is time limited to the interval [a, T], where T is the symbol duration. In this paper we will concentrate on symbol-synchronous CDMA systems. Syn- chronous systems find applications in time slotted channels with the central (base) station transmitting to remote (mobile) terminals and also in relays between cen- tral stations. The synchronous problem will also be construed as providing us with a manageable setting to better understand the issues in the more difficult asyn- chronous situation. In a synchronous CDMA system, the users maintain time syn- chronism so that the relative time delays associated with all users are assumed to be zero. To illustrate the potentials of the proposed multiuser detector, we present the application to binary PSK direct-sequence signals in coherent systems. Therefore, the signal at a given receiver is the superposition of the K transmitted signals in additive channel noise (see [Aazhang 87, Lupas 89] and references within) P K ret) = L L b~i) Akak(t - iT) cos(we[t - iT] + Ok) + nt, t E ~, (1) i=1 k=1 where P is the packet length, Ak is the signal amplitude, We is the carrier frequency, Ok is the phase angle. The symbol b1i) E {-I, + I} denotes the bit that the kth user is transmitting in the ith time interval. In this model, nt is the additive channel noise which is assumed to be a white Gaussian random process. The time-limited code waveform, denoted.by ak(t), is derived from the spreading sequence assigned to the kth user. That is, ak(t) = Ef=-~/ a)k)p(t - jTe) where pet) is the unit rectangular pulse of duration Te and N is the length of the spreading sequence. One code period !!(k) = [a~k),a~k), . . . ,a~~I] is used for spreading the signal per symbol so 274 Paris, Orsak, Varanasi and Aazhang that T = NTc • In this system, spectrum efficiency is measured as the ratio of the number of channel users to the spread factor, K/ N. In the next two sections, we first consider optimum synchronous demodulation of the multiuser spread-spectrum signal. Then, we introduce the application of neural networks to the multiuser detection problem. OPTIMUM RECEIVER Multiuser detection is an active research area with the objective of developing strate- gies for demodulation of information sent by several transmitters sharing a channel [Verdu 86, Poor 88, Varanasi 89, Lupas 89]. In these situations with two or more users of a multiple-access Gaussian channel, one filter matched to the desired signal is no longer optimum since the decision statistics are effected by the other signals (e.g., the statistics are disturbed by cross-correlations with the interfering signals). Employing conventional matched filters, because of its structural simplicity, may still be justified if the system is operating at a low bandwidth efficiency. However, as the number of users in the system with fixed bandwidth grows or as the rel- ative received powers of the interfering signals become large, severe performance degradation of the conventional matched filter is observed [Aazhang 87]. For direct- sequence spread-spectrum systems, optimum receivers obtained by Verdu and Poor require an extremely high degree of software complexity and storage, which may be unacceptable for most multiple-access systems [Verdu 86, Lupas 89]. Despite imple- mentation problems, studies on optimum demodulation illustrate that the effects of interfering signals in a CDMA system, in principle, can be neutralized. A complete study of the suboptimum neural net receiver requires a review of the maximum likelihood sequence detection formulation. Assuming that all possible information sequences are independent and equally likely, and defining !L{ i) = [b~i), b~i), ... , b}2]', it is easy to see that an optimum decision on fL{ i) is a one-shot decision in that it requires the observation of the received signal only in the ith time interval. Without loss of generality, we will therefore focus our attention on i = 0 and drop the time superscript and consider the demodulation of the vector of bits !L with the observation of the received signal in the interval [0,11- In a K -user Gaussian channel, the most likely information vector is chosen as that which maximizes the log of the likelihood function (see [Lupas 89]) where Sk(t) = Akak(t) cos(wct + Ok) is the modulating signal of the kth user. The optimum decision can also be written as ~pt = arg max {2y'IL - !L'HIL} , te{ _l,+l}K - (3) where H is the K x K matrix of signal cross-correlations such that the (k,l)th element is hk,r =< Sk(t), Sr(t) >. The vector of sufficient statistics '[ consists of the Neural Net Receivers in Multiple-Access Communications 275 outputs of a bank of J{ filters each matched to one of the signals Yk = iT r(t)Sit;(t)dt, for k = 1,2, ... ,K. (4) The maximization in (3) has been shown to be NP-complete [Lupas 89], i.e., no algorithm is known that can solve the maximization problem in polynomial time in K. This computational intensity is unacceptable in many applications. In the next section, we consider a suboptimum receiver that employs artificial neural networks for finding a solution to a maximization problem similar to (3). NEURAL NETWORK Until now the application of neural networ,ks to multiple-access communications has not drawn much attention. In this study we employ neural networks for classifying different signals in synchronous additive Gaussian channels. We assume that the information bits of the first of the K signals is of interest, therefore, the phase angle of the desired signal is assumed to be zero (i.e., (}1 = 0). Two configurations with multi-layer perceptrons and sigmoid nonlinearity are considered for multiuser detection of direct-sequence spread-spectrum signals. One structure is depicted in Figure 1.b where a layered network of percep- trons processes the sufficient statistics (4) of the multi-user Gaussian channel. In this structure the first layer of the net (referred to as the hidden layer) processes [Y1, Y2, ... , YK]. The output layer may only have one node since there is only one signal that is being demodulated. This feed-forward structure is then trained using the back-propagation algorithm [Rumelhart 86]. In an alternate configuration, the continuous-time received signal is converted to an N-dimensional vector by sampling the output of the front-end filter at the chip rate Te- 1 as illustrated in Figure 1.a. The input vector to the net can be written so that the demodulation of the first signal is viewed as a classification problem: (5) where £1(1) is the spreading code vector of the first user, 1] is a length-N vector of filtered Gaussian noise samples and L = E[=2 bkA~ COS(8k)!!(k) is the multiple- access interference vector with Ak = AkTel2, Vk = 1,2, ... ,K. The layered neural net is then trained to process the input vector for demodulation of the first user's information bits via the back-propagation algorithm. For this configuration we consider two training methods, first the multi-layer receiver is trained, via the back- propagation algorithm, to classify the parity of the desired signal (referred to as the ""trained"" example) [Lippmann 87]. In another attempt (referred to as the ""preset"" example), the input layer of the net is preset as Gaussian classifiers and the other layers are trained using the back propagation algorithm [Gschwendtner 88]. Since we are interested in understanding the internal representation of knowledge by the weights of the net, a signal space method is developed to illustrate decision regions. In a K -user system where the spreading sequences are not orthogonal, the 276 Paris, Orsak, Varanasi and Aazhang signals can be represented by orthonormal bases using the Gram-Schmidt procedure. The optimum decision regions in the signal space for the demodulation of 61 are known [Poor 88] and can be directly compared to ones for the neural net. Figure 2 illustrates decision regions for the optimum receiver and for ""preset"" and ""trained"" neural net receivers. In this example, two users are sharing a channel with N = 3, signal to noise ratio of user 1 (SN Rd equal to 8dB and relative energies of the two user, E2/ E1 = 6dB. As it is seen in this figure the decision region of the ""preset"" example is almost identical to the optimum boundary, however, the decision boundary for the ""trained"" example is quite conservative. Such comparisons are instrumental not only in identifying the pattern by which decisions are made by the neural networks but also in understanding the characteristics of the training algorithms. PERFORMANCE ANALYSIS In this paper, we motivate the application of neural nets to single-user detection in multiuser channels by comparing the performance of the receivers in Figure 1 to that of the conventional and the optimum [Poor 88]. Since exact analysis of the bit error probabilities for the neural net receivers are analytically intractable, we con- sider Monte Carlo simulations. This method can produce very accurate estimates of bit-error probability if the number of simulations is sufficiently large to ensure occurrence of several erroneous decisions. The fact that these multiuser receivers operate with near optimum error rates puts a tremendous computational burden on the computer system. The new variance reduction scheme, developed by Orsak and Aazhang in [Orsak 89], first shifts the simulated channel noise to bias the simula- tions and then scales the error rate to obtain an unbiased estimate with a reduced variance. This importance sampling technique, which proved to be extremely effec- tive in single-user detection [Orsak 89], is applied to the analysis of the multiuser systems. As discussed in [Orsak 89], the fundamental issue is to generate more errors by biasing the simulations in cases where the error rate is very small. This strategy is better described by the two-user Gaussian example in Figure 2. In this example the simulation is carried out by generating zero-mean Gaussian noise vectors 'I} , random phase (}2 and random values of the interfering bit 62 . Considering 61 = 1. (corresponding to signals +a1 + a2 or +a1 - a2 which are marked by ""+"" in Figure 2) error occurs if the statistics fall on the left side of the decision boundary. It can be shown that the most efficient biasing scheme corresponds to a shift of the mean of the Gaussian noise and the multiple-access interference such that the mean of the statistics are placed on the decision boundary (the shifted signals are marked by ""0"" in Figure 2). Since this strategy generates much more errors than the standard Monte Carlo, errors are weighted to obtain an unbiased estimate of the error rate. The importance sampling technique substantially reduces the number of simulation trials compared to standard Monte Carlo for a given accuracy. In Figure 3 the gain which is defined as the ratio of the number of trials required for a fixed variance using Monte Carlo to that using the importance sampling method, is plotted versus Neural Net Receivers in Multiple-Access Communications 277 the bit-error probability. In this example, the spreading sequence length, N is equal 3 and relative energies of the two user, E2/ El = 6dB. The gain in this example of severe near-far problem is inversely proportional to the error rate. Furthermore, results from extensive analysis indicated that the proposed importance sampling technique is well suited for problems in multi-user communications and less than 100 trials is sufficient for an accurate error probability estimate. NUMERICAL RESULTS The performance of the conventional, optimum [Poor 88] and the neural net re- ceivers are compared via Monte Carlo simulations employing the importance sam- pling method. Except for a difference in length of training periods, the two configu- rations in Figure 1 result in similar average bit-error probabilities. Results presented here correspond to the neural net receiver in Figure l.a. A two-user Gaussian channel is considered with severe near-far problem where E2/ El = 6dB and spreading sequence length N = 3. In Figure 4, the average bit-error probabilities of the four receivers (conventional, optimum, neural nets for the ""trained"" and ""preset"" examples) are plotted versus the signal to noise ratio of the first user (SN RI). It is clear from this figure that the two neural net receivers outperform the matched filter receiver over the range of SN R l . Figure 5 depicts these average error probabilities versus the relative energies of the two users (i.e., E2/ El ) for a fixed SN Rl = 8dB and N = 3. As expected the conventional receiver becomes multiple-access limited as E2 increases, however, the performance of the neural net receivers closely track that of the optimum receiver for all values of E2 • We also considered a three-user Gaussian example with a high bandwidth effi- ciency and severe near-far problem where spreading sequence length N = 3 and first and third users have equal energy and second user has four times more energy (Le., E2/ El = 6dB ). The average error probabilities of the four receivers versus SN Rl are depicted in Figure 6. The neural net receivers maintained their near optimum performance even in this three user example with a spread fae tor of 3 corresponding to a bandwidth efficiency of 1. CONCLUSIONS In this paper, we consider the problem of demodulating a signal in a multiple- access Gaussian channel. The error probability of different neural net receivers were compared with the conventional and optimum receivers in a symbol-synchronous system. As expected the performance of the conventional receiver (matched filter) is very sensitive to the strength of the interfering users. However, the error probability of the neural net receiver is independent of the strength of the other users and is at least one order of magnitude better than the conventional receiver. Except for a difference in the length of training periods, the two configurations in Figure 1 result in similar average bit-error probabilities. However, the training strategies, ""preset"" and ""trained"", resulted in slightly different error rates and decision regions. The multi-layer perceptron was very successful in the classification problem in the presence of interfering signals. In all the examples that were considered, two layers 278 Paris, Orsak, Varanasi and Aazhang of perceptrons proved to be sufficient to closely approximate the decision boundary of the optimum receiver. We anticipate that this application of neural networks will shed more light on the potentials of neural nets in digital communications. The issues facing the project were quite general in nature and are reported in many neural network studies. However, we were able to address these issues in multiple-access communications since the disturbances are structured and the optimum receiver (which is NP-hard) is well understood. References [Aazhang 87] B. Aazhang and H. V. Poor. Performance of DS/SSMA Com- munications in Impulsive Channels-Part I: Linear Correlation Receivers. IEEE Trans. Commun., COM-35(1l):1l79-1188, November 1987. [Gschwendtner 88] A. B. Gschwendtner. DARPA Neural Network Study. AFCEA International Press, 1988. [Lippmann 87] [Lupas 89] [Orsak 89] [Poor 88] [Rumelhart 86] [Varanasi 89] [Verdu 86] R. P. Lippmann and B. Gold. Neural-Net Classifiers Useful for Speech Recognition. In IEEE First Conference on Neural Net- works, pages 417-425, San Diego, CA, June 21-24, 1987. R. Lupas and S. Verdu. Linear Multiuser Detectors for Syn- chronous Code-Division Multiple-Access Channels. IEEE Trans. Info. Theory, IT-34, 1989. G. Orsak and B. Aazhang. On the Theory of Importance Sam- pling Applied to the Analysis of Detection Systems. IEEE Trans. Commun., COM-37, April, 1989. H. V. Poor and S. Verdu. Single-User Detectors for Multiuser Channels. IEEE Trans. Commun., COM-36(1):50-60, January, 1988. D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning Internal Representation by Error Propagation. In D. E. Rumel- hart and J. L. McClelland, editors, Parallel Distributed Pro- cessing: Explorations in the Microstructure of Cognition. Vol. I: Foundations, pages 318-362, MIT Press, 1986. M. K. Varanasi and B. Aazhang. Multistage Detection in Asynchronous Code-Division Multiple-Access Communications. IEEE Trans. Commun., COM-37, 1989. S. Verdu. Optimum Multiuser Asymptotic Efficiency. IEEE Trans. Commun., COM-34(9):890-897, September, 1986. Neural Net Receivers in Multiple-Access Communications 279 Sampler (n+l)T c (a) ret) Figure 1. Two Neural Net Receiver Structures. 4,---------------~.------rr------~ 2 o -2 • o : • Matched Filter .... Neural Net (preset) l , l ,. "" r' ~ l ~' Optimum Receiver A- Neural Net (trained) f I "" I I : I : .... I -4~----~~~~--~----~----~--~ -3 -2 -1 o 2 3 (b) Figure 2. Decision Boundaries of the Various Receivers. 1012~ ________________________________ ~ 10 10 10 8 .~ 10 6 C!' 10 4 10 2 Opt. Receiver Neural Net (preset) Neural Net (trained) Matched Filter 10° ~~--~~~~~~--~~--~~--~~ 10-13 10 -11 10-9 10 -7 10-5 10-3 10 -1 Prob. of Error Figure 3. Importance Sampling Gain versus Error Rate for 2-user Example. ~1 280 Paris, Orsak, Varanasi and Aazhang 10-1 r--::::~~::::=----I 10-2 10-3 15 10-4 .. 10-5 ~ 10-6 'Q 10-7 ~ 10-8 Q 10-9 ""= 10-10 to-11 to-I Matched Filter Neural Net (trained) Neural Net (preset) Opt Receiver to -13+-_...,.._--. __ ,--_...,.._--. __ ,--_ .... 2 4 6 8SNR l~ dB 12 14 16 Figure 4. Prob. of Error as a Function of the SNR (E2/El = 4). 10-1 ~------------------------------~ Matched Filter Neural Net (trained) Neural Net (preset) Opt. Receiver 104+---~--~--~--~--~--,---~~ o 1 2 E2/El 3 4 Figure 5. Influence of MA-Interference (SNR = 8dB). 10-1 '---~~-'--~F.:~~==~====---, 10 -2 10 -3 10-4 10 -5 10 -6 10 -7 10 -8 10 -9 10 -10 10 ~1l 10 -12 Neural Net (trained) Neural Net (preset) Opt. Receiver 10 -13-+-'I'""'""""""II""""""""I""' ........ ~.......,--r' ........ ~......., __ -r-..,--.......,.--.--r--t 2 4 6 8 10 12 14 16 SNR in dB Figure 6. Error Curves for the 3-User Example.","[-0.01931804046034813, -0.012656583450734615, -0.022243479266762733, -0.006042798049747944, -0.003934812266379595, 0.02541000209748745, 0.06695020943880081, -0.02965458855032921, -0.017373280599713326, -0.05072537809610367, -0.04858224838972092, 0.04367094486951828, -0.0021095143165439367, -0.09347005933523178, -0.04290129244327545, -0.0030190187972038984, 0.05333699658513069, 0.003051815088838339, -0.03284216299653053, -0.0419476144015789, 0.011096841655671597, 0.0017394755268469453, 0.0036487088073045015, -0.044584114104509354, 0.008936172351241112, -0.017235644161701202, 0.006747575011104345, 0.017298130318522453, 0.019996054470539093, 0.004740218631923199, 0.02975151501595974, -0.005492661148309708, 0.026193896308541298, 0.007121475413441658, -0.11475541442632675, -0.003570371074602008, -0.051303353160619736, 0.02055760659277439, 0.012405440211296082, 0.04533792659640312, -0.014014351181685925, -0.0044570923782885075, -0.038435280323028564, 0.022828366607427597, 0.056943707168102264, -0.01081772893667221, 0.0007679625996388495, 0.04561099782586098, 0.04784873500466347, -0.08104806393384933, 0.01448191050440073, 0.03285115212202072, -0.007229569833725691, 0.11172490566968918, -0.001309889368712902, -0.024035532027482986, -0.061789244413375854, 0.03634240850806236, -0.07135146111249924, 0.08793678879737854, -0.042930494993925095, 0.044418055564165115, -0.02408689260482788, -0.0868750810623169, 0.01930186152458191, 0.02076050639152527, 0.03789328411221504, 0.04526515677571297, 0.019953692331910133, -0.03029111959040165, -0.017834236845374107, 0.047029945999383926, -0.00045719192712567747, 0.03787379711866379, 0.03689569607377052, 0.0056587099097669125, -0.002998871263116598, -0.026197852566838264, 0.007776571437716484, 0.004922803491353989, 0.004509679041802883, -0.008877262473106384, -0.06031116470694542, -0.07271502166986465, 0.037020131945610046, 0.02744241990149021, -0.06330954283475876, 0.010402808897197247, -0.013698894530534744, -0.05007171258330345, -0.04228341579437256, -0.0007860639598220587, -0.020374735817313194, 0.01960926502943039, 0.046826958656311035, 0.01920795813202858, -0.04446626082062721, -0.022517330944538116, 0.001722179469652474, 0.07861936837434769, 0.009315558709204197, -0.04781344532966614, -0.004834455903619528, -0.05573633685708046, -0.003090196056291461, -0.010924381203949451, 0.05761931464076042, 0.04144006967544556, 0.10342264920473099, -0.0882706269621849, -0.04814776033163071, 0.04423898085951805, -0.054878346621990204, 0.03661452233791351, -0.0038513559848070145, 0.04909659922122955, 0.062016360461711884, 0.06412570178508759, 0.02593994140625, -0.018643001094460487, -0.09869392961263657, -0.02607543393969536, 0.0040493253618478775, 0.06167314574122429, 0.04181919991970062, -0.035471029579639435, -0.07074786722660065, 7.85179073977928e-34, -0.11482309550046921, 0.02897164598107338, -0.0424809530377388, -0.035194236785173416, -0.02257908694446087, -0.015048899687826633, 0.05024155229330063, 0.025964194908738136, 0.045962538570165634, 0.06770957261323929, -0.08742445707321167, 0.0026429505087435246, -0.00635744770988822, -0.03225889801979065, 0.06214073300361633, -0.05895381048321724, -0.006478413473814726, -0.0019036703743040562, -0.005441964138299227, -0.04337942227721214, 0.05979444086551666, -0.05882692337036133, 0.041484441608190536, -0.006824370939284563, 0.07832322269678116, 0.01748473010957241, -0.0026825678069144487, 0.0431220643222332, 0.04697812348604202, 0.019627660512924194, 0.04660560563206673, 0.02762615866959095, -0.011964951641857624, -0.04793127626180649, 0.09491762518882751, -0.059890277683734894, -0.040940698236227036, 0.05834158882498741, 0.05037306249141693, 0.008381123654544353, -0.07206180691719055, -0.04920336976647377, -0.031171530485153198, -0.0007346085621975362, -0.006604465190321207, -0.10285735875368118, 0.0008524778531864285, 0.07992996275424957, -0.016675284132361412, -0.04556263983249664, -0.03157157823443413, -0.022542357444763184, -0.07902402430772781, -0.014495529234409332, 0.04771963134407997, 0.0005552200018428266, 0.06415024399757385, 0.06947162002325058, 0.035600338131189346, 0.14452534914016724, -0.05066899210214615, -0.06014904007315636, 0.01282245572656393, 0.007154218386858702, 0.11281485110521317, -0.014964534901082516, -0.05966459587216377, 0.026378365233540535, 0.031249547377228737, 0.010260753333568573, 0.007538655772805214, 0.036997873336076736, -0.00926939770579338, -0.06422847509384155, 0.008985036984086037, -0.04371968284249306, 0.007509649731218815, 0.12532095611095428, 0.0005513805663213134, 0.004167755600064993, -0.043408650904893875, 0.050107866525650024, -0.10174993425607681, 0.02310679480433464, -0.025387432426214218, 0.137472465634346, 0.020167898386716843, -0.03454968333244324, -0.02279192954301834, 0.023396188393235207, -0.03826531767845154, 0.07166716456413269, -0.005996575113385916, -0.07099495828151703, -0.030156875029206276, -1.0191492858946038e-33, -0.052588049322366714, 0.09190528094768524, -0.06318134069442749, -0.005988575983792543, -0.04394062981009483, -0.06743280589580536, -0.002654709853231907, 0.046361394226551056, -0.015483563765883446, 0.06773550063371658, -0.016054373234510422, -0.02793016843497753, 0.04780125990509987, -0.02409277856349945, 0.010929820127785206, 0.051841817796230316, -0.0342097207903862, 0.011757314205169678, 0.07071904093027115, 0.0031944415532052517, -0.022058019414544106, 0.06521235406398773, -0.0027794819325208664, -0.05553490296006203, -0.030554890632629395, -0.05531078204512596, -0.07556933164596558, 0.1272396296262741, 0.01906058005988598, -0.06540658324956894, -0.006395956035703421, -0.028005890548229218, -0.01868252269923687, -0.005285210441797972, 0.059191133826971054, -0.02645101398229599, 0.07114361971616745, 0.05288344621658325, 0.02802889607846737, 0.07712072134017944, 0.0766618400812149, 0.010924072004854679, -0.03481179475784302, -0.03695736080408096, -0.008225209079682827, -0.023122316226363182, -0.09950579702854156, -0.0046440367586910725, -0.07672623544931412, -0.040525954216718674, 0.11868414282798767, -0.03543626517057419, 0.009308015927672386, -0.05062614753842354, 0.007250735070556402, 0.07857417315244675, 0.03499230742454529, 0.06528786569833755, 0.11717936396598816, 0.02201550267636776, 0.028187915682792664, -0.1404992938041687, 0.10632245242595673, -0.021935509517788887, 0.054782621562480927, 0.044449638575315475, 0.026168067008256912, 0.07689093053340912, 0.07393210381269455, 0.024531735107302666, -0.039820339530706406, -0.028469139710068703, 0.042009007185697556, 0.032673079520463943, -0.07754699885845184, 0.02982088178396225, -0.06027931720018387, -0.002124856226146221, -0.006203086115419865, 0.03379755839705467, -0.06262338906526566, -0.06055663153529167, -0.09806552529335022, -0.02236151322722435, 0.022820182144641876, 0.05052638426423073, 0.10137475281953812, -0.019192412495613098, 0.011405573226511478, -0.06719387322664261, -0.008490069769322872, 0.13494795560836792, -0.009397532790899277, -0.018225261941552162, -0.050993531942367554, -5.574797867780035e-08, -0.09448231011629105, -0.024121118709445, -0.07400964200496674, 0.012688640505075455, 0.09031638503074646, -0.013454793952405453, 0.004669366404414177, -0.09640169143676758, -0.021648939698934555, -0.11074908822774887, 0.032660119235515594, -0.05042142793536186, -0.009450340643525124, 0.01586705446243286, 0.04372456669807434, -0.04196162894368172, -0.06317013502120972, -0.0893203541636467, 0.031662821769714355, 0.03939277306199074, 0.025286953896284103, 0.02620549686253071, 0.03534293174743652, 0.0541590191423893, 0.09017892181873322, 0.0008818572387099266, 0.0003440705477260053, 0.029557004570961, 0.00556530337780714, -0.00820380449295044, 0.031055988743901253, 0.0068716187961399555, 0.03016652911901474, 3.4078497264999896e-05, 0.06008122116327286, 0.11008497327566147, -0.0264480859041214, -0.013254513964056969, -0.04518852382898331, 0.08288803696632385, 0.014327344484627247, 0.01991523616015911, -0.010646219365298748, 0.0467110313475132, 0.030660875141620636, -0.05891608074307442, 0.1261371672153473, -0.061282698065042496, 0.046308476477861404, 0.005542567931115627, 0.10232677310705185, -0.01877610571682453, -0.01962733455002308, -0.06496306508779526, -0.002305918140336871, -0.08218198269605637, -0.0046744090504944324, -0.16136112809181213, -0.018884601071476936, 0.09650922566652298, -0.08455178141593933, 0.08430807292461395, -0.06745963543653488, 0.03535718470811844]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\UsingBackpropagationwithTemporalWindowstoLearntheDynamicsoftheCMUDirectDriveArmII.pdf,Computer Vision,"A Computer Modeling Approach to Understanding 117 A computer modeling approach to understanding the inferior olive and its relationship to the cerebellar cortex in rats Maurice Lee and James M. Bower Computation and Neural Systems Program California Institute of Technology Pasadena, CA 91125 ABSTRACT This paper presents the results of a simulation of the spatial relationship between the inferior olivary nucleus and folium crus IIA of the lateral hemisphere of the rat cerebellum. The principal objective of this modeling effort was to resolve an apparent conflict between a proposed zonal organization of olivary projections to cerebellar cortex suggested by anatomical tract-tracing experiments (Brodal & Kawamura 1980; Campbell & Armstrong 1983) and a more patchy organization apparent with physiological mapping (Robertson 1987). The results suggest that several unique features of the olivocerebellar circuit may contribute to the appearance of zonal organization using anatomical techniques, but that the detailed patterns of patchy tactile projections seen with physiological techniques are a more accurate representation of the afferent organization of this region of cortex. 1 INTRODUCTION Determining the detailed anatomical structure of the nervous system has been a major focus of neurobiology ever since anatomical techniques for looking at the fine structure of individual neurons were developed more than 100 years ago (Ram6n y Cajal 1911). In more recent times, new techniques that allow labeling of the distant targets of groups of neurons have extended this investigation to include studies of the topographic relationships between different brain regions. In general, these so-called ""tract-tracing"" techniques have greatly extended our knowledge of the interrelationships between neural structures, often guiding and reinforcing the results of physiological investigations (DeYoe & Van Essen 1988). However, in some cases, anatomical and physiological techniques have been interpreted as producing conflicting results. One case, considered here, involves the pattern of neuronal projections from the inferior olivary nucleus to the 118 Lee and Bower cerebellar cortex. In this paper we describe the results of a computer modeling effort, based on the structure of the olivocerebellar projection, intended to resolve this conflict. a c b e Figure 1. a: Profile of the rat brain, showing three areas (Cx, cerebral cortex; Po, pons; Tr, spinal trigeminal nucleus) that project to the cerebellum (Cb) via both climbing fiber (CF) pathways through the inferior olive (10) and mossy fiber (MF) pathways. b: Magnified. highly simplified view of the cerebellar cortex, showing a Purkinje cell (P) being supplied with climbing fiber input, directly, and mossy fiber input. through the granule cells (G). c: Zonal organization of the olivocerebellar projection. Different shading patterns represent input from different areas of the inferior olive. Adapted from Campbell & Armstrong 1983. Circled area (crus llNcrus UB) is enlarged in Figure 1d; bracketed area (anterior lobe) is enlarged in Figure Ie. d: Detail of zonal organization. Dark areas represent bands of Purkinje cells that stain positive for monoclonal antibody Zehrin I. According to Gravel et al. 1987, these bands have boundaries similar to those resulting from partial tracer injections in the inferior olive. Adapted from Gundappa-Sulur et al. 1989. e: Patchy organization of the olivocerebellar projection (partial map). Different shading patterns represent input through the olive from different body surfaces. The horizontal and vertical scales are different. Adapted from Logan & Robertson 1986. A Computer Modeling Approach to Understanding 119 2 THE OLIVO CEREBELLAR SYSTEM Purlcinje cells, the principal neurons of the cerebellar cortex, are influenced by two major excitatory afferent projections to the cerebellum, the mLJSSY fiber system and the climbing fiber system (palay & Chan-Palay 1973). As shown in Figures la and Ib, mossy fibers arise from many different nuclei and influence Purkinje cells through granule cells within the cortex. Within the cortex the mossy fiber-granule cell-Purkinje cell circuit is characterized by enormous divergence (a single mossy fiber may influence several thousand Purkinje cells) and convergence (a single Purkinje cell may be influenced by several hundred thousand mossy fibers). In contrast, as also shown in Figures la and Ib, climbing fibers arise from a single source, the inferior olive, and exhibit severely limited divergence (10-15 Purkinje cells) and convergence (I Purkinje cell). Because the inferior olive is the sole source of the climbing fiber projection to the entire cerebellar cortex, and each Purkinje cell receives only one climbing fiber, the spatial organization of the olivocerebellar circuit has been the subject of a large research effort (Brodal & Kawamura 1980). Much of this effort has involved anatomical tract-tracing techniques in which injections of neuron ally absorbed substances are traced from the inferior olive to the cerebellum or vice versa. Based on this work it has been proposed that the entire cerebellum is organized as a series of strips or zones, oriented in a parasagittal plane (Figures Ic, Id: Campbell & Armstrong 1983; Gravel et al. 1987). This principle of organization has served as the basis for several functional speculations on the role of the cerebellum in coordinating movements (Ito 1984; Oscarsson 1980). Unfortunately, as suggested in the introduction, these anatomical results are somewhat at odds with the pattern of organization revealed by detailed electrophysiological mapping studies of olivary projections (Robertson 1987). Physiological results, summarized in Figure Ie, suggest that rather than being strictly zone-like, the olivocerebellar projection is organized more as a mosaic of parasagittally elongated patches. 3 THE MODEL Our specific interests are with the tactilely responsive regions of the lateral hemispheres of the rat cerebellum (Bower et al. 1981; Welker 1987), and the modeling effort described here is a first step in using structural models to explore the functional organization of this region. As with previous modeling efforts in the olfactory system (Bower 1990), the current model is based on features of the anatomy and physiology of the real system. In the following section we will briefly describe these features. 3.1 ANATOMICAL ORGANIZATION Structure of the inferior olive. The inferior olive has a complex, highly folded conformation (Gwyn et al. 1977). The portion of the olive simulated in the model consists of a folded slab of 2520 olivary neurons with a volume of approximately 0.05 mm3 (Figure 2a). Afferent projections to the olive. While inputs of various kinds and origins converge on this nucleus, we have limited those simulated here to tactile afferents from those 120 Lee and Bower perioral regions known to influence the lateral cerebellar hemispheres (Shambes et al. 1978). These have been mapped to the olive following the somatotopically organized pattern suggested by several previous experiments (Gellman et al. 1983). Structure or the cerebellum. The cerebellum is represented in the model by a flat sheet of 2520 Purkinje cells with an area of approximately 2 mm1 (Figure 2a). Within this region. each Purkinje cell receives input from one. and only one. olivary neuron. Details of Purlcinje cells at the cellular level have not been included in the current model. a b Figure 2. a: Basic structure of the model. Folia crus I1A and crus lIB of the cerebellum and a cross section of the inferior olive are shown, roughly to scale. The regions simulated in the model are outlined. Clusters of neighboring olivary neurons project to parasagittal strips of Purkinje cells as indicated. This figure also shows simulated correlation results similar to those in Figure lb. b: Spatial structure of correlations among records of climbing fiber activity in crus IIA. Sizes of filled circles represent cross-correlation coefficients with respect to the ""master"" site (open circle). Sample cross-correlograms are shown for two sites as indicated. The autocorrelogram for the ""master"" site is also shown. Adapted from Sasaki et al. 1989. 3.2 PHYSIOLOGICAL ORGANIZATION Spatially correlated patterns or activity. When the activities of multiple climbing fibers are recorded from within cerebellar cortex, there is a strong tendency for climbing fibers supplying Purkinje cells oriented parasagittally with respect to each other to be correlated in their firing activity (Sasaki et al. 1989: Figure 2b). It has been suggested that these correlations reflect the fact that direct electrotonic couplings exist between olivary neurons (Llinas & Yarom 1981a, b; Benardo & Foster 1986). These physiological results are simulated in two ways in the current model. First. neighboring olivary neurons are electrotonically coupled, thus firing in a correlated manner. Second. small clusters of olivary neurons have been made to project to parasagittally oriented strips of Purkinje A Computer Modeling Approach to Understanding 121 cells. Under these constraints. the model replicates the parasagittal pattern of climbing fiber activity found in certain regions of cerebellar cortex (compare Figures 2a and 2b). Topography or cerebeUar afferents. As discussed above. this model is intended to explore spatial and functional relationships between the inferior olive and the lateral hemispheres of the rat cerebellum. Unfortunately. a physiological map of the climbing fiber projections to this cerebellar region does not yet exist for the rat. However. a detailed map of mossy fiber tactile projections to this region is available (Welker 1987). As in the climbing fiber map in the anterior lobe (Robertson 1987; Figure Ie) and mossy fiber maps in various areas in the cat (Kassel et al. 1984). representations of different parts of the body surface are grouped into patches with adjacent patches receiving input from nonadjacent peripheral regions. On the assumption that the mossy fiber and climbing fiber maps coincide. we have based the modeled topography of the olivary projection to the cerebellum on the well-described mossy fiber map (Figure 3a). In the model, the smoothly varying topography of the olive is transformed to the patchy organization of the cerebellar cortex through the projection pathways taken to the cerebellum by different climbing fibers. a b .-. -.:;:"":. Figure 3. a: Organization of receptive field map in simulated region of crus IIA. Different shading patterns represent input from different perioral surfaces. b: Simulated tract-tracing experiment. Left, tracer visualization (dark areas) in the cerebellum. Right. tracer uptake (dark areas) in the inferior olive. 122 Lee and Bower 4 RESULTS: SIMULATION OF ZONAL ORGANIZATION Having constructed the model to include each of the physiological features described above. we proceeded to replicate anatomical tract-tracing experiments. This was done by simulating the chemical labeling of neurons within restricted areas of inferior olive and following their connections to the cerebellum. As in the biological experiments. in many cases simulated injections included several folds of the olivary nucleus (Figure 3b). The results (Figure 3b) demonstrate patterns of labeling remarkably similar to those seen with real olivary injections in the rat (compare Figures Id and 3b). 5 CONCLUSIONS AND FURTHER WORK These simulation results have demonstrated that a broadly parasagittal organization can be generated in a model system which is actually based on a fine-grained patchy pattern of afferent projections. Further, the simulations allow us to propose that the appearance of parasagittal zonation may result from several unusual features of the olivary nucleus. First. the folding characteristic of the inferior olive likely places neurons with different receptive fields within a common area of tracer uptake in any given anatomical experiment. resulting in co-labeling of functionally different regions. Second. the tendency for local clusters of olivary neurons to project to parasagittal strips of Purkinje cells could serve to extend tracer injection in the parasagittal direction. enhancing the impression of parasagittal zones. This is further reinforced by the tendency of the patches themselves to be somewhat elongated in the parasagittal plane. Finally, the restricted resolution of the anatomical techniques could very well contribute to the overall impression of parasagittal zonation by obscuring small, unlabeled regions more apparent using physiological procedures. Modeling efforts currently under way will extend these results to more than one cerebellar folium in an attempt to account for the appearence of transfolial zones in some preparations. In addition to these interpretations of previous data, this model also provides both directions for further physiological experiments and predictions concerning the results. First, the model assumes that mossy fiber and climbing fiber projections representing the same regions of the rat's body surface overlap in the cerebellum. We take the similarity in modeled and real tract-tracing results (Figures Id and 3b) as suggesting strongly that this is, in fact. the case; however. physiological experiments are currently underway to test this hypothesis. Second, the model predicts that the parasagittal pattern of climbing fiber correlations found in a particular cerebellar region will be dependent on the pattern of tactile patches found in that region. Those regions containing large patches (e.g. the center of crus IIA) should clearly show parasagittal strips of correlated climbing fiber activity. However, in cortical regions containing smaller, more diverse sets of patches (e.g. more medial regions of crus IIA), this correlation structure should not be as clear. Experiments are also under way to test this prediction of the model. A Computer Modeling Approach to Understanding 123 Acknowledgements This model has been constructed using GENESIS, the Caltech neural simulation system. Simulation code for the model presented here can be accessed by registered GENESIS users. Information on the simulator or this model can be obtained from genesiS@caltech.bitnet. This work was supported by NIH grant BNS 22205. References Benardo, L. S., and R. E. Foster 1986. Oscillatory behavior in inferior olive neurons: Mechanism. modulation. cell aggregates. Brain Res. Bull. 17:773-784. Bower. J. M. 1990. Reverse engineering the nervous system: An anatomical. physiological. and computer based approach. In An introduction to neural and electronic networks. ed. S. Zornetzer. J. Davis. and C. Lau, pp. 3-24. Academic Press. Bower, J. M .• and J. Kassel 1989. Variability in tactile projection patterns to crus ITA of the Norway rat. J. Neurosci. (submitted for publication). Bower, J. M., D. H. Beermann, J. M. Gibson. G. M. Shambes. and W. Welker 1981. Principles of organization of a cerebro-cerebellar circuit. Micromapping the projections from cerebral (SI) to cerebellar (granule cell layer) tactile areas of rats. Brain Behav. Evol. 18:1-18. Brodal. A .• and K. Kawamura 1980. Olivocerebellar projection: A review. Adv. Anat. Embryol. Cell Bioi. 64:1-140. Campbell, N. C., and D. M. Armstrong 1983. Topographical localization in the olivocerebellar projection in the rat: An autoradiographic study. Brain Res. 275:235-249. DeYoe. E. A., and D. C. Van Essen 1988. Concurrent processing streams in monkey visual cortex. Trends Neurosci. 11:219-226. Gellman. R, J. C. Hook, and A. R Gibson 1983. Somatosensory properties of the inferior olive of the cat. J. Compo Neurol. 215:228-243. Gravel. C .• L. M. Eisenman. R Sasseville, and R. Hawkes 1987. Parasagittal organization of the rat cerebellar cortex: Direct correlation between antigenic Purkinje cell bands revealed by mabQ 113 and the organization of the olivocerebellar projection. J. Compo Neurol. 265:294-310. Gundappa-Sulur. G., H. Shojaeian. M. Paulin, L. Posakony, R. Hawkes, and J. M. Bower 1989. Variability in and comparisons of: 1) tactile projections to the granule cell layers of cerebellar cortex; and 2) the spatial distribution of Zebrin I-labeled Purkinje cells. Soc. Neurosci. Abstr. 15:612. Gwyn, D. G., G. P. Nicholson, and B. A. Flumerfelt 1977. The inferior olivary nucleus of the rat: A light and electron microscopic study. J. Compo Neurol. 174:489-520. Ito. M. 1984. The cerebellum and neural control. Raven Press. Kassel, J .• G. M. Shambes. and W. Welker 1984. Fractured cutaneous projections to the granule cell layer of the posterior cerebellar hemispheres of the domestic cat. J. Compo Neurol. 225:458-468. Llinas, R., and Y. Yarom 1981a. Electrophysiology of mammalian inferior olivary neurones in vitro. Different types of voltage-dependent ionic conductances. J. 124 Lee and Bower Physiol. (Lond.) 315:549-567. Llinas, R., and Y. Yarom 1981b. Properties and distribution of ionic conductances generating electroresponsiveness of mammalian inferior olivary neurones in vitro. J. Physiol. (Lond.) 315:568-584. Logan, K., and L. T. Robertson 1986. Somatosensory representation of the cerebellar climbing fiber system in the rat. Brain Res. 372:290-300. Oscarsson, O. 1980. Functional organization of olivary projection to the cerebellar anterior lobe. In The inferior olivary nucleus: Anatomy and physiology, ed. J. Courville, C. de Montigny, and Y. Lammare, pp. 279-289. Raven Press. Palay, S. L., and V. Chan-Palay 1973. Cerebellar cortex: Cytology and organization. Springer-Verlag. Ram6n y Cajal, S. 1911. Histologie du systeme nerveux de l' homme et des vertebres. Maloine. Robertson, L. T. 1987. Organization of climbing fiber representation in the anterior lobe. In New concepts in cerebellar neurobiology, ed. J. S. King, pp. 281-320. Alan R. Liss. Sasaki, K., J. M. Bower, and R. Llinas 1989. Multiple Purkinje cell recording in rodent cerebellar cortex. Eur. J. Neurosci. (submitted for publication). Shambes, G. M., J. M. Gibson, and W. Welker 1978. Fractured somatotopy in granule cell tactile areas of rat cerebellar hemispheres revealed by micromapping. Brain Behav. Evol. 15:94-140. Welker, W. 1987. Spatial organization of somatosensory projections to granule cell cerebellar cortex: Functional and connectional implications of fractured somatotopy (summary of Wisconsin studies). In New concepts in cerebellar neurobiology, ed. J. S. King, pp. 239-280. Alan R. Liss.","[0.008180462755262852, -0.11322809010744095, 0.029168182983994484, -0.019661154597997665, -0.07080883532762527, -0.024013282731175423, -0.057504162192344666, 0.03512018918991089, 0.07915057241916656, -0.01796097122132778, 0.020260857418179512, -0.10341968387365341, 0.033961210399866104, -0.0004141523677390069, 0.00566264009103179, -0.13809293508529663, -0.0331256128847599, 0.037660472095012665, -0.028414299711585045, -0.01846109703183174, 0.027556288987398148, 0.0037052645348012447, -0.015244659967720509, 0.024029435589909554, -0.08233930170536041, -0.03638489171862602, -0.046148013323545456, 0.06547054648399353, -0.027975741773843765, -0.0638347938656807, 0.07703934609889984, -0.00814241822808981, 0.03595922514796257, 0.00022172373428475112, -0.04174019768834114, -0.039070483297109604, -0.03282958269119263, -0.09815220534801483, 0.03401680290699005, -0.040623001754283905, -0.02422182448208332, 0.04343228414654732, 0.000495895859785378, 0.059082113206386566, -0.023283736780285835, 0.10801278054714203, 0.08939003199338913, -0.05559266358613968, -0.045718733221292496, -0.058189187198877335, -0.0166670773178339, -0.10350176692008972, 0.00423223664984107, 0.05437251180410385, -0.05061832070350647, 0.15811051428318024, -0.01131939236074686, -0.06157978996634483, 0.01395957637578249, 0.016693785786628723, -0.06505388766527176, -0.0005534248193725944, 0.036781709641218185, 0.04786514490842819, 0.09542892128229141, 0.03739115968346596, -0.008895335718989372, -0.025505537167191505, -0.07745591551065445, -0.07359083741903305, 0.07980242371559143, -0.03325353562831879, 0.03360620513558388, -0.01338282786309719, 0.01534183043986559, -0.011597621254622936, -0.054007045924663544, 0.08256867527961731, 0.03654157370328903, -0.07727372646331787, 0.002968738554045558, 0.12185055017471313, -0.05405798181891441, 0.054061420261859894, 0.011106041260063648, 0.029421953484416008, -0.02008330635726452, 0.002499089576303959, -0.014612388797104359, 0.03807728737592697, 0.07031098753213882, -0.02510119043290615, -0.08978215605020523, -0.02676641196012497, -0.010090605355799198, -0.018469929695129395, 0.10196244716644287, 0.014021758921444416, 0.01590007357299328, -0.0012172588612884283, 0.04629813879728317, -0.02222564071416855, 0.0901794284582138, -0.03297405317425728, 0.05782949551939964, 0.00981864333152771, 0.10837699472904205, -0.0026251538656651974, -0.07641434669494629, -0.024710383266210556, -0.06165873631834984, -0.052773475646972656, -0.023596134036779404, -0.009944834746420383, 0.02951851487159729, -0.050275273621082306, 0.024644948542118073, 0.06838764995336533, 0.07960233837366104, -0.06458774954080582, 0.019027817994356155, -0.02217399701476097, 0.018650852143764496, -0.03142351284623146, -0.022036807611584663, 0.01903536543250084, -0.12555678188800812, 5.5293039190513074e-33, -0.04751751571893692, -0.02154761180281639, -0.023308707401156425, -0.01162730623036623, 0.01427591871470213, -0.016845250502228737, 0.02873685210943222, 0.07038542628288269, 0.05578043684363365, -0.03697844594717026, -0.09219483286142349, 0.01564633660018444, -0.0395624004304409, 0.05038441717624664, 0.02330981008708477, -0.013063011690974236, -0.033341795206069946, -0.012277108617126942, -0.045552413910627365, -0.08794842660427094, 0.017466334626078606, -0.003221317194402218, 0.03585035353899002, -0.041136521846055984, -0.04156825318932533, 0.050179921090602875, -0.12238280475139618, -0.03693542629480362, -0.05861889570951462, 0.01470945030450821, 0.0009223531233146787, 0.01657230593264103, -0.06609191000461578, -0.01742755062878132, 0.009287071414291859, -0.007487548049539328, 0.10937689244747162, -0.0730913057923317, -0.011644511483609676, -0.02509135752916336, 0.023763174191117287, 0.018165869638323784, 0.019101422280073166, 0.0013112099841237068, -0.0024110344238579273, 0.0004950951552018523, 0.002648756606504321, -0.0003206514520570636, 0.08056776225566864, -0.05057288333773613, 0.03473380580544472, -0.02719140239059925, 0.1071271002292633, -0.11598208546638489, 0.02526165172457695, 0.07445786893367767, -0.05179367586970329, -0.02427944727241993, -0.06725314259529114, 0.029356954619288445, 0.012637506239116192, 0.0342516265809536, 0.00717462133616209, -0.015171986073255539, 0.07022195309400558, 0.08240588754415512, -0.10636553913354874, -0.04108573868870735, 0.01694599911570549, -0.0015552145196124911, -0.0550946407020092, -0.039058439433574677, 0.011208373121917248, 0.0076156132854521275, 0.04251080006361008, 0.009715033695101738, -0.02635663002729416, 0.009445530362427235, -0.09869372099637985, -0.09634792059659958, -0.030285626649856567, 0.06468930095434189, -0.027418658137321472, 0.011642548255622387, 0.07944914698600769, 0.03371110185980797, 0.08846483379602432, 0.07918927073478699, -0.014697924256324768, 0.05372016504406929, 0.0111906873062253, -0.03706460818648338, 0.04100975766777992, 0.002453630557283759, -0.03051789477467537, -6.462940674533894e-33, -0.09753329306840897, 0.06314102560281754, 0.03325767442584038, -0.01221531629562378, -0.05105980113148689, -0.049115683883428574, -0.035206686705350876, -0.03809399902820587, -0.01875658705830574, -0.031993649899959564, -0.014364242553710938, 0.05335339531302452, 0.057476188987493515, -0.05520171299576759, 0.09156713634729385, -0.0023939332459121943, -0.033572208136320114, 0.06122999265789986, -0.023571088910102844, -0.12213235348463058, 0.021960429847240448, 0.02493603527545929, -0.03675489127635956, -0.08388432115316391, -0.004494513850659132, 0.03330850973725319, -0.014539974741637707, 0.02874700166285038, 0.042702775448560715, 0.02231903001666069, -0.041711293160915375, 0.012142390012741089, -0.011106711812317371, 0.015455464832484722, -0.0015700246440246701, -0.006863793823868036, -0.046428781002759933, -0.06868904829025269, -0.013632371090352535, -0.05573143810033798, -0.0018600720213726163, 0.036559052765369415, 0.027730535715818405, -0.016642365604639053, 0.04397696256637573, -0.04453199729323387, -0.001262165722437203, 0.030775832012295723, -0.06317169219255447, 0.0837399959564209, -0.05443919077515602, 0.014141809195280075, -0.10243739187717438, -0.07555055618286133, 0.04326438903808594, 0.008704501204192638, 0.0319373793900013, -0.07442908734083176, 0.061672139912843704, -0.07987261563539505, -0.04623333364725113, -0.09075331687927246, -0.01866377890110016, -0.004375795368105173, 0.0013353307731449604, -0.002467050217092037, -0.028354717418551445, 0.04636634141206741, 0.06538669019937515, 0.07543451339006424, 0.013631769455969334, 0.05141349881887436, 0.020129390060901642, -0.02466190978884697, 0.021116239950060844, 0.07133860141038895, 0.010213621892035007, 0.015597278252243996, -0.014765364117920399, 0.0027683067601174116, -0.0382780060172081, -0.019050871953368187, 0.032409701496362686, -0.0127036077901721, 0.032967209815979004, 0.02667725831270218, -0.018093382939696312, -0.019117366522550583, 0.03420396149158478, -0.05811113864183426, 0.04249168187379837, -0.04163050651550293, 0.0312493946403265, 0.03525976091623306, 0.05564943328499794, -5.1033339332207106e-08, -0.04274299368262291, 0.015624422580003738, 0.0551082119345665, 0.0170265045017004, 0.03238267824053764, -0.02083573304116726, 0.04752429574728012, -0.04904395714402199, -0.06857120990753174, -0.01221751980483532, 0.004201049450784922, -0.0028364623431116343, 0.07055581361055374, 0.03486531972885132, -0.03314780071377754, 0.10799407958984375, 0.025644859299063683, 0.06378223747015, -0.01503823883831501, 0.04637289419770241, 0.06515464931726456, -0.05082235485315323, -0.012902318499982357, 0.10842680931091309, 0.06252061575651169, -0.0906803086400032, -0.07726078480482101, 0.10795318335294724, -0.08395132422447205, 0.04018611088395119, 0.09244944900274277, 0.018779009580612183, 0.05629413202404976, -0.01352729182690382, 0.003212441923096776, 0.016690131276845932, -0.005737324245274067, 0.002416854025796056, -0.06523340940475464, 0.04642294719815254, 0.012894156388938427, -0.0873524621129036, 0.04205351695418358, 0.026775212958455086, 0.10877200216054916, 0.05163853242993355, 0.08183842897415161, 0.010268405079841614, 0.012314108200371265, -0.0503477044403553, -0.033571548759937286, 0.06587676703929901, 0.010273594409227371, 0.07405593991279602, -0.04410979524254799, -0.06482608616352081, -0.018300767987966537, 0.028910815715789795, -0.015781495720148087, 0.02588120475411415, 0.026002489030361176, 0.13646601140499115, -0.01417563483119011, -0.07239576429128647]"
papers\Advances in Neural Information Processing Systems 1  (NIPS 1988)\WinnerTakeAllNetworksofONComplexity.pdf,Deep Learning,"160 SCALING AND GENERALIZATION IN NEURAL NETWORKS: A CASE STUDY Subutai Ahmad Center for Complex Systems Research University of Illinois at Urbana-Champaign 508 S. 6th St., Champaign, IL 61820 ABSTRACT Gerald Tesauro IBM Watson Research Center PO Box 704 Yorktown Heights, NY 10598 The issues of scaling and generalization have emerged as key issues in current studies of supervised learning from examples in neural networks. Questions such as how many training patterns and training cycles are needed for a problem of a given size and difficulty, how to represent the inllUh and how to choose useful training exemplars, are of considerable theoretical and practical importance. Several intuitive rules of thumb have been obtained from empirical studies, but as yet there are few rig- orous results. In this paper we summarize a study Qf generalization in the simplest possible case-perceptron networks learning linearly separa- ble functions. The task chosen was the majority function (i.e. return a 1 if a majority of the input units are on), a predicate with a num- ber of useful properties. We find that many aspects of.generalization in multilayer networks learning large, difficult tasks are reproduced in this simple domain, in which concrete numerical results and even some analytic understanding can be achieved. 1 INTRODUCTION In recent years there has been a tremendous growth in the study of machines which learn. One class of learning systems which has been fairly popular is neural net- works. Originally motivated by the study of the nervous system in biological organ- isms and as an abstract model of computation, they have since been applied to a wide variety of real-world problems (for examples see [Sejnowski and Rosenberg, 87] and [Tesauro and Sejnowski, 88]). Although the results have been encouraging, there is actually little understanding of the extensibility of the formalism. In par- ticular, little is known of the resources required when dealing with large problems (i.e. scaling), and the abilities of networks to respond to novel situations (i.e. gen- eraliz ation). The objective of this paper is to gain some insight into the relationships between three fundament~l quantities under a variety of situations. In particular we are in- terested in the relationships between the size of the network, the number of training Scaling and Generalization in Neural Networks 161 instances, and the generalization that the network performs, with an emphasis on the effects of the input representation and the particular patterns present in the training set. As a first step to a detailed understanding, we summarize a study of scaling and generalization in the simplest possible case. Using feed forward networks, the type of networks most common in the literature, we examine the majority function (return a 1 if a majority of the inputs are on), a boolean predicate with a number of useful features. By using a combination of computer simulations and analysis in the limited domain of the majority function, we obtain some concrete numerical results which provide insight into the process of generalization and which will hopefully lead to a better understanding of learning in neural networks in general.· 2 THE MAJORITY FUNCTION The function we have chosen to study is the majority function, a simple predicate whose output is a 1 if and only if more than half of the input units are on. This function has a number of useful properties which facilitate a study of this type. The function has a natural appeal and can occur in several different contexts in the real-world. The problem is linearly separable (i.e. of predicate order 1 [Minsky and Papert, 69]). A version of the perceptron convergence theorem applies, so we are guaranteed that a network with one layer of weights can learn the function. Finally, when there are an odd number of input units, exactly half of the possible inputs results in an output of 1. This property tends to minimize any negative effects that may result from having too many positive or negative training examples. 3 METHODOLOGY The class of networks used are feed forward networks [Rumelhart and McClelland, 86], a general category of networks that include perceptrons and the multi-layered net- works most often used in current research. Since majority is a boolean function of predicate order 1, we use a network with no hidden units. The output function used was a sigmoid with a bias. The basic procedure consisted of three steps. First the network was initialized to some random starting weights. Next it was trained using back propagation on a set of training patterns. Finally, the performance of the network was tested on a set of random test patterns. This performance figure was used as the estimate of the network's generalization. Since there is a large amount of randomness in the procedure, most of our data are averages over several simulations. O. The material contained in this paper is a condensation of portions of the first author's M.S. thesis [Ahmad, 88]. 162 Ahmad and Tesauro f 0.50 0.42 0.33 0.25 0.17 0.08 0.00 0 70 140 210 280 350 5 420 Figure 1: The average failure rate as a function of S. d = 25 Notation. In the following discussion, we denote 5 to be the number of training patterns, d the number of input units, and c the number of cycles through the train- ing set. Let f be the failure rate (the fraction of misclassified training instances), and rr be the set of training patterns. 4 RANDOM TRAINING PATTERNS We first examine the failure rate as a function of 5 and d. Figure 1 shows the graph of the average failure rate as a function of S, for a fixed input size d = 25. Not surprisingly we find that the failure rate decreases fairly monotonically with 5. Our simulations show that in fact, for majority there is a well defined relationship between the failure rate and 5. Figure 2 shows this for a network with 25 input units. The figure indicates that In f is proportional to 5 implying that the failure rate decreases exponentially with 5, i.e., , = ae-fJs . 1/ {3 can be thought of as a characteristic training set size, corresponding to a failure rate of a/e. Obtaining the exact scaling relationship of l/P was somewhat tricky. Plotting {3 on a log-log plot against d showed it to be close to a straight line, indicating that 1/ {3 increases'"" d(J for some constant a. Extracting the exponent by measuring the slope of the log-log graph turned out to be very error prone, since the data only ranged over one order of magnitude. An alternate method for obtaining the exponent is to look for a particular exponent a by setting 5 = ad(J. Since a linear scaling relationship is theoretically plausible, we measured the failure rate of the network In! Scaling and Generalization in Neural Networks 163 G.""'"" -1.000 -Z.OOO -3.000 -4.000 -5.000 -6.000 0.0 70.0 140.0 Z10.0 Z80.0 350.0 4Z0.0 s Figure 2: In f as a function of S. d = 25. The slope was == -0.01 at S = ad for various values of a. As Figure 3 shows, the failure rate remains more or less constant for fixed values of a, indicating a linear scaling relationship with d. Thus O( d) training patterns should be required to learn majority to a fixed level of performance. Note that if we require perfect learning, then the failure rate has to be < 1/(2d - S) ,..,. 1/2d • By substituting this for f in the above formula and solving for S, we get that (1 )(dln 2 + In a) patterns are required. The extra factor of d suggests that O( d2) would be required to learn majority perfectly. We will show in Section 6.1 that this is actually an overestimate. 5 THE INPUT REPRESENTATION So far in our simulations we have used the representation commonly used for boolean predicates. Whenever an input feature has been true, we clamped the corresponding input unit to a 1, and when it has been off we have clamped it to a O. There is no reason, however, why some other representation couldn't have been used. Notice that in back propagation the weight change is proportional to the incoming input signal, hence the weight from a particular input unit to the output unit is changed only when the pattern is misclassified and the input unit is non-zero. The weight remains unchanged when the input unit is O. If the 0,1 representation were changed to a-l,+1 representation each weight will be changed more often, hence the network should learn the training set quicker (simulations in [Stornetta and Huberman, 81] reported such a decrease in training time using a -i, +i representation.) 164 Ahmad and Tesauro f 0.50 0.42 0.33 0.25 0.17 0.08 ~~-------------------- - S=3d S=5d S=7d 0.00 +----+----+-----+---+----+---.... 60 20 27 33 40 47 53 d Figure 3: Failure ra.te VB d with S = 3d, 5d, 7 d. We found that not only did the training time decrease with the new representation, the generalization of the network improved significantly. The scaling of the failure rate with respect to S is unchanged, but for any fixed value of S, the generalization is about 5 - 10% better. Also, the scaling with respect to dis still linear, but the constant for a fixed performance level is smaller. Although the exact reason for the improved generalization is unclear, the following might be a plausible reason. A weight is changed only if the corresponding input is non-zero. By the definition of the majority function, the average number of units that are on for the positive instances is higher than for the negative instances. Hence, using the 0,1 represen- tation, the weight changes are more pronounced for the positive instances than for the negative instances. Since the weights are changed whenever a pattern is mis- classified, the net result is that the weight change is greater when a positive event is misclassified than when a negative event is misclassified. Thus, there seems to be a bias in the 0,1 representation for correcting the hyperplane more when a positive event is misclassified. In the new representation, both positive and negative events are treated equally hence it is unbiased. The basic lesson here seems to be that one should carefully examine every choice that has been made during the design process. The representation of the input, even down to such low level details as deciding whether ""off"" should be represented as 0 or -1, could make a significant difference in the generalization. Scaling and Generalization in Neural Networks 165 6 BORDER PATTERNS We now consider a method for improving the generalization by intelligently selecting the patterns in the training set. Normally, for a given training set, when the inputs are spread evenly around the input space, there can be several generalizations which are consistent with the patterns. The performance of the network on the test set becomes a random event, depending on the initial state of the network. If practical, it makes sense to choose training patterns whic~ can limit the possible generalizations. In particular, if we can find those examples which are closest to the separating surface, we can maximally constrain the number of generalizations. The solution that the network converges to using these ""border"" patterns should have a higher probability of being a good separator. In general finding a perfect set of border patterns can be computationally expensive, however there might exist simple heuristics which can help select good training examples. We explored one heuristic for choosing such points: selecting only those patterns in which the number of 1 's is either one less or one more than half the number of input units. Intuitively, these inputs should be close to the desired separating surface, thereby constraining'the network more than random patterns would. Our results show that using only border patterns in the training set, there is a large increase in the expected performance of the network for a given S. In addition, the scaling behavior as a function of S seems to be very different and is faster than an exponential decrease. (Figure 4 shows typical failure rate vs S curves comparing border patterns, the -1,+1 representation, and the 0,1 representation.) 6.1 BORDER PATTERNS AND PERFECT LEARNING We say the network has perfectly learned a function when the test patterns are never misclassified. For the majority function, one can argue that at least some border patterns must be present in order to guarantee perfect performance. If no border patterns were in the training set, then the network could have learned the f - 1 of d or the f + 1 of d function. Furthermon~, if we know that a certain number of border patterns is guaranteed to give perfect performance, say bed), then given the probability that a random pattern is a border pattern, we can calculate the expected number of random patterns sufficient to learn majority. For odd d, there are 2 * ( ; ) border patterns, so the probability of choosing a border pattern randomly is: ( ; ) 2d- 1 As d gets larger this probability decreases as 1/.fd.* The expected number of ran- domly chosen patterns required before b( d) border patterns are chosen is therefore: 0* This can be shown using Stirling's approximation to d!. 166 Ahmad and Tesauro / --......, '\ 0.50 f 0.42 0.33 0.25 0.17 0.08 0.001 0 58 117 175 233 292 350 S Figure 4: Graph showing the average failure rate vs. S using the 0,1 representation (right), the -1,+1 representation (middle), and using border patterns (left). The network had 23 inputs units and was tested on a test set consisting of 1024 patterns. b( cl)Vd. From our data we find that 3d border patterns are always sufficient to learn the test set perfectly. From this, and from the theoretical results in [Cover, 65], we can be confident that b( cI) is linear in d. Thus, O( fi3/2) random patterns should be sufficient to learn majority perfectly. It should be mentioned that border patterns are not the only patterns which con- tribute to the generalization of the network. Figure 5 shows that the failure rate of the network when trained with random training patterns which happen to contain b border patterns is substantially better than a training set consisting of only b border patterns. Note that perfect performance is achieved at the same point in both cases. 7 CONCLUSION In this paper we have described a systematic study of some of the various factors affecting scaling and generalization in neural networks. Using empirical studies in a simple test domain, we were able to obtain precise scaling relationships between the performance of the network, the number of training patterns, and the size of the network. It was shown that for a fixed network size, the failure rate decreases exponentially with the size of the training set. The number of patterns required to Scaling and Generalization in Neural Networks 167 f •. u •. u •• U •• 17 .... .... • II .. II ,. II N wnber of border patterna. Figure 5: This figure compares the failure rate on a random training set which happens to contain b border patterns (bottom plot) with a training set composed of only b border patterns (top plot). achieve a fixed performance level was shown to increase linearly with the network SIZe. A general finding was that the performance of the network was very sensitive to a number of factors. A slight change in the input representation caused a jump in the performance of the network. The specific patterns in the training set had a large influence on the final weights and on the generalization. By selecting the training patterns intelligently, the performance of the network was increased significantly. The notion of border patterns were introduced as the most interesting patterns in the training set. As far as the number of patterns required to teach a function to the network, these patterns are near optimal. It was shown that a network trained only on border patterns generalizes substantially better than one trained on the same number of random patterns. Border patterns were also used to derive an expected bound on the number of random patterns sufficient to learn majority perfectly. It was shown tha,t on average, O(d3 / 2 ) random patterns are sufficient to learn majority perfectly. In conclusion, this paper advocates a careful study of the process of generalization in neural networks. There are a large number of different factors which can affect the performance. Any assumptions made when applying neural networks to a real- world problem should be made with care. Although much more work needs to be 168 Ahmad and Tesauro done, it was shown that many of the issues can be effectively studied in a simple test domain. Acknowledgements We thank T. Sejnowski, R. Rivest and A. Barron for helpful discussions. We also thank T. Sejnowski and B. Bogstad for assistance in development of the simulator code. This work was partially supported by the National Center for Supercomputing Applications and by National Science Foundation grant Phy 86-58062. References [Ahmad,88] S. Ahmad. A Study of Scaling and Generalization in Neural Networks. Technical Report UIUCDCS-R-88-1454, Department of Computer Science, Uni- versity of Illinois, Urbana-Champaign, IL, 1988. [Cover, 65] T. Cover. Geometric and satistical properties of systems oflinear equa- tions. IEEE Trans. Elect. Comp., 14:326-334, 1965. [Minsky and Papert, 69] Marvin Minsky and Seymour Papert. Perceptrons. MIT Press, Cambridge, Mass., 1969. [Muroga, 71] S Muroga. Threshold Logic and its Applications. Wiley, New York, 1971. [Rumelhart and McClelland, 86] D. E. Rumelhart and J. L. McClelland, editors. Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations. Volume I, MIT Press, Cambridge, Mass., 1986. [Stornetta and Huberman, 87] W.S. Stornetta and B.A. Huberman. An improved three-layer, back propagation algorithm. In Proceedings of the IEEE First In- ternational Conference on Neural Networks, San Diego, CA, 1987. [Sejnowski and Rosenberg, 87] T.J. Sejnowski and C.R. Rosenberg. Parallel net- works that learn to pronounce English text. Complex Systems, 1:145-168, 1987. [Tesauro and Sejnowski, 88] G. Tesauro and T.J. Sejnowski. A Parallel Network that Learns to Play Backgammon. Technical Report CCSR-88-2, Center for Complex Systems Research, University of Illinois, Urbana-Champaign, IL, 1988.","[-0.07839781045913696, -0.07719596475362778, 0.03214721381664276, -0.015932871028780937, 0.03247581049799919, 0.05196384713053703, 0.049590010195970535, -0.009838225319981575, 0.016812583431601524, -0.0141919469460845, -0.0076591456308960915, -0.04242699593305588, 0.054002176970243454, 0.07233288139104843, -0.029492979869246483, -0.01178052369505167, 0.015537217259407043, 0.03852604329586029, -0.12758152186870575, -0.006720209494233131, -0.008077423088252544, 0.020284363999962807, -0.03096960112452507, -0.015063893049955368, 0.04015001654624939, -0.002266105031594634, -0.046849410980939865, -0.01854071393609047, 0.015144401229918003, -0.011018232442438602, 0.058420129120349884, -0.01719241589307785, 0.05653554946184158, 0.017087360844016075, -0.050820980221033096, 0.04865454509854317, -0.07439455389976501, 0.0011204646434634924, 0.01656462624669075, 0.005469274707138538, 0.051377493888139725, 0.006537315435707569, -0.004154163412749767, 0.03860002011060715, 0.15564033389091492, 0.04919862747192383, -0.00752794835716486, -0.033507268875837326, -0.04400335252285004, -0.06359481811523438, 0.005558385513722897, 0.02946959249675274, -0.06678838282823563, 0.0853823646903038, 0.0327143669128418, -0.07183977216482162, 0.024051478132605553, 0.03453531488776207, -0.0625615194439888, -0.014360932633280754, -0.01679375022649765, -0.11375761777162552, -0.022028818726539612, -0.049790095537900925, 0.0768565833568573, -0.0031940490007400513, 0.030995327979326248, -0.056984174996614456, -0.0036399029195308685, 0.010001085698604584, 0.019738124683499336, 0.01691351644694805, -0.08298932760953903, 0.02531457506120205, 0.0784396231174469, 0.04560849443078041, 0.023977147415280342, -0.003968354314565659, -0.054951827973127365, -0.020304711535573006, -0.005620753392577171, 0.02408735826611519, 0.05813857913017273, 0.025181427597999573, 0.10886722058057785, 0.009315382689237595, -0.06832313537597656, 0.01741265319287777, 0.04788890853524208, -0.01534577552229166, -0.059300150722265244, 0.040050581097602844, -0.05758420750498772, -0.04158812016248703, 0.10347650945186615, -0.026030277833342552, -0.040047869086265564, -0.07796862721443176, 0.02123580500483513, 0.11004392057657242, 0.01662394404411316, 0.0341169573366642, 0.06615093350410461, -0.058025505393743515, 0.03978968784213066, 0.05204889923334122, 0.02057999186217785, 0.040058501064777374, 0.09528419375419617, -0.1597016602754593, -0.0789693221449852, 0.028821982443332672, -0.015020166523754597, -0.009968556463718414, 0.03960870951414108, -0.06597542762756348, 0.13853773474693298, -0.007839951664209366, 0.031237950548529625, 0.0562894344329834, -0.029328271746635437, 0.008522862568497658, -0.031908076256513596, 0.02094941772520542, 0.012834010645747185, 0.07023317366838455, -0.055091749876737595, 5.7251912364082104e-33, -0.04831288009881973, 0.025479380041360855, 0.0397985503077507, 0.00401534978300333, 0.07028164714574814, 0.0034230961464345455, 0.0010097784688696265, -0.018778113648295403, 0.025352071970701218, 0.029021941125392914, -0.05588444322347641, 0.08795593678951263, 0.017931440845131874, -0.006745574530214071, 0.07257270067930222, -0.013413657434284687, 0.009472313337028027, -0.015667209401726723, -0.01930742710828781, -0.1334870159626007, 0.06831073760986328, -0.026975858956575394, 0.06481845676898956, -0.051844272762537, -0.002130997134372592, -0.0732518807053566, 0.08551362156867981, -0.004925544373691082, -0.07951424270868301, -0.02686866745352745, -0.04750623553991318, -0.01477687619626522, -0.05673253908753395, 0.021793903782963753, 0.002750630956143141, 0.0042705596424639225, 0.012613002210855484, -0.006548394449055195, 0.0057271933183074, -0.026406005024909973, -0.06451868265867233, 0.0057213194668293, 0.013384432531893253, 0.012234056368470192, -0.019895358011126518, -0.0252684336155653, 0.00094272056594491, 0.060682497918605804, -0.03700786456465721, -0.14438068866729736, -0.008944597095251083, -0.0012942221947014332, 0.033693183213472366, -0.10427133738994598, 0.06594352424144745, 0.07427909225225449, 0.0705995187163353, 0.0861806869506836, 0.06197309121489525, 0.04515647515654564, 0.007071011699736118, 0.0014521440025418997, -0.02646550163626671, 0.0872885212302208, -0.003814881667494774, 0.046061933040618896, -0.0638279840350151, 0.085762158036232, 0.0639912337064743, 0.008007789961993694, 0.05824427306652069, 0.0339454784989357, -0.00720191141590476, -0.12109483778476715, 0.10652727633714676, 0.0011348203988745809, 0.036323439329862595, -0.11710153520107269, -0.045868080109357834, 0.023180020973086357, -0.02914649434387684, 0.04392186552286148, -0.001653045415878296, -0.023110581561923027, -0.07264167070388794, -0.041018903255462646, 0.053848057985305786, -0.06374559551477432, 0.023311736062169075, 0.013400072231888771, -0.052102766931056976, 0.004560902249068022, -0.014413625933229923, -0.03179985284805298, -0.027690071612596512, -5.0537609855040126e-33, -0.06449025869369507, 0.027202846482396126, -0.07182654738426208, -0.0014702372718602419, 0.01809374801814556, -0.032965756952762604, -0.08054932951927185, -0.005099381320178509, -0.060007866472005844, -0.013934632763266563, -0.0009092814871110022, 0.01151546835899353, 0.015039737336337566, -0.03504565358161926, -0.041496116667985916, 0.00475011020898819, -0.02877671644091606, 0.032397180795669556, 0.08610537648200989, 0.019211623817682266, 0.0226433165371418, 0.08262744545936584, -0.06344024091959, 0.06009475514292717, -0.022481361404061317, -0.033526383340358734, -0.08239944279193878, 0.05674370378255844, -0.02452513761818409, -0.024246132001280785, -0.049502428621053696, -0.11910314112901688, -0.05309467017650604, -0.003119061700999737, 0.02009516954421997, 0.06696860492229462, 0.01058212760835886, 0.029001347720623016, -0.04936862736940384, 0.06900691241025925, -0.01531226560473442, -0.051690567284822464, -0.07824119925498962, -0.02704944647848606, 0.06907027214765549, -0.05284520238637924, -0.02033132314682007, -0.009236468002200127, -0.03581532463431358, 0.047747544944286346, -0.009026522748172283, 0.03396505117416382, -0.05683402344584465, -0.011555591598153114, -0.07926011085510254, 0.02813813090324402, 0.04094018042087555, -0.026556458324193954, 0.05917271599173546, -0.0216317567974329, -0.07216289639472961, -0.09206274896860123, -0.029882527887821198, 0.02690182439982891, -0.03037809580564499, -0.051771145313978195, 0.0378655344247818, 0.039892397820949554, 0.08437279611825943, 0.005356556735932827, -0.09695109724998474, -0.02750444784760475, 0.02093726210296154, 0.024563224986195564, -0.09311294555664062, -0.029527397826313972, 0.037805959582328796, -0.07162979245185852, -0.027395125478506088, -0.012590193189680576, 0.041211485862731934, -0.05172531306743622, 0.017595315352082253, 0.01190640963613987, 0.04153769463300705, 0.01845615729689598, 0.1214146539568901, 0.04175768420100212, 0.05826963111758232, -0.06553107500076294, -0.01296878233551979, 0.057241011410951614, -0.014991117641329765, 0.010756880044937134, -0.049630120396614075, -5.401281200079211e-08, -0.10250978171825409, 0.0657828226685524, 0.023611314594745636, 0.022985495626926422, 0.0913289487361908, -0.054343272000551224, -0.00864100269973278, 0.05237411707639694, -0.005096204113215208, 0.03710126876831055, 0.06795454770326614, 0.009789537638425827, 0.00821791123598814, -0.030453424900770187, 0.0011985814198851585, 0.05856817960739136, 0.0038418190088123083, -0.0327821709215641, 0.017808929085731506, -0.04633662849664688, 0.03808419033885002, 0.06451084464788437, 0.006160681135952473, -0.022260071709752083, 0.03584056347608566, -0.10123313963413239, -0.10341616719961166, 0.0712505504488945, 0.0006893558311276138, 0.03744099289178848, 0.00793479010462761, 0.04459629952907562, 0.02609439007937908, 0.034769993275403976, 0.05037630349397659, 0.07687105983495712, 0.0283396914601326, 0.04006531089544296, -0.0039004988502711058, -0.04611095413565636, -0.02180107682943344, 0.016339264810085297, 0.023335212841629982, 0.02318999543786049, -0.005999418906867504, -0.009726758114993572, -0.07837507873773575, -0.06805706769227982, 0.06062797084450722, -0.033371228724718094, 0.11373341083526611, -0.00836423970758915, 0.04644937813282013, 0.07119590044021606, 0.09742505848407745, -0.01754077337682247, -0.012787532061338425, -0.11781887710094452, -0.0029619026463478804, 0.038777533918619156, 0.029361819848418236, 0.05806498974561691, -0.07974231988191605, -0.023384129628539085]"
